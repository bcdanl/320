[
  {
    "objectID": "listing-danl-320-cw.html",
    "href": "listing-danl-320-cw.html",
    "title": "DANL 320 - Classwork",
    "section": "",
    "text": "Title\n\n\n\nSubtitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nClasswork 1\n\n\nBuilding a Personal Website using Git, GitHub, and RStudio with Quarto\n\n\nJanuary 22, 2025\n\n\n\n\n\n\nClasswork 2\n\n\nMarkdown Basics\n\n\nJanuary 27, 2025\n\n\n\n\n\n\nClasswork 3\n\n\nQuarto Website Basics\n\n\nJanuary 27, 2025\n\n\n\n\n\n\nClasswork 4\n\n\nPython Basics\n\n\nJanuary 29, 2025\n\n\n\n\n\n\nClasswork 5\n\n\nPySpark Basics - Loading, Summarizing, Selecting, Counting, and Sorting Data\n\n\nFebruary 10, 2025\n\n\n\n\n\n\nClasswork 6\n\n\nPySpark Basics - Convering Data Types; Filtering Data; Dealing with Missing Values/Duplicates\n\n\nFebruary 12, 2025\n\n\n\n\n\n\nClasswork 7\n\n\nPySpark Basics - Group Operations\n\n\nFebruary 17, 2025\n\n\n\n\n\n\nClasswork 8\n\n\nLinear Regression I\n\n\nApril 14, 2025\n\n\n\n\n\n\nClasswork 9\n\n\nLinear Regression II\n\n\nApril 14, 2025\n\n\n\n\n\n\nClasswork 10\n\n\nLogistic Regression\n\n\nApril 14, 2025\n\n\n\n\n\n\nClasswork 11\n\n\nAddressing Quasi-Separation in Logistic Regression with Regularization\n\n\nApril 14, 2025\n\n\n\n\n\n\nClasswork 12\n\n\nPredicting Housing Price in California\n\n\nApril 14, 2025\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "danl-hw/danl-320-hw-04.html",
    "href": "danl-hw/danl-320-hw-04.html",
    "title": "Homework 4",
    "section": "",
    "text": "Please submit four Jupyter Notebooks for Part 1 and Part 2 of Homework 4 to Brightspace using the following file naming convention:\n\nPart 1 - Model 1\n\ndanl-320-hw4-LASTNAME-FIRSTNAME-pt-1-model-1.ipynb\n\nPart 1 - Model 2\n\ndanl-320-hw4-LASTNAME-FIRSTNAME-pt-1-model-2.ipynb\n\nPart 1 - Model 3\n\ndanl-320-hw4-LASTNAME-FIRSTNAME-pt-1-model-3.ipynb\n\nPart 2\n\ndanl-320-hw4-LASTNAME-FIRSTNAME-pt-2.ipynb\n\nExamples:\n\ndanl-320-hw4-choe-byeonghak-pt-1-model-1.ipynb\ndanl-320-hw4-choe-byeonghak-pt-1-model-2.ipynb\ndanl-320-hw4-choe-byeonghak-pt-1-model-3.ipynb\ndanl-320-hw4-choe-byeonghak-pt-2.ipynb\n\n\nThe due is April 19, 2025, noon.\nPlease send Byeong-Hak an email (bchoe@geneseo.edu) if you have any questions."
  },
  {
    "objectID": "danl-hw/danl-320-hw-04.html#variable-description",
    "href": "danl-hw/danl-320-hw-04.html#variable-description",
    "title": "Homework 4",
    "section": "Variable Description",
    "text": "Variable Description\n\n\n\n\n\n\n\nVariable Name\nDescription\n\n\n\n\nhousehold\nUnique identifier for household\n\n\nX_purchase_desc\nDescription of beer purchase\n\n\nquantity\nNumber of beer packages purchased\n\n\nbrand\nBrand of beer purchased\n\n\ndollar_spent\nTotal amount spent on the purchase\n\n\nbeer_floz\nTotal volume of beer purchased (in fluid ounces)\n\n\nprice_floz\nPrice per fluid ounce of beer\n\n\ncontainer\nType of beer container (e.g., CAN, BOTTLE)\n\n\npromo\nIndicates if the purchase was part of a promotion (True/False)\n\n\nmarket\nMarket region of purchase\n\n\nmarital\nMarital status of household head\n\n\nincome\nIncome level of the household\n\n\nage\nAge group of household head\n\n\nemployment\nEmployment status of household head\n\n\ndegree\nEducation level of household head\n\n\noccupation\nOccupation category of household head\n\n\nethnic\nEthnicity of household head\n\n\nmicrowave\nIndicates if the household owns a microwave (True/False)\n\n\ndishwasher\nIndicates if the household owns a dishwasher (True/False)\n\n\ntvcable\nType of television subscription (e.g., basic, premium)\n\n\nsinglefamilyhome\nIndicates if the household is a single-family home (True/False)\n\n\nnpeople\nNumber of people in the household\n\n\n\n\nFor this homework, please read only one CSV file at a time due to memory limitations in Google Colab. Loading multiple CSV files simultaneously may cause a free-tier Google Colab instance to crash.\nurl_1 = \"https://bcdanl.github.io/data/beer_markets_xbeer_xdemog.zip\"\nurl_2 = \"https://bcdanl.github.io/data/beer_markets_xbeer_brand_xdemog.zip\"\nurl_3 = \"https://bcdanl.github.io/data/beer_markets_xbeer_brand_promo_xdemog.zip\"\n\n## Model 1\nbeer = pd.read_csv(url_1)\n\n## Model 2\nbeer = pd.read_csv(url_2)\n\n## Model 3\nbeer = pd.read_csv(url_3)\n\nDataset Details: Each DataFrame specified in url_1, url_2, and url_3 contains 2,638 demographic dummy variables. These include:\n\nIndividual Demographic Dummies: As described previously.\nInteraction Terms: Constructed by interacting the market dummies with each of the demographic dummies from the beer_markets DataFrame.\n\n\n\n\n\\[\n\\begin{align}\n&\\text{market}\\\\\n&\\text{marital}\\\\\n&\\text{age}\\\\\n&\\text{employment}\\\\\n&\\text{degree}\\\\\n&\\text{occupation}\\\\\n&\\text{ethnic}\\\\\n&\\text{microwave}\\\\\n&\\text{dishwasher}\\\\\n&\\text{tvcable}\\\\\n&\\text{singlefamilyhome}\\\\\n&\\text{npeople}\n\\end{align}\n\\]\n\n\\[\n\\begin{align}\n&\\text{market}\\times \\text{marital}\\\\\n&\\text{market}\\times \\text{income}\\\\\n&\\text{market}\\times \\text{age}\\\\\n&\\text{market}\\times \\text{employment}\\\\\n&\\text{market}\\times \\text{degree}\\\\\n&\\text{market}\\times \\text{occupation}\\\\\n&\\text{market}\\times \\text{ethnic}\\\\\n&\\text{market}\\times \\text{microwave}\\\\\n&\\text{market}\\times \\text{dishwasher}\\\\\n&\\text{market}\\times \\text{tvcable}\\\\\n&\\text{market}\\times \\text{singlefamilyhome}\\\\\n&\\text{market}\\times \\text{npeople}\n\\end{align}\n\\]\n\n\nConsider including all demographic dummy variables from the beer DataFrame in each of the models evaluated in Homework 2 Questions 3-8.\n\nModel 1\n\\[\n\\begin{aligned}\n\\log(\\text{price\\_per\\_floz}) = &\\ \\beta_{0} + \\sum_{j=1}^{4} \\beta_{j} \\,\\text{brand}_{j}\n+ \\beta_{5} \\,\\text{container\\_CAN} \\\\\n&\\,+\\, \\beta_{6} \\log(\\text{beer\\_floz})\\\\\n&\\,+\\, \\sum_{k =7}^{2645}\\beta_{k}\\,\\text{demoghics}_{k}\\\\\n&\\,+\\, \\epsilon\n\\end{aligned}\n\\]\n\n\nModel 2\n\\[\n\\begin{aligned}\n\\log(\\text{price\\_per\\_floz}) \\,=\\, & \\beta_{0} \\,+\\,  \\sum_{j=1}^{4}\\beta_{j}\\,\\text{brand}_{j} \\,+\\, \\beta_{5}\\,\\text{container\\_CAN} \\\\\n&\\,+\\, \\beta_{6}\\log(\\text{beer\\_floz})\\\\\n&\\,+\\, \\sum_{j=1}^{4}\\beta_{j\\times\\text{beer\\_floz}}\\,\\text{brand}_{j}\\times \\log(\\text{beer\\_floz})\\\\\n&\\,+\\, \\sum_{k = 7}^{2645}\\beta_{k}\\,\\text{demoghics}_{k}\\\\\n&\\,+\\, \\epsilon\n\\end{aligned}\n\\]\n\n\nModel 3\n\\[\n\\begin{aligned}\n\\log(\\text{price\\_per\\_floz}) \\,=\\, & \\beta_{0} \\,+\\, \\sum_{j=1}^{4}\\beta_{j}\\,\\text{brand}_{j} \\,+\\, \\beta_{5}\\,\\text{container\\_CAN} \\\\\n&\\,+\\, \\beta_{6}\\log(\\text{beer\\_floz})\\\\\n&\\,+\\, \\beta_{7}\\,\\text{promo} \\times\\log(\\text{beer\\_floz}) \\\\\n&\\,+\\, \\sum_{j=1}^{4}\\beta_{j\\times\\text{beer\\_floz}}\\,\\text{brand}_{j}\\times \\log(\\text{beer\\_floz})\\\\\n&\\,+\\, \\sum_{j=1}^{4}\\beta_{j\\times\\text{promo}}\\,\\text{brand}_{j}\\times \\text{promo}\\\\\n&\\,+\\, \\sum_{j=1}^{4}\\beta_{j\\times\\text{promo}\\times\\text{beer\\_floz}}\\,\\text{brand}_{j}\\times \\text{promo}\\times \\log(\\text{beer\\_floz})\\\\\n&\\,+\\, \\sum_{k = 7}^{2645}\\beta_{k}\\,\\text{demoghics}_{k}\\\\\n&\\,+\\, \\epsilon\n\\end{aligned}\n\\]\n\nPlease fit one model at a time.\n\nOne Lasso training can take around 3 minutes."
  },
  {
    "objectID": "danl-hw/danl-320-hw-04.html#question-1",
    "href": "danl-hw/danl-320-hw-04.html#question-1",
    "title": "Homework 4",
    "section": "Question 1",
    "text": "Question 1\n\nFit a Lasso linear regression model for Models 1, 2, and 3.\nDetermine the optimal value of the alpha parameter for each model.\nCompute and report the Mean Squared Error (MSE) for each model."
  },
  {
    "objectID": "danl-hw/danl-320-hw-04.html#question-2",
    "href": "danl-hw/danl-320-hw-04.html#question-2",
    "title": "Homework 4",
    "section": "Question 2",
    "text": "Question 2\n\nAcross the three models in this homework, how is the percentage change in the price of beer sensitive to the percentage change in the volume of beer purchases for each brand?\nHow does incorporating a broader demographic design into the model affect this?"
  },
  {
    "objectID": "danl-hw/danl-320-hw-04.html#question-3",
    "href": "danl-hw/danl-320-hw-04.html#question-3",
    "title": "Homework 4",
    "section": "Question 3",
    "text": "Question 3\n\nUsing the test dataset, compare the Mean Squared Errors (MSEs) of the models from Homework 2 to those from the current analysis."
  },
  {
    "objectID": "danl-hw/danl-320-hw-04.html#variable-description-1",
    "href": "danl-hw/danl-320-hw-04.html#variable-description-1",
    "title": "Homework 4",
    "section": "Variable Description",
    "text": "Variable Description\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ng\nGames Played: The number of games in which the player appeared.\n\n\npa\nPlate Appearances: Total number of times the player appeared at the plate.\n\n\nhr\nHome Runs: Total number of home runs hit by the player.\n\n\nr\nRuns: Total number of runs scored by the player.\n\n\nrbi\nRuns Batted In (RBI): Number of runs the player batted in.\n\n\nsb\nStolen Bases: Total number of bases stolen by the player.\n\n\nbb_percent\nWalk Percentage: The percentage of plate appearances that result in a base on balls.\n\n\nk_percent\nStrikeout Percentage: The percentage of plate appearances that end in a strikeout.\n\n\niso\nIsolated Power (ISO): A measure of a player’s raw power, calculated as (SLG - AVG).\n\n\nbabip\nBatting Average on Balls In Play (BABIP): The average when excluding home runs and strikeouts.\n\n\navg\nBatting Average (AVG): The ratio of hits to official at-bats.\n\n\nobp\nOn-Base Percentage (OBP): The frequency a player reaches base per plate appearance.\n\n\nslg\nSlugging Percentage (SLG): A weighted measure of total bases per at-bat.\n\n\nw_oba\nWeighted On-Base Average (wOBA): An advanced metric that measures a player’s overall offensive value.\n\n\nxw_oba\nExpected wOBA (xwOBA): A metric estimating wOBA based on the quality of contact.\n\n\nw_rc\nWeighted Runs Created (wRC): An advanced statistic that estimates the number of runs a player creates.\n\n\nbs_r\nBase Running Runs (BsR): A metric quantifying the value of a player’s base running.\n\n\noff\nOffensive Value: A composite metric or rating summarizing the player’s offensive contributions.\n\n\ndef\nDefensive Value: A composite metric or rating summarizing the player’s defensive contributions.\n\n\nwar\nWins Above Replacement (WAR): An overall measure of a player’s total contributions to their team.\n\n\n\n\n\nConsider the tree-based models in Part 2:\n\nOutcome Variable: war\nPredictors: All remaining variables"
  },
  {
    "objectID": "danl-hw/danl-320-hw-04.html#question-4",
    "href": "danl-hw/danl-320-hw-04.html#question-4",
    "title": "Homework 4",
    "section": "Question 4",
    "text": "Question 4\n\nFit a regression tree model with a maximum depth of 3 (max_depth=3).\nProvide an interpretation of the leaf nodes."
  },
  {
    "objectID": "danl-hw/danl-320-hw-04.html#question-5",
    "href": "danl-hw/danl-320-hw-04.html#question-5",
    "title": "Homework 4",
    "section": "Question 5",
    "text": "Question 5\n\nFit a regression tree model without imposing a maximum depth constraint."
  },
  {
    "objectID": "danl-hw/danl-320-hw-04.html#question-6",
    "href": "danl-hw/danl-320-hw-04.html#question-6",
    "title": "Homework 4",
    "section": "Question 6",
    "text": "Question 6\n\nPrune regression trees using cross-validation (CV).\nPlot the CV error versus the number of leaves.\nPlot the pruned tree with the lowest mean CV MSE.\nCompare the pruned tree with the tree from Question 4."
  },
  {
    "objectID": "danl-hw/danl-320-hw-04.html#question-7",
    "href": "danl-hw/danl-320-hw-04.html#question-7",
    "title": "Homework 4",
    "section": "Question 7",
    "text": "Question 7\n\nFit a random forest model.\nPlot the variable importance measures."
  },
  {
    "objectID": "danl-hw/danl-320-hw-04.html#question-8",
    "href": "danl-hw/danl-320-hw-04.html#question-8",
    "title": "Homework 4",
    "section": "Question 8",
    "text": "Question 8\n\nFit an XGBoost model.\nPlot the variable importance measures."
  },
  {
    "objectID": "danl-hw/danl-320-hw-04.html#question-9",
    "href": "danl-hw/danl-320-hw-04.html#question-9",
    "title": "Homework 4",
    "section": "Question 9",
    "text": "Question 9\n\nCompare the Mean Squared Errors (MSEs) on the test data across the different tree-based models.\nAnalyze and discuss the differences in predictive performance among these models."
  },
  {
    "objectID": "danl-hw/danl-320-hw-02.html",
    "href": "danl-hw/danl-320-hw-02.html",
    "title": "Homework 2",
    "section": "",
    "text": "Please submit your Jupyter Notebook for Part 1 in Homework 2 to Brightspace with the name below:\n\ndanl-320-hw2-LASTNAME-FIRSTNAME.ipynb\n( e.g., danl-320-hw2-choe-byeonghak.ipynb )\n\nThe due is March 24, 2025, 3:15 P.M.\n\nIt is recommended to finish it before the Spring Break begins.\n\nPlease send Byeong-Hak an email (bchoe@geneseo.edu) if you have any questions."
  },
  {
    "objectID": "danl-hw/danl-320-hw-02.html#variable-description",
    "href": "danl-hw/danl-320-hw-02.html#variable-description",
    "title": "Homework 2",
    "section": "Variable Description",
    "text": "Variable Description\n\n\n\n\n\n\n\nVariable Name\nDescription\n\n\n\n\nhousehold\nUnique identifier for household\n\n\nX_purchase_desc\nDescription of beer purchase\n\n\nquantity\nNumber of beer packages purchased\n\n\nbrand\nBrand of beer purchased\n\n\ndollar_spent\nTotal amount spent on the purchase\n\n\nbeer_floz\nTotal volume of beer purchased (in fluid ounces)\n\n\nprice_floz\nPrice per fluid ounce of beer\n\n\ncontainer\nType of beer container (e.g., CAN, BOTTLE)\n\n\npromo\nIndicates if the purchase was part of a promotion (True/False)\n\n\nregion\nGeographic region of purchase\n\n\nmarital\nMarital status of household head\n\n\nincome\nIncome level of the household\n\n\nage\nAge group of household head\n\n\nemployment\nEmployment status of household head\n\n\ndegree\nEducation level of household head\n\n\noccupation\nOccupation category of household head\n\n\nethnic\nEthnicity of household head\n\n\nmicrowave\nIndicates if the household owns a microwave (True/False)\n\n\ndishwasher\nIndicates if the household owns a dishwasher (True/False)\n\n\ntvcable\nType of television subscription (e.g., basic, premium)\n\n\nsinglefamilyhome\nIndicates if the household is a single-family home (True/False)\n\n\nnpeople\nNumber of people in the household"
  },
  {
    "objectID": "danl-hw/danl-320-hw-02.html#question-1",
    "href": "danl-hw/danl-320-hw-02.html#question-1",
    "title": "Homework 2",
    "section": "Question 1",
    "text": "Question 1\nCreate the DataFrame that keeps all the observations whose value of container is either ‘CAN’ or ‘NON REFILLABLE BOTTLE’ in the beer_markets DataFrame."
  },
  {
    "objectID": "danl-hw/danl-320-hw-02.html#question-2",
    "href": "danl-hw/danl-320-hw-02.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\nSplit the resulting DataFrame of Question 1 into training and test DataFrames such that approximately 67% of observations in the resulting DataFrame of Question 1 belong to the training DataFrame and the rest observations belong to the test DataFrame."
  },
  {
    "objectID": "danl-hw/danl-320-hw-02.html#questions-3-8",
    "href": "danl-hw/danl-320-hw-02.html#questions-3-8",
    "title": "Homework 2",
    "section": "Questions 3-8",
    "text": "Questions 3-8\nConsider the following three linear regression models:\n\nModel 1\n\\[\n\\begin{aligned}\n\\log(\\text{price\\_per\\_floz}) = &\\ \\beta_{0} + \\sum_{i=1}^{N} \\beta_{i} \\,\\text{market}_{i} + \\sum_{j=N+1}^{N+4} \\beta_{j} \\,\\text{brand}_{j}\n+ \\beta_{N+5} \\,\\text{container\\_CAN} \\\\\n&\\,+\\, \\beta_{N+6} \\log(\\text{beer\\_floz}) + \\epsilon\n\\end{aligned}\n\\]\n\n\nModel 2\n\\[\n\\begin{aligned}\n\\log(\\text{price\\_per\\_floz}) \\,=\\, & \\beta_{0} \\,+\\, \\sum_{i=1}^{N}\\beta_{i}\\,\\text{market}_{i} \\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j}\\,\\text{brand}_{j} \\,+\\, \\beta_{N+5}\\,\\text{container\\_CAN} \\\\\n&\\,+\\, \\beta_{N+6}\\log(\\text{beer\\_floz})\\\\\n&\\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j\\times\\text{beer\\_floz}}\\,\\text{brand}_{j}\\times \\log(\\text{beer\\_floz})\\\\\n&\\,+\\, \\epsilon\n\\end{aligned}\n\\]\n\n\nModel 3\n\\[\n\\begin{aligned}\n\\log(\\text{price\\_per\\_floz}) \\,=\\, & \\beta_{0} \\,+\\, \\sum_{i=1}^{N}\\beta_{i}\\,\\text{market}_{i} \\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j}\\,\\text{brand}_{j} \\,+\\, \\beta_{N+5}\\,\\text{container\\_CAN} \\\\\n&\\,+\\, \\beta_{N+6}\\log(\\text{beer\\_floz})\\\\\n&\\,+\\, \\beta_{N+7}\\,\\text{promo} \\times\\log(\\text{beer\\_floz}) \\\\\n&\\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j\\times\\text{beer\\_floz}}\\,\\text{brand}_{j}\\times \\log(\\text{beer\\_floz})\\\\\n&\\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j\\times\\text{promo}}\\,\\text{brand}_{j}\\times \\text{promo}\\\\\n&\\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j\\times\\text{promo}\\times\\text{beer\\_floz}}\\,\\text{brand}_{j}\\times \\text{promo}\\times \\log(\\text{beer\\_floz})\\\\\n&\\,+\\, \\epsilon\n\\end{aligned}\n\\]\n\nSet “BUD_LIGHT” as the reference level for the \\(\\text{brand}\\) variable.\nSet “BUFFALO-ROCHESTER” as the reference level for the \\(\\text{market}\\) variable..\n\nThere are \\(N+1\\) distinct categories in the \\(\\text{market}\\) variable.\n\n\n\n\n\nQuestion 3\nProvide intuition behind each of the model.\n\n\n\nQuestion 4\n\nTrain each of the three linear regression models using the training DataFrame from Question 2.\n\nProvide the summary of the result for each linear regression model.\nWhat are the predicted beer prices for unseen data from each model?\n\n\n\n\n\nQuestion 5\n\nInterpret the beta estimates of the following variables from the Model 3:\n\nmarket_ALBANY\nmarket_EXURBAN_NY\nmarket_RURAL_NEW_YORK\nmarket_SURBURBAN_NY\nmarket_SYRACUSE\nmarket_URBAN_NY\n\n\n\n\n\nQuestion 6\n\nAcross the three models, how is the percentage change in the price of beer sensitive to the percentage change in the volume of beer purchases for each brand?\nHow does promo affect such sensitivity in the Model 3?\n\n\n\n\nQuestion 7\n\nDraw a residual plot from each of the three models.\n\nOn average, are the prediction correct? Are there systematic errors?\n\n\n\n\n\nQuestion 8\nWhich model do you prefer most and why?"
  },
  {
    "objectID": "danl-hw/danl-320-hw-02.html#variable-description-1",
    "href": "danl-hw/danl-320-hw-02.html#variable-description-1",
    "title": "Homework 2",
    "section": "Variable Description",
    "text": "Variable Description\n\n\n\n\n\n\n\n\n\nVariable Name\nType\nDescription\nCategories / Notes\n\n\n\n\npriceper1\nNumeric\nThe unit price for one product serving (in dollars).\n\n\n\nflavor_descr\nCategorical\nDescription of the ice cream flavor.\n\n\n\nsize1_descr\nCategorical\nThe description of the package or serving size (in fluid ounces or a related measure).\n\n\n\nhousehold_id\nCategorical/ID\nUnique identifier for each household purchasing product(s).\n\n\n\nhousehold_income\nCategorical/Numeric\nThe household income bracket (in dollars).\nThe number (e.g., 60,000) corresponds to predefined income categories. (for instance, “60,000” represents a particular income range from $60,000 to $70,000). Not a categorical variable per se, but can be grouped into categories if desired.\n\n\nhousehold_size\nCategorical\nThe number of persons in the household.\nDiscrete numeric count (e.g., 1, 2, 3…). Not a categorical variable per se, but can be grouped into categories if desired.\n\n\nusecoup\nCategorical\nIndicates whether a coupon was used in the purchase.\nBoolean category: True (coupon used) or False (coupon not used).\n\n\ncouponper1\nNumeric\nThe discount amount per unit applied when a coupon is used.\nContinuous numeric value; often zero if no coupon is used, and a positive number when a discount is applied.\n\n\nregion\nCategorical\nGeographic region where the household is located.\nCategories include “East”, “Central”, “West”, and “South”.\n\n\nmarried\nCategorical\nMarital status of the household head (or the household overall status).\nBoolean category: True (married) or False (not married).\n\n\nrace\nCategorical\nRace of the household head or primary respondent.\nCategories include “white”, “black”, “asian”, and “other”.\n\n\nhispanic_origin\nCategorical\nIndicates whether the household identifies as of Hispanic origin.\nBoolean category: True (Hispanic origin) or False (not Hispanic).\n\n\nmicrowave\nCategorical\nWhether the household owns a microwave.\nBoolean category: True (owns microwave) or False (does not own one).\n\n\ndishwasher\nCategorical\nWhether the household owns a dishwasher.\nBoolean category: True (owns dishwasher) or False (does not own one).\n\n\nsfh\nCategorical\nWhether the household resides in a single-family home.\nBoolean category: True (single-family home) or False (does not live in a single-family home).\n\n\ninternet\nCategorical\nWhether the household has internet service.\nBoolean category: True (has internet) or False (does not).\n\n\ntvcable\nCategorical\nIndicates whether the household subscribes to cable television service.\nBoolean category: True (has cable TV) or False (does not).\n\n\n\n\n\n\n\n\n\n\n\nWrite a blog post about Ben and Jerry’s ice cream in the ice_cream DataFrame using Jupyter Notebook, and add it to your online blog.\n\nIn your blog post, provide your linear regression model, as well as descriptive statistics, counting, filtering, and various group operations.\nOptionally, provide seaborn visualizations."
  },
  {
    "objectID": "econml.html",
    "href": "econml.html",
    "title": "Causal Machine Learning Bookmarks",
    "section": "",
    "text": "Dive into Causal Machine Learning, The World Bank and Pontificia Universidad Católica del Perú, Alexander Quispe et. al. \nMIT 14.388: Inference on Causal and Structural Parameters Using ML and AI, Department of Economics, MIT, Victor Chernozukhov\n\nPython Website\nJulia Website \n\nMGTECON 634: ML-based Causal Inference, Stanford, Susan Athey \nMachine Learning & Causal Inference: A Short Course, Stanford, Susan Athey, Jan Spiess, and Stefan Wager\n\nTutorial\nYouTube \n\n2018 American Economic Association Continuing Education: Machine Learning and Econometrics, Susan Athey and Guido Imbens \nCausal Inference and Machine Learning in Practice with EconML and CausalML: Industrial Use Cases at Microsoft, TripAdvisor, Uber \nDoubleML: Python and R Packages for the Double/Debiased Machine Learning Framework, P. Bach, V. Chernozhukov, M. S. Kurz, and M. Spindler \nEconML: A Python Package for ML-based Heterogeneous Treatment Effects Estimation, Microsoft \nCausalML: A Python Package for ML-based Causal Inference, Uber"
  },
  {
    "objectID": "econml.html#causal-machine-learning",
    "href": "econml.html#causal-machine-learning",
    "title": "Causal Machine Learning Bookmarks",
    "section": "",
    "text": "Dive into Causal Machine Learning, The World Bank and Pontificia Universidad Católica del Perú, Alexander Quispe et. al. \nMIT 14.388: Inference on Causal and Structural Parameters Using ML and AI, Department of Economics, MIT, Victor Chernozukhov\n\nPython Website\nJulia Website \n\nMGTECON 634: ML-based Causal Inference, Stanford, Susan Athey \nMachine Learning & Causal Inference: A Short Course, Stanford, Susan Athey, Jan Spiess, and Stefan Wager\n\nTutorial\nYouTube \n\n2018 American Economic Association Continuing Education: Machine Learning and Econometrics, Susan Athey and Guido Imbens \nCausal Inference and Machine Learning in Practice with EconML and CausalML: Industrial Use Cases at Microsoft, TripAdvisor, Uber \nDoubleML: Python and R Packages for the Double/Debiased Machine Learning Framework, P. Bach, V. Chernozhukov, M. S. Kurz, and M. Spindler \nEconML: A Python Package for ML-based Heterogeneous Treatment Effects Estimation, Microsoft \nCausalML: A Python Package for ML-based Causal Inference, Uber"
  },
  {
    "objectID": "econml.html#machine-learning-and-big-data",
    "href": "econml.html#machine-learning-and-big-data",
    "title": "Causal Machine Learning Bookmarks",
    "section": "Machine Learning and Big Data",
    "text": "Machine Learning and Big Data\n\n2023 American Economic Association Continuing Education: Machine Learning and Big Data, Melissa Dell and Matthew Harding \nMachine Learning for Economists (ml4econ), Bank of Israel, Itamar Caspi and Ariel Mansura"
  },
  {
    "objectID": "econml.html#causal-inference",
    "href": "econml.html#causal-inference",
    "title": "Causal Machine Learning Bookmarks",
    "section": "Causal Inference",
    "text": "Causal Inference\n\nCausal Inference: The Mixtape, Scott Cunningham \nCausal Inference for the Brave and True, Matheus Facure \nCausal Inference and Its Applications in Online Industry, Alex Deng \nApplied Empirical Methods, Yale SOM, Paul Goldsmith-Pinkham\n\nYouTube \n\nCausal Inference with Panel Data, Department of Political Science, Stanford, Yiqing Xu\n\nYouTube \n\nCausal Inference: What If, Miguel A. Hernán and James M. Robins \nRecent Developments in Difference-in-Differences, Vienna University of Economics and Business, Asjad Naqvi \nDifference-in-Differences Blog \nGov 2003: Causal Inference, Department of Government, Harvard, Matthew Blackwell"
  },
  {
    "objectID": "econml.html#researchers-in-causal-machine-learning",
    "href": "econml.html#researchers-in-causal-machine-learning",
    "title": "Causal Machine Learning Bookmarks",
    "section": "Researchers in Causal Machine Learning",
    "text": "Researchers in Causal Machine Learning\n\nSusan Athey \nAlexandre Belloni \nVictor Chernozhukov \nCarlos Cinelli \nChristian Hansen \nGuido Imbens\nJann Spiess \nStefan Wager"
  },
  {
    "objectID": "listing-danl-320-lec.html",
    "href": "listing-danl-320-lec.html",
    "title": "DANL 320 - Lecture",
    "section": "",
    "text": "Title\n\n\n\nSubtitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nLecture 1\n\n\nSyllabus, Course Outline, and DANL Career\n\n\nJanuary 22, 2025\n\n\n\n\n\n\nLecture 2\n\n\nGetting Started with Jupyter Notebook and Quarto\n\n\nJanuary 27, 2025\n\n\n\n\n\n\nLecture 3\n\n\nPython Basics\n\n\nJanuary 29, 2025\n\n\n\n\n\n\nLecture 4\n\n\nBig Data\n\n\nFebruary 3, 2025\n\n\n\n\n\n\nLecture 5\n\n\nDistributed Computing Framework; Apache Hadoop and Spark; PySpark\n\n\nFebruary 5, 2025\n\n\n\n\n\n\nLecture 6\n\n\nPySpark Basics\n\n\nFebruary 10, 2025\n\n\n\n\n\n\nLecture 7\n\n\nLinear Regression\n\n\nFebruary 17, 2025\n\n\n\n\n\n\nLecture 8\n\n\nLogistic Regression\n\n\nMarch 10, 2025\n\n\n\n\n\n\nLecture 9\n\n\nK-fold Cross-Validation; Regularized Regression\n\n\nMarch 26, 2025\n\n\n\n\n\n\nLecture 10\n\n\nTree-based Models\n\n\nApril 7, 2025\n\n\n\n\n\n\nLecture 11\n\n\nUnsupervised Learning\n\n\nApril 30, 2025\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "listing-danl-320-hw.html",
    "href": "listing-danl-320-hw.html",
    "title": "DANL 320 - Homework",
    "section": "",
    "text": "Title\n\n\n\nSubtitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nHomework 1\n\n\nSurvey, Personal Website, and Python Basics\n\n\nMarch 3, 2025\n\n\n\n\n\n\nHomework 2\n\n\nLinear Regression; Jupyter Notebook Blogging\n\n\nMarch 13, 2025\n\n\n\n\n\n\nHomework 3\n\n\nRegression; Jupyter Notebook Blogging\n\n\nMarch 25, 2025\n\n\n\n\n\n\nHomework 4\n\n\nLasso Linear Regression; Tree-based Models\n\n\nApril 16, 2025\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DANL 320-01: Big Data Analytics, Spring 2025",
    "section": "",
    "text": "Welcome! 👋\n\\(-\\) Explore, Learn, and Grow with Data Analytics! 🌟"
  },
  {
    "objectID": "index.html#bullet-lecture-slides",
    "href": "index.html#bullet-lecture-slides",
    "title": "DANL 320-01: Big Data Analytics, Spring 2025",
    "section": "\\(\\bullet\\,\\) Lecture Slides 🚀",
    "text": "\\(\\bullet\\,\\) Lecture Slides 🚀\n\n\n\n\n\n\nTitle\n\n\n\nSubtitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nLecture 1\n\n\nSyllabus, Course Outline, and DANL Career\n\n\nJanuary 22, 2025\n\n\n\n\n\n\nLecture 2\n\n\nGetting Started with Jupyter Notebook and Quarto\n\n\nJanuary 27, 2025\n\n\n\n\n\n\nLecture 3\n\n\nPython Basics\n\n\nJanuary 29, 2025\n\n\n\n\n\n\nLecture 4\n\n\nBig Data\n\n\nFebruary 3, 2025\n\n\n\n\n\n\nLecture 5\n\n\nDistributed Computing Framework; Apache Hadoop and Spark; PySpark\n\n\nFebruary 5, 2025\n\n\n\n\n\n\nLecture 6\n\n\nPySpark Basics\n\n\nFebruary 10, 2025\n\n\n\n\n\n\nLecture 7\n\n\nLinear Regression\n\n\nFebruary 17, 2025\n\n\n\n\n\n\nLecture 8\n\n\nLogistic Regression\n\n\nMarch 10, 2025\n\n\n\n\n\n\nLecture 9\n\n\nK-fold Cross-Validation; Regularized Regression\n\n\nMarch 26, 2025\n\n\n\n\n\n\nLecture 10\n\n\nTree-based Models\n\n\nApril 7, 2025\n\n\n\n\n\n\nLecture 11\n\n\nUnsupervised Learning\n\n\nApril 30, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#bullet-ml-notebooks",
    "href": "index.html#bullet-ml-notebooks",
    "title": "DANL 320-01: Big Data Analytics, Spring 2025",
    "section": "\\(\\bullet\\,\\) ML Notebooks 🔢",
    "text": "\\(\\bullet\\,\\) ML Notebooks 🔢\n\n\n\n\n\n\nTitle\n\n\n\nSubtitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nLinear Regression\n\n\nBikeshare in DC\n\n\nFebruary 19, 2025\n\n\n\n\n\n\nLinear Regression\n\n\nOrange Juice\n\n\nFebruary 26, 2025\n\n\n\n\n\n\nHomework 2\n\n\nBeer Markets\n\n\nMarch 5, 2025\n\n\n\n\n\n\nLogistic Regression\n\n\nNew Born Baby at Risk\n\n\nMarch 24, 2025\n\n\n\n\n\n\nHomework 3\n\n\nAmerican Housing Survey 2004\n\n\nMarch 26, 2025\n\n\n\n\n\n\nQuasi-Separation and Regularized Logistic Regression\n\n\nCar Safety Rating\n\n\nMarch 31, 2025\n\n\n\n\n\n\nLasso Linear Regression\n\n\nOnline Shopping\n\n\nApril 2, 2025\n\n\n\n\n\n\nLasso Logistic Regression\n\n\nNHL Player Evaluation\n\n\nApril 7, 2025\n\n\n\n\n\n\nOmitted Variable Bias\n\n\nOrange Juice\n\n\nApril 9, 2025\n\n\n\n\n\n\nTree-based Models\n\n\nNBC Shows; Boston Housing Markets\n\n\nApril 14, 2025\n\n\n\n\n\n\nFrom Linear Regression to Tree-based Models\n\n\nClaifornia Housing Markets\n\n\nApril 16, 2025\n\n\n\n\n\n\nHomework 4 - Part 1: Lasso Linear Regerssion - Model 1\n\n\nBeer Markets with Big Demographic Design\n\n\nApril 19, 2025\n\n\n\n\n\n\nHomework 4 - Part 1: Lasso Linear Regerssion - Model 2\n\n\nBeer Markets with Big Demographic Design\n\n\nApril 19, 2025\n\n\n\n\n\n\nHomework 4 - Part 1: Lasso Linear Regerssion - Model 3 with Discussions\n\n\nBeer Markets with Big Demographic Design\n\n\nApril 19, 2025\n\n\n\n\n\n\nHomework 4 - Part 2: Tree-based Models\n\n\nMLB Batting\n\n\nApril 19, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#bullet-classwork",
    "href": "index.html#bullet-classwork",
    "title": "DANL 320-01: Big Data Analytics, Spring 2025",
    "section": "\\(\\bullet\\,\\) Classwork ⌨️",
    "text": "\\(\\bullet\\,\\) Classwork ⌨️\n\n\n\n\n\n\nTitle\n\n\n\nSubtitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nClasswork 1\n\n\nBuilding a Personal Website using Git, GitHub, and RStudio with Quarto\n\n\nJanuary 22, 2025\n\n\n\n\n\n\nClasswork 2\n\n\nMarkdown Basics\n\n\nJanuary 27, 2025\n\n\n\n\n\n\nClasswork 3\n\n\nQuarto Website Basics\n\n\nJanuary 27, 2025\n\n\n\n\n\n\nClasswork 4\n\n\nPython Basics\n\n\nJanuary 29, 2025\n\n\n\n\n\n\nClasswork 5\n\n\nPySpark Basics - Loading, Summarizing, Selecting, Counting, and Sorting Data\n\n\nFebruary 10, 2025\n\n\n\n\n\n\nClasswork 6\n\n\nPySpark Basics - Convering Data Types; Filtering Data; Dealing with Missing Values/Duplicates\n\n\nFebruary 12, 2025\n\n\n\n\n\n\nClasswork 7\n\n\nPySpark Basics - Group Operations\n\n\nFebruary 17, 2025\n\n\n\n\n\n\nClasswork 8\n\n\nLinear Regression I\n\n\nApril 14, 2025\n\n\n\n\n\n\nClasswork 9\n\n\nLinear Regression II\n\n\nApril 14, 2025\n\n\n\n\n\n\nClasswork 10\n\n\nLogistic Regression\n\n\nApril 14, 2025\n\n\n\n\n\n\nClasswork 11\n\n\nAddressing Quasi-Separation in Logistic Regression with Regularization\n\n\nApril 14, 2025\n\n\n\n\n\n\nClasswork 12\n\n\nPredicting Housing Price in California\n\n\nApril 14, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#bullet-homework",
    "href": "index.html#bullet-homework",
    "title": "DANL 320-01: Big Data Analytics, Spring 2025",
    "section": "\\(\\bullet\\,\\) Homework 💻",
    "text": "\\(\\bullet\\,\\) Homework 💻\n\n\n\n\n\n\nTitle\n\n\n\nSubtitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nHomework 1\n\n\nSurvey, Personal Website, and Python Basics\n\n\nMarch 3, 2025\n\n\n\n\n\n\nHomework 2\n\n\nLinear Regression; Jupyter Notebook Blogging\n\n\nMarch 13, 2025\n\n\n\n\n\n\nHomework 3\n\n\nRegression; Jupyter Notebook Blogging\n\n\nMarch 25, 2025\n\n\n\n\n\n\nHomework 4\n\n\nLasso Linear Regression; Tree-based Models\n\n\nApril 16, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_OVB.html",
    "href": "danl-ml/danl_320_linear_regression_OVB.html",
    "title": "Omitted Variable Bias",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n\n\n\n\n\n\n\nCode\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper | Exp(95% CI Lower) | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: 95% CI Upper, 8: Exp(95% CI Lower), 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"95% CI Upper\", \"Exp(95% CI Lower)\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))\n\n\n\n\n\n\n\nCode\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\n\n\n\n\n\nCode\ndef add_interaction_terms(var_list1, var_list2, var_list3=None):\n    \"\"\"\n    Creates interaction term columns in the global DataFrames dtrain and dtest.\n\n    For two sets of variable names (which may represent categorical (dummy) or continuous variables),\n    this function creates two-way interactions by multiplying each variable in var_list1 with each\n    variable in var_list2.\n\n    Optionally, if a third list of variable names (var_list3) is provided, the function also creates\n    three-way interactions among each variable in var_list1, each variable in var_list2, and each variable\n    in var_list3.\n\n    Parameters:\n        var_list1 (list): List of column names for the first set of variables.\n        var_list2 (list): List of column names for the second set of variables.\n        var_list3 (list, optional): List of column names for the third set of variables for three-way interactions.\n\n    Returns:\n        A flat list of new interaction column names.\n    \"\"\"\n    global dtrain, dtest\n\n    interaction_cols = []\n\n    # Create two-way interactions between var_list1 and var_list2.\n    for var1 in var_list1:\n        for var2 in var_list2:\n            col_name = f\"{var1}_*_{var2}\"\n            dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list1 and var_list3.\n    if var_list3 is not None:\n      for var1 in var_list1:\n          for var3 in var_list3:\n              col_name = f\"{var1}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list2 and var_list3.\n    if var_list3 is not None:\n      for var2 in var_list2:\n          for var3 in var_list3:\n              col_name = f\"{var2}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # If a third list is provided, create three-way interactions.\n    if var_list3 is not None:\n        for var1 in var_list1:\n            for var2 in var_list2:\n                for var3 in var_list3:\n                    col_name = f\"{var1}_*_{var2}_*_{var3}\"\n                    dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    interaction_cols.append(col_name)\n\n    return interaction_cols\n\n # Example\n # interaction_cols_brand_price = add_interaction_terms(dummy_cols_brand, ['log_price'])\n # interaction_cols_brand_ad_price = add_interaction_terms(dummy_cols_brand, dummy_cols_ad, ['log_price'])\n\n\n\n\n\n\n\nCode\ndef compare_reg_models(models, assemblers, names=None):\n    \"\"\"\n    Produces a single formatted table comparing multiple regression models.\n\n    For each predictor (the union across models, ordered by first appearance), the table shows\n    the beta estimate (with significance stars) from each model (blank if not used).\n    For a predictor, if a model's outcome (model.getLabelCol()) starts with \"log\", the cell displays\n    both the beta and its exponential (separated by \" / \"), except when the predictor's name includes \"log_\".\n    (The intercept row does not display exp(.))\n\n    Additional rows for Intercept, Observations, R², and RMSE are appended.\n\n    The header's first column is labeled \"Predictor\", and subsequent columns are\n    \"y: [outcome] ([name])\" for each model.\n\n    The table is produced in grid format (with vertical lines). A dashed line (using '-' characters)\n    is inserted at the top, immediately after the header, and at the bottom.\n    Additionally, immediately after the Intercept row, the border line is replaced with one using '='\n    (to appear as, for example, \"+==============================================+==========================+...\").\n\n    Parameters:\n        models (list): List of fitted LinearRegression models.\n        assemblers (list): List of corresponding VectorAssembler objects.\n        names (list, optional): List of model names; defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing the combined regression table.\n    \"\"\"\n    # Default model names.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(models))]\n\n    # For each model, get outcome and determine if that model is log-transformed.\n    outcomes = [m.getLabelCol() for m in models]\n    is_log_flags = [out.lower().startswith(\"log\") for out in outcomes]\n\n    # Build an ordered union of predictors based on first appearance.\n    ordered_predictors = []\n    for assembler in assemblers:\n        for feat in assembler.getInputCols():\n            if feat not in ordered_predictors:\n                ordered_predictors.append(feat)\n\n    # Helper for significance stars.\n    def significance_stars(p):\n        if p is None:\n            return \"\"\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build rows for each predictor.\n    rows = []\n    for feat in ordered_predictors:\n        row = [feat]\n        for m, a, is_log in zip(models, assemblers, is_log_flags):\n            feats_model = a.getInputCols()\n            if feat in feats_model:\n                idx = feats_model.index(feat)\n                beta = m.coefficients.toArray()[idx]\n                p_val = m.summary.pValues[idx] if m.summary.pValues is not None else None\n                stars = significance_stars(p_val)\n                cell = f\"{beta:.3f}{stars}\"\n                # Only add exp(beta) if model is log and predictor name does NOT include \"log_\"\n                if is_log and (\"log_\" not in feat.lower()):\n                    cell += f\" / {np.exp(beta):,.3f}\"\n                row.append(cell)\n            else:\n                row.append(\"\")\n        rows.append(row)\n\n    # Build intercept row (do NOT compute exp(intercept)).\n    intercept_row = [\"Intercept\"]\n    for m in models:\n        std_all = np.array(m.summary.coefficientStandardErrors)\n        coeffs = m.coefficients.toArray()\n        if len(std_all) == len(coeffs) + 1:\n            intercept_p = m.summary.pValues[0] if m.summary.pValues is not None else None\n        else:\n            intercept_p = None\n        sig = significance_stars(intercept_p)\n        cell = f\"{m.intercept:.3f}{sig}\"\n        intercept_row.append(cell)\n    rows.append(intercept_row)\n\n    # Add Observations row.\n    obs_row = [\"Observations\"]\n    for m in models:\n        obs = m.summary.numInstances\n        obs_row.append(f\"{int(obs):,}\")\n    rows.append(obs_row)\n\n    # Add R² row.\n    r2_row = [\"R²\"]\n    for m in models:\n        r2_row.append(f\"{m.summary.r2:.3f}\")\n    rows.append(r2_row)\n\n    # Add RMSE row.\n    rmse_row = [\"RMSE\"]\n    for m in models:\n        rmse_row.append(f\"{m.summary.rootMeanSquaredError:.3f}\")\n    rows.append(rmse_row)\n\n    # Build header: first column \"Predictor\", then for each model: \"y: [outcome] ([name])\"\n    header = [\"Predictor\"]\n    for out, name in zip(outcomes, names):\n        header.append(f\"y: {out} ({name})\")\n\n    # Create table string using grid format.\n    table_str = tabulate(rows, headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(models))\n\n    # Split into lines.\n    lines = table_str.split(\"\\n\")\n\n    # Create a dashed line spanning the full width.\n    full_width = len(lines[0])\n    dash_line = '-' * full_width\n    # Create an equals line by replacing '-' with '='.\n    eq_line = dash_line.replace('-', '=')\n\n    # Insert a dashed line after the header row.\n    lines = table_str.split(\"\\n\")\n    # In grid format, header and separator are usually the first two lines.\n    lines.insert(2, dash_line)\n\n    # Insert an equals line after the Intercept row.\n    for i, line in enumerate(lines):\n        if line.startswith(\"|\") and \"Intercept\" in line:\n            if i+1 &lt; len(lines):\n                lines[i+1] = eq_line\n            break\n\n    # Add dashed lines at the very top and bottom.\n    final_table = dash_line + \"\\n\" + \"\\n\".join(lines) + \"\\n\" + dash_line\n\n    return final_table\n\n# Example usage:\n# print(compare_reg_models([model_1, model_2, model_3],\n#                          [assembler_1, assembler_2, assembler_3],\n#                          [\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\n\n\n\n\n\nCode\ndef compare_rmse(test_dfs, label_col, pred_col=\"prediction\", names=None):\n    \"\"\"\n    Computes and compares RMSE values for a list of test DataFrames.\n\n    For each DataFrame in test_dfs, this function calculates the RMSE between the actual outcome\n    (given by label_col) and the predicted value (given by pred_col, default \"prediction\"). It then\n    produces a formatted table where the first column header is empty and the first row's first cell is\n    \"RMSE\", with each model's RMSE in its own column.\n\n    Parameters:\n        test_dfs (list): List of test DataFrames.\n        label_col (str): The name of the outcome column.\n        pred_col (str, optional): The name of the prediction column (default \"prediction\").\n        names (list, optional): List of model names corresponding to the test DataFrames.\n                                Defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing a table that compares RMSE values for each test DataFrame,\n        with one model per column.\n    \"\"\"\n    # Set default model names if none provided.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(test_dfs))]\n\n    rmse_values = []\n    for df in test_dfs:\n        # Create a column for squared error.\n        df = df.withColumn(\"error_sq\", pow(col(label_col) - col(pred_col), 2))\n        # Calculate RMSE: square root of the mean squared error.\n        rmse = df.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n        rmse_values.append(rmse)\n\n    # Build a single row table: first cell \"RMSE\", then one cell per model with the RMSE value.\n    row = [\"RMSE\"] + [f\"{rmse:.3f}\" for rmse in rmse_values]\n\n    # Build header: first column header is empty, then model names.\n    header = [\"\"] + names\n\n    table_str = tabulate([row], headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(names))\n    return table_str\n\n# Example usage:\n# print(compare_rmse([dtest_1, dtest_2, dtest_3], \"log_sales\", names=[\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\n\n\n\n\n\nCode\ndef residual_plot(df, label_col, model_name):\n    \"\"\"\n    Generates a residual plot for a given test dataframe.\n\n    Parameters:\n        df (DataFrame): Spark DataFrame containing the test set with predictions.\n        label_col (str): The column name of the actual outcome variable.\n        title (str): The title for the residual plot.\n\n    Returns:\n        None (displays the plot)\n    \"\"\"\n    # Convert to Pandas DataFrame\n    df_pd = df.select([\"prediction\", label_col]).toPandas()\n    df_pd[\"residual\"] = df_pd[label_col] - df_pd[\"prediction\"]\n\n    # Scatter plot of residuals vs. predicted values\n    plt.scatter(df_pd[\"prediction\"], df_pd[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n    # Use LOWESS smoothing for trend line\n    smoothed = sm.nonparametric.lowess(df_pd[\"residual\"], df_pd[\"prediction\"])\n    plt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\n\n    # Add reference line at y=0\n    plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n\n    # Labels and title (model_name)\n    plt.xlabel(\"Predicted Values\")\n    plt.ylabel(\"Residuals\")\n    model_name = \"Residual Plot for \" + model_name\n    plt.title(model_name)\n\n    # Show plot\n    plt.show()\n\n# Example usage:\n# residual_plot(dtest_1, \"log_sales\", \"Model 1\")"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_OVB.html#required-libraries-and-spark-session",
    "href": "danl-ml/danl_320_linear_regression_OVB.html#required-libraries-and-spark-session",
    "title": "Omitted Variable Bias",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_OVB.html#udfs",
    "href": "danl-ml/danl_320_linear_regression_OVB.html#udfs",
    "title": "Omitted Variable Bias",
    "section": "",
    "text": "Code\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper | Exp(95% CI Lower) | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: 95% CI Upper, 8: Exp(95% CI Lower), 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"95% CI Upper\", \"Exp(95% CI Lower)\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))\n\n\n\n\n\n\n\nCode\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\n\n\n\n\n\nCode\ndef add_interaction_terms(var_list1, var_list2, var_list3=None):\n    \"\"\"\n    Creates interaction term columns in the global DataFrames dtrain and dtest.\n\n    For two sets of variable names (which may represent categorical (dummy) or continuous variables),\n    this function creates two-way interactions by multiplying each variable in var_list1 with each\n    variable in var_list2.\n\n    Optionally, if a third list of variable names (var_list3) is provided, the function also creates\n    three-way interactions among each variable in var_list1, each variable in var_list2, and each variable\n    in var_list3.\n\n    Parameters:\n        var_list1 (list): List of column names for the first set of variables.\n        var_list2 (list): List of column names for the second set of variables.\n        var_list3 (list, optional): List of column names for the third set of variables for three-way interactions.\n\n    Returns:\n        A flat list of new interaction column names.\n    \"\"\"\n    global dtrain, dtest\n\n    interaction_cols = []\n\n    # Create two-way interactions between var_list1 and var_list2.\n    for var1 in var_list1:\n        for var2 in var_list2:\n            col_name = f\"{var1}_*_{var2}\"\n            dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list1 and var_list3.\n    if var_list3 is not None:\n      for var1 in var_list1:\n          for var3 in var_list3:\n              col_name = f\"{var1}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list2 and var_list3.\n    if var_list3 is not None:\n      for var2 in var_list2:\n          for var3 in var_list3:\n              col_name = f\"{var2}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # If a third list is provided, create three-way interactions.\n    if var_list3 is not None:\n        for var1 in var_list1:\n            for var2 in var_list2:\n                for var3 in var_list3:\n                    col_name = f\"{var1}_*_{var2}_*_{var3}\"\n                    dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    interaction_cols.append(col_name)\n\n    return interaction_cols\n\n # Example\n # interaction_cols_brand_price = add_interaction_terms(dummy_cols_brand, ['log_price'])\n # interaction_cols_brand_ad_price = add_interaction_terms(dummy_cols_brand, dummy_cols_ad, ['log_price'])\n\n\n\n\n\n\n\nCode\ndef compare_reg_models(models, assemblers, names=None):\n    \"\"\"\n    Produces a single formatted table comparing multiple regression models.\n\n    For each predictor (the union across models, ordered by first appearance), the table shows\n    the beta estimate (with significance stars) from each model (blank if not used).\n    For a predictor, if a model's outcome (model.getLabelCol()) starts with \"log\", the cell displays\n    both the beta and its exponential (separated by \" / \"), except when the predictor's name includes \"log_\".\n    (The intercept row does not display exp(.))\n\n    Additional rows for Intercept, Observations, R², and RMSE are appended.\n\n    The header's first column is labeled \"Predictor\", and subsequent columns are\n    \"y: [outcome] ([name])\" for each model.\n\n    The table is produced in grid format (with vertical lines). A dashed line (using '-' characters)\n    is inserted at the top, immediately after the header, and at the bottom.\n    Additionally, immediately after the Intercept row, the border line is replaced with one using '='\n    (to appear as, for example, \"+==============================================+==========================+...\").\n\n    Parameters:\n        models (list): List of fitted LinearRegression models.\n        assemblers (list): List of corresponding VectorAssembler objects.\n        names (list, optional): List of model names; defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing the combined regression table.\n    \"\"\"\n    # Default model names.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(models))]\n\n    # For each model, get outcome and determine if that model is log-transformed.\n    outcomes = [m.getLabelCol() for m in models]\n    is_log_flags = [out.lower().startswith(\"log\") for out in outcomes]\n\n    # Build an ordered union of predictors based on first appearance.\n    ordered_predictors = []\n    for assembler in assemblers:\n        for feat in assembler.getInputCols():\n            if feat not in ordered_predictors:\n                ordered_predictors.append(feat)\n\n    # Helper for significance stars.\n    def significance_stars(p):\n        if p is None:\n            return \"\"\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build rows for each predictor.\n    rows = []\n    for feat in ordered_predictors:\n        row = [feat]\n        for m, a, is_log in zip(models, assemblers, is_log_flags):\n            feats_model = a.getInputCols()\n            if feat in feats_model:\n                idx = feats_model.index(feat)\n                beta = m.coefficients.toArray()[idx]\n                p_val = m.summary.pValues[idx] if m.summary.pValues is not None else None\n                stars = significance_stars(p_val)\n                cell = f\"{beta:.3f}{stars}\"\n                # Only add exp(beta) if model is log and predictor name does NOT include \"log_\"\n                if is_log and (\"log_\" not in feat.lower()):\n                    cell += f\" / {np.exp(beta):,.3f}\"\n                row.append(cell)\n            else:\n                row.append(\"\")\n        rows.append(row)\n\n    # Build intercept row (do NOT compute exp(intercept)).\n    intercept_row = [\"Intercept\"]\n    for m in models:\n        std_all = np.array(m.summary.coefficientStandardErrors)\n        coeffs = m.coefficients.toArray()\n        if len(std_all) == len(coeffs) + 1:\n            intercept_p = m.summary.pValues[0] if m.summary.pValues is not None else None\n        else:\n            intercept_p = None\n        sig = significance_stars(intercept_p)\n        cell = f\"{m.intercept:.3f}{sig}\"\n        intercept_row.append(cell)\n    rows.append(intercept_row)\n\n    # Add Observations row.\n    obs_row = [\"Observations\"]\n    for m in models:\n        obs = m.summary.numInstances\n        obs_row.append(f\"{int(obs):,}\")\n    rows.append(obs_row)\n\n    # Add R² row.\n    r2_row = [\"R²\"]\n    for m in models:\n        r2_row.append(f\"{m.summary.r2:.3f}\")\n    rows.append(r2_row)\n\n    # Add RMSE row.\n    rmse_row = [\"RMSE\"]\n    for m in models:\n        rmse_row.append(f\"{m.summary.rootMeanSquaredError:.3f}\")\n    rows.append(rmse_row)\n\n    # Build header: first column \"Predictor\", then for each model: \"y: [outcome] ([name])\"\n    header = [\"Predictor\"]\n    for out, name in zip(outcomes, names):\n        header.append(f\"y: {out} ({name})\")\n\n    # Create table string using grid format.\n    table_str = tabulate(rows, headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(models))\n\n    # Split into lines.\n    lines = table_str.split(\"\\n\")\n\n    # Create a dashed line spanning the full width.\n    full_width = len(lines[0])\n    dash_line = '-' * full_width\n    # Create an equals line by replacing '-' with '='.\n    eq_line = dash_line.replace('-', '=')\n\n    # Insert a dashed line after the header row.\n    lines = table_str.split(\"\\n\")\n    # In grid format, header and separator are usually the first two lines.\n    lines.insert(2, dash_line)\n\n    # Insert an equals line after the Intercept row.\n    for i, line in enumerate(lines):\n        if line.startswith(\"|\") and \"Intercept\" in line:\n            if i+1 &lt; len(lines):\n                lines[i+1] = eq_line\n            break\n\n    # Add dashed lines at the very top and bottom.\n    final_table = dash_line + \"\\n\" + \"\\n\".join(lines) + \"\\n\" + dash_line\n\n    return final_table\n\n# Example usage:\n# print(compare_reg_models([model_1, model_2, model_3],\n#                          [assembler_1, assembler_2, assembler_3],\n#                          [\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\n\n\n\n\n\nCode\ndef compare_rmse(test_dfs, label_col, pred_col=\"prediction\", names=None):\n    \"\"\"\n    Computes and compares RMSE values for a list of test DataFrames.\n\n    For each DataFrame in test_dfs, this function calculates the RMSE between the actual outcome\n    (given by label_col) and the predicted value (given by pred_col, default \"prediction\"). It then\n    produces a formatted table where the first column header is empty and the first row's first cell is\n    \"RMSE\", with each model's RMSE in its own column.\n\n    Parameters:\n        test_dfs (list): List of test DataFrames.\n        label_col (str): The name of the outcome column.\n        pred_col (str, optional): The name of the prediction column (default \"prediction\").\n        names (list, optional): List of model names corresponding to the test DataFrames.\n                                Defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing a table that compares RMSE values for each test DataFrame,\n        with one model per column.\n    \"\"\"\n    # Set default model names if none provided.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(test_dfs))]\n\n    rmse_values = []\n    for df in test_dfs:\n        # Create a column for squared error.\n        df = df.withColumn(\"error_sq\", pow(col(label_col) - col(pred_col), 2))\n        # Calculate RMSE: square root of the mean squared error.\n        rmse = df.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n        rmse_values.append(rmse)\n\n    # Build a single row table: first cell \"RMSE\", then one cell per model with the RMSE value.\n    row = [\"RMSE\"] + [f\"{rmse:.3f}\" for rmse in rmse_values]\n\n    # Build header: first column header is empty, then model names.\n    header = [\"\"] + names\n\n    table_str = tabulate([row], headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(names))\n    return table_str\n\n# Example usage:\n# print(compare_rmse([dtest_1, dtest_2, dtest_3], \"log_sales\", names=[\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\n\n\n\n\n\nCode\ndef residual_plot(df, label_col, model_name):\n    \"\"\"\n    Generates a residual plot for a given test dataframe.\n\n    Parameters:\n        df (DataFrame): Spark DataFrame containing the test set with predictions.\n        label_col (str): The column name of the actual outcome variable.\n        title (str): The title for the residual plot.\n\n    Returns:\n        None (displays the plot)\n    \"\"\"\n    # Convert to Pandas DataFrame\n    df_pd = df.select([\"prediction\", label_col]).toPandas()\n    df_pd[\"residual\"] = df_pd[label_col] - df_pd[\"prediction\"]\n\n    # Scatter plot of residuals vs. predicted values\n    plt.scatter(df_pd[\"prediction\"], df_pd[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n    # Use LOWESS smoothing for trend line\n    smoothed = sm.nonparametric.lowess(df_pd[\"residual\"], df_pd[\"prediction\"])\n    plt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\n\n    # Add reference line at y=0\n    plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n\n    # Labels and title (model_name)\n    plt.xlabel(\"Predicted Values\")\n    plt.ylabel(\"Residuals\")\n    model_name = \"Residual Plot for \" + model_name\n    plt.title(model_name)\n\n    # Show plot\n    plt.show()\n\n# Example usage:\n# residual_plot(dtest_1, \"log_sales\", \"Model 1\")"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_OVB.html#log-transformation",
    "href": "danl-ml/danl_320_linear_regression_OVB.html#log-transformation",
    "title": "Omitted Variable Bias",
    "section": "Log Transformation",
    "text": "Log Transformation\n\ndf = spark.createDataFrame(oj)\ndf = (\n    df\n    .withColumn(\"log_sales\",\n                log(df['sales']) )\n    .withColumn(\"log_price\",\n                log(df['price']) )\n)"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_OVB.html#training-test-split",
    "href": "danl-ml/danl_320_linear_regression_OVB.html#training-test-split",
    "title": "Omitted Variable Bias",
    "section": "Training-Test Split",
    "text": "Training-Test Split\n\ndtrain, dtest = df.randomSplit([0.9, 0.1], seed = 1234)"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_OVB.html#adding-dummies",
    "href": "danl-ml/danl_320_linear_regression_OVB.html#adding-dummies",
    "title": "Omitted Variable Bias",
    "section": "Adding Dummies",
    "text": "Adding Dummies\n\ndummy_cols_brand, ref_category_brand = add_dummy_variables('brand', 0)\n\nReference category (dummy omitted): dominicks"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_OVB.html#betas",
    "href": "danl-ml/danl_320_linear_regression_OVB.html#betas",
    "title": "Omitted Variable Bias",
    "section": "Betas",
    "text": "Betas\n\nprint(\n    compare_reg_models(\n        [model_1, model_2, model_3],\n        [assembler_1, assembler_2, assembler_3]\n        )\n    )\n\n---------------------------------------------------------------------------------------------------------------------------------\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| Predictor                                    |   y: log_sales (Model 1) |   y: log_sales (Model 2) |   y: log_sales (Model 3) |\n---------------------------------------------------------------------------------------------------------------------------------\n+==============================================+==========================+==========================+==========================+\n| log_price                                    |                -3.152*** |                -3.410*** |                -2.797*** |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| brand_minute_maid                            |         0.876*** / 2.400 |         0.868*** / 2.382 |            0.029 / 1.029 |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| brand_tropicana                              |         1.537*** / 4.649 |         0.955*** / 2.598 |         0.681*** / 1.976 |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| brand_minute_maid_*_log_price                |                          |                    0.095 |                 0.813*** |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| brand_tropicana_*_log_price                  |                          |                 0.688*** |                 0.766*** |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| ad_status_Ad                                 |                          |                          |         1.095*** / 2.990 |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| brand_minute_maid_*_ad_status_Ad             |                          |                          |         1.183*** / 3.265 |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| brand_tropicana_*_ad_status_Ad               |                          |                          |         0.814*** / 2.256 |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| ad_status_Ad_*_log_price                     |                          |                          |                -0.499*** |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| brand_minute_maid_*_ad_status_Ad_*_log_price |                          |                          |                -1.096*** |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| brand_tropicana_*_ad_status_Ad_*_log_price   |                          |                          |                -0.975*** |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| Intercept                                    |                10.836*** |                10.972*** |                10.423*** |\n=================================================================================================================================\n| Observations                                 |                   26,040 |                   26,040 |                   26,040 |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| R²                                           |                    0.394 |                    0.398 |                    0.535 |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| RMSE                                         |                    0.794 |                    0.792 |                    0.696 |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n---------------------------------------------------------------------------------------------------------------------------------"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_OVB.html#rmses",
    "href": "danl-ml/danl_320_linear_regression_OVB.html#rmses",
    "title": "Omitted Variable Bias",
    "section": "RMSEs",
    "text": "RMSEs\n\n# Create a new column for squared error\ndtest_1 = dtest_1.withColumn(\"error_sq\", pow(col(\"log_sales\") - col(\"prediction\"), 2))\n\n# Calculate RMSE as the square root of the mean squared error\nrmse_val_1 = dtest_1.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n\nprint(f'RMSE_1: {rmse_val_1:.3f}')\n\nRMSE_1: 0.785\n\n\n\nprint(compare_rmse([dtest_1, dtest_2, dtest_3], \"log_sales\"))\n\n+------+-----------+-----------+-----------+\n|      |   Model 1 |   Model 2 |   Model 3 |\n+======+===========+===========+===========+\n| RMSE |     0.785 |     0.783 |     0.682 |\n+------+-----------+-----------+-----------+"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_OVB.html#residual-plots",
    "href": "danl-ml/danl_320_linear_regression_OVB.html#residual-plots",
    "title": "Omitted Variable Bias",
    "section": "Residual Plots",
    "text": "Residual Plots\n\nresidual_plot(dtest_1, \"log_sales\", \"Model 1\")\n\n\n\n\n\n\n\n\n\nresidual_plot(dtest_2, \"log_sales\", \"Model 2\")\n\n\n\n\n\n\n\n\n\nresidual_plot(dtest_3, \"log_sales\", \"Model 3\")"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_OVB.html#short-regression",
    "href": "danl-ml/danl_320_linear_regression_OVB.html#short-regression",
    "title": "Omitted Variable Bias",
    "section": "Short-regression",
    "text": "Short-regression\n\n# assembling predictors\nconti_cols = [\"log_price\"]\n\nassembler_predictors = (\n    conti_cols\n)\n\nassembler_short = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\n\ndtrain_short = assembler_short.transform(dtrain)\ndtest_short  = assembler_short.transform(dtest)\n\n# training model\nmodel_short = (\n    LinearRegression(featuresCol=\"predictors\",\n                     labelCol=\"log_sales\")\n    .fit(dtrain_short)\n)\n\n# makting regression table\nprint( regression_table(model_short, assembler_short) )\n\n+--------------+--------+-----------+------+------------+---------+--------------+--------------+-------------------+-------------------+\n| y: log_sales |   Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper | Exp(95% CI Lower) | Exp(95% CI Upper) |\n+--------------+--------+-----------+------+------------+---------+--------------+--------------+-------------------+-------------------+\n| log_price    | -1.604 |           | ***  |      0.016 |   0.000 |       -1.635 |       -1.572 |                   |                   |\n| Intercept    | 10.425 |           | ***  |      0.019 |         |       10.387 |              |            10.463 |                   |\n-----------------------------------------------------------------------------------------------------------------------------------------\n| Observations | 26,040 |           |      |            |         |              |              |                   |                   |\n| R²           |  0.208 |           |      |            |         |              |              |                   |                   |\n| RMSE         |  0.909 |           |      |            |         |              |              |                   |                   |\n+--------------+--------+-----------+------+------------+---------+--------------+--------------+-------------------+-------------------+"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_OVB.html#long-regression",
    "href": "danl-ml/danl_320_linear_regression_OVB.html#long-regression",
    "title": "Omitted Variable Bias",
    "section": "Long-regression",
    "text": "Long-regression\n\n# assembling predictors\nconti_cols = [\"log_price\"]\n\nassembler_predictors = (\n    conti_cols +\n    dummy_cols_brand\n)\n\nassembler_long = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\n\ndtrain_long = assembler_long.transform(dtrain)\ndtest_long  = assembler_long.transform(dtest)\n\n# training model\nmodel_long = (\n    LinearRegression(featuresCol=\"predictors\",\n                     labelCol=\"log_sales\")\n    .fit(dtrain_long)\n)\n\n# makting regression table\nprint( regression_table(model_long, assembler_long) )\n\n+-------------------+--------+-----------+------+------------+---------+--------------+--------------+-------------------+-------------------+\n| y: log_sales      |   Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper | Exp(95% CI Lower) | Exp(95% CI Upper) |\n+-------------------+--------+-----------+------+------------+---------+--------------+--------------+-------------------+-------------------+\n| log_price         | -3.152 |           | ***  |      0.014 |   0.000 |       -3.179 |       -3.125 |                   |                   |\n| brand_minute_maid |  0.876 |     2.400 | ***  |      0.017 |   0.000 |        0.842 |        0.909 |             2.321 |             2.483 |\n| brand_tropicana   |  1.537 |     4.649 | ***  |      0.015 |   0.000 |        1.507 |        1.567 |             4.511 |             4.792 |\n| Intercept         | 10.836 |           | ***  |      0.024 |         |       10.788 |              |            10.883 |                   |\n----------------------------------------------------------------------------------------------------------------------------------------------\n| Observations      | 26,040 |           |      |            |         |              |              |                   |                   |\n| R²                |  0.394 |           |      |            |         |              |              |                   |                   |\n| RMSE              |  0.794 |           |      |            |         |              |              |                   |                   |\n+-------------------+--------+-----------+------+------------+---------+--------------+--------------+-------------------+-------------------+"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_OVB.html#ovb",
    "href": "danl-ml/danl_320_linear_regression_OVB.html#ovb",
    "title": "Omitted Variable Bias",
    "section": "OVB",
    "text": "OVB\n\nprint(\n    compare_reg_models(\n        [model_short, model_long],\n        [assembler_short, assembler_long]\n        )\n    )\n\n---------------------------------------------------------------------------\n+-------------------+--------------------------+--------------------------+\n| Predictor         |   y: log_sales (Model 1) |   y: log_sales (Model 2) |\n---------------------------------------------------------------------------\n+===================+==========================+==========================+\n| log_price         |                -1.604*** |                -3.152*** |\n+-------------------+--------------------------+--------------------------+\n| brand_minute_maid |                          |         0.876*** / 2.400 |\n+-------------------+--------------------------+--------------------------+\n| brand_tropicana   |                          |         1.537*** / 4.649 |\n+-------------------+--------------------------+--------------------------+\n| Intercept         |                10.425*** |                10.836*** |\n===========================================================================\n| Observations      |                   26,040 |                   26,040 |\n+-------------------+--------------------------+--------------------------+\n| R²                |                    0.208 |                    0.394 |\n+-------------------+--------------------------+--------------------------+\n| RMSE              |                    0.909 |                    0.794 |\n+-------------------+--------------------------+--------------------------+\n---------------------------------------------------------------------------\n\n\n\novb = -1.604 + 3.152\novb\n\n1.548"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_OVB.html#how-does-ovb-happen",
    "href": "danl-ml/danl_320_linear_regression_OVB.html#how-does-ovb-happen",
    "title": "Omitted Variable Bias",
    "section": "How does OVB happen?",
    "text": "How does OVB happen?\n\n# assembling predictors\n\nassembler_predictors = (\n    dummy_cols_brand\n)\n\nassembler_1st_stage = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\n\ndtrain_1st = assembler_1st_stage.transform(dtrain)\ndtest_1st  = assembler_1st_stage.transform(dtest)\n\n# training model\nmodel_1st = (\n    LinearRegression(featuresCol=\"predictors\",\n                     labelCol=\"log_price\")\n    .fit(dtrain_1st)\n)\n\n# makting regression table\nprint( regression_table(model_1st, assembler_1st_stage) )\n\n+-------------------+--------+-----------+------+------------+---------+--------------+--------------+-------------------+-------------------+\n| y: log_price      |   Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper | Exp(95% CI Lower) | Exp(95% CI Upper) |\n+-------------------+--------+-----------+------+------------+---------+--------------+--------------+-------------------+-------------------+\n| brand_minute_maid |  0.262 |     1.300 | ***  |      0.003 |   0.000 |        0.256 |        0.268 |             1.292 |             1.308 |\n| brand_tropicana   |  0.508 |     1.662 | ***  |      0.002 |   0.000 |        0.504 |        0.512 |             1.655 |             1.669 |\n| Intercept         |  0.528 |           | ***  |      0.003 |         |        0.522 |              |             0.534 |                   |\n----------------------------------------------------------------------------------------------------------------------------------------------\n| Observations      | 26,040 |           |      |            |         |              |              |                   |                   |\n| R²                |  0.511 |           |      |            |         |              |              |                   |                   |\n| RMSE              |  0.203 |           |      |            |         |              |              |                   |                   |\n+-------------------+--------+-----------+------+------------+---------+--------------+--------------+-------------------+-------------------+\n\n\n\n# Adding residual to training data\ndtrain_1st = model_1st.transform(dtrain_1st)  # prediction\ndtrain_1st = dtrain_1st.withColumn(\"residual\", col(\"log_price\") - col(\"prediction\") )\ndtrain_1st.show()\n\n+-----+-----+-----------+---------+------------------+------------------+---------------+-----------------+---------------+----------+------------------+--------------------+\n|sales|price|      brand|ad_status|         log_sales|         log_price|brand_dominicks|brand_minute_maid|brand_tropicana|predictors|        prediction|            residual|\n+-----+-----+-----------+---------+------------------+------------------+---------------+-----------------+---------------+----------+------------------+--------------------+\n| 64.0| 2.69|  dominicks|       Ad|4.1588830833596715|0.9895411936137477|              1|                0|              0| (2,[],[])|0.5277470062036129|  0.4617941874101348|\n| 64.0| 2.69|  dominicks|    No Ad|4.1588830833596715|0.9895411936137477|              1|                0|              0| (2,[],[])|0.5277470062036129|  0.4617941874101348|\n| 64.0| 2.69|  dominicks|    No Ad|4.1588830833596715|0.9895411936137477|              1|                0|              0| (2,[],[])|0.5277470062036129|  0.4617941874101348|\n|128.0| 1.39|  dominicks|    No Ad| 4.852030263919617|0.3293037471426003|              1|                0|              0| (2,[],[])|0.5277470062036129|-0.19844325906101262|\n|128.0| 2.09|  dominicks|    No Ad| 4.852030263919617|0.7371640659767196|              1|                0|              0| (2,[],[])|0.5277470062036129| 0.20941705977310665|\n|192.0| 2.69|  dominicks|    No Ad|5.2574953720277815|0.9895411936137477|              1|                0|              0| (2,[],[])|0.5277470062036129|  0.4617941874101348|\n|192.0| 2.69|  dominicks|    No Ad|5.2574953720277815|0.9895411936137477|              1|                0|              0| (2,[],[])|0.5277470062036129|  0.4617941874101348|\n|192.0| 3.39|  tropicana|    No Ad|5.2574953720277815| 1.220829921392359|              0|                0|              1| [0.0,1.0]|1.0358024337672593| 0.18502748762509968|\n|256.0| 1.59|  dominicks|    No Ad| 5.545177444479562|0.4637340162321402|              1|                0|              0| (2,[],[])|0.5277470062036129| -0.0640129899714727|\n|256.0| 2.69|  dominicks|       Ad| 5.545177444479562|0.9895411936137477|              1|                0|              0| (2,[],[])|0.5277470062036129|  0.4617941874101348|\n|256.0| 2.69|  dominicks|    No Ad| 5.545177444479562|0.9895411936137477|              1|                0|              0| (2,[],[])|0.5277470062036129|  0.4617941874101348|\n|256.0| 2.69|  dominicks|    No Ad| 5.545177444479562|0.9895411936137477|              1|                0|              0| (2,[],[])|0.5277470062036129|  0.4617941874101348|\n|320.0| 2.66|minute_maid|    No Ad| 5.768320995793772|0.9783261227936078|              0|                1|              0| [1.0,0.0]|0.7899962396372924| 0.18832988315631538|\n|320.0| 2.69|  dominicks|    No Ad| 5.768320995793772|0.9895411936137477|              1|                0|              0| (2,[],[])|0.5277470062036129|  0.4617941874101348|\n|320.0| 2.69|  dominicks|    No Ad| 5.768320995793772|0.9895411936137477|              1|                0|              0| (2,[],[])|0.5277470062036129|  0.4617941874101348|\n|320.0| 2.69|  dominicks|    No Ad| 5.768320995793772|0.9895411936137477|              1|                0|              0| (2,[],[])|0.5277470062036129|  0.4617941874101348|\n|384.0| 2.19|  dominicks|       Ad| 5.950642552587727|0.7839015438284094|              1|                0|              0| (2,[],[])|0.5277470062036129| 0.25615453762479645|\n|384.0| 2.69|  dominicks|       Ad| 5.950642552587727|0.9895411936137477|              1|                0|              0| (2,[],[])|0.5277470062036129|  0.4617941874101348|\n|384.0| 2.69|  dominicks|    No Ad| 5.950642552587727|0.9895411936137477|              1|                0|              0| (2,[],[])|0.5277470062036129|  0.4617941874101348|\n|384.0| 2.69|  dominicks|    No Ad| 5.950642552587727|0.9895411936137477|              1|                0|              0| (2,[],[])|0.5277470062036129|  0.4617941874101348|\n+-----+-----+-----------+---------+------------------+------------------+---------------+-----------------+---------------+----------+------------------+--------------------+\nonly showing top 20 rows\n\n\n\n\ndtrain_2nd = dtrain_1st\ndtrain_2nd = dtrain_2nd.drop('prediction', 'predictors')\ndtrain_2nd.show()\n\n+-----+-----+-----------+---------+------------------+------------------+---------------+-----------------+---------------+--------------------+\n|sales|price|      brand|ad_status|         log_sales|         log_price|brand_dominicks|brand_minute_maid|brand_tropicana|            residual|\n+-----+-----+-----------+---------+------------------+------------------+---------------+-----------------+---------------+--------------------+\n| 64.0| 2.69|  dominicks|       Ad|4.1588830833596715|0.9895411936137477|              1|                0|              0|  0.4617941874101348|\n| 64.0| 2.69|  dominicks|    No Ad|4.1588830833596715|0.9895411936137477|              1|                0|              0|  0.4617941874101348|\n| 64.0| 2.69|  dominicks|    No Ad|4.1588830833596715|0.9895411936137477|              1|                0|              0|  0.4617941874101348|\n|128.0| 1.39|  dominicks|    No Ad| 4.852030263919617|0.3293037471426003|              1|                0|              0|-0.19844325906101262|\n|128.0| 2.09|  dominicks|    No Ad| 4.852030263919617|0.7371640659767196|              1|                0|              0| 0.20941705977310665|\n|192.0| 2.69|  dominicks|    No Ad|5.2574953720277815|0.9895411936137477|              1|                0|              0|  0.4617941874101348|\n|192.0| 2.69|  dominicks|    No Ad|5.2574953720277815|0.9895411936137477|              1|                0|              0|  0.4617941874101348|\n|192.0| 3.39|  tropicana|    No Ad|5.2574953720277815| 1.220829921392359|              0|                0|              1| 0.18502748762509968|\n|256.0| 1.59|  dominicks|    No Ad| 5.545177444479562|0.4637340162321402|              1|                0|              0| -0.0640129899714727|\n|256.0| 2.69|  dominicks|       Ad| 5.545177444479562|0.9895411936137477|              1|                0|              0|  0.4617941874101348|\n|256.0| 2.69|  dominicks|    No Ad| 5.545177444479562|0.9895411936137477|              1|                0|              0|  0.4617941874101348|\n|256.0| 2.69|  dominicks|    No Ad| 5.545177444479562|0.9895411936137477|              1|                0|              0|  0.4617941874101348|\n|320.0| 2.66|minute_maid|    No Ad| 5.768320995793772|0.9783261227936078|              0|                1|              0| 0.18832988315631538|\n|320.0| 2.69|  dominicks|    No Ad| 5.768320995793772|0.9895411936137477|              1|                0|              0|  0.4617941874101348|\n|320.0| 2.69|  dominicks|    No Ad| 5.768320995793772|0.9895411936137477|              1|                0|              0|  0.4617941874101348|\n|320.0| 2.69|  dominicks|    No Ad| 5.768320995793772|0.9895411936137477|              1|                0|              0|  0.4617941874101348|\n|384.0| 2.19|  dominicks|       Ad| 5.950642552587727|0.7839015438284094|              1|                0|              0| 0.25615453762479645|\n|384.0| 2.69|  dominicks|       Ad| 5.950642552587727|0.9895411936137477|              1|                0|              0|  0.4617941874101348|\n|384.0| 2.69|  dominicks|    No Ad| 5.950642552587727|0.9895411936137477|              1|                0|              0|  0.4617941874101348|\n|384.0| 2.69|  dominicks|    No Ad| 5.950642552587727|0.9895411936137477|              1|                0|              0|  0.4617941874101348|\n+-----+-----+-----------+---------+------------------+------------------+---------------+-----------------+---------------+--------------------+\nonly showing top 20 rows\n\n\n\n\n# assembling predictors\nconti_cols = [\"residual\"]\n\nassembler_predictors = (\n    conti_cols\n)\n\nassembler_2nd = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\n\ndtrain_2nd = assembler_2nd.transform(dtrain_2nd)\n\n# training model\nmodel_2nd = (\n    LinearRegression(featuresCol=\"predictors\",\n                     labelCol=\"log_sales\")\n    .fit(dtrain_2nd)\n)\n\n# makting regression table\nprint( regression_table(model_2nd, assembler_2nd) )\n\n+--------------+--------+-----------+------+------------+---------+--------------+--------------+-------------------+-------------------+\n| y: log_sales |   Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper | Exp(95% CI Lower) | Exp(95% CI Upper) |\n+--------------+--------+-----------+------+------------+---------+--------------+--------------+-------------------+-------------------+\n| residual     | -3.152 |     0.043 | ***  |      0.005 |   0.000 |       -3.162 |       -3.142 |             0.042 |             0.043 |\n| Intercept    |  9.167 |           | ***  |      0.024 |         |        9.120 |              |             9.215 |                   |\n-----------------------------------------------------------------------------------------------------------------------------------------\n| Observations | 26,040 |           |      |            |         |              |              |                   |                   |\n| R²           |  0.392 |           |      |            |         |              |              |                   |                   |\n| RMSE         |  0.796 |           |      |            |         |              |              |                   |                   |\n+--------------+--------+-----------+------+------------+---------+--------------+--------------+-------------------+-------------------+\n\n\n\nprint(\n    compare_reg_models(\n        [model_2nd, model_long],\n        [assembler_2nd, assembler_long]\n        )\n    )\n\n---------------------------------------------------------------------------\n+-------------------+--------------------------+--------------------------+\n| Predictor         |   y: log_sales (Model 1) |   y: log_sales (Model 2) |\n---------------------------------------------------------------------------\n+===================+==========================+==========================+\n| residual          |        -3.152*** / 0.043 |                          |\n+-------------------+--------------------------+--------------------------+\n| log_price         |                          |                -3.152*** |\n+-------------------+--------------------------+--------------------------+\n| brand_minute_maid |                          |         0.876*** / 2.400 |\n+-------------------+--------------------------+--------------------------+\n| brand_tropicana   |                          |         1.537*** / 4.649 |\n+-------------------+--------------------------+--------------------------+\n| Intercept         |                 9.167*** |                10.836*** |\n===========================================================================\n| Observations      |                   26,040 |                   26,040 |\n+-------------------+--------------------------+--------------------------+\n| R²                |                    0.392 |                    0.394 |\n+-------------------+--------------------------+--------------------------+\n| RMSE              |                    0.796 |                    0.794 |\n+-------------------+--------------------------+--------------------------+\n---------------------------------------------------------------------------"
  },
  {
    "objectID": "danl-ml/danl_320_hw4_part_1_lasso_beer_model_2.html",
    "href": "danl-ml/danl_320_hw4_part_1_lasso_beer_model_2.html",
    "title": "Homework 4 - Part 1: Lasso Linear Regerssion - Model 2",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import scale # zero mean & one s.d.\nfrom sklearn.linear_model import LassoCV, lasso_path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n\n#beer = pd.read_csv(\"https://bcdanl.github.io/data/beer_markets_xbeer_xdemog.zip\")\nbeer = pd.read_csv(\"https://bcdanl.github.io/data/beer_markets_xbeer_brand_xdemog.zip\")\n#beer = pd.read_csv(\"https://bcdanl.github.io/data/beer_markets_xbeer_brand_promo_xdemog.zip\")\n\n\nX = beer.drop('ylogprice', axis = 1)\ny = beer['ylogprice']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ny_train = y_train.values\ny_test = y_test.values\n\n\n# LassoCV with a range of alpha values\nlasso_cv = LassoCV(n_alphas = 100,\n                   alphas = None, # alphas=None automatically generate 100 candidate alpha values\n                   cv = 5,\n                   random_state=42,\n                   max_iter=100000)\n\n\nlasso_cv.fit(X_train.values, y_train)\n#lasso_cv.fit(X_train.values, y_train.ravel())\nprint(\"LassoCV - Best alpha:\", lasso_cv.alpha_)\n\nLassoCV - Best alpha: 0.00022539867869301466\n\n\n\n# Create a DataFrame including the intercept and the coefficients:\ncoef_lasso_beer = pd.DataFrame({\n    'predictor': list(X_train.columns),\n    'coefficient':  list(lasso_cv.coef_),\n    'exp_coefficient': np.exp(  list(lasso_cv.coef_) )\n})\n\n\n# Evaluate\ny_pred_lasso = lasso_cv.predict(X_test)\nmse_lasso = mean_squared_error(y_test, y_pred_lasso)\nprint(\"LassoCV - MSE:\", mse_lasso)\n\nLassoCV - MSE: 0.028972019742897575\n\n\n\ncoef_lasso_beer_n0 = coef_lasso_beer[coef_lasso_beer['coefficient'] != 0]\n\n\nX_train.shape[1]\n\n2645\n\n\n\ncoef_lasso_beer_n0.shape[0]\n\n160\n\n\n\ncoef_lasso_beer_n0\n\n\n    \n\n\n\n\n\n\npredictor\ncoefficient\nexp_coefficient\n\n\n\n\n0\nlogquantity\n-0.140849\n0.868621\n\n\n1\ncontainer_CAN\n-0.054905\n0.946575\n\n\n2\nbrandBUSCH_LIGHT\n-0.085638\n0.917927\n\n\n4\nbrandMILLER_LITE\n0.000932\n1.000932\n\n\n5\nbrandNATURAL_LIGHT\n-0.474588\n0.622141\n\n\n...\n...\n...\n...\n\n\n2362\nmarketRURAL_WEST_VIRGINIA:npeople2\n-0.030961\n0.969513\n\n\n2375\nmarketTAMPA:npeople2\n0.018641\n1.018816\n\n\n2376\nmarketURBAN_NY:npeople2\n0.008943\n1.008983\n\n\n2416\nmarketRALEIGH-DURHAM:npeople3\n-0.005085\n0.994928\n\n\n2572\nmarketEXURBAN_NJ:npeople5plus\n0.079611\n1.082865\n\n\n\n\n160 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# Compute the mean and standard deviation of the CV errors for each alpha.\nmean_cv_errors = np.mean(lasso_cv.mse_path_, axis=1)\nstd_cv_errors = np.std(lasso_cv.mse_path_, axis=1)\n\nplt.figure(figsize=(8, 6))\nplt.errorbar(lasso_cv.alphas_, mean_cv_errors, yerr=std_cv_errors, marker='o', linestyle='-', capsize=5)\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Mean CV Error (MSE)')\nplt.title('Cross-Validation Error vs. Alpha')\n#plt.gca().invert_xaxis()  # Optionally invert the x-axis so lower alphas (less regularization) appear to the right.\nplt.axvline(x=lasso_cv.alpha_, color='red', linestyle='--', label='Best alpha')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Compute the coefficient path over the alpha grid that LassoCV used\nalphas, coefs, _ = lasso_path(X_train, y_train,\n                              alphas=lasso_cv.alphas_,\n                              max_iter=100000)\n\n# Count nonzero coefficients for each alpha (coefs shape: (n_features, n_alphas))\nnonzero_counts = np.sum(coefs != 0, axis=0)\n\n# Plot the number of nonzero coefficients versus alpha\nplt.figure(figsize=(8,6))\nplt.plot(alphas, nonzero_counts, marker='o', linestyle='-')\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Number of nonzero coefficients')\nplt.title('Nonzero Coefficients vs. Alpha')\n#plt.gca().invert_xaxis()  # Lower alphas (less regularization) on the right\nplt.axvline(x=lasso_cv.alpha_, color='red', linestyle='--', label='Best alpha')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Compute the lasso path. Note: we use np.log(y_train) because that's what you used in LassoCV.\nalphas, coefs, _ = lasso_path(X_train, y_train,\n                              alphas=lasso_cv.alphas_,\n                              max_iter=100000)\nplt.figure(figsize=(8, 6))\n# Iterate over each predictor and plot its coefficient path.\nfor i, col in enumerate(X_train.columns):\n    plt.plot(alphas, coefs[i, :], label=col)\n\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Coefficient value')\nplt.title('Lasso Coefficient Paths')\n#plt.gca().invert_xaxis()  # Lower alphas (weaker regularization) to the right.\n#plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.axvline(x=lasso_cv.alpha_, color='red', linestyle='--', label='Best alpha')\n#plt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "danl-ml/danl_320_lasso_linear_regression_online_shopping.html",
    "href": "danl-ml/danl_320_lasso_linear_regression_online_shopping.html",
    "title": "Lasso Linear Regression",
    "section": "",
    "text": "from google.colab import data_table\ndata_table.enable_dataframe_formatter()  # Enabling an interactive DataFrame display\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import scale # zero mean & one s.d.\nfrom sklearn.linear_model import LassoCV, lasso_path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\ndf = pd.read_csv(\"https://bcdanl.github.io/data/browser-online-shopping.zip\")\ndf\n\nWarning: Total number of columns (1001) exceeds max_columns (20). Falling back to pandas display.\n\n\n\n    \n\n\n\n\n\n\nspend\natdmt.com\nyahoo.com\nwhenu.com\nweatherbug.com\nmsn.com\ngoogle.com\naol.com\nquestionmarket.com\ngooglesyndication.com-o02\n...\nugo.com\ncox.com\nspicymint.com\nreal.com-o01\ntargetnet.com\neffectivebrand.com\ndallascowboys.com\nleadgenetwork.com\nin.us\nvistaprint.com\n\n\n\n\n0\n424\n4.052026\n11.855928\n0.000000\n0.000000\n0.250125\n6.528264\n0.150075\n1.350675\n3.401701\n...\n0.0\n0.025013\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.000000\n\n\n1\n2335\n4.448743\n8.446164\n0.000000\n0.000000\n0.644745\n0.451322\n0.128949\n0.967118\n1.225016\n...\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.000000\n\n\n2\n279\n7.678815\n0.487234\n0.000000\n31.904112\n13.213798\n0.954980\n0.000000\n2.124342\n2.514130\n...\n0.0\n0.000000\n0.000000\n0.019489\n0.019489\n0.0\n0.0\n0.0\n0.0\n0.000000\n\n\n3\n829\n13.547802\n18.509289\n0.045310\n0.045310\n0.294517\n1.110104\n0.067966\n3.194382\n3.149071\n...\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.067966\n\n\n4\n221\n2.879581\n10.558464\n0.000000\n0.000000\n3.606748\n1.396161\n0.000000\n0.727167\n0.988947\n...\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9995\n102\n6.040454\n26.350790\n0.000000\n0.000000\n0.000000\n0.055417\n4.710446\n2.909393\n0.554170\n...\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.000000\n\n\n9996\n5096\n8.044292\n3.012539\n9.102752\n0.032568\n1.612115\n1.840091\n9.037616\n3.289367\n1.009608\n...\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.000000\n\n\n9997\n883\n7.053942\n1.659751\n0.000000\n0.000000\n9.543568\n0.414938\n13.692946\n2.074689\n2.074689\n...\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.000000\n\n\n9998\n256\n10.408043\n0.473093\n0.000000\n0.000000\n18.568894\n0.887049\n3.370787\n3.548196\n0.236546\n...\n0.0\n0.532229\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.000000\n\n\n9999\n6\n2.937832\n11.789234\n0.018954\n7.827900\n0.113723\n2.710387\n0.151630\n0.777104\n0.398029\n...\n0.0\n0.000000\n0.037908\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.000000\n\n\n\n\n10000 rows × 1001 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nX = df.drop('spend', axis = 1)\ny = df['spend']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nX_train_np = X_train.values\nX_test_np = X_test.values\ny_train_np = y_train.values\ny_test_np = y_test.values\n\n\n# LassoCV with a range of alpha values\nlasso_cv = LassoCV(n_alphas = 100, # default is 100\n                   alphas = None, # alphas=None automatically generate 100 candidate alpha values\n                   cv = 5,\n                   random_state=42,\n                   max_iter=100000)\nlasso_cv.fit(X_train.values, np.log(y_train.values))\n\nprint(\"LassoCV - Best alpha:\", lasso_cv.alpha_)\n\n\n# Create a DataFrame including the intercept and the coefficients:\ncoef_lasso = pd.DataFrame({\n    'predictor': list(X_train.columns),\n    'coefficient':  list(lasso_cv.coef_),\n    'exp_coefficient': np.exp(  list(lasso_cv.coef_) )\n})\n\n\n# Evaluate\ny_pred_lasso = lasso_cv.predict(X_test.values)\nmse_lasso = mean_squared_error(y_test, y_pred_lasso)\nprint(\"LassoCV - MSE:\", mse_lasso)\n\nLassoCV - Best alpha: 0.004098154620620373\nLassoCV - MSE: 88099012.81854005\n\n\n\n# LassoCV with a range of alpha values\nlasso_cv = LassoCV(n_alphas = 100, # default is 100\n                   alphas = None, # alphas=None automatically generate 100 candidate alpha values\n                   cv = 5,\n                   random_state=42,\n                   max_iter=100000)\nlasso_cv.fit(X_train_np, np.log(y_train_np))\n\nprint(\"LassoCV - Best alpha:\", lasso_cv.alpha_)\n\n\n# Create a DataFrame including the intercept and the coefficients:\ncoef_lasso = pd.DataFrame({\n    'predictor': list(X_train.columns),\n    'coefficient':  list(lasso_cv.coef_),\n    'exp_coefficient': np.exp(  list(lasso_cv.coef_) )\n})\n\n\n# Evaluate\ny_pred_lasso = lasso_cv.predict(X_test_np)\nmse_lasso = mean_squared_error(y_test_np, y_pred_lasso)\nprint(\"LassoCV - MSE:\", mse_lasso)\n\nLassoCV - Best alpha: 0.004098154620620373\nLassoCV - MSE: 88099012.81854005\n\n\n\ncoef_lasso = coef_lasso.query('coefficient != 0')\ncoef_lasso.shape[0]\n\n307\n\n\n\ncoef_lasso.sort_values('coefficient', ascending = False)\n\n\n    \n\n\n\n\n\n\npredictor\ncoefficient\nexp_coefficient\n\n\n\n\n895\nbizrate.com-o01\n1.388699\n4.009628\n\n\n770\nstaples.com\n0.757120\n2.132127\n\n\n690\ntravelhook.net\n0.688146\n1.990022\n\n\n843\nunited.com\n0.610771\n1.841850\n\n\n506\nvictoriassecret.com\n0.584030\n1.793251\n\n\n...\n...\n...\n...\n\n\n279\nnew.net\n-0.181475\n0.834039\n\n\n374\ncoolsavings.com\n-0.187875\n0.828718\n\n\n851\nrsc03.net\n-0.231599\n0.793264\n\n\n443\ncheckm8.com\n-0.240327\n0.786371\n\n\n481\nmacromedia.com\n-0.243739\n0.783693\n\n\n\n\n307 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# Compute the mean and standard deviation of the CV errors for each alpha.\nmean_cv_errors = np.mean(lasso_cv.mse_path_, axis=1)\nstd_cv_errors = np.std(lasso_cv.mse_path_, axis=1)\n\nplt.figure(figsize=(8, 6))\nplt.errorbar(lasso_cv.alphas_, mean_cv_errors, yerr=std_cv_errors, marker='o', linestyle='-', capsize=5)\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Mean CV Error (MSE)')\nplt.title('Cross-Validation Error vs. Alpha')\n#plt.gca().invert_xaxis()  # Optionally invert the x-axis so lower alphas (less regularization) appear to the right.\nplt.axvline(x=lasso_cv.alpha_, color='red', linestyle='--', label='Best alpha')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Compute the coefficient path over the alpha grid that LassoCV used\nalphas, coefs, _ = lasso_path(X_train, np.log(y_train),\n                              alphas=lasso_cv.alphas_,\n                              max_iter=100000)\n\n# Count nonzero coefficients for each alpha (coefs shape: (n_features, n_alphas))\nnonzero_counts = np.sum(coefs != 0, axis=0)\n\n# Plot the number of nonzero coefficients versus alpha\nplt.figure(figsize=(8,6))\nplt.plot(alphas, nonzero_counts, marker='o', linestyle='-')\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Number of nonzero coefficients')\nplt.title('Nonzero Coefficients vs. Alpha')\n#plt.gca().invert_xaxis()  # Lower alphas (less regularization) on the right\nplt.axvline(x=lasso_cv.alpha_, color='red', linestyle='--', label='Best alpha')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Compute the lasso path. Note: we use np.log(y_train) because that's what you used in LassoCV.\nalphas, coefs, _ = lasso_path(X_train.values, np.log(y_train.values), alphas=lasso_cv.alphas_, max_iter=100000)\n\nplt.figure(figsize=(8, 6))\n# Iterate over each predictor and plot its coefficient path.\nfor i, col in enumerate(X_train.columns):\n    plt.plot(alphas, coefs[i, :], label=col)\n\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Coefficient value')\nplt.title('Lasso Coefficient Paths')\n#plt.gca().invert_xaxis()  # Lower alphas (weaker regularization) to the right.\n#plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.axvline(x=lasso_cv.alpha_, color='red', linestyle='--', label='Best alpha')\n#plt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# RidgeCV with a list of alpha values and 3-fold cross-validation\nfrom sklearn.linear_model import RidgeCV, Ridge\n\nalpha_max = 10\nalpha_min_ratio = 1e-4\nalpha_min = alpha_max * alpha_min_ratio\n\n# Define candidate alpha values\nalphas = np.logspace(np.log(alpha_max), np.log(alpha_min), num=5)\nalphas\n\narray([2.00717432e+02, 1.00000000e+00, 4.98212830e-03, 2.48216024e-05,\n       1.23664407e-07])\n\n\n\n\nridge_cv = RidgeCV(alphas=alphas, cv=3, scoring='neg_mean_squared_error')\nridge_cv.fit(X_train_np, np.log(y_train_np))\n\nprint(\"RidgeCV - Best alpha:\", ridge_cv.alpha_)\n\n# Create a DataFrame including the intercept and the coefficients:\ncoef_ridge = pd.DataFrame({\n    'predictor': list(X_train.columns),\n    'coefficient': list(ridge_cv.coef_),\n    'exp_coefficient': np.exp( list(ridge_cv.coef_) )\n})\n\n# Evaluate\ny_pred_ridge = ridge_cv.predict(X_test_np)\nmse_ridge = mean_squared_error(y_test_np, y_pred_ridge)\nprint(\"RidgeCV - MSE:\", mse_ridge)\n\n\nRidgeCV - Best alpha: 200.71743249053017\nRidgeCV - MSE: 88099136.93994579\n\n\n\ncoef_ridge\n\n\n    \n\n\n\n\n\n\npredictor\ncoefficient\nexp_coefficient\n\n\n\n\n0\natdmt.com\n-0.004977\n0.995035\n\n\n1\nyahoo.com\n-0.017194\n0.982953\n\n\n2\nwhenu.com\n-0.008336\n0.991699\n\n\n3\nweatherbug.com\n-0.005399\n0.994616\n\n\n4\nmsn.com\n-0.004900\n0.995112\n\n\n...\n...\n...\n...\n\n\n995\neffectivebrand.com\n-0.098469\n0.906223\n\n\n996\ndallascowboys.com\n-0.039989\n0.960800\n\n\n997\nleadgenetwork.com\n0.009274\n1.009317\n\n\n998\nin.us\n-0.033527\n0.967029\n\n\n999\nvistaprint.com\n0.066156\n1.068394\n\n\n\n\n1000 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\ncoef_ridge.query('coefficient != 0').shape[0]\n\n1000\n\n\n\nridge = Ridge()\ncoefs = []\n\nfor a in alphas:\n    ridge.set_params(alpha=a)\n    ridge.fit(X_train_np, y_train_np)\n    coefs.append(ridge.coef_)\n\nax = plt.gca()\nax.plot(alphas, coefs)\nax.set_xscale('log')\n#ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\nplt.axis('tight')\nplt.xlabel('alpha')\nplt.ylabel('weights')\nplt.title('Ridge coefficients as a function of the regularization');\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "danl-ml/danl_320_hw4_part_1_lasso_beer_model_1.html",
    "href": "danl-ml/danl_320_hw4_part_1_lasso_beer_model_1.html",
    "title": "Homework 4 - Part 1: Lasso Linear Regerssion - Model 1",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import scale # zero mean & one s.d.\nfrom sklearn.linear_model import LassoCV, lasso_path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n\nbeer = pd.read_csv(\"https://bcdanl.github.io/data/beer_markets_xbeer_xdemog.zip\")\n#beer = pd.read_csv(\"https://bcdanl.github.io/data/beer_markets_xbeer_brand_xdemog.zip\")\n#beer = pd.read_csv(\"https://bcdanl.github.io/data/beer_markets_xbeer_brand_promo_xdemog.zip\")\n\n\nX = beer.drop('ylogprice', axis = 1)\ny = beer['ylogprice']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ny_train = y_train.values\ny_test = y_test.values\n\n\n# LassoCV with a range of alpha values\nlasso_cv = LassoCV(n_alphas = 100,\n                   alphas = None, # alphas=None automatically generate 100 candidate alpha values\n                   cv = 5,\n                   random_state=42,\n                   max_iter=100000)\n\n\nlasso_cv.fit(X_train.values, y_train)\n#lasso_cv.fit(X_train.values, y_train.ravel())\nprint(\"LassoCV - Best alpha:\", lasso_cv.alpha_)\n\nLassoCV - Best alpha: 8.170612772733373e-05\n\n\n\n# Create a DataFrame including the intercept and the coefficients:\ncoef_lasso_beer = pd.DataFrame({\n    'predictor': list(X_train.columns),\n    'coefficient':  list(lasso_cv.coef_),\n    'exp_coefficient': np.exp(  list(lasso_cv.coef_) )\n})\n\n\n# Evaluate\ny_pred_lasso = lasso_cv.predict(X_test)\nmse_lasso = mean_squared_error(y_test, y_pred_lasso)\nprint(\"LassoCV - MSE:\", mse_lasso)\n\nLassoCV - MSE: 0.027828386344520374\n\n\n\ncoef_lasso_beer_n0 = coef_lasso_beer[coef_lasso_beer['coefficient'] != 0]\n\n\nX_train.shape[1]\n\n2641\n\n\n\ncoef_lasso_beer_n0.shape[0]\n\n394\n\n\n\ncoef_lasso_beer_n0\n\n\n    \n\n\n\n\n\n\npredictor\ncoefficient\nexp_coefficient\n\n\n\n\n0\nlogquantity\n-0.142898\n0.866842\n\n\n1\ncontainer_CAN\n-0.054903\n0.946577\n\n\n2\nbrandBUSCH_LIGHT\n-0.258170\n0.772464\n\n\n4\nbrandMILLER_LITE\n-0.015062\n0.985051\n\n\n5\nbrandNATURAL_LIGHT\n-0.315142\n0.729685\n\n\n...\n...\n...\n...\n\n\n2557\nmarketBOSTON:npeople5plus\n-0.006056\n0.993963\n\n\n2563\nmarketCOLUMBUS:npeople5plus\n-0.032305\n0.968211\n\n\n2564\nmarketDALLAS:npeople5plus\n0.045550\n1.046603\n\n\n2568\nmarketEXURBAN_NJ:npeople5plus\n0.127266\n1.135720\n\n\n2628\nmarketSACRAMENTO:npeople5plus\n0.016272\n1.016405\n\n\n\n\n394 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# Compute the mean and standard deviation of the CV errors for each alpha.\nmean_cv_errors = np.mean(lasso_cv.mse_path_, axis=1)\nstd_cv_errors = np.std(lasso_cv.mse_path_, axis=1)\n\nplt.figure(figsize=(8, 6))\nplt.errorbar(lasso_cv.alphas_, mean_cv_errors, yerr=std_cv_errors, marker='o', linestyle='-', capsize=5)\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Mean CV Error (MSE)')\nplt.title('Cross-Validation Error vs. Alpha')\n#plt.gca().invert_xaxis()  # Optionally invert the x-axis so lower alphas (less regularization) appear to the right.\nplt.axvline(x=lasso_cv.alpha_, color='red', linestyle='--', label='Best alpha')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Compute the coefficient path over the alpha grid that LassoCV used\nalphas, coefs, _ = lasso_path(X_train, y_train,\n                              alphas=lasso_cv.alphas_,\n                              max_iter=100000)\n\n# Count nonzero coefficients for each alpha (coefs shape: (n_features, n_alphas))\nnonzero_counts = np.sum(coefs != 0, axis=0)\n\n# Plot the number of nonzero coefficients versus alpha\nplt.figure(figsize=(8,6))\nplt.plot(alphas, nonzero_counts, marker='o', linestyle='-')\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Number of nonzero coefficients')\nplt.title('Nonzero Coefficients vs. Alpha')\n#plt.gca().invert_xaxis()  # Lower alphas (less regularization) on the right\nplt.axvline(x=lasso_cv.alpha_, color='red', linestyle='--', label='Best alpha')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Compute the lasso path. Note: we use np.log(y_train) because that's what you used in LassoCV.\nalphas, coefs, _ = lasso_path(X_train, y_train,\n                              alphas=lasso_cv.alphas_,\n                              max_iter=100000)\nplt.figure(figsize=(8, 6))\n# Iterate over each predictor and plot its coefficient path.\nfor i, col in enumerate(X_train.columns):\n    plt.plot(alphas, coefs[i, :], label=col)\n\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Coefficient value')\nplt.title('Lasso Coefficient Paths')\n#plt.gca().invert_xaxis()  # Lower alphas (weaker regularization) to the right.\n#plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.axvline(x=lasso_cv.alpha_, color='red', linestyle='--', label='Best alpha')\n#plt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_bikeshare.html",
    "href": "danl-ml/danl_320_linear_regression_bikeshare.html",
    "title": "Linear Regression",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n\n\n\n\n\n\n\nCode\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                ci_upper,\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: Exp(95% CI Lower), 8: 95% CI Upper, 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"Exp(95% CI Lower)\", \"95% CI Upper\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))\n\n\n\n\n\n\n\nCode\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\n\n\n\n\n\nCode\ndef residual_plot(df, label_col, model_name):\n    \"\"\"\n    Generates a residual plot for a given test dataframe.\n\n    Parameters:\n        df (DataFrame): Spark DataFrame containing the test set with predictions.\n        label_col (str): The column name of the actual outcome variable.\n        title (str): The title for the residual plot.\n\n    Returns:\n        None (displays the plot)\n    \"\"\"\n    # Convert to Pandas DataFrame\n    df_pd = df.select([\"prediction\", label_col]).toPandas()\n    df_pd[\"residual\"] = df_pd[label_col] - df_pd[\"prediction\"]\n\n    # Scatter plot of residuals vs. predicted values\n    plt.scatter(df_pd[\"prediction\"], df_pd[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n    # Use LOWESS smoothing for trend line\n    smoothed = sm.nonparametric.lowess(df_pd[\"residual\"], df_pd[\"prediction\"])\n    plt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\n\n    # Add reference line at y=0\n    plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n\n    # Labels and title (model_name)\n    plt.xlabel(\"Predicted Values\")\n    plt.ylabel(\"Residuals\")\n    model_name = \"Residual Plot for \" + model_name\n    plt.title(model_name)\n\n    # Show plot\n    plt.show()\n\n# Example usage:\n# residual_plot(dtest_1, \"log_sales\", \"Model 1\")"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_bikeshare.html#required-libraries-and-spark-session",
    "href": "danl-ml/danl_320_linear_regression_bikeshare.html#required-libraries-and-spark-session",
    "title": "Linear Regression",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_bikeshare.html#udfs",
    "href": "danl-ml/danl_320_linear_regression_bikeshare.html#udfs",
    "title": "Linear Regression",
    "section": "",
    "text": "Code\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                ci_upper,\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: Exp(95% CI Lower), 8: 95% CI Upper, 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"Exp(95% CI Lower)\", \"95% CI Upper\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))\n\n\n\n\n\n\n\nCode\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\n\n\n\n\n\nCode\ndef residual_plot(df, label_col, model_name):\n    \"\"\"\n    Generates a residual plot for a given test dataframe.\n\n    Parameters:\n        df (DataFrame): Spark DataFrame containing the test set with predictions.\n        label_col (str): The column name of the actual outcome variable.\n        title (str): The title for the residual plot.\n\n    Returns:\n        None (displays the plot)\n    \"\"\"\n    # Convert to Pandas DataFrame\n    df_pd = df.select([\"prediction\", label_col]).toPandas()\n    df_pd[\"residual\"] = df_pd[label_col] - df_pd[\"prediction\"]\n\n    # Scatter plot of residuals vs. predicted values\n    plt.scatter(df_pd[\"prediction\"], df_pd[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n    # Use LOWESS smoothing for trend line\n    smoothed = sm.nonparametric.lowess(df_pd[\"residual\"], df_pd[\"prediction\"])\n    plt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\n\n    # Add reference line at y=0\n    plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n\n    # Labels and title (model_name)\n    plt.xlabel(\"Predicted Values\")\n    plt.ylabel(\"Residuals\")\n    model_name = \"Residual Plot for \" + model_name\n    plt.title(model_name)\n\n    # Show plot\n    plt.show()\n\n# Example usage:\n# residual_plot(dtest_1, \"log_sales\", \"Model 1\")"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_bikeshare.html#spark-dataframe",
    "href": "danl-ml/danl_320_linear_regression_bikeshare.html#spark-dataframe",
    "title": "Linear Regression",
    "section": "Spark DataFrame",
    "text": "Spark DataFrame\n\n# 1. Read CSV data from URL\ndf_pd = pd.read_csv('https://bcdanl.github.io/data/bikeshare_cleaned.csv')\ndf = spark.createDataFrame(df_pd)\ndf.show()\n\n+---+----+-----+----+---+--------+-------+-------+--------------------+------------------+-----------------+------------------+\n|cnt|year|month|date| hr|   wkday|holiday|seasons|        weather_cond|              temp|              hum|         windspeed|\n+---+----+-----+----+---+--------+-------+-------+--------------------+------------------+-----------------+------------------+\n| 16|2011|    1|   1|  0|saturday|      0| spring| Clear or Few Cloudy| -1.33460918694128|0.947345243330896|  -1.5538438052971|\n| 40|2011|    1|   1|  1|saturday|      0| spring| Clear or Few Cloudy| -1.43847500990342|0.895512927978679|  -1.5538438052971|\n| 32|2011|    1|   1|  2|saturday|      0| spring| Clear or Few Cloudy| -1.43847500990342|0.895512927978679|  -1.5538438052971|\n| 13|2011|    1|   1|  3|saturday|      0| spring| Clear or Few Cloudy| -1.33460918694128|0.636351351217591|  -1.5538438052971|\n|  1|2011|    1|   1|  4|saturday|      0| spring| Clear or Few Cloudy| -1.33460918694128|0.636351351217591|  -1.5538438052971|\n|  1|2011|    1|   1|  5|saturday|      0| spring|      Mist or Cloudy| -1.33460918694128|0.636351351217591|-0.821460017517193|\n|  2|2011|    1|   1|  6|saturday|      0| spring| Clear or Few Cloudy| -1.43847500990342|0.895512927978679|  -1.5538438052971|\n|  3|2011|    1|   1|  7|saturday|      0| spring| Clear or Few Cloudy| -1.54234083286556| 1.20650682009198|  -1.5538438052971|\n|  8|2011|    1|   1|  8|saturday|      0| spring| Clear or Few Cloudy| -1.33460918694128|0.636351351217591|  -1.5538438052971|\n| 14|2011|    1|   1|  9|saturday|      0| spring| Clear or Few Cloudy|-0.919145895092722|0.688183666569809|  -1.5538438052971|\n| 36|2011|    1|   1| 10|saturday|      0| spring| Clear or Few Cloudy|-0.607548426206302|0.688183666569809| 0.519881272378821|\n| 56|2011|    1|   1| 11|saturday|      0| spring| Clear or Few Cloudy|-0.711414249168442|0.947345243330896| 0.764281665845554|\n| 84|2011|    1|   1| 12|saturday|      0| spring| Clear or Few Cloudy|-0.399816780282022|0.740015981922026| 0.764281665845554|\n| 94|2011|    1|   1| 13|saturday|      0| spring|      Mist or Cloudy|-0.192085134357741|0.480854405160939| 0.886073166268775|\n|106|2011|    1|   1| 14|saturday|      0| spring|      Mist or Cloudy|-0.192085134357741|0.480854405160939| 0.764281665845554|\n|110|2011|    1|   1| 15|saturday|      0| spring|      Mist or Cloudy|-0.295950957319881|0.740015981922026| 0.886073166268775|\n| 93|2011|    1|   1| 16|saturday|      0| spring|      Mist or Cloudy|-0.399816780282022|0.999177558683113| 0.886073166268775|\n| 67|2011|    1|   1| 17|saturday|      0| spring|      Mist or Cloudy|-0.295950957319881|0.999177558683113| 0.764281665845554|\n| 35|2011|    1|   1| 18|saturday|      0| spring|Light Snow or Lig...|-0.399816780282022| 1.31017145079642| 0.519881272378821|\n| 37|2011|    1|   1| 19|saturday|      0| spring|Light Snow or Lig...|-0.399816780282022| 1.31017145079642| 0.519881272378821|\n+---+----+-----+----+---+--------+-------+-------+--------------------+------------------+-----------------+------------------+\nonly showing top 20 rows"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_bikeshare.html#training-test-split",
    "href": "danl-ml/danl_320_linear_regression_bikeshare.html#training-test-split",
    "title": "Linear Regression",
    "section": "Training-Test Split",
    "text": "Training-Test Split\n\ndtrain, dtest = df.randomSplit([0.6, 0.4], seed = 1234)"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_bikeshare.html#adding-dummies",
    "href": "danl-ml/danl_320_linear_regression_bikeshare.html#adding-dummies",
    "title": "Linear Regression",
    "section": "Adding Dummies",
    "text": "Adding Dummies\n\ndummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\ndummy_cols_month, ref_category_month = add_dummy_variables('month', 0)\ndummy_cols_hr, ref_category_hr = add_dummy_variables('hr', 0)\n\ncustom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday'] # Custom category_order\ndummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\ndummy_cols_holiday, ref_category_holiday = add_dummy_variables('holiday', 0)\n\ncustom_order_seasons = ['spring', 'summer', 'fall', 'winter']  # Custom category_order\ndummy_cols_seasons, ref_category_seasons = add_dummy_variables('seasons', 0, custom_order_seasons)\n\ndummy_cols_weather_cond, ref_category_weather_cond = add_dummy_variables('weather_cond', 0)\n\nReference category (dummy omitted): 2011\nReference category (dummy omitted): 1\nReference category (dummy omitted): 0\nReference category (dummy omitted): sunday\nReference category (dummy omitted): 0\nReference category (dummy omitted): spring\nReference category (dummy omitted): Clear or Few Cloudy"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_bikeshare.html#assembling-predictors",
    "href": "danl-ml/danl_320_linear_regression_bikeshare.html#assembling-predictors",
    "title": "Linear Regression",
    "section": "Assembling Predictors",
    "text": "Assembling Predictors\n\nconti_cols = [\"temp\", \"hum\", \"windspeed\"]\nassembler_predictors = (\n    conti_cols +\n    dummy_cols_year + dummy_cols_month +\n    dummy_cols_hr + dummy_cols_wkday +\n    dummy_cols_holiday + dummy_cols_seasons + dummy_cols_weather_cond\n)\n\nassembler_dum = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\ndtrain_dum = assembler_dum.transform(dtrain)\ndtest_dum  = assembler_dum.transform(dtest)"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_bikeshare.html#fitting-regression",
    "href": "danl-ml/danl_320_linear_regression_bikeshare.html#fitting-regression",
    "title": "Linear Regression",
    "section": "Fitting Regression",
    "text": "Fitting Regression\n\nmodel_dum = (\n    LinearRegression(featuresCol=\"predictors\",\n                     labelCol=\"cnt\")\n    .fit(dtrain_dum)\n)"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_bikeshare.html#regression-table",
    "href": "danl-ml/danl_320_linear_regression_bikeshare.html#regression-table",
    "title": "Linear Regression",
    "section": "Regression Table",
    "text": "Regression Table\n\nprint( regression_table(model_dum, assembler_dum) )\n\n+---------------------------------------+---------+------+------------+---------+--------------+--------------+\n| y: cnt                                |    Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper |\n+---------------------------------------+---------+------+------------+---------+--------------+--------------+\n| temp                                  |  45.307 | ***  |      1.395 |   0.000 |       42.573 |       48.042 |\n| hum                                   | -17.193 | ***  |      1.095 |   0.000 |      -19.339 |      -15.046 |\n| windspeed                             |  -4.617 | ***  |      2.029 |   0.000 |       -8.595 |       -0.639 |\n| year_2012                             |  85.361 | ***  |      5.129 |   0.000 |       75.308 |       95.414 |\n| month_2                               |   8.508 |  *   |      5.765 |   0.097 |       -2.792 |       19.809 |\n| month_3                               |  18.322 | ***  |      8.557 |   0.001 |        1.550 |       35.095 |\n| month_4                               |  10.784 |      |      9.177 |   0.208 |       -7.205 |       28.772 |\n| month_5                               |  25.990 | ***  |      9.402 |   0.005 |        7.561 |       44.420 |\n| month_6                               |   4.947 |      |     10.543 |   0.599 |      -15.720 |       25.613 |\n| month_7                               | -10.677 |      |     10.250 |   0.311 |      -30.768 |        9.415 |\n| month_8                               |  10.144 |      |      9.119 |   0.322 |       -7.732 |       28.020 |\n| month_9                               |  34.887 | ***  |      8.440 |   0.000 |       18.343 |       51.432 |\n| month_10                              |  18.646 |  **  |      8.105 |   0.027 |        2.758 |       34.534 |\n| month_11                              |  -6.450 |      |      6.482 |   0.426 |      -19.155 |        6.256 |\n| month_12                              |  -5.341 |      |      6.938 |   0.410 |      -18.940 |        8.258 |\n| hr_1                                  | -19.935 | ***  |      6.964 |   0.004 |      -33.585 |       -6.285 |\n| hr_2                                  | -26.234 | ***  |      6.986 |   0.000 |      -39.929 |      -12.540 |\n| hr_3                                  | -40.899 | ***  |      6.919 |   0.000 |      -54.461 |      -27.336 |\n| hr_4                                  | -41.142 | ***  |      6.974 |   0.000 |      -54.813 |      -27.472 |\n| hr_5                                  | -24.888 | ***  |      6.922 |   0.000 |      -38.456 |      -11.321 |\n| hr_6                                  |  31.898 | ***  |      6.991 |   0.000 |       18.194 |       45.601 |\n| hr_7                                  | 166.868 | ***  |      6.959 |   0.000 |      153.228 |      180.508 |\n| hr_8                                  | 294.758 | ***  |      6.915 |   0.000 |      281.203 |      308.312 |\n| hr_9                                  | 163.694 | ***  |      6.967 |   0.000 |      150.037 |      177.351 |\n| hr_10                                 | 109.802 | ***  |      7.056 |   0.000 |       95.971 |      123.634 |\n| hr_11                                 | 134.459 | ***  |      7.085 |   0.000 |      120.571 |      148.347 |\n| hr_12                                 | 176.590 | ***  |      7.132 |   0.000 |      162.609 |      190.570 |\n| hr_13                                 | 166.123 | ***  |      7.152 |   0.000 |      152.105 |      180.142 |\n| hr_14                                 | 151.832 | ***  |      7.053 |   0.000 |      138.006 |      165.657 |\n| hr_15                                 | 160.555 | ***  |      7.208 |   0.000 |      146.427 |      174.683 |\n| hr_16                                 | 224.717 | ***  |      7.129 |   0.000 |      210.742 |      238.691 |\n| hr_17                                 | 378.531 | ***  |      7.007 |   0.000 |      364.796 |      392.265 |\n| hr_18                                 | 339.843 | ***  |      6.962 |   0.000 |      326.196 |      353.490 |\n| hr_19                                 | 233.479 | ***  |      6.963 |   0.000 |      219.830 |      247.127 |\n| hr_20                                 | 156.485 | ***  |      6.922 |   0.000 |      142.917 |      170.053 |\n| hr_21                                 | 109.363 | ***  |      6.866 |   0.000 |       95.904 |      122.821 |\n| hr_22                                 |  69.689 | ***  |      6.903 |   0.000 |       56.157 |       83.220 |\n| hr_23                                 |  29.770 | ***  |      3.861 |   0.000 |       22.201 |       37.339 |\n| wkday_monday                          |  11.570 | ***  |      3.760 |   0.003 |        4.200 |       18.940 |\n| wkday_tuesday                         |  11.127 | ***  |      3.798 |   0.003 |        3.682 |       18.571 |\n| wkday_wednesday                       |  11.043 | ***  |      3.770 |   0.004 |        3.653 |       18.433 |\n| wkday_thursday                        |  15.072 | ***  |      3.757 |   0.000 |        7.707 |       22.437 |\n| wkday_friday                          |  18.413 | ***  |      3.757 |   0.000 |       11.048 |       25.778 |\n| wkday_saturday                        |  22.575 | ***  |      6.324 |   0.000 |       10.179 |       34.971 |\n| holiday_1                             | -27.635 | ***  |      6.360 |   0.000 |      -40.102 |      -15.169 |\n| seasons_summer                        |  37.101 | ***  |      7.501 |   0.000 |       22.398 |       51.803 |\n| seasons_fall                          |  32.195 | ***  |      6.345 |   0.000 |       19.758 |       44.631 |\n| seasons_winter                        |  68.404 | ***  |      4.235 |   0.000 |       60.102 |       76.706 |\n| weather_cond_Light_Snow_or_Light_Rain | -57.274 | ***  |      2.495 |   0.000 |      -62.166 |      -52.383 |\n| weather_cond_Mist_or_Cloudy           | -10.256 | ***  |      7.526 |   0.000 |      -25.008 |        4.496 |\n| Intercept                             | -25.277 | ***  |      2.362 |   0.000 |      -29.906 |      -20.648 |\n---------------------------------------------------------------------------------------------------------------\n| Observations                          |  10,431 |      |            |         |              |              |\n| R²                                    |   0.685 |      |            |         |              |              |\n| RMSE                                  | 102.177 |      |            |         |              |              |\n+---------------------------------------+---------+------+------------+---------+--------------+--------------+"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_bikeshare.html#making-predictions",
    "href": "danl-ml/danl_320_linear_regression_bikeshare.html#making-predictions",
    "title": "Linear Regression",
    "section": "Making Predictions",
    "text": "Making Predictions\n\ndtest_dum = model_dum.transform(dtest_dum)"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_bikeshare.html#coefficient-plots",
    "href": "danl-ml/danl_320_linear_regression_bikeshare.html#coefficient-plots",
    "title": "Linear Regression",
    "section": "Coefficient Plots",
    "text": "Coefficient Plots\n\nPandas DataFrame for the Plots\n\nterms = assembler_dum.getInputCols()\ncoefs = model_dum.coefficients.toArray()[:len(terms)]\nstdErrs = model_dum.summary.coefficientStandardErrors[:len(terms)]\n\ndf_summary = pd.DataFrame({\n    \"term\": terms,\n    \"estimate\": coefs,\n    \"std_error\": stdErrs\n})\n\n\n\ntemp, hum and windspeed variables.\n\n# Filter df_summary if needed\ncond = df_summary['term'].isin(['temp', 'hum', 'windspeed'])\n\ndf_summary_1 = df_summary[cond]\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary_1[\"term\"], df_summary_1[\"estimate\"],\n             yerr = 1.96 * df_summary_1[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nmonth variables.\n\n# Filter df_summary if needed\nmonth_list = ['month_' + str(i) for i in range(2, 13)]\ncond = df_summary['term'].isin( month_list )\n\ndf_summary_2 = df_summary[cond]\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary_2[\"term\"], df_summary_2[\"estimate\"],\n             yerr = 1.96 * df_summary_2[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nhr variables\n\n# Filter df_summary if needed\nhr_list = ['hr_' + str(i) for i in range(1, 25)]\ncond = df_summary['term'].isin( hr_list )\n\ndf_summary_3 = df_summary[cond]\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary_3[\"term\"], df_summary_3[\"estimate\"],\n             yerr = 1.96 * df_summary_3[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nwkday variables\n\n# Filter df_summary if needed\nwkday_list = [ 'wkday_' + day for day in ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday'] ]\ncond = df_summary['term'].isin( wkday_list )\ndf_summary_4 = df_summary[cond]\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary_4[\"term\"], df_summary_4[\"estimate\"],\n             yerr = 1.96 * df_summary_4[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\nDraw a coefficient plot for seasons variables.\n\n\n# Filter df_summary if needed\nseasons_list = [ 'seasons_' + s for s in ['summer', 'fall', 'winter'] ]\ncond = df_summary['term'].isin( seasons_list )\ndf_summary_5 = df_summary[cond]\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary_5[\"term\"], df_summary_5[\"estimate\"],\n             yerr = 1.96 * df_summary_5[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\nDraw a coefficient plot for weather_cond variables.\n\n\n# Filter df_summary if needed\nweather_cond_list = [ 'weather_cond_' + s for s in ['Light_Snow_or_Light_Rain', 'Mist_or_Cloudy'] ]\ncond = df_summary['term'].isin( weather_cond_list )\ndf_summary_6 = df_summary[cond]\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary_6[\"term\"], df_summary_6[\"estimate\"],\n             yerr = 1.96 * df_summary_6[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_bikeshare.html#residual-plot",
    "href": "danl-ml/danl_320_linear_regression_bikeshare.html#residual-plot",
    "title": "Linear Regression",
    "section": "Residual Plot",
    "text": "Residual Plot\n\nresidual_plot(dtest_dum, \"cnt\", \"the Model\")"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_bikeshare.html#rmse",
    "href": "danl-ml/danl_320_linear_regression_bikeshare.html#rmse",
    "title": "Linear Regression",
    "section": "RMSE",
    "text": "RMSE\n\ndtest_dum = dtest_dum.withColumn(\"error_sq\", pow(col('cnt') - col('prediction'), 2))\nrmse_dum = dtest_dum.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\nrmse_dum\n\n101.18558392860682"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_bikeshare.html#cnt",
    "href": "danl-ml/danl_320_linear_regression_bikeshare.html#cnt",
    "title": "Linear Regression",
    "section": "cnt",
    "text": "cnt\n\n# Create a histogram\ndfpd = df.select([\"cnt\"]).toPandas()\nsns.histplot(dfpd[\"cnt\"], bins=10, kde=True)"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_bikeshare.html#log_cnt",
    "href": "danl-ml/danl_320_linear_regression_bikeshare.html#log_cnt",
    "title": "Linear Regression",
    "section": "log_cnt",
    "text": "log_cnt\n\n# Create a histogram\ndfpd = df.select([\"log_cnt\"]).toPandas()\nsns.histplot(dfpd[\"log_cnt\"], bins=10, kde=True)"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_bikeshare.html#data-preparation-1",
    "href": "danl-ml/danl_320_linear_regression_bikeshare.html#data-preparation-1",
    "title": "Linear Regression",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nTraining-Test Split\n\ndtrain, dtest = df.randomSplit([0.6, 0.4], seed = 1234)\n\n\n\nAdding Dummies\n\ndummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\ndummy_cols_month, ref_category_month = add_dummy_variables('month', 0)\ndummy_cols_hr, ref_category_hr = add_dummy_variables('hr', 0)\n\ncustom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday'] # Custom category_order\ndummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\ndummy_cols_holiday, ref_category_holiday = add_dummy_variables('holiday', 0)\n\ncustom_order_seasons = ['spring', 'summer', 'fall', 'winter'] # Custom category_order\ndummy_cols_seasons, ref_category_seasons = add_dummy_variables('seasons', 0, custom_order_seasons)\n\ndummy_cols_weather_cond, ref_category_weather_cond = add_dummy_variables('weather_cond', 0)\n\nReference category (dummy omitted): 2011\nReference category (dummy omitted): 1\nReference category (dummy omitted): 0\nReference category (dummy omitted): sunday\nReference category (dummy omitted): 0\nReference category (dummy omitted): spring\nReference category (dummy omitted): Clear or Few Cloudy"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_bikeshare.html#assembling-predictors-1",
    "href": "danl-ml/danl_320_linear_regression_bikeshare.html#assembling-predictors-1",
    "title": "Linear Regression",
    "section": "Assembling Predictors",
    "text": "Assembling Predictors\n\nconti_cols = [\"temp\", \"hum\", \"windspeed\"]\nassembler_predictors = (\n    conti_cols +\n    dummy_cols_year + dummy_cols_month +\n    dummy_cols_hr + dummy_cols_wkday +\n    dummy_cols_holiday + dummy_cols_seasons + dummy_cols_weather_cond\n)\n\nassembler_log = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\ndtrain_log = assembler_log.transform(dtrain)\ndtest_log  = assembler_log.transform(dtest)"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_bikeshare.html#fitting-regression-1",
    "href": "danl-ml/danl_320_linear_regression_bikeshare.html#fitting-regression-1",
    "title": "Linear Regression",
    "section": "Fitting Regression",
    "text": "Fitting Regression\n\nmodel_log = (\n    LinearRegression(featuresCol=\"predictors\",\n                     labelCol=\"log_cnt\")\n    .fit(dtrain_log)\n)"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_bikeshare.html#regression-table-1",
    "href": "danl-ml/danl_320_linear_regression_bikeshare.html#regression-table-1",
    "title": "Linear Regression",
    "section": "Regression Table",
    "text": "Regression Table\n\nprint( regression_table(model_log, assembler_log) )\n\n+---------------------------------------+--------+-----------+------+------------+---------+--------------+-------------------+--------------+-------------------+\n| y: log_cnt                            |   Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper) |\n+---------------------------------------+--------+-----------+------+------------+---------+--------------+-------------------+--------------+-------------------+\n| temp                                  |  0.271 |     1.311 | ***  |      0.008 |   0.000 |        0.254 |             1.290 |        0.288 |             1.333 |\n| hum                                   | -0.056 |     0.946 | ***  |      0.007 |   0.000 |       -0.069 |             0.933 |       -0.043 |             0.958 |\n| windspeed                             | -0.028 |     0.972 | ***  |      0.012 |   0.000 |       -0.053 |             0.949 |       -0.004 |             0.996 |\n| year_2012                             |  0.463 |     1.589 | ***  |      0.031 |   0.000 |        0.402 |             1.494 |        0.524 |             1.689 |\n| month_2                               |  0.146 |     1.157 | ***  |      0.035 |   0.000 |        0.077 |             1.080 |        0.214 |             1.239 |\n| month_3                               |  0.208 |     1.231 | ***  |      0.052 |   0.000 |        0.106 |             1.112 |        0.310 |             1.364 |\n| month_4                               |  0.177 |     1.194 | ***  |      0.056 |   0.001 |        0.068 |             1.070 |        0.287 |             1.332 |\n| month_5                               |  0.314 |     1.369 | ***  |      0.057 |   0.000 |        0.202 |             1.224 |        0.426 |             1.532 |\n| month_6                               |  0.188 |     1.207 | ***  |      0.064 |   0.001 |        0.063 |             1.065 |        0.314 |             1.369 |\n| month_7                               |  0.046 |     1.047 |      |      0.062 |   0.478 |       -0.077 |             0.926 |        0.168 |             1.183 |\n| month_8                               |  0.123 |     1.131 |  **  |      0.056 |   0.049 |        0.014 |             1.014 |        0.232 |             1.261 |\n| month_9                               |  0.204 |     1.227 | ***  |      0.051 |   0.000 |        0.104 |             1.109 |        0.305 |             1.357 |\n| month_10                              |  0.085 |     1.088 |  *   |      0.049 |   0.100 |       -0.012 |             0.988 |        0.181 |             1.199 |\n| month_11                              | -0.015 |     0.985 |      |      0.039 |   0.765 |       -0.092 |             0.912 |        0.063 |             1.065 |\n| month_12                              | -0.022 |     0.978 |      |      0.042 |   0.574 |       -0.105 |             0.900 |        0.061 |             1.062 |\n| hr_1                                  | -0.642 |     0.526 | ***  |      0.042 |   0.000 |       -0.725 |             0.484 |       -0.559 |             0.572 |\n| hr_2                                  | -1.247 |     0.287 | ***  |      0.043 |   0.000 |       -1.331 |             0.264 |       -1.164 |             0.312 |\n| hr_3                                  | -1.776 |     0.169 | ***  |      0.042 |   0.000 |       -1.858 |             0.156 |       -1.693 |             0.184 |\n| hr_4                                  | -2.057 |     0.128 | ***  |      0.042 |   0.000 |       -2.140 |             0.118 |       -1.974 |             0.139 |\n| hr_5                                  | -0.934 |     0.393 | ***  |      0.042 |   0.000 |       -1.017 |             0.362 |       -0.852 |             0.427 |\n| hr_6                                  |  0.232 |     1.261 | ***  |      0.043 |   0.000 |        0.149 |             1.160 |        0.315 |             1.371 |\n| hr_7                                  |  1.216 |     3.375 | ***  |      0.042 |   0.000 |        1.133 |             3.106 |        1.299 |             3.667 |\n| hr_8                                  |  1.807 |     6.090 | ***  |      0.042 |   0.000 |        1.724 |             5.607 |        1.889 |             6.614 |\n| hr_9                                  |  1.560 |     4.760 | ***  |      0.042 |   0.000 |        1.477 |             4.380 |        1.643 |             5.173 |\n| hr_10                                 |  1.234 |     3.436 | ***  |      0.043 |   0.000 |        1.150 |             3.159 |        1.319 |             3.738 |\n| hr_11                                 |  1.330 |     3.780 | ***  |      0.043 |   0.000 |        1.245 |             3.473 |        1.414 |             4.113 |\n| hr_12                                 |  1.532 |     4.628 | ***  |      0.043 |   0.000 |        1.447 |             4.250 |        1.617 |             5.039 |\n| hr_13                                 |  1.494 |     4.454 | ***  |      0.044 |   0.000 |        1.408 |             4.090 |        1.579 |             4.851 |\n| hr_14                                 |  1.433 |     4.191 | ***  |      0.043 |   0.000 |        1.349 |             3.853 |        1.517 |             4.560 |\n| hr_15                                 |  1.462 |     4.315 | ***  |      0.044 |   0.000 |        1.376 |             3.959 |        1.548 |             4.702 |\n| hr_16                                 |  1.728 |     5.627 | ***  |      0.043 |   0.000 |        1.642 |             5.168 |        1.813 |             6.127 |\n| hr_17                                 |  2.116 |     8.302 | ***  |      0.043 |   0.000 |        2.033 |             7.636 |        2.200 |             9.026 |\n| hr_18                                 |  2.026 |     7.580 | ***  |      0.042 |   0.000 |        1.942 |             6.976 |        2.109 |             8.237 |\n| hr_19                                 |  1.763 |     5.827 | ***  |      0.042 |   0.000 |        1.679 |             5.363 |        1.846 |             6.332 |\n| hr_20                                 |  1.461 |     4.311 | ***  |      0.042 |   0.000 |        1.379 |             3.969 |        1.544 |             4.682 |\n| hr_21                                 |  1.222 |     3.395 | ***  |      0.042 |   0.000 |        1.140 |             3.128 |        1.304 |             3.685 |\n| hr_22                                 |  0.952 |     2.590 | ***  |      0.042 |   0.000 |        0.869 |             2.385 |        1.034 |             2.812 |\n| hr_23                                 |  0.536 |     1.709 | ***  |      0.024 |   0.000 |        0.490 |             1.632 |        0.582 |             1.790 |\n| wkday_monday                          | -0.024 |     0.977 |      |      0.023 |   0.317 |       -0.068 |             0.934 |        0.021 |             1.022 |\n| wkday_tuesday                         | -0.032 |     0.969 |      |      0.023 |   0.165 |       -0.077 |             0.926 |        0.014 |             1.014 |\n| wkday_wednesday                       | -0.028 |     0.973 |      |      0.023 |   0.234 |       -0.073 |             0.930 |        0.017 |             1.018 |\n| wkday_thursday                        |  0.026 |     1.026 |      |      0.023 |   0.262 |       -0.019 |             0.981 |        0.071 |             1.073 |\n| wkday_friday                          |  0.134 |     1.143 | ***  |      0.023 |   0.000 |        0.089 |             1.093 |        0.179 |             1.196 |\n| wkday_saturday                        |  0.145 |     1.156 | ***  |      0.039 |   0.000 |        0.069 |             1.072 |        0.220 |             1.247 |\n| holiday_1                             | -0.146 |     0.865 | ***  |      0.039 |   0.000 |       -0.221 |             0.801 |       -0.070 |             0.933 |\n| seasons_summer                        |  0.277 |     1.320 | ***  |      0.046 |   0.000 |        0.188 |             1.207 |        0.367 |             1.443 |\n| seasons_fall                          |  0.342 |     1.407 | ***  |      0.039 |   0.000 |        0.266 |             1.305 |        0.417 |             1.518 |\n| seasons_winter                        |  0.618 |     1.855 | ***  |      0.026 |   0.000 |        0.567 |             1.764 |        0.669 |             1.951 |\n| weather_cond_Light_Snow_or_Light_Rain | -0.548 |     0.578 | ***  |      0.015 |   0.000 |       -0.578 |             0.561 |       -0.518 |             0.596 |\n| weather_cond_Mist_or_Cloudy           | -0.042 |     0.959 | ***  |      0.046 |   0.005 |       -0.132 |             0.876 |        0.048 |             1.049 |\n| Intercept                             |  3.117 |           | ***  |      0.014 |         |        3.088 |                   |        3.145 |                   |\n------------------------------------------------------------------------------------------------------------------------------------------------------------------\n| Observations                          | 10,431 |           |      |            |         |              |                   |              |                   |\n| R²                                    |  0.826 |           |      |            |         |              |                   |              |                   |\n| RMSE                                  |  0.622 |           |      |            |         |              |                   |              |                   |\n+---------------------------------------+--------+-----------+------+------------+---------+--------------+-------------------+--------------+-------------------+"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_bikeshare.html#making-predictions-1",
    "href": "danl-ml/danl_320_linear_regression_bikeshare.html#making-predictions-1",
    "title": "Linear Regression",
    "section": "Making Predictions",
    "text": "Making Predictions\n\ndtest_log = model_log.transform(dtest_log)"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_bikeshare.html#coefficient-plots-1",
    "href": "danl-ml/danl_320_linear_regression_bikeshare.html#coefficient-plots-1",
    "title": "Linear Regression",
    "section": "Coefficient Plots",
    "text": "Coefficient Plots\n\nPandas DataFrame for the Plots\n\nterms = assembler_log.getInputCols()\ncoefs = model_log.coefficients.toArray()[:len(terms)]\nstdErrs = model_log.summary.coefficientStandardErrors[:len(terms)]\n\ndf_summary = pd.DataFrame({\n    \"term\": terms,\n    \"estimate\": coefs,\n    \"std_error\": stdErrs\n})\n\n\n\ntemp, hum and windspeed variables.\n\n# Filter df_summary if needed\ncond = df_summary['term'].isin(['temp', 'hum', 'windspeed'])\n\ndf_summary_1 = df_summary[cond]\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary_1[\"term\"],\n             df_summary_1[\"estimate\"],\n             yerr = 1.96 * df_summary_1[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nmonth variables.\n\n# Filter df_summary if needed\nmonth_list = ['month_' + str(i) for i in range(2, 13)]\ncond = df_summary['term'].isin( month_list )\n\ndf_summary_2 = df_summary[cond]\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary_2[\"term\"], df_summary_2[\"estimate\"],\n             yerr = 1.96 * df_summary_2[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nhr variables\n\n# Filter df_summary if needed\nhr_list = ['hr_' + str(i) for i in range(1, 25)]\ncond = df_summary['term'].isin( hr_list )\n\ndf_summary_3 = df_summary[cond]\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary_3[\"term\"], df_summary_3[\"estimate\"],\n             yerr = 1.96 * df_summary_3[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nwkday variables\n\n# Filter df_summary if needed\nwkday_list = [ 'wkday_' + day for day in ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday'] ]\ncond = df_summary['term'].isin( wkday_list )\ndf_summary_4 = df_summary[cond]\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary_4[\"term\"], df_summary_4[\"estimate\"],\n             yerr = 1.96 * df_summary_4[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_bikeshare.html#residual-plot-1",
    "href": "danl-ml/danl_320_linear_regression_bikeshare.html#residual-plot-1",
    "title": "Linear Regression",
    "section": "Residual Plot",
    "text": "Residual Plot\n\nresidual_plot(dtest_log, \"log_cnt\", \"the Model\")"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_bikeshare.html#rmse-1",
    "href": "danl-ml/danl_320_linear_regression_bikeshare.html#rmse-1",
    "title": "Linear Regression",
    "section": "RMSE",
    "text": "RMSE\n\ndtest_log = dtest_log.withColumn(\"error_sq\", pow(col('log_cnt') - col('prediction'), 2))\nrmse_log = dtest_log.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\nrmse_log\n\n0.6288655799916469\n\n\n\n# transform fitted log_cnt to fitted cnt\ndtest_log = dtest_log.withColumn(\"prediction_cnt\", exp(col('prediction')))\n\n\ndtest_log = dtest_log.withColumn(\"error_sq_transform\", pow(col('cnt') - col('prediction_cnt'), 2))\nrmse_log = dtest_log.agg(sqrt(avg(\"error_sq_transform\")).alias(\"rmse\")).collect()[0][\"rmse\"]\nrmse_log\n\n98.60168448259316"
  },
  {
    "objectID": "danl-ml/danl_320_tree_based_models.html",
    "href": "danl-ml/danl_320_tree_based_models.html",
    "title": "Tree-based Models",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor, plot_importance\nfrom sklearn.inspection import PartialDependenceDisplay, plot_partial_dependence"
  },
  {
    "objectID": "danl-ml/danl_320_tree_based_models.html#classification-tree",
    "href": "danl-ml/danl_320_tree_based_models.html#classification-tree",
    "title": "Tree-based Models",
    "section": "Classification Tree",
    "text": "Classification Tree\n\n# Use the demographic variables (excluding the first column) as features\nX = demog.iloc[:, 1:]\ny = nbc[\"Genre\"]\n\n# Build the classification tree.\n# The 'mincut = 1' in R is analogous to a very low min_samples_split in scikit-learn (default=2).\nclf = DecisionTreeClassifier(min_samples_split=2, random_state=42)\nclf.fit(X, y)\n\n# Generate predictions for the 'Genre' and store them in the nbc DataFrame\nnbc[\"genrepred\"] = clf.predict(X)\n\n# Plot the decision tree\nplt.figure(figsize=(12, 8))\nplot_tree(clf, feature_names=X.columns, class_names=clf.classes_, filled=True, rounded=True)\nplt.title(\"Classification Tree for Genre\")\nplt.show()"
  },
  {
    "objectID": "danl-ml/danl_320_tree_based_models.html#regression-tree",
    "href": "danl-ml/danl_320_tree_based_models.html#regression-tree",
    "title": "Tree-based Models",
    "section": "Regression Tree",
    "text": "Regression Tree\n\n# Prepare the predictor set and target variable.\n# We want to model: PE ~ Genre + GRP using all columns except the first.\n# Here, we select the 'Genre' and 'GRP' columns as predictors and 'PE' as the target.\nX = nbc[['Genre', 'GRP']]\ny = nbc['PE']\n\n# If 'Genre' is categorical, convert it to dummy variables.\n# This is necessary because scikit-learn models require numerical inputs.\nX = pd.get_dummies(X, columns=['Genre'], drop_first=True)\n\n\n# Build and fit the regression tree.\nreg_tree = DecisionTreeRegressor(max_depth=3, min_samples_split=2, random_state=42)\nreg_tree.fit(X, y)\n\n# Generate predictions for PE and store them in the DataFrame.\nnbc['PEpred'] = reg_tree.predict(X)\n\n# Plot the regression tree.\nplt.figure(figsize=(16, 10), dpi = 300)\nplot_tree(reg_tree, feature_names=X.columns, filled=True, rounded=True)\nplt.title(\"Regression Tree for PE\")\nplt.show()"
  },
  {
    "objectID": "danl-ml/danl_320_tree_based_models.html#splitting-data-into-training-and-testing",
    "href": "danl-ml/danl_320_tree_based_models.html#splitting-data-into-training-and-testing",
    "title": "Tree-based Models",
    "section": "Splitting Data into Training and Testing",
    "text": "Splitting Data into Training and Testing\n\n# Set a random seed for reproducibility (like R's set.seed)\nnp.random.seed(42120532)\ntrain, test = train_test_split(boston, test_size=0.20, random_state=42120532)\n\nX_train = train.drop(columns=[\"medv\"])\ny_train = train[\"medv\"]\n\nX_test = test.drop(columns=[\"medv\"])\ny_test = test[\"medv\"]"
  },
  {
    "objectID": "danl-ml/danl_320_tree_based_models.html#build-the-regression-tree-on-the-training-data",
    "href": "danl-ml/danl_320_tree_based_models.html#build-the-regression-tree-on-the-training-data",
    "title": "Tree-based Models",
    "section": "Build the Regression Tree on the Training Data",
    "text": "Build the Regression Tree on the Training Data\n\n# In scikit-learn, we can use min_impurity_decrease=0.005 for a similar effect.\ntree_model = DecisionTreeRegressor(min_impurity_decrease=0.005, random_state=42)\n# Fit the model using all predictors (all columns except 'medv')\ntree_model.fit(X_train, y_train)\n\n\n# Predict on training and test sets\ny_train_pred = tree_model.predict(X_train)\ny_test_pred = tree_model.predict(X_test)\n\n# Calculate MSE\nmse_train = mean_squared_error(y_train, y_train_pred)\nmse_test = mean_squared_error(y_test, y_test_pred)\n\n# Print the results\nprint(f\"Training MSE: {mse_train:.3f}\")\nprint(f\"Test MSE: {mse_test:.3f}\")\n\n# Plot the initial regression tree\nplt.figure(figsize=(12, 8))\nplot_tree(tree_model, feature_names=X_train.columns, filled=True, rounded=True)\nplt.title(\"Regression Tree for medv (Initial Fit)\")\nplt.show()\n\nTraining MSE: 0.250\nTest MSE: 13.859\n\n\n\n\n\n\n\n\n\n\n# In scikit-learn, we can use min_impurity_decrease=0.005 for a similar effect.\ntree_model = DecisionTreeRegressor(max_depth=3, min_impurity_decrease=0.005, random_state=42)\n# Fit the model using all predictors (all columns except 'medv')\ntree_model.fit(X_train, y_train)\n\n# Plot the initial regression tree\nplt.figure(figsize=(9, 8), dpi = 300)\nplot_tree(tree_model, feature_names=X_train.columns, filled=True, rounded=True)\nplt.title(\"Regression Tree for medv (Initial Fit)\")\nplt.show()"
  },
  {
    "objectID": "danl-ml/danl_320_tree_based_models.html#cross-validation-and-cost-complexity-pruning-cv-tree",
    "href": "danl-ml/danl_320_tree_based_models.html#cross-validation-and-cost-complexity-pruning-cv-tree",
    "title": "Tree-based Models",
    "section": "Cross-Validation and Cost Complexity Pruning (CV Tree)",
    "text": "Cross-Validation and Cost Complexity Pruning (CV Tree)\n\n# Obtain the cost-complexity pruning path from the initial tree\npath = tree_model.cost_complexity_pruning_path(X_train, y_train)  # Get candidate ccp_alpha values and corresponding impurities\nccp_alphas = path.ccp_alphas  # Candidate pruning parameters (alpha values)\nimpurities = path.impurities  # Impurity values at each candidate alpha\n\n# Exclude the maximum alpha value to avoid the trivial tree (a tree with only the root)\nccp_alphas = ccp_alphas[:-1]  # Remove the last alpha value which would prune the tree to a single node\n\n# Set up 10-fold cross-validation\nkf = KFold(n_splits=10, shuffle=True, random_state=42)  # Initialize 10-fold CV with shuffling and fixed random state\ncv_scores = []  # List to store mean cross-validated scores (negative MSE)\nleaf_nodes = []  # List to record the number of leaves for each pruned tree\nsse = []         # List to record the sum of squared errors (SSE) on the training set\n\n# Loop over each candidate alpha value to evaluate its performance\nfor ccp_alpha in ccp_alphas:\n    # Create a DecisionTreeRegressor with the current ccp_alpha and other specified parameters\n    clf = DecisionTreeRegressor(random_state=42,\n                                ccp_alpha=ccp_alpha,\n                                min_impurity_decrease=0.005)\n\n    # Perform 10-fold cross-validation and compute negative mean squared error (MSE)\n    scores = cross_val_score(clf, X_train, y_train,\n                             cv=kf, scoring=\"neg_mean_squared_error\")\n    cv_scores.append(np.mean(scores))  # Append the mean CV score for the current alpha\n\n    # Fit the tree on the training data to record additional metrics\n    clf.fit(X_train, y_train)\n    leaf_nodes.append(clf.get_n_leaves())  # Record the number of leaf nodes in the tree\n\n    # Compute SSE (sum of squared errors) on the training set\n    preds = clf.predict(X_train)  # Predict target values on training data\n    sse.append(np.sum((y_train - preds) ** 2))  # Calculate and record SSE for training set\n\n\n# Select the best alpha based on the highest (least negative) mean CV score\nbest_alpha = ccp_alphas[np.argmax(cv_scores)]  # Identify the alpha with the best CV performance\nprint(\"Best alpha:\", best_alpha)  # Print the best alpha value\n\n# Train the final pruned tree using the best alpha found\nfinal_tree = DecisionTreeRegressor(random_state=42,\n                                   ccp_alpha=best_alpha,\n                                   min_impurity_decrease=0.005)\nfinal_tree.fit(X_train, y_train)  # Fit the final model on the training data\n\nBest alpha: 0.2329619817674513\n\n\nDecisionTreeRegressor(ccp_alpha=np.float64(0.2329619817674513),\n                      min_impurity_decrease=0.005, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFittedDecisionTreeRegressor(ccp_alpha=np.float64(0.2329619817674513),\n                      min_impurity_decrease=0.005, random_state=42) \n\n\n\nlen(ccp_alphas)\n\n146"
  },
  {
    "objectID": "danl-ml/danl_320_tree_based_models.html#evaluate-the-final-tree-on-the-test-data",
    "href": "danl-ml/danl_320_tree_based_models.html#evaluate-the-final-tree-on-the-test-data",
    "title": "Tree-based Models",
    "section": "Evaluate the Final Tree on the Test Data",
    "text": "Evaluate the Final Tree on the Test Data\n\npreds = final_tree.predict(test.drop(columns=[\"medv\"]))\nmse = mean_squared_error(y_test, preds)\nprint(\"Test MSE:\", mse)\n\nTest MSE: 12.755442672992647"
  },
  {
    "objectID": "danl-ml/danl_320_tree_based_models.html#prunned-tree",
    "href": "danl-ml/danl_320_tree_based_models.html#prunned-tree",
    "title": "Tree-based Models",
    "section": "Prunned Tree",
    "text": "Prunned Tree\n\n# Plot the pruned tree.\nplt.figure(figsize=(16, 12), dpi=300)\nplot_tree(final_tree, feature_names=X_train.columns, filled=True, rounded=True)\nplt.title(\"Pruned Regression Tree for medv\")\nplt.show()\n\n# Summary of the final tree\nprint(\"Number of leaves in the pruned tree:\", final_tree.get_n_leaves())\nprint(\"Tree depth:\", final_tree.get_depth())\n\n\n\n\n\n\n\n\nNumber of leaves in the pruned tree: 21\nTree depth: 6\n\n\n\n# Plot the average cross-validated MSE against the number of leaf nodes\nnegative_cv_scores = -np.array(cv_scores)\n\nplt.figure(figsize=(8, 6), dpi=150)\nplt.plot(leaf_nodes, negative_cv_scores, marker='o', linestyle='-')\nplt.axvline(x=final_tree.get_n_leaves(), color='red', linestyle='--', label='Leaf Nodes = 21')  # Add vertical line at 21 leaf nodes\nplt.xlabel(\"Number of Leaf Nodes\")\nplt.ylabel(\"Mean CV MSE\")\nplt.title(\"CV Error vs. Number of Leaf Nodes\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plot the SSE on the training against the number of leaf nodes\nplt.figure(figsize=(8, 6), dpi=150)\nplt.plot(leaf_nodes, sse, marker='o', linestyle='-')\nplt.xlabel(\"Number of Leaf Nodes\")\nplt.ylabel(\"SSE\")\nplt.title(\"SSE vs. Number of Leaf Nodes\")\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "danl-ml/danl_320_tree_based_models.html#variable-importance",
    "href": "danl-ml/danl_320_tree_based_models.html#variable-importance",
    "title": "Tree-based Models",
    "section": "Variable Importance",
    "text": "Variable Importance\n\n# Get feature importances from the model (equivalent to importance(bag.boston) in R)\nimportances = rf.feature_importances_\nfeature_names = X_train.columns\n\nprint(\"Variable Importances:\")\nfor name, imp in zip(feature_names, importances):\n    print(f\"{name}: {imp:.4f}\")\n\n# Plot the feature importances, similar to varImpPlot(bag.boston) in R\n# Sort the features by importance for a nicer plot.\nindices = np.argsort(importances)[::-1]\n\nplt.figure(figsize=(10, 6), dpi=150)\nplt.title(\"Variable Importances\")\nplt.bar(range(len(feature_names)), importances[indices], align='center')\nplt.xticks(range(len(feature_names)), feature_names[indices], rotation=90)\nplt.xlabel(\"Variables\")\nplt.ylabel(\"Importance\")\nplt.tight_layout()\nplt.show()\n\nVariable Importances:\ncrim: 0.0609\nzn: 0.0039\nindus: 0.0468\nchas: 0.0063\nnox: 0.0549\nrm: 0.3493\nage: 0.0236\ndis: 0.0560\nrad: 0.0080\ntax: 0.0266\nptratio: 0.0569\nblack: 0.0177\nlstat: 0.2892\n\n\n\n\n\n\n\n\n\n\nPartialDependenceDisplay.from_estimator(rf, X_train, ['rm'], kind='both')\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-1-019c12d0cfbb&gt; in &lt;cell line: 0&gt;()\n----&gt; 1 PartialDependenceDisplay.from_estimator(rf, X_train, ['rm'], kind='both')\n\nNameError: name 'PartialDependenceDisplay' is not defined\n\n\n\n\nPartialDependenceDisplay.from_estimator(rf, X_train, ['lstat'], kind='individual')\n\n\n\n\n\n\n\n\n\ndisp = PartialDependenceDisplay.from_estimator(rf, X_train, ['lstat'], kind='both')\n\n# Access the line representing the average PDP (it's typically the last Line2D object)\n# and change its color manually\nfor ax in disp.axes_.ravel():\n    lines = ax.get_lines()\n    if lines:  # In case the axis has line objects\n        # The last line is usually the average PDP\n        pdp_line = lines[-1]\n        pdp_line.set_color(\"red\")  # Change to any color you like\n\nplt.show()\n\n\n\n\n\n\n\n\n\ndisp = PartialDependenceDisplay.from_estimator(rf, X_train, ['crim'], kind='both')\n\n# Access the line representing the average PDP (it's typically the last Line2D object)\n# and change its color manually\nfor ax in disp.axes_.ravel():\n    lines = ax.get_lines()\n    if lines:  # In case the axis has line objects\n        # The last line is usually the average PDP\n        pdp_line = lines[-1]\n        pdp_line.set_color(\"red\")  # Change to any color you like\n\nplt.show()"
  },
  {
    "objectID": "danl-ml/danl_320_hw2_linear_regression_beer.html",
    "href": "danl-ml/danl_320_hw2_linear_regression_beer.html",
    "title": "Homework 2",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n\n\n\n\n\n\n\nCode\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper | Exp(95% CI Lower) | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: 95% CI Upper, 8: Exp(95% CI Lower), 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"95% CI Upper\", \"Exp(95% CI Lower)\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))\n\n\n\n\n\n\n\nCode\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\n\n\n\n\n\nCode\ndef add_interaction_terms(var_list1, var_list2, var_list3=None):\n    \"\"\"\n    Creates interaction term columns in the global DataFrames dtrain and dtest.\n\n    For two sets of variable names (which may represent categorical (dummy) or continuous variables),\n    this function creates two-way interactions by multiplying each variable in var_list1 with each\n    variable in var_list2.\n\n    Optionally, if a third list of variable names (var_list3) is provided, the function also creates\n    three-way interactions among each variable in var_list1, each variable in var_list2, and each variable\n    in var_list3.\n\n    Parameters:\n        var_list1 (list): List of column names for the first set of variables.\n        var_list2 (list): List of column names for the second set of variables.\n        var_list3 (list, optional): List of column names for the third set of variables for three-way interactions.\n\n    Returns:\n        A flat list of new interaction column names.\n    \"\"\"\n    global dtrain, dtest\n\n    interaction_cols = []\n\n    # Create two-way interactions between var_list1 and var_list2.\n    for var1 in var_list1:\n        for var2 in var_list2:\n            col_name = f\"{var1}_*_{var2}\"\n            dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list1 and var_list3.\n    if var_list3 is not None:\n      for var1 in var_list1:\n          for var3 in var_list3:\n              col_name = f\"{var1}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list2 and var_list3.\n    if var_list3 is not None:\n      for var2 in var_list2:\n          for var3 in var_list3:\n              col_name = f\"{var2}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # If a third list is provided, create three-way interactions.\n    if var_list3 is not None:\n        for var1 in var_list1:\n            for var2 in var_list2:\n                for var3 in var_list3:\n                    col_name = f\"{var1}_*_{var2}_*_{var3}\"\n                    dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    interaction_cols.append(col_name)\n\n    return interaction_cols\n\n # Example\n # interaction_cols_brand_price = add_interaction_terms(dummy_cols_brand, ['log_price'])\n # interaction_cols_brand_ad_price = add_interaction_terms(dummy_cols_brand, dummy_cols_ad, ['log_price'])\n\n\n\n\n\n\n\nCode\ndef compare_reg_models(models, assemblers, names=None):\n    \"\"\"\n    Produces a single formatted table comparing multiple regression models.\n\n    For each predictor (the union across models, ordered by first appearance), the table shows\n    the beta estimate (with significance stars) from each model (blank if not used).\n    For a predictor, if a model's outcome (model.getLabelCol()) starts with \"log\", the cell displays\n    both the beta and its exponential (separated by \" / \"), except when the predictor's name includes \"log_\".\n    (The intercept row does not display exp(.))\n\n    Additional rows for Intercept, Observations, R², and RMSE are appended.\n\n    The header's first column is labeled \"Predictor\", and subsequent columns are\n    \"y: [outcome] ([name])\" for each model.\n\n    The table is produced in grid format (with vertical lines). A dashed line (using '-' characters)\n    is inserted at the top, immediately after the header, and at the bottom.\n    Additionally, immediately after the Intercept row, the border line is replaced with one using '='\n    (to appear as, for example, \"+==============================================+==========================+...\").\n\n    Parameters:\n        models (list): List of fitted LinearRegression models.\n        assemblers (list): List of corresponding VectorAssembler objects.\n        names (list, optional): List of model names; defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing the combined regression table.\n    \"\"\"\n    # Default model names.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(models))]\n\n    # For each model, get outcome and determine if that model is log-transformed.\n    outcomes = [m.getLabelCol() for m in models]\n    is_log_flags = [out.lower().startswith(\"log\") for out in outcomes]\n\n    # Build an ordered union of predictors based on first appearance.\n    ordered_predictors = []\n    for assembler in assemblers:\n        for feat in assembler.getInputCols():\n            if feat not in ordered_predictors:\n                ordered_predictors.append(feat)\n\n    # Helper for significance stars.\n    def significance_stars(p):\n        if p is None:\n            return \"\"\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build rows for each predictor.\n    rows = []\n    for feat in ordered_predictors:\n        row = [feat]\n        for m, a, is_log in zip(models, assemblers, is_log_flags):\n            feats_model = a.getInputCols()\n            if feat in feats_model:\n                idx = feats_model.index(feat)\n                beta = m.coefficients.toArray()[idx]\n                p_val = m.summary.pValues[idx] if m.summary.pValues is not None else None\n                stars = significance_stars(p_val)\n                cell = f\"{beta:.3f}{stars}\"\n                # Only add exp(beta) if model is log and predictor name does NOT include \"log_\"\n                if is_log and (\"log_\" not in feat.lower()):\n                    cell += f\" / {np.exp(beta):,.3f}\"\n                row.append(cell)\n            else:\n                row.append(\"\")\n        rows.append(row)\n\n    # Build intercept row (do NOT compute exp(intercept)).\n    intercept_row = [\"Intercept\"]\n    for m in models:\n        std_all = np.array(m.summary.coefficientStandardErrors)\n        coeffs = m.coefficients.toArray()\n        if len(std_all) == len(coeffs) + 1:\n            intercept_p = m.summary.pValues[0] if m.summary.pValues is not None else None\n        else:\n            intercept_p = None\n        sig = significance_stars(intercept_p)\n        cell = f\"{m.intercept:.3f}{sig}\"\n        intercept_row.append(cell)\n    rows.append(intercept_row)\n\n    # Add Observations row.\n    obs_row = [\"Observations\"]\n    for m in models:\n        obs = m.summary.numInstances\n        obs_row.append(f\"{int(obs):,}\")\n    rows.append(obs_row)\n\n    # Add R² row.\n    r2_row = [\"R²\"]\n    for m in models:\n        r2_row.append(f\"{m.summary.r2:.3f}\")\n    rows.append(r2_row)\n\n    # Add RMSE row.\n    rmse_row = [\"RMSE\"]\n    for m in models:\n        rmse_row.append(f\"{m.summary.rootMeanSquaredError:.3f}\")\n    rows.append(rmse_row)\n\n    # Build header: first column \"Predictor\", then for each model: \"y: [outcome] ([name])\"\n    header = [\"Predictor\"]\n    for out, name in zip(outcomes, names):\n        header.append(f\"y: {out} ({name})\")\n\n    # Create table string using grid format.\n    table_str = tabulate(rows, headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(models))\n\n    # Split into lines.\n    lines = table_str.split(\"\\n\")\n\n    # Create a dashed line spanning the full width.\n    full_width = len(lines[0])\n    dash_line = '-' * full_width\n    # Create an equals line by replacing '-' with '='.\n    eq_line = dash_line.replace('-', '=')\n\n    # Insert a dashed line after the header row.\n    lines = table_str.split(\"\\n\")\n    # In grid format, header and separator are usually the first two lines.\n    lines.insert(2, dash_line)\n\n    # Insert an equals line after the Intercept row.\n    for i, line in enumerate(lines):\n        if line.startswith(\"|\") and \"Intercept\" in line:\n            if i+1 &lt; len(lines):\n                lines[i+1] = eq_line\n            break\n\n    # Add dashed lines at the very top and bottom.\n    final_table = dash_line + \"\\n\" + \"\\n\".join(lines) + \"\\n\" + dash_line\n\n    return final_table\n\n# Example usage:\n# print(compare_reg_models([model_1, model_2, model_3],\n#                          [assembler_1, assembler_2, assembler_3],\n#                          [\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\n\n\n\n\n\nCode\ndef compare_rmse(test_dfs, label_col, pred_col=\"prediction\", names=None):\n    \"\"\"\n    Computes and compares RMSE values for a list of test DataFrames.\n\n    For each DataFrame in test_dfs, this function calculates the RMSE between the actual outcome\n    (given by label_col) and the predicted value (given by pred_col, default \"prediction\"). It then\n    produces a formatted table where the first column header is empty and the first row's first cell is\n    \"RMSE\", with each model's RMSE in its own column.\n\n    Parameters:\n        test_dfs (list): List of test DataFrames.\n        label_col (str): The name of the outcome column.\n        pred_col (str, optional): The name of the prediction column (default \"prediction\").\n        names (list, optional): List of model names corresponding to the test DataFrames.\n                                Defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing a table that compares RMSE values for each test DataFrame,\n        with one model per column.\n    \"\"\"\n    # Set default model names if none provided.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(test_dfs))]\n\n    rmse_values = []\n    for df in test_dfs:\n        # Create a column for squared error.\n        df = df.withColumn(\"error_sq\", pow(col(label_col) - col(pred_col), 2))\n        # Calculate RMSE: square root of the mean squared error.\n        rmse = df.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n        rmse_values.append(rmse)\n\n    # Build a single row table: first cell \"RMSE\", then one cell per model with the RMSE value.\n    row = [\"RMSE\"] + [f\"{rmse:.3f}\" for rmse in rmse_values]\n\n    # Build header: first column header is empty, then model names.\n    header = [\"\"] + names\n\n    table_str = tabulate([row], headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(names))\n    return table_str\n\n# Example usage:\n# print(compare_rmse([dtest_1, dtest_2, dtest_3], \"log_sales\", names=[\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\n\n\n\n\n\nCode\ndef residual_plot(df, label_col, model_name):\n    \"\"\"\n    Generates a residual plot for a given test dataframe.\n\n    Parameters:\n        df (DataFrame): Spark DataFrame containing the test set with predictions.\n        label_col (str): The column name of the actual outcome variable.\n        title (str): The title for the residual plot.\n\n    Returns:\n        None (displays the plot)\n    \"\"\"\n    # Convert to Pandas DataFrame\n    df_pd = df.select([\"prediction\", label_col]).toPandas()\n    df_pd[\"residual\"] = df_pd[label_col] - df_pd[\"prediction\"]\n\n    # Scatter plot of residuals vs. predicted values\n    plt.scatter(df_pd[\"prediction\"], df_pd[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n    # Use LOWESS smoothing for trend line\n    smoothed = sm.nonparametric.lowess(df_pd[\"residual\"], df_pd[\"prediction\"])\n    plt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\n\n    # Add reference line at y=0\n    plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n\n    # Labels and title (model_name)\n    plt.xlabel(\"Predicted Values\")\n    plt.ylabel(\"Residuals\")\n    model_name = \"Residual Plot for \" + model_name\n    plt.title(model_name)\n\n    # Show plot\n    plt.show()\n\n# Example usage:\n# residual_plot(dtest_1, \"log_sales\", \"Model 1\")"
  },
  {
    "objectID": "danl-ml/danl_320_hw2_linear_regression_beer.html#required-libraries-and-spark-session",
    "href": "danl-ml/danl_320_hw2_linear_regression_beer.html#required-libraries-and-spark-session",
    "title": "Homework 2",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
  },
  {
    "objectID": "danl-ml/danl_320_hw2_linear_regression_beer.html#udfs",
    "href": "danl-ml/danl_320_hw2_linear_regression_beer.html#udfs",
    "title": "Homework 2",
    "section": "",
    "text": "Code\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper | Exp(95% CI Lower) | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: 95% CI Upper, 8: Exp(95% CI Lower), 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"95% CI Upper\", \"Exp(95% CI Lower)\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))\n\n\n\n\n\n\n\nCode\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\n\n\n\n\n\nCode\ndef add_interaction_terms(var_list1, var_list2, var_list3=None):\n    \"\"\"\n    Creates interaction term columns in the global DataFrames dtrain and dtest.\n\n    For two sets of variable names (which may represent categorical (dummy) or continuous variables),\n    this function creates two-way interactions by multiplying each variable in var_list1 with each\n    variable in var_list2.\n\n    Optionally, if a third list of variable names (var_list3) is provided, the function also creates\n    three-way interactions among each variable in var_list1, each variable in var_list2, and each variable\n    in var_list3.\n\n    Parameters:\n        var_list1 (list): List of column names for the first set of variables.\n        var_list2 (list): List of column names for the second set of variables.\n        var_list3 (list, optional): List of column names for the third set of variables for three-way interactions.\n\n    Returns:\n        A flat list of new interaction column names.\n    \"\"\"\n    global dtrain, dtest\n\n    interaction_cols = []\n\n    # Create two-way interactions between var_list1 and var_list2.\n    for var1 in var_list1:\n        for var2 in var_list2:\n            col_name = f\"{var1}_*_{var2}\"\n            dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list1 and var_list3.\n    if var_list3 is not None:\n      for var1 in var_list1:\n          for var3 in var_list3:\n              col_name = f\"{var1}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list2 and var_list3.\n    if var_list3 is not None:\n      for var2 in var_list2:\n          for var3 in var_list3:\n              col_name = f\"{var2}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # If a third list is provided, create three-way interactions.\n    if var_list3 is not None:\n        for var1 in var_list1:\n            for var2 in var_list2:\n                for var3 in var_list3:\n                    col_name = f\"{var1}_*_{var2}_*_{var3}\"\n                    dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    interaction_cols.append(col_name)\n\n    return interaction_cols\n\n # Example\n # interaction_cols_brand_price = add_interaction_terms(dummy_cols_brand, ['log_price'])\n # interaction_cols_brand_ad_price = add_interaction_terms(dummy_cols_brand, dummy_cols_ad, ['log_price'])\n\n\n\n\n\n\n\nCode\ndef compare_reg_models(models, assemblers, names=None):\n    \"\"\"\n    Produces a single formatted table comparing multiple regression models.\n\n    For each predictor (the union across models, ordered by first appearance), the table shows\n    the beta estimate (with significance stars) from each model (blank if not used).\n    For a predictor, if a model's outcome (model.getLabelCol()) starts with \"log\", the cell displays\n    both the beta and its exponential (separated by \" / \"), except when the predictor's name includes \"log_\".\n    (The intercept row does not display exp(.))\n\n    Additional rows for Intercept, Observations, R², and RMSE are appended.\n\n    The header's first column is labeled \"Predictor\", and subsequent columns are\n    \"y: [outcome] ([name])\" for each model.\n\n    The table is produced in grid format (with vertical lines). A dashed line (using '-' characters)\n    is inserted at the top, immediately after the header, and at the bottom.\n    Additionally, immediately after the Intercept row, the border line is replaced with one using '='\n    (to appear as, for example, \"+==============================================+==========================+...\").\n\n    Parameters:\n        models (list): List of fitted LinearRegression models.\n        assemblers (list): List of corresponding VectorAssembler objects.\n        names (list, optional): List of model names; defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing the combined regression table.\n    \"\"\"\n    # Default model names.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(models))]\n\n    # For each model, get outcome and determine if that model is log-transformed.\n    outcomes = [m.getLabelCol() for m in models]\n    is_log_flags = [out.lower().startswith(\"log\") for out in outcomes]\n\n    # Build an ordered union of predictors based on first appearance.\n    ordered_predictors = []\n    for assembler in assemblers:\n        for feat in assembler.getInputCols():\n            if feat not in ordered_predictors:\n                ordered_predictors.append(feat)\n\n    # Helper for significance stars.\n    def significance_stars(p):\n        if p is None:\n            return \"\"\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build rows for each predictor.\n    rows = []\n    for feat in ordered_predictors:\n        row = [feat]\n        for m, a, is_log in zip(models, assemblers, is_log_flags):\n            feats_model = a.getInputCols()\n            if feat in feats_model:\n                idx = feats_model.index(feat)\n                beta = m.coefficients.toArray()[idx]\n                p_val = m.summary.pValues[idx] if m.summary.pValues is not None else None\n                stars = significance_stars(p_val)\n                cell = f\"{beta:.3f}{stars}\"\n                # Only add exp(beta) if model is log and predictor name does NOT include \"log_\"\n                if is_log and (\"log_\" not in feat.lower()):\n                    cell += f\" / {np.exp(beta):,.3f}\"\n                row.append(cell)\n            else:\n                row.append(\"\")\n        rows.append(row)\n\n    # Build intercept row (do NOT compute exp(intercept)).\n    intercept_row = [\"Intercept\"]\n    for m in models:\n        std_all = np.array(m.summary.coefficientStandardErrors)\n        coeffs = m.coefficients.toArray()\n        if len(std_all) == len(coeffs) + 1:\n            intercept_p = m.summary.pValues[0] if m.summary.pValues is not None else None\n        else:\n            intercept_p = None\n        sig = significance_stars(intercept_p)\n        cell = f\"{m.intercept:.3f}{sig}\"\n        intercept_row.append(cell)\n    rows.append(intercept_row)\n\n    # Add Observations row.\n    obs_row = [\"Observations\"]\n    for m in models:\n        obs = m.summary.numInstances\n        obs_row.append(f\"{int(obs):,}\")\n    rows.append(obs_row)\n\n    # Add R² row.\n    r2_row = [\"R²\"]\n    for m in models:\n        r2_row.append(f\"{m.summary.r2:.3f}\")\n    rows.append(r2_row)\n\n    # Add RMSE row.\n    rmse_row = [\"RMSE\"]\n    for m in models:\n        rmse_row.append(f\"{m.summary.rootMeanSquaredError:.3f}\")\n    rows.append(rmse_row)\n\n    # Build header: first column \"Predictor\", then for each model: \"y: [outcome] ([name])\"\n    header = [\"Predictor\"]\n    for out, name in zip(outcomes, names):\n        header.append(f\"y: {out} ({name})\")\n\n    # Create table string using grid format.\n    table_str = tabulate(rows, headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(models))\n\n    # Split into lines.\n    lines = table_str.split(\"\\n\")\n\n    # Create a dashed line spanning the full width.\n    full_width = len(lines[0])\n    dash_line = '-' * full_width\n    # Create an equals line by replacing '-' with '='.\n    eq_line = dash_line.replace('-', '=')\n\n    # Insert a dashed line after the header row.\n    lines = table_str.split(\"\\n\")\n    # In grid format, header and separator are usually the first two lines.\n    lines.insert(2, dash_line)\n\n    # Insert an equals line after the Intercept row.\n    for i, line in enumerate(lines):\n        if line.startswith(\"|\") and \"Intercept\" in line:\n            if i+1 &lt; len(lines):\n                lines[i+1] = eq_line\n            break\n\n    # Add dashed lines at the very top and bottom.\n    final_table = dash_line + \"\\n\" + \"\\n\".join(lines) + \"\\n\" + dash_line\n\n    return final_table\n\n# Example usage:\n# print(compare_reg_models([model_1, model_2, model_3],\n#                          [assembler_1, assembler_2, assembler_3],\n#                          [\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\n\n\n\n\n\nCode\ndef compare_rmse(test_dfs, label_col, pred_col=\"prediction\", names=None):\n    \"\"\"\n    Computes and compares RMSE values for a list of test DataFrames.\n\n    For each DataFrame in test_dfs, this function calculates the RMSE between the actual outcome\n    (given by label_col) and the predicted value (given by pred_col, default \"prediction\"). It then\n    produces a formatted table where the first column header is empty and the first row's first cell is\n    \"RMSE\", with each model's RMSE in its own column.\n\n    Parameters:\n        test_dfs (list): List of test DataFrames.\n        label_col (str): The name of the outcome column.\n        pred_col (str, optional): The name of the prediction column (default \"prediction\").\n        names (list, optional): List of model names corresponding to the test DataFrames.\n                                Defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing a table that compares RMSE values for each test DataFrame,\n        with one model per column.\n    \"\"\"\n    # Set default model names if none provided.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(test_dfs))]\n\n    rmse_values = []\n    for df in test_dfs:\n        # Create a column for squared error.\n        df = df.withColumn(\"error_sq\", pow(col(label_col) - col(pred_col), 2))\n        # Calculate RMSE: square root of the mean squared error.\n        rmse = df.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n        rmse_values.append(rmse)\n\n    # Build a single row table: first cell \"RMSE\", then one cell per model with the RMSE value.\n    row = [\"RMSE\"] + [f\"{rmse:.3f}\" for rmse in rmse_values]\n\n    # Build header: first column header is empty, then model names.\n    header = [\"\"] + names\n\n    table_str = tabulate([row], headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(names))\n    return table_str\n\n# Example usage:\n# print(compare_rmse([dtest_1, dtest_2, dtest_3], \"log_sales\", names=[\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\n\n\n\n\n\nCode\ndef residual_plot(df, label_col, model_name):\n    \"\"\"\n    Generates a residual plot for a given test dataframe.\n\n    Parameters:\n        df (DataFrame): Spark DataFrame containing the test set with predictions.\n        label_col (str): The column name of the actual outcome variable.\n        title (str): The title for the residual plot.\n\n    Returns:\n        None (displays the plot)\n    \"\"\"\n    # Convert to Pandas DataFrame\n    df_pd = df.select([\"prediction\", label_col]).toPandas()\n    df_pd[\"residual\"] = df_pd[label_col] - df_pd[\"prediction\"]\n\n    # Scatter plot of residuals vs. predicted values\n    plt.scatter(df_pd[\"prediction\"], df_pd[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n    # Use LOWESS smoothing for trend line\n    smoothed = sm.nonparametric.lowess(df_pd[\"residual\"], df_pd[\"prediction\"])\n    plt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\n\n    # Add reference line at y=0\n    plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n\n    # Labels and title (model_name)\n    plt.xlabel(\"Predicted Values\")\n    plt.ylabel(\"Residuals\")\n    model_name = \"Residual Plot for \" + model_name\n    plt.title(model_name)\n\n    # Show plot\n    plt.show()\n\n# Example usage:\n# residual_plot(dtest_1, \"log_sales\", \"Model 1\")"
  },
  {
    "objectID": "danl-ml/danl_320_hw2_linear_regression_beer.html#log-transformation",
    "href": "danl-ml/danl_320_hw2_linear_regression_beer.html#log-transformation",
    "title": "Homework 2",
    "section": "Log Transformation",
    "text": "Log Transformation\n\ndf = spark.createDataFrame(beer)\ndf = (\n    df\n    .withColumn(\"log_beer_floz\",\n                log(df['beer_floz']) )\n    .withColumn(\"log_price_floz\",\n                log(df['price_floz']) )\n)"
  },
  {
    "objectID": "danl-ml/danl_320_hw2_linear_regression_beer.html#question-1---filter",
    "href": "danl-ml/danl_320_hw2_linear_regression_beer.html#question-1---filter",
    "title": "Homework 2",
    "section": "Question 1 - Filter",
    "text": "Question 1 - Filter\n\ndf = df.filter(\n    (col(\"container\") == \"CAN\") |\n    (col(\"container\") == \"NON_REFILLABLE_BOTTLE\"))"
  },
  {
    "objectID": "danl-ml/danl_320_hw2_linear_regression_beer.html#question-2---training-test-split",
    "href": "danl-ml/danl_320_hw2_linear_regression_beer.html#question-2---training-test-split",
    "title": "Homework 2",
    "section": "Question 2 - Training-Test Split",
    "text": "Question 2 - Training-Test Split\n\ndtrain, dtest = df.randomSplit([0.67, 0.33], seed = 1234)\n\n\n(\n    df\n    .groupBy(\"market\")\n    .count()\n    .orderBy(\"market\")\n    .show(n = df.select(\"market\").distinct().count())\n)\n\n+--------------------+-----+\n|              market|count|\n+--------------------+-----+\n|              ALBANY|  487|\n|             ATLANTA| 1279|\n|           BALTIMORE|  374|\n|          BIRMINGHAM| 1137|\n|              BOSTON|  872|\n|   BUFFALO-ROCHESTER|  607|\n|           CHARLOTTE| 1246|\n|             CHICAGO| 1879|\n|          CINCINNATI| 1270|\n|           CLEVELAND| 1226|\n|            COLUMBUS| 1862|\n|              DALLAS| 2098|\n|              DENVER|  796|\n|          DES_MOINES|  716|\n|             DETROIT| 1731|\n|          EXURBAN_NJ|  223|\n|          EXURBAN_NY|   98|\n|        GRAND_RAPIDS|  739|\n|  HARTFORD-NEW_HAVEN|  370|\n|             HOUSTON| 1673|\n|        INDIANAPOLIS| 1213|\n|        JACKSONVILLE|  501|\n|         KANSAS_CITY|  663|\n|         LITTLE_ROCK|  452|\n|         LOS_ANGELES| 1564|\n|          LOUISVILLE|  833|\n|             MEMPHIS|  530|\n|               MIAMI| 2616|\n|           MILWAUKEE|  728|\n|         MINNEAPOLIS|  801|\n|           NASHVILLE|  989|\n|  NEW_ORLEANS-MOBILE|  852|\n| OKLAHOMA_CITY-TULSA|  800|\n|               OMAHA| 1017|\n|             ORLANDO| 1135|\n|        PHILADELPHIA|  433|\n|             PHOENIX| 2263|\n|          PITTSBURGH|  352|\n|            PORTLAND|  552|\n|      RALEIGH-DURHAM| 1126|\n|            RICHMOND| 1063|\n|       RURAL_ALABAMA|  305|\n|      RURAL_ARKANSAS|  160|\n|    RURAL_CALIFORNIA|  848|\n|      RURAL_COLORADO|   21|\n|       RURAL_FLORIDA|  522|\n|       RURAL_GEORGIA|  460|\n|         RURAL_IDAHO|  154|\n|      RURAL_ILLINOIS| 1195|\n|       RURAL_INDIANA|  481|\n|          RURAL_IOWA| 1060|\n|        RURAL_KANSAS|  179|\n|      RURAL_KENTUCKY|  225|\n|     RURAL_LOUISIANA|  381|\n|         RURAL_MAINE|  353|\n|      RURAL_MICHIGAN|  754|\n|     RURAL_MINNESOTA|  138|\n|   RURAL_MISSISSIPPI|  354|\n|      RURAL_MISSOURI|  640|\n|       RURAL_MONTANA|  354|\n|      RURAL_NEBRASKA|  110|\n|        RURAL_NEVADA|  557|\n| RURAL_NEW_HAMPSHIRE|   25|\n|    RURAL_NEW_MEXICO|  427|\n|      RURAL_NEW_YORK|   13|\n|RURAL_NORTH_CAROLINA|  909|\n|  RURAL_NORTH_DAKOTA|  129|\n|          RURAL_OHIO|  257|\n|      RURAL_OKLAHOMA|   54|\n|        RURAL_OREGON|   38|\n|  RURAL_PENNSYLVANIA|  298|\n|RURAL_SOUTH_CAROLINA| 1295|\n|  RURAL_SOUTH_DAKOTA|  153|\n|     RURAL_TENNESSEE|  423|\n|         RURAL_TEXAS| 1771|\n|       RURAL_VERMONT|  139|\n|      RURAL_VIRGINIA|  185|\n|    RURAL_WASHINGTON|  330|\n| RURAL_WEST_VIRGINIA|  265|\n|     RURAL_WISCONSIN| 1306|\n|       RURAL_WYOMING|   39|\n|          SACRAMENTO|  981|\n|      SALT_LAKE_CITY|  320|\n|         SAN_ANTONIO| 2615|\n|           SAN_DIEGO|  656|\n|       SAN_FRANCISCO|  871|\n|             SEATTLE|  903|\n|            ST_LOUIS| 1347|\n|        SURBURBAN_NJ|  399|\n|        SURBURBAN_NY|  473|\n|            SYRACUSE|  294|\n|               TAMPA| 3180|\n|            URBAN_NY|  735|\n|       WASHINGTON_DC|  863|\n+--------------------+-----+\n\n\n\n\n(\n    df\n    .groupBy(\"brand\")\n    .count()\n    .orderBy(\"brand\")\n    .show(n = df.select(\"brand\").distinct().count())\n)\n\n+-------------+-----+\n|        brand|count|\n+-------------+-----+\n|    BUD_LIGHT|21170|\n|  BUSCH_LIGHT| 8671|\n|  COORS_LIGHT|12865|\n|  MILLER_LITE|16788|\n|NATURAL_LIGHT|12616|\n+-------------+-----+\n\n\n\n\n(\n    df\n    .groupBy(\"container\")\n    .count()\n    .orderBy(\"container\")\n    .show(n = df.select(\"container\").distinct().count(), truncate=False)\n)\n\n+---------------------+-----+\n|container            |count|\n+---------------------+-----+\n|CAN                  |53015|\n|NON_REFILLABLE_BOTTLE|19095|\n+---------------------+-----+\n\n\n\n\n(\n    df\n    .groupBy(\"promo\")\n    .count()\n    .orderBy(\"promo\")\n    .show(n = df.select(\"promo\").distinct().count())\n)\n\n+-----+-----+\n|promo|count|\n+-----+-----+\n|false|57621|\n| true|14489|\n+-----+-----+"
  },
  {
    "objectID": "danl-ml/danl_320_hw2_linear_regression_beer.html#adding-dummies",
    "href": "danl-ml/danl_320_hw2_linear_regression_beer.html#adding-dummies",
    "title": "Homework 2",
    "section": "Adding Dummies",
    "text": "Adding Dummies\n\ndummy_cols_market, ref_category_market = add_dummy_variables('market', 5)\ndummy_cols_brand, ref_category_brand = add_dummy_variables('brand', 0)\ndummy_cols_container, ref_category_container = add_dummy_variables('container', 1)\ndummy_cols_promo, ref_category_promo = add_dummy_variables('promo', 0)\n\nReference category (dummy omitted): BUFFALO-ROCHESTER\nReference category (dummy omitted): BUD_LIGHT\nReference category (dummy omitted): NON_REFILLABLE_BOTTLE\nReference category (dummy omitted): False"
  },
  {
    "objectID": "danl-ml/danl_320_hw2_linear_regression_beer.html#adding-interaction-terms",
    "href": "danl-ml/danl_320_hw2_linear_regression_beer.html#adding-interaction-terms",
    "title": "Homework 2",
    "section": "Adding Interaction Terms",
    "text": "Adding Interaction Terms\n\ninteraction_cols_brand_quantity = add_interaction_terms(dummy_cols_brand, ['log_beer_floz'])\ninteraction_cols_brand_promo_quantity = add_interaction_terms(dummy_cols_brand, dummy_cols_promo, ['log_beer_floz'])"
  },
  {
    "objectID": "danl-ml/danl_320_hw2_linear_regression_beer.html#question-4",
    "href": "danl-ml/danl_320_hw2_linear_regression_beer.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4\n\nprint(\n    compare_reg_models(\n        [model_1, model_2, model_3],\n        [assembler_1, assembler_2, assembler_3]\n        )\n    )\n\n----------------------------------------------------------------------------------------------------------------------------------------------------\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| Predictor                                        |   y: log_price_floz (Model 1) |   y: log_price_floz (Model 2) |   y: log_price_floz (Model 3) |\n----------------------------------------------------------------------------------------------------------------------------------------------------\n+==================================================+===============================+===============================+===============================+\n| log_beer_floz                                    |                     -0.142*** |                     -0.146*** |                     -0.140*** |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_ALBANY                                    |               0.027** / 1.027 |               0.029** / 1.029 |                0.022* / 1.023 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_ATLANTA                                   |              0.083*** / 1.087 |              0.083*** / 1.087 |              0.079*** / 1.083 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_BALTIMORE                                 |              0.100*** / 1.105 |              0.104*** / 1.109 |              0.093*** / 1.098 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_BIRMINGHAM                                |              0.124*** / 1.132 |              0.130*** / 1.139 |              0.124*** / 1.133 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_BOSTON                                    |              0.127*** / 1.136 |              0.127*** / 1.135 |              0.123*** / 1.131 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_CHARLOTTE                                 |                0.020* / 1.020 |                 0.015 / 1.015 |               0.024** / 1.024 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_CHICAGO                                   |                -0.008 / 0.992 |                -0.013 / 0.987 |                -0.007 / 0.993 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_CINCINNATI                                |              0.084*** / 1.088 |              0.079*** / 1.082 |              0.077*** / 1.080 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_CLEVELAND                                 |              0.050*** / 1.051 |              0.045*** / 1.046 |              0.041*** / 1.041 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_COLUMBUS                                  |              0.069*** / 1.072 |              0.066*** / 1.068 |              0.066*** / 1.068 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_DALLAS                                    |              0.203*** / 1.226 |              0.214*** / 1.238 |              0.218*** / 1.244 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_DENVER                                    |              0.123*** / 1.131 |              0.121*** / 1.129 |              0.130*** / 1.139 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_DES_MOINES                                |              0.129*** / 1.138 |              0.125*** / 1.134 |              0.119*** / 1.126 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_DETROIT                                   |              0.087*** / 1.091 |              0.083*** / 1.086 |              0.084*** / 1.088 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_EXURBAN_NJ                                |              0.221*** / 1.247 |              0.216*** / 1.242 |              0.207*** / 1.229 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_EXURBAN_NY                                |              0.122*** / 1.130 |              0.119*** / 1.126 |              0.112*** / 1.119 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_GRAND_RAPIDS                              |              0.082*** / 1.086 |              0.078*** / 1.081 |              0.077*** / 1.080 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_HARTFORD-NEW_HAVEN                        |              0.148*** / 1.160 |              0.146*** / 1.157 |              0.143*** / 1.154 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_HOUSTON                                   |              0.113*** / 1.120 |              0.110*** / 1.117 |              0.114*** / 1.121 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_INDIANAPOLIS                              |              0.042*** / 1.043 |              0.041*** / 1.042 |              0.042*** / 1.043 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_JACKSONVILLE                              |              0.113*** / 1.120 |              0.105*** / 1.111 |              0.108*** / 1.114 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_KANSAS_CITY                               |              0.076*** / 1.079 |              0.070*** / 1.073 |              0.065*** / 1.067 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_LITTLE_ROCK                               |              0.092*** / 1.096 |              0.088*** / 1.092 |              0.085*** / 1.088 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_LOS_ANGELES                               |              0.032*** / 1.033 |               0.026** / 1.026 |              0.034*** / 1.035 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_LOUISVILLE                                |              0.068*** / 1.070 |              0.063*** / 1.065 |              0.067*** / 1.069 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_MEMPHIS                                   |              0.128*** / 1.137 |              0.127*** / 1.136 |              0.121*** / 1.128 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_MIAMI                                     |              0.108*** / 1.114 |              0.106*** / 1.112 |              0.107*** / 1.113 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_MILWAUKEE                                 |               0.028** / 1.028 |               0.027** / 1.027 |               0.029** / 1.029 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_MINNEAPOLIS                               |              0.128*** / 1.137 |              0.129*** / 1.138 |              0.125*** / 1.134 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_NASHVILLE                                 |              0.143*** / 1.153 |              0.142*** / 1.152 |              0.140*** / 1.150 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_NEW_ORLEANS-MOBILE                        |              0.128*** / 1.136 |              0.118*** / 1.125 |              0.112*** / 1.118 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_OKLAHOMA_CITY-TULSA                       |              0.145*** / 1.156 |              0.142*** / 1.153 |              0.135*** / 1.144 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_OMAHA                                     |              0.131*** / 1.140 |              0.129*** / 1.137 |              0.130*** / 1.139 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_ORLANDO                                   |              0.098*** / 1.103 |              0.096*** / 1.101 |              0.099*** / 1.104 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_PHILADELPHIA                              |              0.115*** / 1.121 |              0.114*** / 1.121 |              0.103*** / 1.108 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_PHOENIX                                   |              0.142*** / 1.152 |              0.144*** / 1.155 |              0.153*** / 1.165 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_PITTSBURGH                                |              0.100*** / 1.105 |              0.098*** / 1.102 |              0.091*** / 1.096 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_PORTLAND                                  |              0.115*** / 1.122 |              0.113*** / 1.120 |              0.116*** / 1.123 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RALEIGH-DURHAM                            |              0.090*** / 1.094 |              0.090*** / 1.094 |              0.084*** / 1.088 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RICHMOND                                  |              0.043*** / 1.044 |              0.041*** / 1.041 |              0.035*** / 1.035 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_ALABAMA                             |              0.157*** / 1.170 |              0.156*** / 1.169 |              0.151*** / 1.163 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_ARKANSAS                            |              0.157*** / 1.171 |              0.161*** / 1.174 |              0.152*** / 1.164 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_CALIFORNIA                          |              0.044*** / 1.045 |              0.041*** / 1.042 |              0.044*** / 1.045 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_COLORADO                            |              0.137*** / 1.147 |              0.136*** / 1.145 |              0.144*** / 1.155 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_FLORIDA                             |              0.059*** / 1.061 |              0.050*** / 1.052 |              0.047*** / 1.049 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_GEORGIA                             |              0.132*** / 1.141 |              0.128*** / 1.137 |              0.122*** / 1.130 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_IDAHO                               |              0.142*** / 1.152 |              0.135*** / 1.144 |              0.134*** / 1.144 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_ILLINOIS                            |                 0.014 / 1.014 |                 0.013 / 1.013 |                 0.010 / 1.010 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_INDIANA                             |              0.073*** / 1.076 |              0.076*** / 1.078 |              0.075*** / 1.077 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_IOWA                                |              0.058*** / 1.060 |              0.055*** / 1.056 |              0.051*** / 1.052 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_KANSAS                              |              0.134*** / 1.143 |              0.133*** / 1.142 |              0.125*** / 1.133 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_KENTUCKY                            |              0.157*** / 1.170 |              0.156*** / 1.169 |              0.152*** / 1.164 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_LOUISIANA                           |              0.060*** / 1.061 |              0.055*** / 1.057 |              0.045*** / 1.046 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_MAINE                               |              0.091*** / 1.096 |              0.088*** / 1.092 |              0.086*** / 1.090 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_MICHIGAN                            |              0.085*** / 1.089 |              0.081*** / 1.084 |              0.076*** / 1.079 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_MINNESOTA                           |              0.165*** / 1.180 |              0.169*** / 1.185 |              0.162*** / 1.176 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_MISSISSIPPI                         |              0.043*** / 1.044 |              0.039*** / 1.040 |              0.038*** / 1.039 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_MISSOURI                            |              0.106*** / 1.112 |              0.104*** / 1.110 |              0.097*** / 1.102 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_MONTANA                             |              0.127*** / 1.135 |              0.124*** / 1.132 |              0.130*** / 1.139 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_NEBRASKA                            |              0.138*** / 1.148 |              0.138*** / 1.148 |              0.133*** / 1.142 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_NEVADA                              |              0.051*** / 1.052 |              0.051*** / 1.052 |              0.048*** / 1.049 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_NEW_HAMPSHIRE                       |                 0.028 / 1.028 |                 0.017 / 1.017 |                 0.011 / 1.011 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_NEW_MEXICO                          |              0.154*** / 1.166 |              0.148*** / 1.160 |              0.145*** / 1.156 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_NEW_YORK                            |                -0.013 / 0.987 |                -0.010 / 0.990 |                -0.024 / 0.976 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_NORTH_CAROLINA                      |                -0.002 / 0.998 |               0.027** / 1.027 |                 0.014 / 1.014 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_NORTH_DAKOTA                        |              0.223*** / 1.250 |              0.222*** / 1.248 |              0.220*** / 1.246 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_OHIO                                |              0.096*** / 1.100 |              0.093*** / 1.098 |              0.090*** / 1.094 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_OKLAHOMA                            |              0.130*** / 1.139 |              0.130*** / 1.139 |              0.120*** / 1.128 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_OREGON                              |               0.074** / 1.077 |               0.070** / 1.073 |               0.071** / 1.074 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_PENNSYLVANIA                        |              0.131*** / 1.140 |              0.132*** / 1.141 |              0.122*** / 1.130 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_SOUTH_CAROLINA                      |              0.055*** / 1.056 |              0.054*** / 1.055 |              0.056*** / 1.057 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_SOUTH_DAKOTA                        |              0.077*** / 1.081 |              0.076*** / 1.079 |              0.071*** / 1.074 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_TENNESSEE                           |              0.170*** / 1.186 |              0.170*** / 1.185 |              0.174*** / 1.190 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_TEXAS                               |              0.169*** / 1.184 |              0.167*** / 1.182 |              0.164*** / 1.179 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_VERMONT                             |              0.077*** / 1.080 |              0.066*** / 1.068 |              0.067*** / 1.069 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_VIRGINIA                            |                 0.018 / 1.019 |                 0.015 / 1.015 |                 0.011 / 1.011 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_WASHINGTON                          |              0.116*** / 1.123 |              0.114*** / 1.121 |              0.131*** / 1.140 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_WEST_VIRGINIA                       |              -0.037** / 0.964 |             -0.041*** / 0.960 |             -0.050*** / 0.951 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_WISCONSIN                           |              0.039*** / 1.040 |              0.037*** / 1.038 |              0.036*** / 1.036 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_RURAL_WYOMING                             |              0.147*** / 1.158 |              0.144*** / 1.155 |              0.138*** / 1.148 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_SACRAMENTO                                |                0.020* / 1.020 |                0.018* / 1.019 |               0.026** / 1.026 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_SALT_LAKE_CITY                            |              0.120*** / 1.128 |              0.114*** / 1.121 |              0.113*** / 1.120 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_SAN_ANTONIO                               |              0.138*** / 1.148 |              0.133*** / 1.142 |              0.129*** / 1.138 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_SAN_DIEGO                                 |                 0.012 / 1.012 |                 0.010 / 1.010 |                 0.015 / 1.015 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_SAN_FRANCISCO                             |              0.069*** / 1.071 |              0.066*** / 1.068 |              0.073*** / 1.076 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_SEATTLE                                   |              0.109*** / 1.116 |              0.101*** / 1.106 |              0.113*** / 1.120 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_ST_LOUIS                                  |              0.042*** / 1.042 |              0.038*** / 1.038 |              0.041*** / 1.042 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_SURBURBAN_NJ                              |                -0.008 / 0.992 |                -0.010 / 0.990 |                -0.021 / 0.979 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_SURBURBAN_NY                              |              0.101*** / 1.106 |              0.097*** / 1.102 |              0.097*** / 1.102 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_SYRACUSE                                  |              -0.035** / 0.966 |             -0.040*** / 0.961 |             -0.048*** / 0.953 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_TAMPA                                     |              0.103*** / 1.109 |              0.099*** / 1.104 |              0.100*** / 1.105 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_URBAN_NY                                  |              0.173*** / 1.189 |              0.172*** / 1.188 |              0.170*** / 1.186 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| market_WASHINGTON_DC                             |              0.097*** / 1.102 |              0.091*** / 1.095 |              0.085*** / 1.089 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| brand_BUSCH_LIGHT                                |             -0.260*** / 0.771 |             -0.185*** / 0.831 |             -0.149*** / 0.862 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| brand_COORS_LIGHT                                |              -0.005** / 0.995 |                 0.019 / 1.019 |               0.042** / 1.043 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| brand_MILLER_LITE                                |             -0.013*** / 0.987 |              0.080*** / 1.083 |              0.111*** / 1.117 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| brand_NATURAL_LIGHT                              |             -0.319*** / 0.727 |             -0.601*** / 0.548 |             -0.519*** / 0.595 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| container_CAN                                    |             -0.053*** / 0.948 |             -0.052*** / 0.949 |             -0.053*** / 0.948 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| brand_BUSCH_LIGHT_*_log_beer_floz                |                               |                     -0.013*** |                     -0.021*** |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| brand_COORS_LIGHT_*_log_beer_floz                |                               |                        -0.004 |                      -0.008** |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| brand_MILLER_LITE_*_log_beer_floz                |                               |                     -0.017*** |                     -0.023*** |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| brand_NATURAL_LIGHT_*_log_beer_floz              |                               |                      0.052*** |                      0.037*** |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| brand_BUSCH_LIGHT_*_promo_True                   |                               |                               |             -0.253*** / 0.776 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| brand_COORS_LIGHT_*_promo_True                   |                               |                               |             -0.227*** / 0.797 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| brand_MILLER_LITE_*_promo_True                   |                               |                               |             -0.286*** / 0.751 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| brand_NATURAL_LIGHT_*_promo_True                 |                               |                               |             -0.400*** / 0.671 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| promo_True_*_log_beer_floz                       |                               |                               |                     -0.008*** |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| brand_BUSCH_LIGHT_*_promo_True_*_log_beer_floz   |                               |                               |                      0.047*** |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| brand_COORS_LIGHT_*_promo_True_*_log_beer_floz   |                               |                               |                      0.037*** |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| brand_MILLER_LITE_*_promo_True_*_log_beer_floz   |                               |                               |                      0.052*** |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| brand_NATURAL_LIGHT_*_promo_True_*_log_beer_floz |                               |                               |                      0.071*** |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| Intercept                                        |                     -2.117*** |                     -2.093*** |                     -2.111*** |\n====================================================================================================================================================\n| Observations                                     |                        48,115 |                        48,115 |                        48,115 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| R²                                               |                         0.547 |                         0.552 |                         0.560 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| RMSE                                             |                         0.170 |                         0.169 |                         0.167 |\n+--------------------------------------------------+-------------------------------+-------------------------------+-------------------------------+\n----------------------------------------------------------------------------------------------------------------------------------------------------\n\n\n\nQuestion 4 - RMSEs on Test Data\n\nprint(compare_rmse([dtest_1, dtest_2, dtest_3], \"log_price_floz\"))\n\n+------+-----------+-----------+-----------+\n|      |   Model 1 |   Model 2 |   Model 3 |\n+======+===========+===========+===========+\n| RMSE |     0.167 |     0.166 |     0.165 |\n+------+-----------+-----------+-----------+"
  },
  {
    "objectID": "danl-ml/danl_320_hw2_linear_regression_beer.html#question-5",
    "href": "danl-ml/danl_320_hw2_linear_regression_beer.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5\n\nBelow provides a list of beta esimates and exponential function of those beta estimates from Model 3:\n\nmarket_ALBANY (\\(\\hat{\\beta}=0.022^*\\) / \\(e^{\\hat{\\beta}}=1.023\\))\nCeteris paribus, beer prices in Albany are 2.3 % higher than in the Buffalo–Rochester market.\nmarket_EXURBAN_NY (\\(\\hat{\\beta}=0.112^{***}\\) / \\(e^{\\hat{\\beta}}=1.119\\))\nCeteris paribus, beer prices in Exurban NY are 11.9 % higher than in the Buffalo–Rochester market.\nmarket_RURAL_NEW_YORK (\\(\\hat{\\beta}=-0.024\\) / \\(e^{\\hat{\\beta}}=0.976\\))\nCeteris paribus, beer prices in Rural NY are not statistically different than in the Buffalo–Rochester market.\nmarket_SUBURBAN_NY (\\(\\hat{\\beta}=0.097^{***}\\) / \\(e^{\\hat{\\beta}}=1.102\\))\nCeteris paribus, beer prices in Suburban NY are 10.2 % higher than in the Buffalo–Rochester market.\nmarket_SYRACUSE (\\(\\hat{\\beta}=-0.048^{***}\\) / \\(e^{\\hat{\\beta}}=0.953\\))\nCeteris paribus, beer prices in Syracuse are 4.7 % lower than in the Buffalo–Rochester market.\nmarket_URBAN_NY (\\(\\hat{\\beta}=0.170^{***}\\) / \\(e^{\\hat{\\beta}}=1.186\\))\nCeteris paribus, beer prices in Urban NY are 18.6 % higher than in the Buffalo–Rochester market."
  },
  {
    "objectID": "danl-ml/danl_320_hw2_linear_regression_beer.html#question-6",
    "href": "danl-ml/danl_320_hw2_linear_regression_beer.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\n\nWe should focus on the beta esimates for predictors with \\(\\log(beer floz)\\) to calculate the inverse price elasticity of beer demand across brands.\n\n\n\n\n\n\n\n\n\n\nPredictor\nModel 1\nModel 2\nModel 3\n\n\n\n\nlog_beer_floz\n-0.142***\n-0.146***\n-0.140***\n\n\nbrand_BUSCH_LIGHT_*_log_beer_floz\n\n-0.013***\n-0.021***\n\n\nbrand_COORS_LIGHT_*_log_beer_floz\n\n-0.004\n-0.008**\n\n\nbrand_MILLER_LITE_*_log_beer_floz\n\n-0.017***\n-0.023***\n\n\nbrand_NATURAL_LIGHT_*_log_beer_floz\n\n0.052***\n0.037***\n\n\npromo_True_*_log_beer_floz\n\n\n-0.008***\n\n\nbrand_BUSCH_LIGHT_promo_True_log_beer_floz\n\n\n0.047***\n\n\nbrand_COORS_LIGHT_promo_True_log_beer_floz\n\n\n0.037***\n\n\nbrand_MILLER_LITE_promo_True_log_beer_floz\n\n\n0.052***\n\n\nbrand_NATURAL_LIGHT_promo_True_log_beer_floz\n\n\n0.071***\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel 1\nModel 2\nModel 3 (no Promo)\nModel 3 (with Promo)\n\n\n\n\nBUD\n-0.142\n-0.146\n-0.140\n-0.148  = −0.140 − 0.008\n\n\nBUSCH\n-0.142\n-0.159  = −0.146 − 0.013\n-0.161  = −0.140 − 0.021\n-0.122  = −0.140 − 0.021 − 0.008 + 0.047\n\n\nCOORS\n-0.142\n-0.146  = −0.146 − 0\n-0.148  = −0.140 − 0.008\n-0.119  = −0.140 − 0.008 − 0.008 + 0.037\n\n\nMILLER\n-0.142\n-0.163  = −0.146 − 0.017\n-0.163  = −0.140 − 0.023\n-0.119  = −0.140 − 0.023 − 0.008 + 0.052\n\n\nNATURAL\n-0.142\n-0.094  = −0.146 + 0.052\n-0.103  = −0.140 + 0.037\n-0.040  = −0.140 + 0.037 − 0.008 + 0.071\n\n\n\n\nModel 1\n\nA 1% increase in sales volume (across any of the five brands) is associated with a 0.142% decrease in price.\n\nModel 2\n\nA 1% increase in BUD sales volume is associated with a 0.146% decrease in its price.\nA 1% increase in BUSCH sales volume is associated with a 0.159 decrease in its price.\nA 1% increase in COORS sales volume is associated with a 0.146% decrease in its price.\nA 1% increase in MILLER sales volume is associated with a 0.163% decrease in its price.\nA 1% increase in NATURAL sales volume is associated with a 0.094% decrease in its price.\n\nModel 3 (no Promo)\n\nA 1% increase in BUD sales volume is associated with a 0.140% decrease in its price.\nA 1% increase in BUSCH sales volume is associated with a 0.161% decrease in its price.\nA 1% increase in COORS sales volume is associated with a 0.148% decrease in its price.\nA 1% increase in MILLER sales volume is associated with a 0.163% decrease in its price.\nA 1% increase in NATURAL sales volume is associated with a 0.103% decrease in its price.\n\nModel 3 (with Promo)\n\nA 1% increase in BUD sales volume is associated with a 0.148% decrease in its price.\nA 1% increase in BUSCH sales volume is associated with a 0.122% decrease in its price.\nA 1% increase in COORS sales volume is associated with a 0.119% decrease in its price.\nA 1% increase in MILLER sales volume is associated with a 0.119% decrease in its price.\nA 1% increase in NATURAL sales volume is associated with a 0.040% decrease in its price.\n\n\n\n# Slopes (inverse elasticities)\nmodel1_slope = -0.142\nmodel2_slopes = {\n    'Bud': -0.146,\n    'Busch': -0.159,\n    'Coors': -0.146,\n    'Miller': -0.163,\n    'Natural': -0.094\n}\nmodel3_full_slopes = {\n    'Bud': -0.140,\n    'Busch': -0.161,\n    'Coors': -0.148,\n    'Miller': -0.163,\n    'Natural': -0.103\n}\nmodel3_promo_slopes = {\n    'Bud': -0.148,\n    'Busch': -0.122,\n    'Coors': -0.119,\n    'Miller': -0.096,\n    'Natural': -0.077\n}\n\n# Create range of log(sales) values\nx = np.linspace(0, 10, 100)\n\n# Set up a 2x2 grid of subplots\nfig, axes = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n\n# Model 1: Common Elasticity\nax = axes[0, 0]\nax.plot(x, model1_slope * x, color='tab:blue', label='All Brands')\nax.set_title('Model 1: Common Elasticity')\nax.set_xlabel('log(sales)')\nax.set_ylabel('log(price)')\nax.grid(True)\nax.legend()\n\n# Model 2: Brand-Specific Elasticity\nax = axes[0, 1]\nfor brand, slope in model2_slopes.items():\n    ax.plot(x, slope * x, label=brand)\nax.set_title('Model 2: Brand-Specific')\nax.set_xlabel('log(sales)')\nax.grid(True)\nax.legend()\n\n# Model 3: Full-Price (No Promo)\nax = axes[1, 0]\nfor brand, slope in model3_full_slopes.items():\n    ax.plot(x, slope * x, label=brand)\nax.set_title('Model 3: Full-Price')\nax.set_xlabel('log(sales)')\nax.set_ylabel('log(price)')\nax.grid(True)\nax.legend()\n\n# Model 3: Promo-Price\nax = axes[1, 1]\nfor brand, slope in model3_promo_slopes.items():\n    ax.plot(x, slope * x, label=brand)\nax.set_title('Model 3: Promotional Price')\nax.set_xlabel('log(sales)')\nax.grid(True)\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nMagnitude (|coefficient|): Larger absolute values mean more price sensitivity (higher elasticity).\nPromotions: For most brands, promotions reduce inverse coefficients (in absolute terms), implying lower sensitivity of price to volume during deals—firms can sell more with smaller price cuts.\nBrand differences: Miller and Busch tend to be the most elastic brands, while Natural Light is consistently the least elastic (i.e. its drinkers are relatively price‐insensitive, especially on promotion).\n\nGiven a fixed price increase, Natural Light experiences the largest decline in sales volume.\nFor the same price increase without promotion, Miller Lite and Busch Light exhibit the smallest declines in sales volume.\nFor the same price decrease with promotion, Bud Light shows the smallest increase in sales volume."
  },
  {
    "objectID": "danl-ml/danl_320_hw2_linear_regression_beer.html#question-7",
    "href": "danl-ml/danl_320_hw2_linear_regression_beer.html#question-7",
    "title": "Homework 2",
    "section": "Question 7",
    "text": "Question 7"
  },
  {
    "objectID": "danl-ml/danl_320_hw2_linear_regression_beer.html#residual-plots",
    "href": "danl-ml/danl_320_hw2_linear_regression_beer.html#residual-plots",
    "title": "Homework 2",
    "section": "Residual Plots",
    "text": "Residual Plots\n\nresidual_plot(dtest_1, \"log_price_floz\", \"Model 1\")\n\n\n\n\n\n\n\n\n\nresidual_plot(dtest_2, \"log_price_floz\", \"Model 2\")\n\n\n\n\n\n\n\n\n\nresidual_plot(dtest_3, \"log_price_floz\", \"Model 3\")"
  },
  {
    "objectID": "danl-ml/danl_320_hw2_linear_regression_beer.html#question-8",
    "href": "danl-ml/danl_320_hw2_linear_regression_beer.html#question-8",
    "title": "Homework 2",
    "section": "Question 8",
    "text": "Question 8\nI prefer Model 3 for three simple reasons:\n\nRealistic Market Setting\n\n\nIt captures both the unique sensitivity of each brand and how that sensitivity changes when a beer is on promotion.\n\n\nPractical Pricing Strategies\n\n\nBy distinguishing full-price from promotional periods, it tells you exactly how much to adjust each brand’s price under each scenario.\n\n\nBetter Fit with the lowest MSE on test data\n\n\nAllowing elasticities to vary by brand and promotion status typically explains sales patterns more accurately than the cruder alternatives.\n\nIn short, Model 3 reflects realistic market settings with brand heterogeneity and promotion effect, along with the best prediction quality."
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_regularization_cars.html",
    "href": "danl-ml/danl_320_logistic_regression_regularization_cars.html",
    "title": "Quasi-Separation and Regularized Logistic Regression",
    "section": "",
    "text": "# Below is for an interactive display of Pandas DataFrame in Colab\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\nimport pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_curve\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n\n\n\n\n\n\nCode\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\n\n\n\n\n\nCode\ndef marginal_effects(model, means):\n    \"\"\"\n    Compute marginal effects for all predictors in a PySpark GeneralizedLinearRegression model (logit)\n    and return a formatted table with statistical significance and standard errors.\n\n    Parameters:\n        model: Fitted GeneralizedLinearRegression model (with binomial family and logit link).\n        means: List of mean values for the predictor variables.\n\n    Returns:\n        - A formatted string containing the marginal effects table.\n        - A Pandas DataFrame with marginal effects, standard errors, confidence intervals, and significance stars.\n    \"\"\"\n    global assembler_predictors  # Use the global assembler_predictors list\n\n    # Extract model coefficients, standard errors, and intercept\n    coeffs = np.array(model.coefficients)\n    std_errors = np.array(model.summary.coefficientStandardErrors)\n    intercept = model.intercept\n\n    # Compute linear combination of means and coefficients (XB)\n    XB = np.dot(means, coeffs) + intercept\n\n    # Compute derivative of logistic function (G'(XB))\n    G_prime_XB = np.exp(XB) / ((1 + np.exp(XB)) ** 2)\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Create lists to store results\n    results = []\n    df_results = []  # For Pandas DataFrame\n\n    for i, predictor in enumerate(assembler_predictors):\n        # Compute marginal effect\n        marginal_effect = G_prime_XB * coeffs[i]\n\n        # Compute standard error of the marginal effect\n        std_error = G_prime_XB * std_errors[i]\n\n        # Compute z-score and p-value\n        z_score = marginal_effect / std_error if std_error != 0 else np.nan\n        p_value = 2 * (1 - norm.cdf(abs(z_score))) if not np.isnan(z_score) else np.nan\n\n        # Compute confidence interval (95%)\n        ci_lower = marginal_effect - 1.96 * std_error\n        ci_upper = marginal_effect + 1.96 * std_error\n\n        # Append results for table formatting\n        results.append([\n            predictor,\n            f\"{marginal_effect: .6f}\",\n            significance_stars(p_value),\n            f\"{std_error: .6f}\",\n            f\"{ci_lower: .6f}\",\n            f\"{ci_upper: .6f}\"\n        ])\n\n        # Append results for Pandas DataFrame\n        df_results.append({\n            \"Variable\": predictor,\n            \"Marginal Effect\": marginal_effect,\n            \"Significance\": significance_stars(p_value),\n            \"Std. Error\": std_error,\n            \"95% CI Lower\": ci_lower,\n            \"95% CI Upper\": ci_upper\n        })\n\n    # Convert results to formatted table\n    table_str = tabulate(results, headers=[\"Variable\", \"Marginal Effect\", \"Significance\", \"Std. Error\", \"95% CI Lower\", \"95% CI Upper\"],\n                         tablefmt=\"pretty\", colalign=(\"left\", \"decimal\", \"left\", \"decimal\", \"decimal\", \"decimal\"))\n\n    # Convert results to Pandas DataFrame\n    df_results = pd.DataFrame(df_results)\n\n    return table_str, df_results\n\n# Example usage:\n# means = [0.5, 30]  # Mean values for x1 and x2\n# assembler_predictors = ['x1', 'x2']  # Define globally before calling the function\n# table_output, df_output = marginal_effects(fitted_model, means)\n# print(table_output)\n# display(df_output)"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_regularization_cars.html#udfs",
    "href": "danl-ml/danl_320_logistic_regression_regularization_cars.html#udfs",
    "title": "Quasi-Separation and Regularized Logistic Regression",
    "section": "",
    "text": "Code\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\n\n\n\n\n\nCode\ndef marginal_effects(model, means):\n    \"\"\"\n    Compute marginal effects for all predictors in a PySpark GeneralizedLinearRegression model (logit)\n    and return a formatted table with statistical significance and standard errors.\n\n    Parameters:\n        model: Fitted GeneralizedLinearRegression model (with binomial family and logit link).\n        means: List of mean values for the predictor variables.\n\n    Returns:\n        - A formatted string containing the marginal effects table.\n        - A Pandas DataFrame with marginal effects, standard errors, confidence intervals, and significance stars.\n    \"\"\"\n    global assembler_predictors  # Use the global assembler_predictors list\n\n    # Extract model coefficients, standard errors, and intercept\n    coeffs = np.array(model.coefficients)\n    std_errors = np.array(model.summary.coefficientStandardErrors)\n    intercept = model.intercept\n\n    # Compute linear combination of means and coefficients (XB)\n    XB = np.dot(means, coeffs) + intercept\n\n    # Compute derivative of logistic function (G'(XB))\n    G_prime_XB = np.exp(XB) / ((1 + np.exp(XB)) ** 2)\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Create lists to store results\n    results = []\n    df_results = []  # For Pandas DataFrame\n\n    for i, predictor in enumerate(assembler_predictors):\n        # Compute marginal effect\n        marginal_effect = G_prime_XB * coeffs[i]\n\n        # Compute standard error of the marginal effect\n        std_error = G_prime_XB * std_errors[i]\n\n        # Compute z-score and p-value\n        z_score = marginal_effect / std_error if std_error != 0 else np.nan\n        p_value = 2 * (1 - norm.cdf(abs(z_score))) if not np.isnan(z_score) else np.nan\n\n        # Compute confidence interval (95%)\n        ci_lower = marginal_effect - 1.96 * std_error\n        ci_upper = marginal_effect + 1.96 * std_error\n\n        # Append results for table formatting\n        results.append([\n            predictor,\n            f\"{marginal_effect: .6f}\",\n            significance_stars(p_value),\n            f\"{std_error: .6f}\",\n            f\"{ci_lower: .6f}\",\n            f\"{ci_upper: .6f}\"\n        ])\n\n        # Append results for Pandas DataFrame\n        df_results.append({\n            \"Variable\": predictor,\n            \"Marginal Effect\": marginal_effect,\n            \"Significance\": significance_stars(p_value),\n            \"Std. Error\": std_error,\n            \"95% CI Lower\": ci_lower,\n            \"95% CI Upper\": ci_upper\n        })\n\n    # Convert results to formatted table\n    table_str = tabulate(results, headers=[\"Variable\", \"Marginal Effect\", \"Significance\", \"Std. Error\", \"95% CI Lower\", \"95% CI Upper\"],\n                         tablefmt=\"pretty\", colalign=(\"left\", \"decimal\", \"left\", \"decimal\", \"decimal\", \"decimal\"))\n\n    # Convert results to Pandas DataFrame\n    df_results = pd.DataFrame(df_results)\n\n    return table_str, df_results\n\n# Example usage:\n# means = [0.5, 30]  # Mean values for x1 and x2\n# assembler_predictors = ['x1', 'x2']  # Define globally before calling the function\n# table_output, df_output = marginal_effects(fitted_model, means)\n# print(table_output)\n# display(df_output)"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_regularization_cars.html#loading-data",
    "href": "danl-ml/danl_320_logistic_regression_regularization_cars.html#loading-data",
    "title": "Quasi-Separation and Regularized Logistic Regression",
    "section": "Loading Data",
    "text": "Loading Data\n\ndfpd = pd.read_csv('https://bcdanl.github.io/data/car-data.csv')\ndf = spark.createDataFrame(dfpd)\ndfpd\n\n\n    \n\n\n\n\n\n\nbuying\nmaint\ndoors\npersons\nlug_boot\nsafety\nrating\nfail\n\n\n\n\n0\nvhigh\nvhigh\n2\n2\nsmall\nlow\nunacc\n1\n\n\n1\nvhigh\nvhigh\n2\n2\nsmall\nmed\nunacc\n1\n\n\n2\nvhigh\nvhigh\n2\n2\nsmall\nhigh\nunacc\n1\n\n\n3\nvhigh\nvhigh\n2\n2\nmed\nlow\nunacc\n1\n\n\n4\nvhigh\nvhigh\n2\n2\nmed\nmed\nunacc\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1723\nlow\nlow\n5more\nmore\nmed\nmed\ngood\n0\n\n\n1724\nlow\nlow\n5more\nmore\nmed\nhigh\nvgood\n0\n\n\n1725\nlow\nlow\n5more\nmore\nbig\nlow\nunacc\n1\n\n\n1726\nlow\nlow\n5more\nmore\nbig\nmed\ngood\n0\n\n\n1727\nlow\nlow\n5more\nmore\nbig\nhigh\nvgood\n0\n\n\n\n\n1728 rows × 8 columns"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_regularization_cars.html#training-test-data-split",
    "href": "danl-ml/danl_320_logistic_regression_regularization_cars.html#training-test-data-split",
    "title": "Quasi-Separation and Regularized Logistic Regression",
    "section": "Training-Test Data Split",
    "text": "Training-Test Data Split\n\ndtrain, dtest = df.randomSplit([0.7, 0.3], seed = 1234)"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_regularization_cars.html#adding-dummies",
    "href": "danl-ml/danl_320_logistic_regression_regularization_cars.html#adding-dummies",
    "title": "Quasi-Separation and Regularized Logistic Regression",
    "section": "Adding Dummies",
    "text": "Adding Dummies\n\ndfpd['buying'].unique() # to see categories in GESTREC3 using the pandas' unique() method\n\narray(['vhigh', 'high', 'med', 'low'], dtype=object)\n\n\n\ndfpd['maint'].unique() # to see categories in DPLURAL\n\narray(['vhigh', 'high', 'med', 'low'], dtype=object)\n\n\n\ndfpd['doors'].unique()\n\narray(['2', '3', '4', '5more'], dtype=object)\n\n\n\ndfpd['persons'].unique()\n\narray(['2', '4', 'more'], dtype=object)\n\n\n\ndfpd['lug_boot'].unique()\n\narray(['small', 'med', 'big'], dtype=object)\n\n\n\ndfpd['safety'].unique()\n\narray(['low', 'med', 'high'], dtype=object)\n\n\n\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\ncustom_order_buying = ['low', 'med', 'high', 'vhigh']\ncustom_order_maint = ['low', 'med', 'high', 'vhigh']\ncustom_order_persons = ['2', '4', 'more']\ncustom_order_lug_boot = ['small', 'med', 'big']\ncustom_order_safety = ['low', 'med', 'high']\ndummy_cols_buying, ref_category_buying = add_dummy_variables('buying', 0, category_order = custom_order_buying)\ndummy_cols_maint, ref_category_maint = add_dummy_variables('maint', 0, category_order = custom_order_maint)\ndummy_cols_persons, ref_category_persons = add_dummy_variables('persons', 0, category_order = custom_order_persons)\ndummy_cols_lug_boot, ref_category_lug_boot = add_dummy_variables('lug_boot', 0, category_order = custom_order_lug_boot)\ndummy_cols_safety, ref_category_safety = add_dummy_variables('safety', 0, category_order = custom_order_safety)\n\nReference category (dummy omitted): low\nReference category (dummy omitted): low\nReference category (dummy omitted): 2\nReference category (dummy omitted): small\nReference category (dummy omitted): low"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_regularization_cars.html#assembling-predictors",
    "href": "danl-ml/danl_320_logistic_regression_regularization_cars.html#assembling-predictors",
    "title": "Quasi-Separation and Regularized Logistic Regression",
    "section": "Assembling Predictors",
    "text": "Assembling Predictors\n\n# Keep the name assembler_predictors unchanged,\n#   as it will be used as a global variable in the marginal_effects UDF.\nassembler_predictors = (\n    dummy_cols_buying +\n    dummy_cols_maint + dummy_cols_persons + dummy_cols_lug_boot + dummy_cols_safety\n)\n\nassembler_1 = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\n\ndtrain_1 = assembler_1.transform(dtrain)\ndtest_1  = assembler_1.transform(dtest)"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_regularization_cars.html#model-fitting",
    "href": "danl-ml/danl_320_logistic_regression_regularization_cars.html#model-fitting",
    "title": "Quasi-Separation and Regularized Logistic Regression",
    "section": "Model Fitting",
    "text": "Model Fitting\n\n# training the model\nmodel_1 = (\n    GeneralizedLinearRegression(featuresCol=\"predictors\",\n                                labelCol=\"fail\",\n                                family=\"binomial\",\n                                link=\"logit\")\n    .fit(dtrain_1)\n)"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_regularization_cars.html#making-predictions",
    "href": "danl-ml/danl_320_logistic_regression_regularization_cars.html#making-predictions",
    "title": "Quasi-Separation and Regularized Logistic Regression",
    "section": "Making Predictions",
    "text": "Making Predictions\n\n# making prediction on both training and test\ndtrain_1 = model_1.transform(dtrain_1)\ndtest_1 = model_1.transform(dtest_1)\n\ndtrain_1 = dtrain_1.withColumnRenamed(\"prediction\", \"prediction_lr\")\ndtest_1 = dtest_1.withColumnRenamed(\"prediction\", \"prediction_lr\")"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_regularization_cars.html#model-summary",
    "href": "danl-ml/danl_320_logistic_regression_regularization_cars.html#model-summary",
    "title": "Quasi-Separation and Regularized Logistic Regression",
    "section": "Model Summary",
    "text": "Model Summary\n\nmodel_1.summary\n\nCoefficients:\n     Feature Estimate  Std Error T Value P Value\n (Intercept)  58.0307 18024.5736  0.0032  0.9974\n  buying_med   1.2332     0.5087  2.4244  0.0153\n buying_high   4.0189     0.5564  7.2235  0.0000\nbuying_vhigh   6.1115     0.6466  9.4511  0.0000\n   maint_med   0.7337     0.4891  1.5001  0.1336\n  maint_high   3.3288     0.5057  6.5825  0.0000\n maint_vhigh   5.5843     0.6280  8.8919  0.0000\n   persons_4 -31.9209 12536.9296 -0.0025  0.9980\npersons_more -31.5619 12536.9296 -0.0025  0.9980\nlug_boot_med  -2.3314     0.3962 -5.8847  0.0000\nlug_boot_big  -3.7045     0.4693 -7.8938  0.0000\n  safety_med -30.2105 12950.2144 -0.0023  0.9981\n safety_high -32.3235 12950.2144 -0.0025  0.9980\n\n(Dispersion parameter for binomial family taken to be 1.0000)\n   Null deviance: 1508.4768 on 1230 degrees of freedom\nResidual deviance: 291.5923 on 1230 degrees of freedom\nAIC: 317.5923"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_regularization_cars.html#classifier-threshold---double-density-plot",
    "href": "danl-ml/danl_320_logistic_regression_regularization_cars.html#classifier-threshold---double-density-plot",
    "title": "Quasi-Separation and Regularized Logistic Regression",
    "section": "Classifier Threshold - Double Density Plot",
    "text": "Classifier Threshold - Double Density Plot\n\n# Filter training data for atRisk == 1 and atRisk == 0\npdf = dtrain_1.select(\"prediction_lr\", \"fail\").toPandas()\n\ntrain_true = pdf[pdf[\"fail\"] == 1]\ntrain_false = pdf[pdf[\"fail\"] == 0]\n\n# Create the first density plot\nplt.figure(figsize=(8, 6))\nsns.kdeplot(train_true[\"prediction_lr\"], label=\"TRUE\", color=\"red\", fill=True)\nsns.kdeplot(train_false[\"prediction_lr\"], label=\"FALSE\", color=\"blue\", fill=True)\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Density\")\nplt.title(\"Density Plot of Predictions\")\nplt.legend(title=\"fail\")\nplt.show()\n\n# Define threshold for vertical line\nthreshold = 0.79  # Replace with actual value\n\n# Create the second density plot with vertical line\nplt.figure(figsize=(8, 6))\nsns.kdeplot(train_true[\"prediction_lr\"], label=\"TRUE\", color=\"red\", fill=True)\nsns.kdeplot(train_false[\"prediction_lr\"], label=\"FALSE\", color=\"blue\", fill=True)\nplt.axvline(x=threshold, color=\"blue\", linestyle=\"dashed\", label=f\"Threshold = {threshold}\")\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Density\")\nplt.title(\"Density Plot of Predictions with Threshold\")\nplt.legend(title=\"fail\")\nplt.show()"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_regularization_cars.html#performance-of-classifier",
    "href": "danl-ml/danl_320_logistic_regression_regularization_cars.html#performance-of-classifier",
    "title": "Quasi-Separation and Regularized Logistic Regression",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\n\nConfusion Matrix & Performance Metrics\n\n# Compute confusion matrix\ndtest_1 = dtest_1.withColumn(\"predicted_class\", when(col(\"prediction_lr\") &gt; .79, 1).otherwise(0))\nconf_matrix = dtest_1.groupBy(\"fail\", \"predicted_class\").count().orderBy(\"fail\", \"predicted_class\")\n\nTP = dtest_1.filter((col(\"fail\") == 1) & (col(\"predicted_class\") == 1)).count()\nFP = dtest_1.filter((col(\"fail\") == 0) & (col(\"predicted_class\") == 1)).count()\nFN = dtest_1.filter((col(\"fail\") == 1) & (col(\"predicted_class\") == 0)).count()\nTN = dtest_1.filter((col(\"fail\") == 0) & (col(\"predicted_class\") == 0)).count()\n\naccuracy = (TP + TN) / (TP + FP + FN + TN)\nprecision = TP / (TP + FP)\nrecall = TP / (TP + FN)\nspecificity = TN / (TN + FP)\naverage_rate = (TP + FN) / (TP + TN + FP + FN)  # Proportion of actual at-risk babies\nenrichment = precision / average_rate\n\n\n# Print formatted confusion matrix with labels\nprint(\"\\n Confusion Matrix:\\n\")\nprint(\"                     Predicted\")\nprint(\"            |  Negative  |  Positive  \")\nprint(\"------------+------------+------------\")\nprint(f\"Actual Neg. |    {TN:5}   |    {FP:5}  |\")\nprint(\"------------+------------+------------\")\nprint(f\"Actual Pos. |    {FN:5}   |    {TP:5}  |\")\nprint(\"------------+------------+------------\")\n\n\nprint(f\"Accuracy:  {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall (Sensitivity): {recall:.4f}\")\nprint(f\"Specificity:  {specificity:.4f}\")\nprint(f\"Average Rate: {average_rate:.4f}\")\nprint(f\"Enrichment:   {enrichment:.4f} (Relative Precision)\")\n\n\n Confusion Matrix:\n\n                     Predicted\n            |  Negative  |  Positive  \n------------+------------+------------\nActual Neg. |      150   |        1  |\n------------+------------+------------\nActual Pos. |       20   |      314  |\n------------+------------+------------\nAccuracy:  0.9567\nPrecision: 0.9968\nRecall (Sensitivity): 0.9401\nSpecificity:  0.9934\nAverage Rate: 0.6887\nEnrichment:   1.4475 (Relative Precision)\n\n\n\n\nTrade-off Between Recall and Precision/Enrichment\n\npdf = dtest_1.select(\"prediction_lr\", \"fail\").toPandas()\n\n# Extract predictions and true labels\ny_true = pdf[\"fail\"]  # True labels\ny_scores = pdf[\"prediction_lr\"]  # Predicted probabilities\n\n# Compute precision, recall, and thresholds\nprecision_plot, recall_plot, thresholds = precision_recall_curve(y_true, y_scores)\n\n# Compute enrichment: precision divided by average at-risk rate\naverage_rate = np.mean(y_true)\nenrichment_plot = precision_plot / average_rate\n\n# Define optimal threshold (example: threshold where recall ≈ enrichment balance)\nthreshold = 0.79  # Adjust based on the plot\n\n# Plot Enrichment vs. Recall vs. Threshold\nplt.figure(figsize=(8, 6))\nplt.plot(thresholds, enrichment_plot[:-1], label=\"Enrichment\", color=\"blue\", linestyle=\"--\")\nplt.plot(thresholds, recall_plot[:-1], label=\"Recall\", color=\"red\", linestyle=\"-\")\n\n# Add vertical line for chosen threshold\nplt.axvline(x=threshold, color=\"black\", linestyle=\"dashed\", label=f\"Optimal Threshold = {threshold}\")\n\n# Labels and legend\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Score\")\nplt.title(\"Enrichment vs. Recall\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAUC and ROC\n\n# Use probability of the positive class (y=1)\nevaluator = BinaryClassificationEvaluator(labelCol=\"fail\", rawPredictionCol=\"prediction_lr\", metricName=\"areaUnderROC\")\n\n# Evaluate AUC\nauc = evaluator.evaluate(dtest_1)\n\nprint(f\"AUC: {auc:.4f}\")  # Higher is better (closer to 1)\n\n# Convert to Pandas\npdf = dtest_1.select(\"prediction_lr\", \"fail\").toPandas()\n\n# Compute ROC curve\nfpr, tpr, _ = roc_curve(pdf[\"fail\"], pdf[\"prediction_lr\"])\n\n# Plot ROC curve\nplt.figure(figsize=(8,6))\nplt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc:.4f})\")\nplt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\nplt.show()\n\nAUC: 0.9906"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_regularization_cars.html#ridge-regression-l2-penalty",
    "href": "danl-ml/danl_320_logistic_regression_regularization_cars.html#ridge-regression-l2-penalty",
    "title": "Quasi-Separation and Regularized Logistic Regression",
    "section": "Ridge Regression (L2 Penalty)",
    "text": "Ridge Regression (L2 Penalty)\n\n# LogisticRegressionCV automatically selects the best regularization strength.\nridge_cv = LogisticRegressionCV(\n    Cs=100, cv=5, penalty='l2', solver='lbfgs', max_iter=1000, scoring='neg_log_loss'\n)\nridge_cv.fit(X_train, y_train)\n\nprint(\"Ridge Regression - Best C (inverse of regularization strength):\", ridge_cv.C_[0])\nintercept = float(ridge_cv.intercept_)\ncoef_ridge = pd.DataFrame({\n    'predictor': list(X_train.columns),\n    'coefficient': list(ridge_cv.coef_[0])\n})\nprint(\"Ridge Regression Coefficients:\")\nprint(coef_ridge)\n\n# Force an order for the y-axis (using the feature names as they appear in coef_ridge)\norder = coef_ridge['predictor'].tolist()\n\nplt.figure(figsize=(8,6))\nax = sns.pointplot(x=\"coefficient\", y=\"predictor\", data=coef_ridge, order=order, join=False)\nplt.title(\"Coefficients of Ridge Logistic Regression Model\")\nplt.xlabel(\"Coefficient value\")\nplt.ylabel(\"Predictor\")\n\n# Draw horizontal lines from 0 to each coefficient.\nfor _, row in coef_ridge.iterrows():\n    # Get the y-axis position from the order list.\n    y_pos = order.index(row['predictor'])\n    plt.hlines(y=y_pos, xmin=0, xmax=row['coefficient'], color='gray', linestyle='--')\n\n# Draw a vertical line at 0.\nplt.axvline(0, color='black', linestyle='--')\nplt.show()\n\n# Prediction and evaluation for ridge model\ny_pred_prob_ridge = ridge_cv.predict_proba(X_test)[:, 1]\ny_pred_ridge = (y_pred_prob_ridge &gt; 0.5).astype(int)\nctab_ridge = confusion_matrix(y_test, y_pred_ridge)\naccuracy_ridge = accuracy_score(y_test, y_pred_ridge)\nprecision_ridge = precision_score(y_test, y_pred_ridge)\nrecall_ridge = recall_score(y_test, y_pred_ridge)\nauc_ridge = roc_auc_score(y_test, y_pred_prob_ridge)\n\nprint(\"Confusion Matrix (Ridge):\\n\", ctab_ridge)\nprint(\"Ridge Accuracy:\", accuracy_ridge)\nprint(\"Ridge Precision:\", precision_ridge)\nprint(\"Ridge Recall:\", recall_ridge)\n\n\n# Plot ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_ridge)\nplt.figure(figsize=(8,6))\nplt.plot(fpr, tpr, label=f'Ridge (AUC = {auc_ridge:.2f})')\nplt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Ridge Logistic Regression Model')\nplt.legend(loc='best')\nplt.show()\n\nRidge Regression - Best C (inverse of regularization strength): 25.950242113997373\nRidge Regression Coefficients:\n         predictor  coefficient\n0      buying_high     2.088163\n1       buying_low    -3.290036\n2       buying_med    -1.826384\n3     buying_vhigh     3.870633\n4       maint_high     1.410526\n5        maint_low    -2.232305\n6        maint_med    -2.169268\n7      maint_vhigh     3.833423\n8          doors_2     1.572662\n9          doors_3     0.197084\n10         doors_4    -0.455927\n11     doors_5more    -0.471443\n12       persons_2     9.931083\n13       persons_4    -4.765592\n14    persons_more    -4.323115\n15    lug_boot_big    -1.346707\n16    lug_boot_med    -0.209493\n17  lug_boot_small     2.398576\n18     safety_high    -5.770078\n19      safety_low     9.416235\n20      safety_med    -2.803781\n\n\n\n\n\n\n\n\n\nConfusion Matrix (Ridge):\n [[156  16]\n [  8 339]]\nRidge Accuracy: 0.953757225433526\nRidge Precision: 0.9549295774647887\nRidge Recall: 0.9769452449567724"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_regularization_cars.html#lasso-regression-l1-penalty",
    "href": "danl-ml/danl_320_logistic_regression_regularization_cars.html#lasso-regression-l1-penalty",
    "title": "Quasi-Separation and Regularized Logistic Regression",
    "section": "Lasso Regression (L1 Penalty)",
    "text": "Lasso Regression (L1 Penalty)\n\n# Note: solver='saga' supports L1 regularization.\nlasso_cv = LogisticRegressionCV(\n    Cs=100, cv=5, penalty='l1', solver='saga', max_iter=1000, scoring='neg_log_loss'\n)\nlasso_cv.fit(X_train, y_train)\n\nintercept = float(lasso_cv.intercept_)\ncoef_lasso = pd.DataFrame({\n    'predictor': list(X_train.columns),\n    'coefficient': list(lasso_cv.coef_[0])\n})\n\nprint(\"Lasso Regression Coefficients:\")\nprint(coef_lasso)\n\n# Force an order for the y-axis (using the feature names as they appear in coef_lasso)\norder = coef_lasso['predictor'].tolist()\n\nplt.figure(figsize=(8,6))\nax = sns.pointplot(x=\"coefficient\", y=\"predictor\", data=coef_lasso, order=order, join=False)\nplt.title(\"Coefficients of Lasso Logistic Regression Model\")\nplt.xlabel(\"Coefficient value\")\nplt.ylabel(\"Predictor\")\n\n# Draw horizontal lines from 0 to each coefficient.\nfor _, row in coef_lasso.iterrows():\n    # Get the y-axis position from the order list.\n    y_pos = order.index(row['predictor'])\n    plt.hlines(y=y_pos, xmin=0, xmax=row['coefficient'], color='gray', linestyle='--')\n\n# Draw a vertical line at 0.\nplt.axvline(0, color='black', linestyle='--')\n\nplt.show()\n\n# Prediction and evaluation for lasso model\ny_pred_prob_lasso = lasso_cv.predict_proba(X_test)[:, 1]\ny_pred_lasso = (y_pred_prob_lasso &gt; 0.5).astype(int)\nctab_lasso = confusion_matrix(y_test, y_pred_lasso)\naccuracy_lasso = accuracy_score(y_test, y_pred_lasso)\nprecision_lasso = precision_score(y_test, y_pred_lasso)\nrecall_lasso = recall_score(y_test, y_pred_lasso)\nauc_lasso = roc_auc_score(y_test, y_pred_prob_lasso)\n\nprint(\"Confusion Matrix (Lasso):\\n\", ctab_lasso)\nprint(\"Lasso Accuracy:\", accuracy_lasso)\nprint(\"Lasso Precision:\", precision_lasso)\nprint(\"Lasso Recall:\", recall_lasso)\n\n\n# Plot ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_lasso)\nplt.figure(figsize=(8,6))\nplt.plot(fpr, tpr, label=f'Lasso (AUC = {auc_lasso:.2f})')\nplt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Lasso Logistic Regression Model')\nplt.legend(loc='best')\nplt.show()\n\nLasso Regression Coefficients:\n         predictor  coefficient\n0      buying_high     1.787892\n1       buying_low    -3.593632\n2       buying_med    -2.081475\n3     buying_vhigh     3.567813\n4       maint_high     1.225046\n5        maint_low    -2.391010\n6        maint_med    -2.326250\n7      maint_vhigh     3.654059\n8          doors_2     1.515634\n9          doors_3     0.133811\n10         doors_4    -0.473203\n11     doors_5more    -0.488640\n12       persons_2    14.899017\n13       persons_4    -0.422268\n14    persons_more     0.000000\n15    lug_boot_big    -1.138295\n16    lug_boot_med     0.000000\n17  lug_boot_small     2.590292\n18     safety_high    -2.953921\n19      safety_low    12.819173\n20      safety_med     0.000000\n\n\n\n\n\n\n\n\n\nConfusion Matrix (Lasso):\n [[156  16]\n [  9 338]]\nLasso Accuracy: 0.9518304431599229\nLasso Precision: 0.9548022598870056\nLasso Recall: 0.9740634005763689"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_regularization_cars.html#elastic-net-regression",
    "href": "danl-ml/danl_320_logistic_regression_regularization_cars.html#elastic-net-regression",
    "title": "Quasi-Separation and Regularized Logistic Regression",
    "section": "Elastic Net Regression",
    "text": "Elastic Net Regression\n\n# LogisticRegressionCV supports elastic net penalty with solver 'saga'.\n# l1_ratio specifies the mix between L1 and L2 (0 = ridge, 1 = lasso).\nenet_cv = LogisticRegressionCV(\n    Cs=10, cv=5, penalty='elasticnet', solver='saga',\n    l1_ratios=[0.5, 0.7, 0.9], max_iter=1000, scoring='neg_log_loss'\n)\nenet_cv.fit(X_train, y_train)\n\nprint(\"Elastic Net Regression - Best C:\", enet_cv.C_[0])\nprint(\"Elastic Net Regression - Best l1 ratio:\", enet_cv.l1_ratio_[0])\n\nintercept = float(enet_cv.intercept_)\ncoef_enet = pd.DataFrame({\n    'predictor': list(X_train.columns),\n    'coefficient': list(enet_cv.coef_[0])\n})\nprint(\"Elastic Net Regression Coefficients:\")\nprint(coef_enet)\n\n\n# Force an order for the y-axis (using the feature names as they appear in coef_lasso)\norder = coef_enet['predictor'].tolist()\n\nplt.figure(figsize=(8,6))\nax = sns.pointplot(x=\"coefficient\", y=\"predictor\", data=coef_enet, order=order, join=False)\nplt.title(\"Coefficients of Elastic Net Logistic Regression Model\")\nplt.xlabel(\"Coefficient value\")\nplt.ylabel(\"Predictor\")\n\n# Draw horizontal lines from 0 to each coefficient.\nfor _, row in coef_enet.iterrows():\n    # Get the y-axis position from the order list.\n    y_pos = order.index(row['predictor'])\n    plt.hlines(y=y_pos, xmin=0, xmax=row['coefficient'], color='gray', linestyle='--')\n\n# Draw a vertical line at 0.\nplt.axvline(0, color='black', linestyle='--')\n\nplt.show()\n\n# Prediction and evaluation for elastic net model\ny_pred_prob_enet = enet_cv.predict_proba(X_test)[:, 1]\ny_pred_enet = (y_pred_prob_enet &gt; 0.5).astype(int)\nctab_enet = confusion_matrix(y_test, y_pred_enet)\naccuracy_enet = accuracy_score(y_test, y_pred_enet)\nprecision_enet = precision_score(y_test, y_pred_enet)\nrecall_enet = recall_score(y_test, y_pred_enet)\n\nprint(\"Confusion Matrix (Elastic Net):\\n\", ctab_enet)\nprint(\"Elastic Net Accuracy:\", accuracy_enet)\nprint(\"Elastic Net Precision:\", precision_enet)\nprint(\"Elastic Net Recall:\", recall_enet)\n\n\n# Plot ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_enet)\nplt.figure(figsize=(8,6))\nplt.plot(fpr, tpr, label=f'Lasso (AUC = {auc_ridge:.2f})')\nplt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Elastic Net Logistic Regression Model')\nplt.legend(loc='best')\nplt.show()\n\nElastic Net Regression - Best C: 21.54434690031882\nElastic Net Regression - Best l1 ratio: 0.5\nElastic Net Regression Coefficients:\n         predictor  coefficient\n0        Intercept     5.607714\n1      buying_high     2.155057\n2       buying_low    -3.449854\n3       buying_med    -1.896770\n4     buying_vhigh     4.001764\n5       maint_high     1.448343\n6        maint_low    -2.337030\n7        maint_med    -2.269264\n8      maint_vhigh     3.968147\n9          doors_2     1.628000\n10         doors_3     0.208917\n11         doors_4    -0.471257\n12     doors_5more    -0.490440\n13       persons_2    10.914849\n14       persons_4    -4.779511\n15    persons_more    -4.325142\n16    lug_boot_big    -1.210976\n17    lug_boot_med    -0.008686\n18  lug_boot_small     2.681634\n19     safety_high    -5.815110\n20      safety_low    10.355200\n21      safety_med    -2.729894\n\n\n\n\n\n\n\n\n\nConfusion Matrix (Elastic Net):\n [[156  16]\n [  8 339]]\nElastic Net Accuracy: 0.953757225433526\nElastic Net Precision: 0.9549295774647887\nElastic Net Recall: 0.9769452449567724"
  },
  {
    "objectID": "danl-cw/danl-320-cw-02.html",
    "href": "danl-cw/danl-320-cw-02.html",
    "title": "Classwork 2",
    "section": "",
    "text": "Markdown is a lightweight markup language with plain-text formatting syntax. Its main goal is to be readable and easy to write, even when viewed as plain text. Markdown is widely used for creating formatted text on the web and in various applications such as Quarto.\n\n\n\n\nHeadings are created by adding one or more # symbols before your heading text. The number of # symbols indicates the level of the heading.\n# Heading 1\n## Heading 2\n### Heading 3\n\n\n\nYou can make text bold by wrapping it with two asterisks **, and italic by using one asterisk *.\n*italic* or _italic_\n**bold** or __bold__\n\n\n\nUnordered lists are created using *, -, or +, while ordered lists are numbered.\n- Item 1\n- Item 2\n  - Subitem 2.1\n  - Subitem 2.2\n1. First item\n2. Second item\n\n\n\nLinks are created using [Link Text](URL)\n[DANL 320](https://bcdanl.github.io/320)\n\n\n\nImages are created using ![Alt Text](Image URL).\n![Geneseo Logo](https://bcdanl.github.io/img/geneseo-logo.gif)\n\n\n\n\n\n\n&gt; Be yourself. Everyone else is already taken. - Oscar Wilde.\n\n\n\n\nA ton of markdown emojis are available here 😄 (:smile:)\n\nhttps://github.com/ikatyang/emoji-cheat-sheet\n\n\n\n\n\nCode blocks are created by using triple backticks (```). Optionally, you can specify the language for syntax highlighting.\n```\n\"string\"\n```\n```python\n# Python code block\nimport numpy as np\n```\n\n\n\n\n\nDo the following tasks on this Classwork 2 Discussion Board:\n\nBasic Syntax: Write a comment with a heading, an unordered list, an ordered list, a link, and an image.\nAdvanced Syntax: Write a comment that includes a Python code block, a blockquote, and an emoji.\n\n\n\n\n\n\nQuarto Markdown Basics\nStart writing on GitHub"
  },
  {
    "objectID": "danl-cw/danl-320-cw-02.html#basic-syntax",
    "href": "danl-cw/danl-320-cw-02.html#basic-syntax",
    "title": "Classwork 2",
    "section": "",
    "text": "Headings are created by adding one or more # symbols before your heading text. The number of # symbols indicates the level of the heading.\n# Heading 1\n## Heading 2\n### Heading 3\n\n\n\nYou can make text bold by wrapping it with two asterisks **, and italic by using one asterisk *.\n*italic* or _italic_\n**bold** or __bold__\n\n\n\nUnordered lists are created using *, -, or +, while ordered lists are numbered.\n- Item 1\n- Item 2\n  - Subitem 2.1\n  - Subitem 2.2\n1. First item\n2. Second item\n\n\n\nLinks are created using [Link Text](URL)\n[DANL 320](https://bcdanl.github.io/320)\n\n\n\nImages are created using ![Alt Text](Image URL).\n![Geneseo Logo](https://bcdanl.github.io/img/geneseo-logo.gif)"
  },
  {
    "objectID": "danl-cw/danl-320-cw-02.html#advanced-syntax",
    "href": "danl-cw/danl-320-cw-02.html#advanced-syntax",
    "title": "Classwork 2",
    "section": "",
    "text": "&gt; Be yourself. Everyone else is already taken. - Oscar Wilde.\n\n\n\n\nA ton of markdown emojis are available here 😄 (:smile:)\n\nhttps://github.com/ikatyang/emoji-cheat-sheet\n\n\n\n\n\nCode blocks are created by using triple backticks (```). Optionally, you can specify the language for syntax highlighting.\n```\n\"string\"\n```\n```python\n# Python code block\nimport numpy as np\n```"
  },
  {
    "objectID": "danl-cw/danl-320-cw-02.html#practice-problems",
    "href": "danl-cw/danl-320-cw-02.html#practice-problems",
    "title": "Classwork 2",
    "section": "",
    "text": "Do the following tasks on this Classwork 2 Discussion Board:\n\nBasic Syntax: Write a comment with a heading, an unordered list, an ordered list, a link, and an image.\nAdvanced Syntax: Write a comment that includes a Python code block, a blockquote, and an emoji."
  },
  {
    "objectID": "danl-cw/danl-320-cw-02.html#references",
    "href": "danl-cw/danl-320-cw-02.html#references",
    "title": "Classwork 2",
    "section": "",
    "text": "Quarto Markdown Basics\nStart writing on GitHub"
  },
  {
    "objectID": "danl-cw/danl-320-cw-01.html",
    "href": "danl-cw/danl-320-cw-01.html",
    "title": "Classwork 1",
    "section": "",
    "text": "Getting a GitHub account\nStep 1. Create the GitHub account with your Geneseo email.\n\nGo to GitHub.\nClick “Sign up for GitHub”.\n\n\nChoose your GitHub username carefully:\n\nhttps://USERNAME.github.io will be the address for your website.\nByeong-Hak’s GitHub username is bcdanl, so that Byeong-Hak owns the web address https://bcdanl.github.io.\n\nIt is recommended to have a username with all lower cases.\n\n\n\n\n\nInstalling git if you do not have one.\nStep 2.\n\nCheck whether git is installed in your laptop.\n\n\nFrom the Console Pane in RStudio, click Terminal tab.\n\n\n\n\n\nFrom the Terminal, run the following command to check if your laptop has git installed.\n\ngit --version\n\nIf your computer has git installed, you will see the message below and you do not need to install git:\n\ngit version 2.xx\n\nIf your computer does not have git installed, you will see the message below and you need to install git:\n\n'git' is not recognized as an internal or external command\n\n\nInstall git if you do not have one. Move to the next step if you have git installed in your laptop.\n\n\n\n\nMac\n\nGo to http://git-scm.com/downloads, and download the file.\nClick “macOS”, scroll down the webpage, and then click “installer” from the Binary installer section.\nRun the downloaded file.\n\n\n\n\nWindows\n\nGo to https://gitforwindows.org, and download the file.\nRun the downloaded file.\n\n\n\n\n\nKeep clicking “Next” to complete the installation of git.\nAfter the git installation is done, close RStudio and re-open it.\n\n\nHow to open git installation file on Mac?\n\nRun the downloaded file.\nClick Okay\nGo to “Setting” &gt; “Privacy and Security”\nGo to “General” or scroll down\nClick “Open Anyway”\n\n\n\n\n\n\n\n\nSetting up GitHub Credential on your local Git.\nStep 3. In Terminal, run the following commands one by one:\ngit config --global user.email \"YOUR_EMAIL_ADDRESS\"\ngit config --global user.name \"YOUR_USERNAME\"\nFor example, the email address for my GitHub account is bchoe@geneseo.edu, and my GitHub username is bcdanl, so that I ran below:\ngit config --global user.email \"bchoe@geneseo.edu\"\ngit config --global user.name \"bcdanl\"\n\nStep 4. Obtain a personal access token (PAT) from GitHub.\n\nIn RStudio Console, run the followings line by line:\n\ninstall.packages(\"usethis\")\nusethis::create_github_token()\n\nThen, click “Generate token” in the pop-upped web browser.\nWe can think of GitHub’s personal access token as a password that expires. You can decide how long it remains valid. My recommendation is to set its expiration for May 31, 2025, or later.\n\n\n\n\n\nThen, copy the generated PAT, and paste it to your clipboard or R script.\n\n\nStep 5. Set the GitHub credential using the PAT.\n\nIn RStudio Console, run the followings line by line:\n\ninstall.packages(\"gitcreds\")\ngitcreds::gitcreds_set()\n\nYou will be asked to provide your PAT.\nPaste your PAT to the RStudio Console, and then hit Enter.\n\n\n\n\n\n\n\nNote\n\n\n\n\nIt does not harm to create multiple PAT for one GitHub account.\nAfter the PAT expires, you should repeat the following if you want to update your GitHub website:\n\n\nCreate a new PAT:\n\nusethis::create_github_token()\n\nReplace the current PAT with the new PAT:\n\ngitcreds::gitcreds_set()\n\nSelect the option 2: Replace these credentials by typing 2 and hitting Enter on R Console.\n\n\n\n\n\n\nEstablishing the Connection between GitHub repo and your local Git\nStep 6. Login to your GitHib and make the repository.\n\nFrom https://github.com, click the plus [+] icon in the upper right corner and select “New repository”.\nName this repo USERNAME.github.io, which will be the domain for your website.\n\n\ne.g., If your GitHub username is abc9, the name of your repo should be abc9.github.io, not abc_9.github.io.\n\n\nThen, copy the web address of your GitHub repo, https://github.com/USERNAME/USERNAME.github.io\n\n\nFor example, the web address for Byeong-Hak’s GitHub repo is https://github.com/bcdanl/bcdanl.github.io.\n\n\nStep 7. Create a RStudio project with Version Control\n\n\n\n\nClick “Project (None)” at the top-right corner in RStudio.\nClick “New Project” &gt; “Version Control” &gt; “Git”\nPaste the web address of your GitHub repo to the Repository URL menu.\nClick “Browse” to select the parent directory for your local project directory (I recommend “Documents” folder.)\nClick “Create”\n\n\n\n\n\n\n\nNote\n\n\n\nIf Step 7 does not work on your laptop, try below Steps 7-1 and 7-2 instead. If Step 7 DOES work well, skip Steps 7-1 and 7-2.\n\n\nStep 7-1. Use git clone to establish the connection between GitHub repo and your local laptop:\n\nChange directory to “Documents” in Terminal using cd command.\n\ncd &lt;pathname of \"Documents\" directory&gt;\n\nHere, you need to know the pathname of “Documents” directory.\nFor example, LAPTOP_USERNAME below is not your GitHub username but one for your local laptop.\n\nMac\ncd /Users/LAPTOP_USERNAME/Documents\nWindows\ncd C:/Users/LAPTOP_USERNAME/Documents\n\nUse git clone to creates a local copy of the GitHub Repository.\n\ngit clone &lt;repository-url&gt;\n\nFor example,\n\ngit clone https://github.com/USERNAME/USERNAME.github.io\n\nStep 7-2. Create a RStudio project from Existing Directory\n\nClick “Project (None)” at the top-right corner in RStudio.\nClick “New Project” &gt; “Existing Directory”\nClick “Browse” to select the local copy of the GitHub Repository\nClick “Create Project”\n\n\n\n\nDownloading Website Template Files\nStep 8. Download the files of website template:\n\nGo to the following webpage: https://github.com/bcdanl/danl-website-template\nFrom the webpage above, click the green icon &lt; &gt; Code, and then click “Download Zip”\nExtract the Zip file you have downloaded\nIf there are the files, .gitignore, .DS_Store, or *.Rproj, in the folder, delete all of them.\nMove all the files that were compressed in the Zip file to your local project directory, USERNAME.github.io.\n\n\nSelect all the files in the danl-website-template folder (Tip: Ctrl + A (Windows) / command + A (Mac) selects all files in a directory).\nThen, Ctrl + C (Windows) / command + C (Mac) to copy them.\nThen, go to your local project directory USERNAME.github.io.\nThen, Ctrl + V (Windows) / command + V (Mac) to paste them to your local project directory USERNAME.github.io.\n\n\nRemove the danl-website-template directory from your local project directory, if you have one.\n\n\nAll the website files should be located at the same level with the R Project file (USERNAME.github.io.Rproj), shown below.\n\n\n\n\n\n\n\nPushing the Website Files to the GitHub repository\n\n\n\nStep 8. Push the files to your GitHub repository\n\nOn Terminal within RStudio, execute the following 3-step git commands, which will stage, commit, and push all the files in the local working directory to your GitHub repository:\n\n\ngit add . adds changes in your local working directory (e.g., edited files, new files, deleted files) to the staging area, which is a temporary area where you can prepare your next commit\n\ngit add .\n\ngit commit -m \"...\" records the changes in the staging area as a new snapshot in the local working directory, along with a message describing the changes.\n\ngit commit -m \"any message to describe the changes\"\n\ngit push uploads the local changes to the online repository in GitHub.\n\ngit push\n\nStep 9. Check whether the files are well uploaded.\n\nGo to the webpages of your GitHub repository and your website:\n\nhttps://github.com/USERNAME/USERNAME.github.io.git\nhttps://USERNAME.github.io\nRefresh the webpages (Ctrl + R for Windows users; cmd + R for Mac users)\n\nAdd a URL for your website (https://YOUR_GITHUB_USERNAME.github.io/) in About section in your GihtHub repository webpage by clicking the setting. Below describes how to do it:\n\n\n\n\nDiscussion\nWelcome to our Classwork 1 Discussion Board! 👋 \nThis space is designed for you to engage with your classmates about the material covered in Classwork 1.\nWhether you are looking to delve deeper into the content, share insights, or have questions about the content, this is the perfect place for you.\nIf you have any specific questions for Byeong-Hak (@bcdanl) regarding the Classwork 1 materials or need clarification on any points, don’t hesitate to ask here.\nAll comments will be stored here.\nLet’s collaborate and learn from each other!\n\n\n\n\n Back to top"
  },
  {
    "objectID": "danl-cw/danl-320-cw-10.html",
    "href": "danl-cw/danl-320-cw-10.html",
    "title": "Classwork 10",
    "section": "",
    "text": "# Below is for an interactive display of Pandas DataFrame in Colab\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\nimport pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_curve\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n\n\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\n\n# Increase figure size to prevent overlapping\nplt.figure(figsize=(10, 6))\n\n# Plot using the DataFrame columns\nplt.errorbar(df_ME[\"Variable\"], df_ME[\"Marginal Effect\"],\n             yerr=1.96 * df_ME[\"Std. Error\"], fmt='o', capsize=5)\n\n# Labels and title\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Marginal Effect\")\nplt.title(\"Marginal Effect at the Mean\")\n\n# Add horizontal line at 0 for reference\nplt.axhline(0, color=\"red\", linestyle=\"--\")\n\n# Adjust x-axis labels to avoid overlap\nplt.xticks(rotation=45, ha=\"right\")  # Rotate and align labels to the right\nplt.tight_layout()  # Adjust layout to prevent overlap\n\n# Show plot\nplt.show()"
  },
  {
    "objectID": "danl-cw/danl-320-cw-10.html#required-libraries-and-sparksession-entry-point",
    "href": "danl-cw/danl-320-cw-10.html#required-libraries-and-sparksession-entry-point",
    "title": "Classwork 10",
    "section": "",
    "text": "# Below is for an interactive display of Pandas DataFrame in Colab\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\nimport pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_curve\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
  },
  {
    "objectID": "danl-cw/danl-320-cw-10.html#udf-for-adding-dummy-variables",
    "href": "danl-cw/danl-320-cw-10.html#udf-for-adding-dummy-variables",
    "title": "Classwork 10",
    "section": "",
    "text": "def add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)"
  },
  {
    "objectID": "danl-cw/danl-320-cw-10.html#marginal-effect-plot",
    "href": "danl-cw/danl-320-cw-10.html#marginal-effect-plot",
    "title": "Classwork 10",
    "section": "",
    "text": "# Increase figure size to prevent overlapping\nplt.figure(figsize=(10, 6))\n\n# Plot using the DataFrame columns\nplt.errorbar(df_ME[\"Variable\"], df_ME[\"Marginal Effect\"],\n             yerr=1.96 * df_ME[\"Std. Error\"], fmt='o', capsize=5)\n\n# Labels and title\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Marginal Effect\")\nplt.title(\"Marginal Effect at the Mean\")\n\n# Add horizontal line at 0 for reference\nplt.axhline(0, color=\"red\", linestyle=\"--\")\n\n# Adjust x-axis labels to avoid overlap\nplt.xticks(rotation=45, ha=\"right\")  # Rotate and align labels to the right\nplt.tight_layout()  # Adjust layout to prevent overlap\n\n# Show plot\nplt.show()"
  },
  {
    "objectID": "danl-cw/danl-320-cw-10.html#variable-description",
    "href": "danl-cw/danl-320-cw-10.html#variable-description",
    "title": "Classwork 10",
    "section": "Variable description",
    "text": "Variable description\n\n\n\n\n\n\n\n\nVariable\nType\nDescription\n\n\n\n\natRisk\nBoolean\nTRUE (1) if 5-minute Apgar score &lt; 7; FALSE (0) otherwise\n\n\nPWGT\nNumeric\nMother’s prepregnancy weight\n\n\nUPREVIS\nNumeric (integer)\nNumber of prenatal medical visits\n\n\nCIG_REC\nBoolean\nTRUE (1) if smoker; FALSE (0) otherwise\n\n\nGESTREC3\nCategorical\nTwo categories: &lt;37 weeks (premature) and &gt;=37 weeks\n\n\nDPLURAL\nCategorical\nBirth plurality, three categories: single/twin/triplet+\n\n\nULD_MECO\nBoolean\nTRUE (1) if moderate/heavy fecal staining of amniotic fluid\n\n\nULD_PRECIP\nBoolean\nTRUE (1) for unusually short labor (&lt; three hours)\n\n\nULD_BREECH\nBoolean\nTRUE (1) for breech (pelvis first) birth position\n\n\nURF_DIAB\nBoolean\nTRUE (1) if mother is diabetic\n\n\nURF_CHYPER\nBoolean\nTRUE (1) if mother has chronic hypertension\n\n\nURF_PHYPER\nBoolean\nTRUE (1) if mother has pregnancy-related hypertension\n\n\nURF_ECLAM\nBoolean\nTRUE (1) if mother experienced eclampsia: pregnancy-related seizures"
  },
  {
    "objectID": "danl-cw/danl-320-cw-05.html",
    "href": "danl-cw/danl-320-cw-05.html",
    "title": "Classwork 5",
    "section": "",
    "text": "Direction\n\n\n\nThe nfl.csv file contains a list of players in the National Football League with similar Name, Team, Position, Birthday, and Salary variables in the nba.csv file.\n\nimport pandas as pd\nfrom pyspark.sql import SparkSession\ndf = pd.read_csv(\"https://bcdanl.github.io/data/nfl.csv\")\nnfl = spark.createDataFrame(df)\n\n\n\n\n\n\n\n\n\n\nQuestion 1\n\nHow can we read the nfl.csv file, and assign it to a PySpark DataFrame object, nfl?\n\nAnswer:\n\n\n\nQuestion 2\n\nHow many observations are in nfl?\nWhat are the mean, standard deviation, minimum, and maximum of Salary in nfl?\n\nAnswer:\n\n\n\nQuestion 3\n\nHow can we count the number of players per team in nfl?\nHow many unique teams are in nfl?\n\nAnswer:\n\n\n\nQuestion 4\n\nWhat is an effective way to convert the values in its Birthday variable to date?\n\nThe format of Birthday is “M/d/yy”\n\n\nAnswer:\n\n\n\nQuestion 5\n\nWho are the five highest-paid players?\nWho is the oldest player?\n\nAnswer:\n\n\n\nQuestion 6\nHow can we sort the DataFrame first by Team in alphabetical order and then by Salary in descending order?\nAnswer:\n\n\n\nQuestion 7\nWho is the oldest player on the Kansas City Chiefs roster, and what is his birthday?\nAnswer:\n\n\n\nQuestion 8\n\nWhat is the median of Salary in nfl?\n\nAnswer:\n\n\n\nDiscussion\nWelcome to our Classwork 5 Discussion Board! 👋 \nThis space is designed for you to engage with your classmates about the material covered in Classwork 5.\nWhether you are looking to delve deeper into the content, share insights, or have questions about the content, this is the perfect place for you.\nIf you have any specific questions for Byeong-Hak (@bcdanl) regarding the Classwork 5 materials or need clarification on any points, don’t hesitate to ask here.\nAll comments will be stored here.\nLet’s collaborate and learn from each other!\n\n\n\n\n Back to top"
  },
  {
    "objectID": "danl-cw/danl-320-cw-06.html",
    "href": "danl-cw/danl-320-cw-06.html",
    "title": "Classwork 6",
    "section": "",
    "text": "The netflix.csv file (with its pathname https://bcdanl.github.io/data/netflix.csv) contains a list of 6,000 titles that were available to watch in November 2019 on the video streaming service Netflix. It includes four variables: the video’s title, director, the date Netflix added it (date_added), and its type (category).\n\nimport pandas as pd\nfrom pyspark.sql import SparkSession\ndf = pd.read_csv(\"https://bcdanl.github.io/data/netflix.csv\")\ndf = df.where(pd.notnull(df), None)\nnetflix = spark.createDataFrame(df)"
  },
  {
    "objectID": "danl-cw/danl-320-cw-06.html#direction",
    "href": "danl-cw/danl-320-cw-06.html#direction",
    "title": "Classwork 6",
    "section": "",
    "text": "The netflix.csv file (with its pathname https://bcdanl.github.io/data/netflix.csv) contains a list of 6,000 titles that were available to watch in November 2019 on the video streaming service Netflix. It includes four variables: the video’s title, director, the date Netflix added it (date_added), and its type (category).\n\nimport pandas as pd\nfrom pyspark.sql import SparkSession\ndf = pd.read_csv(\"https://bcdanl.github.io/data/netflix.csv\")\ndf = df.where(pd.notnull(df), None)\nnetflix = spark.createDataFrame(df)"
  },
  {
    "objectID": "danl-cw/danl-320-cw-06.html#question-1",
    "href": "danl-cw/danl-320-cw-06.html#question-1",
    "title": "Classwork 6",
    "section": "Question 1",
    "text": "Question 1\n\nOptimize the DataFrame for limited memory use and maximum utility by using the cast() method.\n\nThe format of date_added is “dd-MMM-yy”.\n\n\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-06.html#question-2",
    "href": "danl-cw/danl-320-cw-06.html#question-2",
    "title": "Classwork 6",
    "section": "Question 2",
    "text": "Question 2\nFind all observations with a director of “Martin Scorsese”.\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-06.html#question-3",
    "href": "danl-cw/danl-320-cw-06.html#question-3",
    "title": "Classwork 6",
    "section": "Question 3",
    "text": "Question 3\nFind all observations with a title of “Limitless” and a type of “Movie”.\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-06.html#question-4",
    "href": "danl-cw/danl-320-cw-06.html#question-4",
    "title": "Classwork 6",
    "section": "Question 4",
    "text": "Question 4\nFind all observations with either a date_added of “2018-06-15” or a director of “Bong Joon Ho”.\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-06.html#question-5",
    "href": "danl-cw/danl-320-cw-06.html#question-5",
    "title": "Classwork 6",
    "section": "Question 5",
    "text": "Question 5\nFind all observations with a director of “Ethan Coen”, “Joel Coen”, and “Quentin Tarantino”.\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-06.html#question-6",
    "href": "danl-cw/danl-320-cw-06.html#question-6",
    "title": "Classwork 6",
    "section": "Question 6",
    "text": "Question 6\nFind all observations with a date_added value between January 1, 2019 and February 1, 2019.\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-06.html#question-7",
    "href": "danl-cw/danl-320-cw-06.html#question-7",
    "title": "Classwork 6",
    "section": "Question 7",
    "text": "Question 7\nDrop all observations with a NULL value in the director variable.\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-06.html#question-8",
    "href": "danl-cw/danl-320-cw-06.html#question-8",
    "title": "Classwork 6",
    "section": "Question 8",
    "text": "Question 8\nIdentify the days when Netflix added only one movie to its catalog.\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-08.html",
    "href": "danl-cw/danl-320-cw-08.html",
    "title": "Classwork 8",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, when, log\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n\n\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                ci_upper,\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: Exp(95% CI Lower), 8: 95% CI Upper, 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"Exp(95% CI Lower)\", \"95% CI Upper\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))\n\n\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n    \n  \n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\n\n\nterms = assembler3.getInputCols()\ncoefs = model3.coefficients.toArray()[:len(terms)]\nstdErrs = model3.summary.coefficientStandardErrors[:len(terms)]\n\ndf_summary = pd.DataFrame({\n    \"term\": terms,\n    \"estimate\": coefs,\n    \"std_error\": stdErrs\n})\n\n# Filter df_summary if needed\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary[\"term\"], df_summary[\"estimate\"],\n             yerr = 1.96 * df_summary[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates (Model 2)\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n# Convert test predictions to Pandas\ndfpd = TEST_DATAFRAME.select([\"prediction\", \"Y_VARIABLE\"]).toPandas()\ndfpd[\"residual\"] = dfpd[\"Y_VARIABLE\"] - dfpd[\"prediction\"]\nplt.scatter(dfpd[\"prediction\"], dfpd[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n# Use lowess smoothing for the trend line\nsmoothed = sm.nonparametric.lowess(dfpd[\"residual\"], dfpd[\"prediction\"])\nplt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\nplt.axhline(y=0, color=\"red\", linestyle=\"--\")\nplt.xlabel(\"Predicted y (Model)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residual Plot for Model\")\nplt.show()\n\n\n\n\n# Create a histogram\ndfpd = DATAFRAME.select([\"Y_VARIABLE\"]).toPandas()\nsns.histplot(dfpd[\"Y_VARIABLE\"], bins=10, kde=True)"
  },
  {
    "objectID": "danl-cw/danl-320-cw-08.html#required-libraries-and-sparksession-entry-point",
    "href": "danl-cw/danl-320-cw-08.html#required-libraries-and-sparksession-entry-point",
    "title": "Classwork 8",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, when, log\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
  },
  {
    "objectID": "danl-cw/danl-320-cw-08.html#udf-for-regression-tables",
    "href": "danl-cw/danl-320-cw-08.html#udf-for-regression-tables",
    "title": "Classwork 8",
    "section": "",
    "text": "def regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                ci_upper,\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: Exp(95% CI Lower), 8: 95% CI Upper, 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"Exp(95% CI Lower)\", \"95% CI Upper\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))"
  },
  {
    "objectID": "danl-cw/danl-320-cw-08.html#udf-for-adding-dummy-variables",
    "href": "danl-cw/danl-320-cw-08.html#udf-for-adding-dummy-variables",
    "title": "Classwork 8",
    "section": "",
    "text": "def add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n    \n  \n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)"
  },
  {
    "objectID": "danl-cw/danl-320-cw-08.html#coefficient-plots",
    "href": "danl-cw/danl-320-cw-08.html#coefficient-plots",
    "title": "Classwork 8",
    "section": "",
    "text": "terms = assembler3.getInputCols()\ncoefs = model3.coefficients.toArray()[:len(terms)]\nstdErrs = model3.summary.coefficientStandardErrors[:len(terms)]\n\ndf_summary = pd.DataFrame({\n    \"term\": terms,\n    \"estimate\": coefs,\n    \"std_error\": stdErrs\n})\n\n# Filter df_summary if needed\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary[\"term\"], df_summary[\"estimate\"],\n             yerr = 1.96 * df_summary[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates (Model 2)\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()"
  },
  {
    "objectID": "danl-cw/danl-320-cw-08.html#residual-plots",
    "href": "danl-cw/danl-320-cw-08.html#residual-plots",
    "title": "Classwork 8",
    "section": "",
    "text": "# Convert test predictions to Pandas\ndfpd = TEST_DATAFRAME.select([\"prediction\", \"Y_VARIABLE\"]).toPandas()\ndfpd[\"residual\"] = dfpd[\"Y_VARIABLE\"] - dfpd[\"prediction\"]\nplt.scatter(dfpd[\"prediction\"], dfpd[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n# Use lowess smoothing for the trend line\nsmoothed = sm.nonparametric.lowess(dfpd[\"residual\"], dfpd[\"prediction\"])\nplt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\nplt.axhline(y=0, color=\"red\", linestyle=\"--\")\nplt.xlabel(\"Predicted y (Model)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residual Plot for Model\")\nplt.show()"
  },
  {
    "objectID": "danl-cw/danl-320-cw-08.html#histogram",
    "href": "danl-cw/danl-320-cw-08.html#histogram",
    "title": "Classwork 8",
    "section": "",
    "text": "# Create a histogram\ndfpd = DATAFRAME.select([\"Y_VARIABLE\"]).toPandas()\nsns.histplot(dfpd[\"Y_VARIABLE\"], bins=10, kde=True)"
  },
  {
    "objectID": "danl-cw/danl-320-cw-08.html#variable-description",
    "href": "danl-cw/danl-320-cw-08.html#variable-description",
    "title": "Classwork 8",
    "section": "Variable description",
    "text": "Variable description\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ncnt\nCount of total rental bikes\n\n\nyear\nYear\n\n\nmonth\nMonth\n\n\ndate\nDate\n\n\nhr\nHour\n\n\nwkday\nWeekday\n\n\nholiday\nHoliday indicator (1 if holiday, 0 otherwise)\n\n\nseasons\nSeason\n\n\nweather_cond\nWeather condition\n\n\ntemp\nTemperature (measured in standard deviations from average)\n\n\nhum\nHumidity (measured in standard deviations from average)\n\n\nwindspeed\nWind speed (measured in standard deviations from average)"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html",
    "href": "posts/beer-markets/beer-markets.html",
    "title": "Beer Markets",
    "section": "",
    "text": "Diving into the complex world of what people like in their beer, the beer_markets.csv dataset comes across as a goldmine of data, showing us the detailed interactions between buyers and their favorite beers. This dataset covers everything from how much and at what price people are buying beer to how deals and brand loyalty influence their decisions, across different types of people and places. As we start digging into this dataset, we aim to uncover the patterns that show what really influences the modern beer drinker’s choices, offering up valuable insights for marketers, industry watchers, and beer lovers. By breaking down the data, our exploration will shine a light on the factors that drive consumer behavior in the beer market, giving us a full picture of the trends that shape this lively industry.\nCode\n# Creating an interactive table\n!pip install itables\nfrom itables import init_notebook_mode\nfrom itables import show\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Reading the CSV file\nbeer_data = pd.read_csv(\"https://bcdanl.github.io/data/beer_markets.csv\")\nshow(beer_data)\n\n\n\n\n\n\n    \n      \n      hh\n      _purchase_desc\n      quantity\n      brand\n      dollar_spent\n      beer_floz\n      price_per_floz\n      container\n      promo\n      market\n      buyertype\n      income\n      childrenUnder6\n      children6to17\n      age\n      employment\n      degree\n      cow\n      race\n      microwave\n      dishwasher\n      tvcable\n      singlefamilyhome\n      npeople\n    \n  Loading... (need help?)\nCode\n# Setting up the visualization settings\nsns.set(style=\"whitegrid\")\n\n# Calculate total quantity and spending for each brand\nbrand_summary = beer_data.groupby('brand').agg({'quantity':'sum', 'dollar_spent':'sum'}).reset_index()\n\n# Sort by total quantity and spending\nbrand_summary_sorted_quantity = brand_summary.sort_values('quantity', ascending=False)\nbrand_summary_sorted_spent = brand_summary.sort_values('dollar_spent', ascending=False)\nCode\n# Plotting total quantity for each brand\nplt.figure(figsize=(10, 8))\nsns.barplot(x='quantity', y='brand', data=brand_summary_sorted_quantity, palette='viridis')\nplt.title('Total Quantity of Beer Purchased by Brand')\nplt.xlabel('Total Quantity')\nplt.ylabel('Brand')\nplt.show()\nThe bar charts above display the total quantity of beer purchased and the total spending by brand. From the looks of it, certain brands dominate in terms of quantity sold and total spending, indicating their popularity.\nNow, let’s calculate the average quantity purchased and average spending per purchase. For this, we’ll consider each row in the dataset as a separate purchase and compute the averages accordingly.\nCode\n# Calculate average quantity purchased and average spending per purchase\naverage_purchase = beer_data.groupby('brand').agg({\n    'quantity': 'mean',\n    'dollar_spent': 'mean'\n}).reset_index()\n\n# Sort by average quantity and average spending\naverage_purchase_sorted_quantity = average_purchase.sort_values('quantity', ascending=False)\naverage_purchase_sorted_spent = average_purchase.sort_values('dollar_spent', ascending=False)\n\n# Plotting average quantity for each brand\nplt.figure(figsize=(10, 8))\nsns.barplot(x='quantity', y='brand', data=average_purchase_sorted_quantity, palette='viridis')\nplt.title('Average Quantity of Beer Purchased by Brand')\nplt.xlabel('Average Quantity')\nplt.ylabel('Brand')\nplt.show()\nThe visualizations above depict the average quantity of beer purchased per brand and the average spending per brand. This shows which brands tend to be bought in larger quantities on average and which brands tend to have higher spending per purchase, which could be indicative of their price point or the purchase of premium products.\nNext, we’ll look at the total spending across different markets to see if there are any notable differences in spending habits geographically. To do this, we’ll sum up the spending in each market and visualize it.\nCode\n# Calculate total spending in each market\nmarket_spending_summary = beer_data.groupby('market').agg({'dollar_spent':'sum'}).reset_index()\n\n# Sort by total spending\nmarket_spending_summary_sorted = market_spending_summary.sort_values('dollar_spent', ascending=False)\n\n# Plotting total spending in each market\nplt.figure(figsize=(12, 18))\nsns.barplot(x='dollar_spent', y='market', data=market_spending_summary_sorted, palette='viridis')\nplt.title('Total Spending on Beer by Market')\nplt.xlabel('Total Spending')\nplt.ylabel('Market')\nplt.show()\nThe bar chart illustrates the total spending on beer by market, showcasing the differences in spending habits across various regions. Some markets have significantly higher spending, which could be due to a variety of factors including market size, consumer preferences, or economic factors.\nNow, let’s move on to the second analysis:"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html#demographic-analysis",
    "href": "posts/beer-markets/beer-markets.html#demographic-analysis",
    "title": "Beer Markets",
    "section": "Demographic Analysis",
    "text": "Demographic Analysis\nWe will examine which demographics are buying what kind of beer and whether spending habits vary by demographics such as age, employment, and race. For this, we could look at:\n\nSpending by age group\nSpending by employment status\nSpending by race\n\nI’ll start by analyzing spending by age group.\n\n\nCode\n# Calculate total spending by age group\nage_group_spending = beer_data.groupby('age').agg({'dollar_spent':'sum'}).reset_index()\n\n# Sort by total spending\nage_group_spending_sorted = age_group_spending.sort_values('dollar_spent', ascending=False)\n\n# Plotting total spending by age group\nplt.figure(figsize=(10, 6))\nsns.barplot(x='dollar_spent', y='age', data=age_group_spending_sorted, palette='viridis')\nplt.title('Total Spending on Beer by Age Group')\nplt.xlabel('Total Spending')\nplt.ylabel('Age Group')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe bar chart demonstrates the total spending on beer segmented by age group, highlighting which age groups spend the most on beer. It appears that certain age groups are more dominant in beer spending, which may align with the purchasing power or preferences of those groups.\nNext, we will examine spending by employment status.\n\n\nCode\n# Calculate total spending by employment status\nemployment_spending = beer_data.groupby('employment').agg({'dollar_spent':'sum'}).reset_index()\n\n# Sort by total spending\nemployment_spending_sorted = employment_spending.sort_values('dollar_spent', ascending=False)\n\n# Plotting total spending by employment status\nplt.figure(figsize=(10, 6))\nsns.barplot(x='dollar_spent', y='employment', data=employment_spending_sorted, palette='viridis')\nplt.title('Total Spending on Beer by Employment Status')\nplt.xlabel('Total Spending')\nplt.ylabel('Employment Status')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe visualization shows the total spending on beer by employment status. We can see that certain employment groups, such as full-time workers, are spending more on beer, which might be related to their disposable income.\nFinally, let’s look at spending by race to complete the demographic analysis.\n\n\nCode\n# Calculate total spending by race\nrace_spending = beer_data.groupby('race').agg({'dollar_spent':'sum'}).reset_index()\n\n# Sort by total spending\nrace_spending_sorted = race_spending.sort_values('dollar_spent', ascending=False)\n\n# Plotting total spending by race\nplt.figure(figsize=(10, 6))\nsns.barplot(x='dollar_spent', y='race', data=race_spending_sorted, palette='viridis')\nplt.title('Total Spending on Beer by Race')\nplt.xlabel('Total Spending')\nplt.ylabel('Race')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe bar chart above indicates the total spending on beer broken down by race, highlighting which racial groups account for the most beer spending within the dataset. This could reflect both the demographics of the regions where the data was collected and cultural preferences regarding beer.\nNow, let’s proceed to the third analysis:"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html#price-sensitivity",
    "href": "posts/beer-markets/beer-markets.html#price-sensitivity",
    "title": "Beer Markets",
    "section": "Price Sensitivity",
    "text": "Price Sensitivity\nWe’ll look at the price per fluid ounce and see if there are any trends or correlations with the quantity purchased or the brand popularity. To do this, we’ll visualize how the price is sensitive to the quantity purchased by brand.\n\n\nCode\n# Ensure there's no entries with 0 for 'price_per_floz' or 'quantity' to avoid log(0) issues\nfiltered_data = beer_data[(beer_data['price_per_floz'] &gt; 0) & (beer_data['quantity'] &gt; 0)]\n\n# Calculate log values for both 'price_per_floz' and 'quantity'\nfiltered_data['log_price_per_floz'] = np.log(filtered_data['price_per_floz'])\nfiltered_data['log_quantity'] = np.log(filtered_data['quantity'])\n\n# Use seaborn to create a scatterplot with fitted lines, facetted by 'brand'\ng = sns.lmplot(data=filtered_data, x='log_quantity', y='log_price_per_floz', col='brand', col_wrap=4, height=3, line_kws={'color': 'red'}, scatter_kws={'alpha':0.5}, aspect = .75)\n\n# Adjusting plot aesthetics\ng.set_titles(\"{col_name}\")\ng.set_axis_labels(\"Log of Quantity\", \"Log of Price per Floz\")\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle('Log of Price per Floz vs. Log of Quantity')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nHere’s the scatterplot with fitted straight lines for the log of price_per_floz versus the log of quantity, facetted by brands. Each subplot represents a different brand, showing the relationship between these two logarithmic variables along with a fitted line to illustrate the trend within each brand’s data.\n\n\nCode\n# Adjust the facetting to split rows by 'brand' and columns by 'promo' for a more detailed comparative analysis\ng = sns.lmplot(data=filtered_data, x='log_quantity', y='log_price_per_floz', row='brand', col='promo', height=3, aspect=.75, line_kws={'color': 'red'}, scatter_kws={'alpha':0.5})\n\n# Adjusting plot aesthetics\ng.set_titles(\"Brand: {row_name}\\n Promo: {col_name}\")\ng.set_axis_labels(\"Log of Quantity\", \"Log of Price per Floz\")\nplt.subplots_adjust(top=0.9, wspace = .4, hspace = .4)\ng.fig.suptitle('Log of Price per Floz vs. Log of Quantity')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThe scatterplot has been reorganized to split rows by brand and columns by promo status, offering a comprehensive view across different brands and their promotional status. Each subplot now provides a clear comparison of the log of price_per_floz versus the log of quantity for purchases made on promotion versus those that were not, across various beer brands.\nThis layout facilitates an easier comparison across brands and how promotion impacts the relationship between quantity and price per fluid ounce within each brand.\nLastly, let’s move to the fourth analysis:"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html#promotional-impact-on-quantity-purchased",
    "href": "posts/beer-markets/beer-markets.html#promotional-impact-on-quantity-purchased",
    "title": "Beer Markets",
    "section": "Promotional Impact on Quantity Purchased",
    "text": "Promotional Impact on Quantity Purchased\nWe’ll assess the impact of promotions on the quantity of beer purchased. For this analysis, we can calculate the average quantity purchased with and without promotions and visualize the difference. We’ll do this for each brand to see which brands are most affected by promotions.\nLet’s begin this analysis by looking at the average quantity purchased with and without promotions for each brand.\n\n\nCode\n# Calculate average quantity purchased with and without promotions for each brand\npromo_impact = beer_data.groupby(['brand', 'promo']).agg({'quantity':'mean'}).reset_index()\n\n# Pivot the data to have promo and non-promo side by side for each brand\npromo_impact_pivot = promo_impact.pivot(index='brand', columns='promo', values='quantity').reset_index()\npromo_impact_pivot.columns = ['brand', 'non_promo', 'promo']\n\n# Calculate the difference in average quantity purchased between promo and non-promo\npromo_impact_pivot['promo_impact'] = promo_impact_pivot['promo'] - promo_impact_pivot['non_promo']\n\n# Sort by the impact of promo\npromo_impact_pivot_sorted = promo_impact_pivot.sort_values('promo_impact', ascending=False)\n\n# Plotting the difference in average quantity purchased between promo and non-promo for each brand\nplt.figure(figsize=(12, 10))\nsns.barplot(x='promo_impact', y='brand', data=promo_impact_pivot_sorted, palette='viridis')\nplt.title('Impact of Promotions on Average Quantity Purchased by Brand')\nplt.xlabel('Difference in Average Quantity Purchased (Promo - Non-Promo)')\nplt.ylabel('Brand')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe bar chart illustrates the impact of promotions on the average quantity of beer purchased by brand. A positive value indicates that, on average, more beer is purchased when there is a promotion compared to when there isn’t. Some brands appear to be significantly more influenced by promotions, with customers buying more when the products are on sale or promotion.\nThis comprehensive analysis has provided insights into purchase patterns, demographic preferences, price sensitivity, and the impact of promotions on beer purchases."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\n\n\n\n Back to top"
  },
  {
    "objectID": "danl-rw/danl-320-team-project.html",
    "href": "danl-rw/danl-320-team-project.html",
    "title": "Team Project - Guideline",
    "section": "",
    "text": "Each team will deliver a 15-20-minute presentation, followed by a 1–2 minute Q&A session:\n\nThe order of team presentations will be determined by a random draw during the last class.\n\nIf multiple teams choose the same topic, I will try to schedule these teams separately to minimize repetition within a presentation session.\n\nTo ensure fairness and equal participation, each student must contribute evenly to the presentation.\nNew Techniques: If your presentation content involves machine learning methods not covered in class, your team must provide a brief explanation of the method along with the accompanying code during the presentation.\n\n\n\n\nEach team must email the presentation slides (in Microsoft PowerPoint or Google Slides format) for the project by May 13, 2025, Tuesday, 3:00 P.M. (Eastern Time)\n\n\n\n\n\nFeel free to reach out to Byeong-Hak if you need guidance with data transformation/visualization tasks for your team project."
  },
  {
    "objectID": "danl-rw/danl-320-team-project.html#submission",
    "href": "danl-rw/danl-320-team-project.html#submission",
    "title": "Team Project - Guideline",
    "section": "",
    "text": "Each team must email the presentation slides (in Microsoft PowerPoint or Google Slides format) for the project by May 13, 2025, Tuesday, 3:00 P.M. (Eastern Time)"
  },
  {
    "objectID": "danl-rw/danl-320-team-project.html#data-transformationvisualization-support",
    "href": "danl-rw/danl-320-team-project.html#data-transformationvisualization-support",
    "title": "Team Project - Guideline",
    "section": "",
    "text": "Feel free to reach out to Byeong-Hak if you need guidance with data transformation/visualization tasks for your team project."
  },
  {
    "objectID": "danl-rw/danl-320-team-project.html#presentation-1",
    "href": "danl-rw/danl-320-team-project.html#presentation-1",
    "title": "Team Project - Guideline",
    "section": "Presentation",
    "text": "Presentation\n\n\n\n\n\n\n\n\n\n\n\nAttribute\nVery Deficient (1)\nSomewhat Deficient (2)\nAcceptable (3)\nVery Good (4)\nOutstanding (5)\n\n\n\n\n1. Quality of Data Transformation and Descriptive Statistics\n- No transformation or cleaning applied- Very poor data transformation- Contains significant errors\n- Minimal transformation or cleaning- Basic data transformation with errors- Contains several errors\n- Basic transformation applied- Adequate data transformation- Contains minor errors\n- Effective transformation- Thorough data transformation- Data is accurate\n- Advanced transformation- Exceptional data transformation- Data is impeccable\n\n\n2. Quality of Data Visualization\n- Visualizations are missing or unclear- Misrepresents data\n- Visualizations are basic and lack clarity- Some misrepresentation\n- Visualizations are clear and accurate- Data is appropriately represented\n- Visualizations are insightful and enhance understanding- Data is accurately represented\n- Visualizations are highly creative and compelling- Data representation is impeccable\n\n\n3. Quality of Machine Learning Models\n- No model used or entirely inappropriate- No explanation of choice or results\n- Basic model used with minimal justification- Limited understanding of model performance\n- Appropriate model applied- Adequate explanation of choice and basic interpretation of results\n- Well-chosen model with thoughtful justification- Good interpretation of results and performance\n- Highly appropriate and sophisticated model- Excellent justification and deep insight into results and implications\n\n\n4. Effectiveness of Data Storytelling\n- No narrative or storyline- Insights are absent or irrelevant- Fails to engage the audience\n- Weak narrative structure- Insights are superficial- Minimal audience engagement\n- Clear narrative present- Insights are relevant- Audience is adequately engaged\n- Compelling narrative- Insights are significant- Engages audience effectively\n- Exceptional and captivating narrative- Insights are profound and impactful- Audience is highly engaged\n\n\n5. Quality of Slides and Visual Materials\n- Very poorly organized- Difficult to read and understand- Numerous errors present\n- Somewhat disorganized- Some slides are unclear- Several errors present\n- Well organized- Mostly clear and understandable- Few errors present\n- Very well organized- Clear and visually appealing- Very few errors\n- Exceptionally well organized- Highly clear and visually compelling- No errors\n\n\n6. Quality of Team Presentation\n- Presentation is disjointed- Poor team coordination- Unable to address questions\n- Lacks flow- Some coordination issues- Difficulty with several questions\n- Cohesive presentation- Team works well together- Addresses most questions adequately\n- Engaging presentation- Team is well-coordinated- Addresses almost all questions professionally\n- Highly engaging and polished presentation- Excellent team coordination- Addresses all questions expertly"
  },
  {
    "objectID": "danl-rw/danl-320-team-project.html#write-up",
    "href": "danl-rw/danl-320-team-project.html#write-up",
    "title": "Team Project - Guideline",
    "section": "Write-up",
    "text": "Write-up\n\n\n\n\n\n\n\n\n\n\n\n\nAttribute\nVery Deficient (1)\nSomewhat Deficient (2)\nAcceptable (3)\nVery Good (4)\nOutstanding (5)\n\n\n\n\n1. Quality of research question\nNot stated, or very unclear  Entirely derivative  Anticipate no contribution\nStated somewhat confusingly  Slightly interesting, but largely derivative  Anticipate minor contributions\nStated explicitly  Somewhat interesting and creative  Anticipate limited contributions\nStated explicitly and clearly  Clearly interesting and creative  Anticipate at least one good contribution\nArticulated very clearly  Highly interesting and creative  Anticipate several important contributions\n\n\n2. Quality of data visualization\nVery poorly visualized  Unclear  Was unable to interpret figures\nSomewhat visualized  Somewhat unclear  Had difficulty in interpreting figures\nMostly well visualized  Mostly clear visualization  Acceptably interpretable\nWell organized  Well thought-out visualization  Almost all figures are clearly interpretable\nVery well visualized  Outstanding visualization  All figures are clearly interpretable\n\n\n3. Quality of proposed business/economic analysis\nDemonstrates little or no critical thinking  Little/no understanding of business/economic concepts  Proposes inappropriate tools\nRudimentary critical thinking  Somewhat shaky understanding of business/economic concepts  Misses some important tools\nAverage critical thinking  Understanding of business/economic concepts  Proposes appropriate tools\nMature critical thinking  Clear understanding of business/economic concepts  Proposes advanced tools\nSophisticated critical thinking  Superior understanding of business/economic concepts  Proposes highly advanced tools\n\n\n4. Quality of proposed modeling analysis \nLittle or no critical thinking  Little/no understanding of theoretical concepts  Proposes inappropriate tools\nRudimentary critical thinking  Somewhat shaky understanding of theoretical concepts  Misses some important tools\nAverage critical thinking  Understanding of theoretical concepts  Proposes appropriate tools\nMature critical thinking  Clear understanding of theoretical concepts  Proposes advanced tools\nSophisticated critical thinking  Superior understanding of theoretical concepts  Proposes highly advanced tools\n\n\n5. Quality of writing\nVery poorly organized  Very difficult to read and understand  Teems with typos and grammatical errors\nSomewhat disorganized  Somewhat difficult to read and understand  Numerous typos and grammatical errors\nMostly well organized  Mostly easy to read and understand  Some typos and grammatical errors\nWell organized  Easy to read and understand  Very few typos or grammatical errors\nVery well organized  Very easy to read and understand  No typos or grammatical errors\n\n\n6. Quality of Jupyter Notebook/Quarto usages\nVery poorly organized  Teems with redundant messages of warning/error from running Python/R code  Provides inappropriate programming codes\nSomewhat disorganized  Numerous messages of warning/error from running Python/R code  Misses some important programming codes\nMostly well organized  Some messages of warning/error from running Python/R code  Provides appropriate programming codes\nWell organized  Very few messages of warning/error from running Python code  Provides advanced programming codes\nVery well organized  No messages of warning/error from running Python/R code  Proposes highly advanced programming codes"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-1",
    "title": "Lecture 6",
    "section": "PySpark Basics",
    "text": "PySpark Basics\nLearning Objectives\n\nLoading DataFrame with Spark’s read API (analogous to read_csv())\nGetting a Summary with printSchema() and describe()\nSelecting and Reordering Variables with select()\nCounting Values with groupBy().count(), countDistinct(), and count()\nSorting with orderBy()\nAdding a New Variable with withColumn()\nRemoving a Variable with drop()\nRenaming a Variable with withColumnRenamed()\nAggregation and Math Operations with selectExpr()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-2",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-2",
    "title": "Lecture 6",
    "section": "PySpark Basics",
    "text": "PySpark Basics\nLearning Objectives\n\nConverting Data Types with cast()\nFiltering Observations with filter()\nDealing with Missing Values (na.drop(), na.fill(), etc.)\nDealing with Duplicates (dropDuplicates())    \nGrouping DataFrames with groupBy().agg()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#spark-dataframe",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#spark-dataframe",
    "title": "Lecture 6",
    "section": "Spark DataFrame",
    "text": "Spark DataFrame\n\nIn PySpark, a DataFrame is a distributed collection of data organized into named columns.\nUnlike Pandas DataFrame, a Spark DataFrame is evaluated lazily: many transformations are “planned” but not executed until an action (e.g., count(), collect()) triggers a computation on the cluster.\nNo dedicated row index is maintained like in Pandas; rows are conceptually identified by their values, not by a numeric position."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#the-sparksession-entry-point",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#the-sparksession-entry-point",
    "title": "Lecture 6",
    "section": "The SparkSession Entry Point",
    "text": "The SparkSession Entry Point\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\nThe SparkSession entry point provides the functionality for data transformation with data frames and SQL.\nfrom pyspark.sql import SparkSession\n\nImports the SparkSession class from PySpark’s SQL module.\n\nSparkSession.builder\n\nBegins the configuration of a new SparkSession.\n\n.master(\"local[*]\")\n\nConfigures Spark to run locally using all available CPU cores.\n\n.getOrCreate()\n\nRetrieves an existing SparkSession if one exists, or creates a new one otherwise."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#reading-a-csv-file-into-the-spark-framework",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#reading-a-csv-file-into-the-spark-framework",
    "title": "Lecture 6",
    "section": "Reading a CSV file into the Spark Framework",
    "text": "Reading a CSV file into the Spark Framework\npath = '/content/drive/MyDrive/lecture-data/cces.csv'\ndf = spark.read.csv(path, \n                    inferSchema=True,\n                    header=True)\ndf.show()  # Displays the first 20 rows\n\nspark.read.csv(path, ...):\n\nRead a CSV file from the location specified by the path variable.\n\ninferSchema=True:\n\nWhen set to True, Spark will automatically detect (or “infer”) the data types of the columns in the CSV file.\n\nWithout this, Spark would treat all columns as strings by default.\n\nheader=True:\n\nThe first row of the CSV file contains the column headers, and will be used as names of the columns."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#reading-a-csv-file-into-the-spark-framework-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#reading-a-csv-file-into-the-spark-framework-1",
    "title": "Lecture 6",
    "section": "Reading a CSV file into the Spark Framework",
    "text": "Reading a CSV file into the Spark Framework\npath = 'https://bcdanl.github.io/data/df.csv'\ndf = spark.read.csv(path, \n                    inferSchema=True,\n                    header=True)\ndf.show()\n\nSpark’s spark.read.csv() function relies on the Hadoop FileSystem API to access files.\nBy default, Hadoop does not support reading files directly from HTTPS URLs.\n\nIt expects a local file system path, HDFS path, or another supported distributed file system."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#reading-a-web-csv-file-into-the-spark-framework",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#reading-a-web-csv-file-into-the-spark-framework",
    "title": "Lecture 6",
    "section": "Reading a Web CSV file into the Spark Framework",
    "text": "Reading a Web CSV file into the Spark Framework\n\nWhat should we do then?\n\nWe can convert the Pandas DataFrame to a Spark DataFrame\ndf = spark.createDataFrame(df_pd)\n\n\nimport pandas as pd\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\ndf_pd = pd.read_csv('https://bcdanl.github.io/data/nba.csv')\ndf = spark.createDataFrame(df_pd)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#spark-dataframe-overview-methods",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#spark-dataframe-overview-methods",
    "title": "Lecture 6",
    "section": "Spark DataFrame Overview Methods",
    "text": "Spark DataFrame Overview Methods\n\ndf.printSchema(): prints the schema (column names and data types).\ndf.columns: returns the list of columns.\ndf.dtypes: returns a list of tuples (columnName, dataType).\ndf.count(): returns the total number of rows.\ndf.describe(): returns basic statistics of numerical/string columns (mean, count, std, min, max)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#displaying-the-first-rows-with-df.show",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#displaying-the-first-rows-with-df.show",
    "title": "Lecture 6",
    "section": "Displaying the First Rows with df.show()",
    "text": "Displaying the First Rows with df.show()\nThree Optional Parameters\ndf.show(5)\ndf.show(truncate = False)\ndf.show(truncate = 3)\ndf.show(vertical = True)\n\nBy default, displays the first 20 rows.\ndf.show(n, truncate, vertical) accepts three optional parameters:\n\nn (int): Number of rows to display.\n\n\nExample: df.show(5)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#displaying-the-first-rows-with-df.show-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#displaying-the-first-rows-with-df.show-1",
    "title": "Lecture 6",
    "section": "Displaying the First Rows with df.show()",
    "text": "Displaying the First Rows with df.show()\nThree Optional Parameters\n\ntruncate (bool or int):\n\n\nIf True (default), long strings are truncated to 20 characters.\nIf False, displays full column contents.\nIf an integer is provided, it specifies the maximum number of characters to display.\nExample: df.show(truncate=False), df.show(truncate=30)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#displaying-the-first-rows-with-df.show-2",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#displaying-the-first-rows-with-df.show-2",
    "title": "Lecture 6",
    "section": "Displaying the First Rows with df.show()",
    "text": "Displaying the First Rows with df.show()\nThree Optional Parameters\n\nvertical (bool):\n\n\nIf True, displays each row vertically (useful for wide tables).\nExample: df.show(vertical=True)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#displaying-variable-information-with-df.printschema-df.dtypes-and-df.columns",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#displaying-variable-information-with-df.printschema-df.dtypes-and-df.columns",
    "title": "Lecture 6",
    "section": "Displaying Variable Information with df.printSchema(), df.dtypes, and df.columns",
    "text": "Displaying Variable Information with df.printSchema(), df.dtypes, and df.columns\ndf.printSchema()\ndf.dtypes\ndf.columns\n\ndf.printSchema() prints the DataFrame schema in a tree format.\n\nnullable = true means that a column can contain null values.\nWe may need to handle missing data appropriately.\n\ndf.dtypes returns a list of tuples representing each column’s name and data type.\ndf.columns returns a list of colunm names."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#generating-descriptive-statistics-with-df.describe.show",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#generating-descriptive-statistics-with-df.describe.show",
    "title": "Lecture 6",
    "section": "Generating Descriptive Statistics with df.describe().show()",
    "text": "Generating Descriptive Statistics with df.describe().show()\ndf.describe().show()\n\ndf.describe() computes summary statistics (e.g., count, mean, stddev, min, max) for the DataFrame’s numeric columns.\n.show() prints these statistics in a readable table format."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#selecting-a-column-by-its-name",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#selecting-a-column-by-its-name",
    "title": "Lecture 6",
    "section": "Selecting a Column by its Name",
    "text": "Selecting a Column by its Name\n# Single column -&gt; returns a DataFrame with one column\ndf.select(\"Name\").show(5)\n\n# Multiple columns -&gt; pass a list-like of column names\ndf.select(\"Name\", \"Team\", \"Salary\").show(5)\n\nIn PySpark, we use select() to choose variables.\nIt returns a new DataFrame projection of the specified variables."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#counting-observations-distinct-values",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#counting-observations-distinct-values",
    "title": "Lecture 6",
    "section": "Counting Observations / Distinct Values",
    "text": "Counting Observations / Distinct Values\n# Counting how many total rows\nnba_count = df.count()\n\n# Count distinct values in one column\nfrom pyspark.sql.functions import countDistinct\nnum_teams = df.select(countDistinct(\"Team\")).collect()[0][0]\n\n# GroupBy a column and count occurrences\ndf.groupBy(\"Team\").count().show(5)\n\ndf.count() returns the number of rows in df.\nUnlike Pandas, there is no direct .value_counts() or .nunique() in PySpark.\n\nHowever, we can replicate these operations using Spark’s aggregations (groupBy().count(), countDistinct, etc.)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-3",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-3",
    "title": "Lecture 6",
    "section": "PySpark Basics",
    "text": "PySpark Basics\nLet’s do Questions 1-3 in Classwork 5!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#sorting-by-one-or-more-variables",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#sorting-by-one-or-more-variables",
    "title": "Lecture 6",
    "section": "Sorting by One or More Variables",
    "text": "Sorting by One or More Variables\n# Sort by a single column ascending\ndf.orderBy(\"Name\").show(5)\n\n# Sort by descending\nfrom pyspark.sql.functions import desc\ndf.orderBy(desc(\"Salary\")).show(5)\n\n# Sort by multiple columns\ndf.orderBy([\"Team\", desc(\"Salary\")]).show(5)\n\norderBy() can accept column names and ascending/descending instructions."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#equivalent-of-pandas-nsmallest-or-nlargest",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#equivalent-of-pandas-nsmallest-or-nlargest",
    "title": "Lecture 6",
    "section": "Equivalent of Pandas nsmallest or nlargest",
    "text": "Equivalent of Pandas nsmallest or nlargest\n# nsmallest example:\ndf.orderBy(\"Salary\").limit(5).show()\n\n# nlargest example:\ndf.orderBy(desc(\"Salary\")).limit(5).show()\n\nSpark does not have nsmallest() or nlargest(), but we can use limit() after sorting."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#row-based-access-in-pyspark",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#row-based-access-in-pyspark",
    "title": "Lecture 6",
    "section": "Row-Based Access in PySpark",
    "text": "Row-Based Access in PySpark\n\nUnlike Pandas, PySpark DataFrames do not use a row-based index, so there is no direct .loc[] or .iloc[].\nWe typically filter rows by conditions (using .filter()) or use transformations (limit(), take(), collect()) to access row data.\n\n# Example: filter by condition\ndf.filter(\"Team == 'New York Knicks'\").show()\ndf.limit(5).show()\ndf.take(5)\ndf.collect()\n\nTo get the first n rows, use df.limit(n) or df.take(n), which returns a list of Row objects.\ndf.collect(): Returns all the records as a list of Row."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#adding-removing-renaming-and-relocating-variables-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#adding-removing-renaming-and-relocating-variables-1",
    "title": "Lecture 6",
    "section": "Adding, Removing, Renaming, and Relocating Variables",
    "text": "Adding, Removing, Renaming, and Relocating Variables\nAdding Columns with withColumn()\n# Add a column \"Salary_k\" using a column expression col()\ndf = df.withColumn(\"Salary_k\", col(\"Salary\") / 1000) \nRemoving Columns with drop()\ndf = df.drop(\"Salary_k\")  # remove a single column\ndf = df.drop(\"Salary_2x\", \"Salary_3x\")  # remove multiple columns\nRenaming Columns with withColumnRenamed()\ndf = df.withColumnRenamed(\"Birthday\", \"DateOfBirth\")\nRearranging Columns\ndf = df.select(\"Name\", \"Team\", \"Position\", \"Salary\", \"DateOfBirth\")"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#aggregations-math-methods",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#aggregations-math-methods",
    "title": "Lecture 6",
    "section": "Aggregations & Math Methods",
    "text": "Aggregations & Math Methods\n# Summaries for numeric columns\ndf.selectExpr(\n    \"mean(Salary) as mean_salary\",\n    \"min(Salary) as min_salary\",\n    \"max(Salary) as max_salary\",\n    \"stddev_pop(Salary) as std_salary\"\n).show()\n\nSpark has many SQL functions that can be used with selectExpr()."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#creating-or-transforming-columns",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#creating-or-transforming-columns",
    "title": "Lecture 6",
    "section": "Creating or Transforming Columns",
    "text": "Creating or Transforming Columns\nfrom pyspark.sql import functions as F\n# Pre-compute the average salary (pulls it back as a Python float)\nsalary_mean = df.select(F.avg(\"Salary\").alias(\"mean_salary\")).collect()[0][\"mean_salary\"]\n\ndf2 = (\n    df\n    .withColumn(\"Salary_2x\", F.col(\"Salary\") * 2)    # Add Salary_2x\n    .withColumn(\n        \"Name_w_Position\",           # Concatenate Name and Position\n        F.concat(F.col(\"Name\"), F.lit(\" (\"), F.col(\"Position\"), F.lit(\")\")))\n    .withColumn(\n        \"Salary_minus_Mean\",        # Subtract mean salary\n        F.col(\"Salary\") - F.lit(salary_mean))\n)\n\nAll transformations in Spark are “lazy” until an action (show(), collect(), count()) is called.\n.alias() method is a way to give a temporary (or alternate) name to the column."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-cast-method-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-cast-method-1",
    "title": "Lecture 6",
    "section": "Converting Data Types with the cast() Method",
    "text": "Converting Data Types with the cast() Method\n\nSpark columns can be cast to other data types using cast():\n\n# Convert Salary to integer\ndf = df.withColumn(\"Salary_int\", col(\"Salary\").cast(\"int\"))"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-to_date-method",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-to_date-method",
    "title": "Lecture 6",
    "section": "Converting Data Types with the to_date() Method",
    "text": "Converting Data Types with the to_date() Method\n\nto_date() can be used with a given string format (e.g., “M/d/yy”)\n\n# Convert to date or timestamp\nfrom pyspark.sql.functions import to_date\n\n# To have 19xx years, not 20xx ones.\nspark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \n\n# Casting the \"Birthday\" column to a date type\ndf = df.withColumn(\"DateOfBirth_ts\", to_date(\"Birthday\", \"M/d/yy\"))"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types",
    "title": "Lecture 6",
    "section": "Converting Data Types",
    "text": "Converting Data Types\n\n\n\n\nIntegers\n\nByteType — byte (8-bit)\nShortType — short (16-bit)\nIntegerType — int (32-bit)\nLongType — long (64-bit)\n\nFloating points\n\nFloatType — float (32-bit floating point)\nDoubleType — double (64-bit floating point)\nDecimalType — decimal (Arbitrary precision numeric type)\n\n\n\n\nStringType — string (Text data)\nBooleanType — boolean (Boolean values (True/False))\nDateType — date (Represents a date (year, month, day))\nTimestampType — timestamp (Represents a timestamp (date and time))"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-4",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-4",
    "title": "Lecture 6",
    "section": "PySpark Basics",
    "text": "PySpark Basics\n\nLet’s do\n\nQuestions 4-8 in Classwork 5!\nQuestion 1 in Classwork 6!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-rows-by-a-condition",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-rows-by-a-condition",
    "title": "Lecture 6",
    "section": "Filtering Rows by a Condition",
    "text": "Filtering Rows by a Condition\nimport pandas as pd\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\ndf_pd = pd.read_csv(\"https://bcdanl.github.io/data/employment.csv\")\ndf_pd = df_pd.where(pd.notnull(df_pd), None)  # Convert NaN to None\ndf = spark.createDataFrame(df_pd)\n\ndf.filter(col(\"Salary\") &gt; 100000).show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-by-a-condition-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-by-a-condition-1",
    "title": "Lecture 6",
    "section": "Filtering by a Condition",
    "text": "Filtering by a Condition\n\n\ndf.filter(\n    ( col(\"Team\") == \"Finance\" ) & \n    ( col(\"Salary\") &gt;= 100000 )\n).show()\n\ndf.filter(\n    (col(\"Team\") == \"Finance\") | \n    (col(\"Team\") == \"Legal\")   | \n    (col(\"Team\") == \"Sales\")\n).show()\n\n\nWe could combine multiple separate Boolean conditions with logical operators."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-with-the-isin-method",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-with-the-isin-method",
    "title": "Lecture 6",
    "section": "Filtering with the isin() method",
    "text": "Filtering with the isin() method\n# Checking membership with \"isin\"\ndf.filter(col(\"Team\").isin(\"Finance\", \"Legal\", \"Sales\")).show()\n\nA better solution is the isin() method."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-with-the-between-method",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-with-the-between-method",
    "title": "Lecture 6",
    "section": "Filtering with the between() method",
    "text": "Filtering with the between() method\ndf_between = df.filter(col(\"Salary\").between(90000, 100000))\ndf_between.show()\n\nIt returns a Boolean Series where True denotes that an observation’s value falls between the specified interval."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-5",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-5",
    "title": "Lecture 6",
    "section": "PySpark Basics",
    "text": "PySpark Basics\nLet’s do Questions 2-6 in Classwork 6!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#checking-for-missing-values-the-isnull-and-isnotnull-methods",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#checking-for-missing-values-the-isnull-and-isnotnull-methods",
    "title": "Lecture 6",
    "section": "Checking for Missing Values: The isNull() and isNotNull() methods",
    "text": "Checking for Missing Values: The isNull() and isNotNull() methods\n\nIn PySpark, missing values often appear as null.\n\n# Count how many null values in a given column\ndf.filter(col(\"Team\").isNull()).count()\n\n# Similarly, you can filter non-null\ndf.filter(col(\"Team\").isNotNull()).count()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dropping-rows-with-null-values-the-.na.drop-method",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dropping-rows-with-null-values-the-.na.drop-method",
    "title": "Lecture 6",
    "section": "Dropping Rows with Null Values: The .na.drop() method",
    "text": "Dropping Rows with Null Values: The .na.drop() method\n# Drop any row that has a null value in any column\ndf_drop_any = df.na.drop()\n\nThe .na.drop() method removes observations that hold any NULL values."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dropping-rows-with-null-values-the-.na.drop-method-with-how",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dropping-rows-with-null-values-the-.na.drop-method-with-how",
    "title": "Lecture 6",
    "section": "Dropping Rows with Null Values: The .na.drop() method with how",
    "text": "Dropping Rows with Null Values: The .na.drop() method with how\n# Drop rows that have all columns null\ndf_drop_all = df.na.drop(how=\"all\")\n\nWe can pass the how parameter an argument of \"all\" to remove observations in which all values are missing.\nNote that the how parameter’s default argument is \"any\"."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dropping-rows-with-null-values-the-.na.drop-method-with-subset",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dropping-rows-with-null-values-the-.na.drop-method-with-subset",
    "title": "Lecture 6",
    "section": "Dropping Rows with Null Values: The .na.drop() method with subset",
    "text": "Dropping Rows with Null Values: The .na.drop() method with subset\n# Drop rows with nulls in specific columns:\ndf_drop_subset = df.na.drop(subset=[\"Gender\", \"Team\"])\n\nWe can use the subset parameter to target observations with a missing value in a specific variable.\n\nThe above example removes observations that have a missing value in the Gender and Team variables."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#filling-null-values-the-.na.fill-method",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#filling-null-values-the-.na.fill-method",
    "title": "Lecture 6",
    "section": "Filling Null Values: The .na.fill() method",
    "text": "Filling Null Values: The .na.fill() method\n\n\n# Fill a specific column’s nulls with 0\ndf_fill = (\n  df.na\n  .fill(value = 0, \n        subset = [\"Salary\"])\n)\n\nWe can specify value and subset parameters to fill a specific column’s NULLs with a specific value\n\n\n# Fill multiple columns with a dictionary\ndf_fill_multi = (\n  df.na\n  .fill({\"Salary\": 0, \n           \"Team\": \"Unknown\"})\n)\n\nWe can fill multiple columns’ NULLs with a dictionary."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#removing-duplicates-with-distinct",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#removing-duplicates-with-distinct",
    "title": "Lecture 6",
    "section": "Removing Duplicates with distinct()",
    "text": "Removing Duplicates with distinct()\n\nIn PySpark, the distinct() method returns a new DataFrame with duplicate rows removed.\nIt is similar to the SQL SELECT DISTINCT command.\n\ndf.select(\"Team\", \"Position\").distinct().show()\n\nAfter applying distinct(), only unique observation remain."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-duplicates-with-the-dropduplicates-method",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-duplicates-with-the-dropduplicates-method",
    "title": "Lecture 6",
    "section": "Dealing with Duplicates with the dropDuplicates() method",
    "text": "Dealing with Duplicates with the dropDuplicates() method\n\nMissing values are a common occurrence in messy data sets, and so are duplicate values.\n\n# Drop all rows that are exact duplicates across all columns\ndf_no_dups = df.dropDuplicates()\n\n# Drop duplicates based on subset of columns\ndf_no_dups_subset = df.dropDuplicates([\"Team\"])\n\nBy default, dropDuplicates() keeps the first occurrence of each distinct combination."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-6",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-6",
    "title": "Lecture 6",
    "section": "PySpark Basics",
    "text": "PySpark Basics\nLet’s do Questions 7-8 in Classwork 6!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#group-operations-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#group-operations-1",
    "title": "Lecture 6",
    "section": "Group Operations",
    "text": "Group Operations\n\nIn PySpark, we use groupBy() (similar to Pandas’ groupby()) to aggregate, analyze, or transform data at a grouped level.\nA GroupedData object is returned, which can then be used with aggregation methods such as sum(), avg(), count(), etc."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#creating-a-groupby-object",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#creating-a-groupby-object",
    "title": "Lecture 6",
    "section": "Creating a groupBy Object",
    "text": "Creating a groupBy Object\n\nWe can create a Spark DataFrame from a list (or other data sources like CSV, Parquet, etc.).\nThen call groupBy(\"Type\") on the DataFrame.\n\nfood_data = [\n    (\"Apple\", \"Fruit\", 1.05),\n    (\"Onion\", \"Vegie\", 1.00),\n    (\"Orange\", \"Fruit\", 1.25),\n    (\"Tomato\", \"Vegie\", 0.85),\n    (\"Watermelon\", \"Fruit\", 4.15)\n]\n\nfood_df = spark.createDataFrame(food_data, [\"Item\", \"Type\", \"Price\"])\n\n# Group by \"Type\"\ngroups = food_df.groupBy(\"Type\")\n\nIn this example, there are two types of items: “Fruit” and “Vegie”."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#aggregation-on-groups",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#aggregation-on-groups",
    "title": "Lecture 6",
    "section": "Aggregation on Groups",
    "text": "Aggregation on Groups\n# Calculate the average Price for each Type\ngroups.avg(\"Price\").show()\n\n# Calculate the sum of the Price for each Type\ngroups.sum(\"Price\").show()\n\n# Count how many rows in each Type\ngroups.count().show()\n\nThese group-based operations are executed once an action (like .show()) is called."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#group-aggregation-with-multiple-columns",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#group-aggregation-with-multiple-columns",
    "title": "Lecture 6",
    "section": "Group Aggregation with Multiple Columns",
    "text": "Group Aggregation with Multiple Columns\nfrom pyspark.sql.functions import min, max, mean\n\nfood_df.groupBy(\"Type\").agg(\n    min(\"Price\").alias(\"min_price\"),\n    max(\"Price\").alias(\"max_price\"),\n    mean(\"Price\").alias(\"mean_price\")\n).show()\n\nWe can pass multiple aggregations to .agg() to get multiple results at once."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#adding-group-level-statistics-to-each-row",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#adding-group-level-statistics-to-each-row",
    "title": "Lecture 6",
    "section": "Adding Group-Level Statistics to Each Row",
    "text": "Adding Group-Level Statistics to Each Row\n\nIn Pandas, .transform() is often used to add group-level statistics back onto the original DataFrame.\nIn PySpark, we typically use a Window function with the aggregated DataFrame.\n\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import avg, col\n\n# Define a window partitioned by \"Type\"\nw = Window.partitionBy(\"Type\")\n\nfood_df_with_mean = food_df.withColumn(\n    \"mean_price_by_type\",\n    avg(col(\"Price\")).over(w)\n)\nfood_df_with_mean.show()\n\nThis keeps each original row, adding the group-level mean price for its corresponding Type."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-7",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-7",
    "title": "Lecture 6",
    "section": "PySpark Basics",
    "text": "PySpark Basics\nLet’s do Classwork 7!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#motivation",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#motivation",
    "title": "Lecture 8",
    "section": "Motivation",
    "text": "Motivation\n\nRelationship: Suppose we want to know how much the flight’s origin/destination, weather, and air carrier is associated with the probability that a flight will be delayed.\nPrediction: Suppose we also want to predict whether or not a flight will be delayed, based on facts like the flight’s origin/destination, weather, and air carrier.\nFor every flight \\(\\texttt{i}\\), you want to predict \\(\\texttt{flight_delayed[i]}\\), a binary variable ( \\(\\texttt{TRUE}\\) or \\(\\texttt{FALSE}\\)), based on \\(\\texttt{origin[i]}\\), \\(\\texttt{destination[i]}\\), \\(\\texttt{weather[i]}\\), and \\(\\texttt{air_carrier[i]}\\).\n\nPrediction of a binary variable \\(y_{i}\\) (0 or 1) is the expected value of \\(y_{i}\\)—the predicted probability that \\(y_{i} = 1\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#motivation-1",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#motivation-1",
    "title": "Lecture 8",
    "section": "Motivation",
    "text": "Motivation\n\nThe logistic regression model for the probability that a flight will be delayed is formulated as: \\[\n\\begin{align}\n&\\quad\\texttt{ Prob( flight_delayed[i] == TRUE ) } \\\\[.5em]\n&=\\, \\texttt{G( b$_{\\texttt{0}}$ + b$_{\\texttt{origin}}$*origin[i]  }\\\\\n&\\qquad\\quad\\,\\, + \\texttt{b$_{\\texttt{destination}}$*destination[i]  }\\\\\n&\\qquad\\quad\\,\\, + \\texttt{b$_{\\texttt{weather}}$*weather[i]  }\\\\\n&\\qquad\\quad\\,\\, + \\texttt{b$_{\\texttt{air_carrier}}$*air_carrier[i] )}.\n\\end{align}\n\\]\n\n\n\n\\(G(z_i)\\): the logistic function\n\n\n\n\n\\[\n\\begin{align}\n\\texttt{G(z[i]) = } \\dfrac{\\texttt{exp(z[i])}}{\\texttt{1 + exp(z[i])}}.\\notag\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#properties-of-the-logistic-function",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#properties-of-the-logistic-function",
    "title": "Lecture 8",
    "section": "Properties of the Logistic Function",
    "text": "Properties of the Logistic Function\n\nThe logistic function \\(G(z_i)\\) maps the linear combination of predictors to the probability that the outcome \\(y_{i}\\) is \\(1\\).\n\\(z_{i} = b_{0} + b_{1}x_{1, i} + b_{2}x_{2, i} + \\,\\cdots\\, + x_{k, i}\\)\n\\(z_{i} \\overset{G}{\\rightarrow} \\texttt{ Prob} ( y_{i} = 1 )\\)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#properties-of-the-logistic-function-1",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#properties-of-the-logistic-function-1",
    "title": "Lecture 8",
    "section": "Properties of the Logistic Function",
    "text": "Properties of the Logistic Function\n\n\nThe logistic function \\(G(z_i)\\) \\[\n\\begin{align}\nG(z_i) = \\frac{\\exp(z_i)}{1 + \\exp(z_i)}\n\\end{align}\n\\] ranges between 0 and 1 for any value of \\(z_i\\)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#what-is-the-logistic-regression-doing",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#what-is-the-logistic-regression-doing",
    "title": "Lecture 8",
    "section": "What is the Logistic Regression doing?",
    "text": "What is the Logistic Regression doing?\n\nThe logistic regression finds the beta coefficients, \\(b_0\\), \\(b_1\\), \\(b_2\\), \\(\\cdots\\), \\(b_{k}\\) such that the logistic function \\[\nG(b_0 + b_{1}x_{1,i} + b_{2}x_{2,i} + \\,\\cdots\\, + b_{k}x_{k,i})\n\\] is the best possible estimate of the binary outcome \\(y_{i}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#what-is-the-logistic-regression-doing-1",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#what-is-the-logistic-regression-doing-1",
    "title": "Lecture 8",
    "section": "What is the Logistic Regression doing?",
    "text": "What is the Logistic Regression doing?\n\nThe function \\(G(z_i)\\) is called the logistic function because the function \\(G(z_i)\\) is the inverse function of a logit (or a log-odd) of the probability that the outcome \\(y_{i}\\) is 1. \\[\n\\begin{align}\nG^{-1}(z_i) &\\,\\equiv\\, \\text{logit} (\\text{Prob}(y_{i} = 1))\\\\\n&\\,\\equiv \\log\\left(\\, \\frac{\\text{Prob}(y_{i} = 1)}{\\text{Prob}(y_{i} = 0)} \\,\\right)\\\\\n&\\,=\\, b_0 + b_{1}x_{1,i} + b_{2}x_{2,i} + \\,\\cdots\\, + b_{k}x_{k,i}\n\\end{align}\n\\]\nLogistic regression is a linear regression model for log odds."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#what-is-the-logistic-regression-doing-2",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#what-is-the-logistic-regression-doing-2",
    "title": "Lecture 8",
    "section": "What is the Logistic Regression doing?",
    "text": "What is the Logistic Regression doing?\n\nLogistic regression can be expressed as linear regression of log odds of \\(y_{i} = 1\\) on predictors \\(x_1, x_2, \\cdots, x_k\\):\n\n\\[\n\\begin{align}\n\\text{Prob}(y_{i} = 1) &\\,=\\, G( b_0 + b_{1}x_{1,i} + b_{2}x_{2,i} + \\,\\cdots\\, +  b_{k}x_{k,i} )\\\\\n\\text{ }\\\\\n\\Leftrightarrow\\qquad \\log\\left(\\dfrac{\\text{Prob}( y_i = 1 )}{\\text{Prob}( y_i = 0 )}\\right) &\\,=\\,  b_0 + b_{1}x_{1,i} + b_{2}x_{2,i} + \\,\\cdots\\, +  b_{k}x_{k,i}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#assessing-newborn-babies-at-risk",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#assessing-newborn-babies-at-risk",
    "title": "Lecture 8",
    "section": "Assessing Newborn Babies at Risk",
    "text": "Assessing Newborn Babies at Risk\n\nNewborn babies are assessed at one and five minutes after birth using what’s called the Apgar test, which is designed to determine if a baby needs immediate emergency care or extra medical attention.\n\nA baby who scores below 7 (on a scale from 0 to 10) on the Apgar scale needs extra attention.\n\nSuch at-risk babies are rare, so the hospital doesn’t want to provision extra emergency equipment for every delivery.\n\nOn the other hand, at-risk babies may need attention quickly, so provisioning resources proactively to appropriate deliveries can save lives."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#assessing-newborn-babies-at-risk-1",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#assessing-newborn-babies-at-risk-1",
    "title": "Lecture 8",
    "section": "Assessing Newborn Babies at Risk",
    "text": "Assessing Newborn Babies at Risk\ndfpd = pd.read_csv('https://bcdanl.github.io/data/NatalRiskData.csv')\n\nWe’ll use a sample dataset from the 2010 CDC natality public-use data file.\nThe data set records information about all US births, including facts about the mother and father, and about the delivery.\nThe sample has just over 26,000 births in a DataFrame."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#assessing-newborn-babies-at-risk-2",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#assessing-newborn-babies-at-risk-2",
    "title": "Lecture 8",
    "section": "Assessing Newborn Babies at Risk",
    "text": "Assessing Newborn Babies at Risk\n\n\n\n\nVariable\n\n\nType\n\n\nDescription\n\n\n\n\natRisk\n\n\nBool\n\n\n1 if Apgar &lt; 7, 0 otherwise\n\n\n\n\nPWGT\n\n\nNum\n\n\nPrepregnancy weight\n\n\n\n\nUPREVIS\n\n\nInt\n\n\nPrenatal visits\n\n\n\n\nCIG_REC\n\n\nBool\n\n\n1 if smoker, 0 otherwise\n\n\n\n\nGESTREC3\n\n\nCat\n\n\n&lt; 37 weeks or ≥ 37 weeks\n\n\n\n\nDPLURAL\n\n\nCat\n\n\nSingle / Twin / Triplet+\n\n\n\n\nULD_MECO\n\n\nBool\n\n\n1 if heavy meconium\n\n\n\n\nULD_PRECIP\n\n\nBool\n\n\n1 if labor &lt; 3 hours\n\n\n\n\nULD_BREECH\n\n\nBool\n\n\n1 if breech birth\n\n\n\n\nURF_DIAB\n\n\nBool\n\n\n1 if diabetic\n\n\n\n\nURF_CHYPER\n\n\nBool\n\n\n1 if chronic hypertension\n\n\n\n\nURF_PHYPER\n\n\nBool\n\n\n1 if pregnancy hypertension\n\n\n\n\nURF_ECLAM\n\n\nBool\n\n\n1 if eclampsia"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#assessing-newborn-babies-at-risk-3",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#assessing-newborn-babies-at-risk-3",
    "title": "Lecture 8",
    "section": "Assessing Newborn Babies at Risk",
    "text": "Assessing Newborn Babies at Risk\n\nTask 1. Identify the relationship between a predictor and the probability of \\(\\texttt{atRisk == TRUE}\\).\nTask 2. Identify ahead of time situations with a higher probability of \\(\\texttt{atRisk == TRUE}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#building-a-logistic-regression-model-in-pyspark",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#building-a-logistic-regression-model-in-pyspark",
    "title": "Lecture 8",
    "section": "Building a Logistic Regression Model in PySpark",
    "text": "Building a Logistic Regression Model in PySpark\n\nThe function to build a logistic regression model in PySpark is \\(\\texttt{GeneralizedLinearRegression(family=\"binomial\", link=\"logit\")}\\).\nThe outcome variable \\(y\\) is the binary variable \\(\\texttt{atRisk}\\) (0 or 1).\nThe other variables in the table in the previous slide are predictors \\(x_{k}\\).\nThe arguments \\(\\texttt{family=\"binomial\"}\\) and \\(\\texttt{link=\"logit\"}\\) specify the logistic distribution of the outcome variable \\(y\\).\n\nfrom pyspark.ml.regression import GeneralizedLinearRegression\n\ndummy_cols_GESTREC3, ref_category_GESTREC3 = add_dummy_variables('GESTREC3', 1)\ndummy_cols_DPLURAL, ref_category_DPLURAL = add_dummy_variables('DPLURAL', 0)\n\n# assembling predictors\nx_cols = ['PWGT', 'UPREVIS', 'CIG_REC', \n          'ULD_MECO', 'ULD_PRECIP', 'ULD_BREECH', 'URF_DIAB',\n          'URF_CHYPER', 'URF_PHYPER', 'URF_ECLAM']\n\nassembler_predictors = (\n    x_cols +\n    dummy_cols_GESTREC3 + dummy_cols_DPLURAL\n)\n\nassembler_1 = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\n\ndtrain_1 = assembler_1.transform(dtrain)\ndtest_1  = assembler_1.transform(dtest)\n\n# training model\nmodel_1 = (\n    GeneralizedLinearRegression(featuresCol=\"predictors\",\n                                labelCol=\"atRisk\",\n                                family=\"binomial\", \n                                link=\"logit\")\n    .fit(dtrain_1)\n)\n\n# making prediction\ndtrain_1 = model_1.transform(dtrain_1)\ndtest_1 = model_1.transform(dtest_1)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#deviance-and-likelihood",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#deviance-and-likelihood",
    "title": "Lecture 8",
    "section": "Deviance and Likelihood",
    "text": "Deviance and Likelihood\nmodel_1.summary\n\nDeviance is a measure of the distance between the data and the estimated model.\n\n\\[\n\\text{Deviance} = -2 \\log(\\text{Likelihood}) + C,\n\\] where \\(C\\) is constant that we can ignore."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#deviance-and-likelihood-1",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#deviance-and-likelihood-1",
    "title": "Lecture 8",
    "section": "Deviance and Likelihood",
    "text": "Deviance and Likelihood\n\nLogistic regression finds the beta coefficients, \\(b_0, b_1, \\,\\cdots, b_k\\) , such that the logistic function\n\n\\[\nG(b_0 + b_{1}x_{1,i} + b_{2}x_{2,i} + \\,\\cdots\\, + b_{k}x_{k,i})\n\\]\nis the best possible estimate of the binary outcome \\(y_i\\).\n\nLogistic regression finds the beta parameters that maximize the log likelihood of the data, given the model, which is equivalent to minimizing the sum of the residual deviances.\n\nWhen you minimize the deviance, you are fitting the parameters to make the model and data look as close as possible."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#likelihood-function",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#likelihood-function",
    "title": "Lecture 8",
    "section": "Likelihood Function",
    "text": "Likelihood Function\n\nLikelihood is the probability of your data given the model.\n\n\n\n\n\n\n\n\nThe probability that the seven data points would be observed:\n\n\\(L = (1-P1)*(1-P2)* P3*(1-P4)*P5*P6*P7\\)\n\nThe log of the likelihood: \\[\n\\begin{align}\n\\log(L) &= \\log(1-P1) + \\log(1-P2) + \\log(P3) \\\\\n&\\quad+ \\log(1-P4) + \\log(P5) + \\log(P6) + \\log(P7)\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#marginal-effect-of-x_k-i-on-textproby_i-1",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#marginal-effect-of-x_k-i-on-textproby_i-1",
    "title": "Lecture 8",
    "section": "Marginal Effect of \\(x_{k, i}\\) on \\(\\text{Prob}(y_{i} = 1)\\)?",
    "text": "Marginal Effect of \\(x_{k, i}\\) on \\(\\text{Prob}(y_{i} = 1)\\)?\nmodel_1.summary\n\nIf the baby is prematurely born, the log-odds of being at risk increases by 1.539 relative to the non-premature baby."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#marginal-effect-of-x_k-i-on-textproby_i-1-1",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#marginal-effect-of-x_k-i-on-textproby_i-1-1",
    "title": "Lecture 8",
    "section": "Marginal Effect of \\(x_{k, i}\\) on \\(\\text{Prob}(y_{i} = 1)\\)",
    "text": "Marginal Effect of \\(x_{k, i}\\) on \\(\\text{Prob}(y_{i} = 1)\\)\n\nIn logistic regression, the effect of \\(x_{k, i}\\) on \\(\\text{Prob}(y_{i} = 1)\\) is different for each observation \\(i\\).\n\n\n\n\n\n\n\n\nLinear regression\n\n\n\n\n\n\n\n\n\n\nLogistic regression"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#marginal-effect-of-x_k-i-on-textproby_i-1-2",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#marginal-effect-of-x_k-i-on-textproby_i-1-2",
    "title": "Lecture 8",
    "section": "Marginal Effect of \\(x_{k, i}\\) on \\(\\text{Prob}(y_{i} = 1)\\)",
    "text": "Marginal Effect of \\(x_{k, i}\\) on \\(\\text{Prob}(y_{i} = 1)\\)\n\nHow can we calculate the effect of \\(x_{k, i}\\) on the probability of \\(y_{i} = 1\\)?\n\nMarginal effect at the mean (MEM): We can obtain the marginal effect at an average observation or representative observations in the training data (MEM) or at representative values (MER)).\nAverage marginal effect (AME): We can average the marginal effects across the training data.\n\n\ndef marginal_effects(model, means):\n    \"\"\"\n    Compute marginal effects for all predictors in a PySpark GeneralizedLinearRegression model (logit)\n    and return a formatted table with statistical significance.\n    \n    Parameters:\n        model: Fitted GeneralizedLinearRegression model (with binomial family and logit link).\n        means: List of mean values for the predictor variables.\n    \n    Returns:\n        A formatted string containing the marginal effects table.\n    \"\"\"\n    global assembler_predictors  # Use the global assembler_predictors list\n    \n    # Extract model coefficients, standard errors, and intercept\n    coeffs = np.array(model.coefficients)\n    std_errors = np.array(model.summary.coefficientStandardErrors)\n    intercept = model.intercept\n    \n    # Compute linear combination of means and coefficients (XB)\n    XB = np.dot(means, coeffs) + intercept\n    \n    # Compute derivative of logistic function (G'(XB))\n    G_prime_XB = np.exp(XB) / ((1 + np.exp(XB)) ** 2)\n    \n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n    \n    # Create table to store results\n    results = []\n    \n    for i, predictor in enumerate(assembler_predictors):\n        # Compute marginal effect\n        marginal_effect = G_prime_XB * coeffs[i]\n        \n        # Compute standard error of the marginal effect\n        std_error = G_prime_XB * std_errors[i]\n        \n        # Compute z-score and p-value\n        z_score = marginal_effect / std_error if std_error != 0 else np.nan\n        p_value = 2 * (1 - norm.cdf(abs(z_score))) if not np.isnan(z_score) else np.nan\n        \n        # Compute confidence interval (95%)\n        ci_lower = marginal_effect - 1.96 * std_error\n        ci_upper = marginal_effect + 1.96 * std_error\n        \n        # Append results\n        results.append([predictor, f\"{marginal_effect: .4f}\", significance_stars(p_value), f\"{ci_lower: .4f}\", f\"{ci_upper: .4f}\"])\n    \n    # Convert results to tabulated format\n    table_str = tabulate(results, headers=[\"Variable\", \"Marginal Effect\", \"Significance\", \"95% CI Lower\", \"95% CI Upper\"], \n                         tablefmt=\"pretty\", colalign=(\"left\", \"decimal\", \"left\", \"decimal\", \"decimal\"))\n    \n    return table_str\n\n# Example usage:\n# means = [0.5, 30]  # Mean values for x1 and x2\n# assembler_predictors = ['x1', 'x2']  # Define globally before calling the function\n# table_output = mfx_glm(fitted_model, means)\n# print(table_output)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#vs.-point",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#vs.-point",
    "title": "Lecture 8",
    "section": "% vs. % point",
    "text": "% vs. % point\n\nLet’s say you have money in a savings account. The interest is 3%.\nNow consider two scenarios:\n\nThe bank increases the interest rate by one percent.\nThe bank increases the interest rate by one percentage point.\n\nWhat is the new interest rate in each scenario? Which is better?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#logistic-regression-1",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#logistic-regression-1",
    "title": "Lecture 8",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nInterpreting the Marginal Effect\n\nHow do we interpret the ME? All else being equal,\n\nThere is a 2.05 percentage point increase in the probability of a newborn being at risk if the baby is prematurely born.\nThere is a 0.04 percentage point decrease in the probability of a newborn being at risk for each additional parental medical visit."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#classifier",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#classifier",
    "title": "Lecture 8",
    "section": "Classifier",
    "text": "Classifier\n\nYour goal is to use the logistic regression model to classify newborn babies into one of two categories—at-risk or not.\nPrediction from the logistic regression with a threshold on the predicted probabilities can be used as a classifier.\n\nIf the predicted probability that the baby \\(\\texttt{i}\\) is at risk is greater than the threshold, the baby \\(\\texttt{i}\\) is classified as at-risk.\nOtherwise, the baby \\(\\texttt{i}\\) is classified as not-at-risk."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#double-density-plot-choosing-the-optimal-threshold",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#double-density-plot-choosing-the-optimal-threshold",
    "title": "Lecture 8",
    "section": "Double Density Plot – Choosing the Optimal Threshold",
    "text": "Double Density Plot – Choosing the Optimal Threshold\n\nDouble density plot is useful when picking the classifier threshold.\n\nSince the classifier is built using the training data, the threshold should also be selected using the training data.\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Filter training data for atRisk == 1 and atRisk == 0\npdf = dtrain_1.select(\"prediction\", \"atRisk\").toPandas()\n\ntrain_true = pdf[pdf[\"atRisk\"] == 1]\ntrain_false = pdf[pdf[\"atRisk\"] == 0]\n\n# Create the first density plot\nplt.figure(figsize=(8, 6))\nsns.kdeplot(train_true[\"prediction\"], label=\"TRUE\", color=\"red\", fill=True)\nsns.kdeplot(train_false[\"prediction\"], label=\"FALSE\", color=\"blue\", fill=True)\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Density\")\nplt.title(\"Density Plot of Predictions\")\nplt.legend(title=\"atRisk\")\nplt.show()\n\n# Define threshold for vertical line\nthreshold = 0.02  # Replace with actual value\n\n# Create the second density plot with vertical line\nplt.figure(figsize=(8, 6))\nsns.kdeplot(train_true[\"prediction\"], label=\"TRUE\", color=\"red\", fill=True)\nsns.kdeplot(train_false[\"prediction\"], label=\"FALSE\", color=\"blue\", fill=True)\nplt.axvline(x=threshold, color=\"blue\", linestyle=\"dashed\", label=f\"Threshold = {threshold}\")\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Density\")\nplt.title(\"Density Plot of Predictions with Threshold\")\nplt.legend(title=\"atRisk\")\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nConfusion matrix\n\nThe confusion matrix summarizes the classifier’s predictions against the actual known data categories.\n\nSuppose the threshold is set as 0.02.\n\n\n# Compute confusion matrix\ndtest_1 = dtest_1.withColumn(\"predicted_class\", when(col(\"prediction\") &gt; .02, 1).otherwise(0))\nconf_matrix = dtest_1.groupBy(\"atRisk\", \"predicted_class\").count().orderBy(\"atRisk\", \"predicted_class\")\n\nTP = dtest_1.filter((col(\"atRisk\") == 1) & (col(\"predicted_class\") == 1)).count()\nFP = dtest_1.filter((col(\"atRisk\") == 0) & (col(\"predicted_class\") == 1)).count()\nFN = dtest_1.filter((col(\"atRisk\") == 1) & (col(\"predicted_class\") == 0)).count()\nTN = dtest_1.filter((col(\"atRisk\") == 0) & (col(\"predicted_class\") == 0)).count()\n\n# Print formatted confusion matrix with labels\nprint(\"\\n Confusion Matrix:\\n\")\nprint(\"                     Predicted\")\nprint(\"            |  Negative  |  Positive  \")\nprint(\"------------+------------+------------\")\nprint(f\"Actual Neg. |    {TN:5}   |    {FP:5}  |\")\nprint(\"------------+------------+------------\")\nprint(f\"Actual Pos. |    {FN:5}   |    {TP:5}  |\")\nprint(\"------------+------------+------------\")"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-1",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-1",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nAccuracy\n\n\nAccuracy: When the classifier says this newborn baby is at risk or is not at risk, what is the probability that the model is correct?\n\nAccuracy is defined as the number of items categorized correctly divided by the total number of items."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-2",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-2",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nFalse positive/negative\n\nFalse positive rate (FPR): If the classifier says this newborn baby is at risk, what’s the probability that the baby is not really at risk?\n\nFPR is defined as the ratio of false positives to predicted positives.\n\nFalse negative rate (FNR): If the classifier says this newborn baby is not at risk, what’s the probability that the baby is really at risk?\n\nFNR is defined as the ratio of false negatives to predicted negatives."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-3",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-3",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nPrecision\n\n\nPrecision: If the classifier says this newborn baby is at risk, what’s the probability that the baby is really at risk?\n\nPrecision is defined as the ratio of true positives to predicted positives."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-4",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-4",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nRecall (or Sensitivity)\n\n\nRecall (or sensitivity): Of all the babies at risk, what fraction did the classifier detect?\n\nRecall (or sensitivity) is also called the true positive rate (TPR), the ratio of true positives over all actual positives."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-5",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-5",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nSpecificity\n\n\nSpecificity: Of all the not-at-risk babies, what fraction did the classifier detect?\n\nSpecificity is also called the true negative rate (TNR), the ratio of true negatives over all actual negatives."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-6",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-6",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nEnrichment\n\nAverage: Average rate of new born babies being at risk\nEnrichment: How does the classifier precisely choose babies at risk relative to the average rate of new born babies being at risk?\n\nWe want a classifier whose enrichment is greater than 2.\n\n\n\naccuracy = (TP + TN) / (TP + FP + FN + TN)\nprecision = TP / (TP + FP)\nrecall = TP / (TP + FN)\nspecificity = TN / (TN + FP)\naverage_rate = (TP + FN) / (TP + TN + FP + FN)  # Proportion of actual at-risk babies\nenrichment = precision / average_rate\n\nprint(f\"Accuracy:  {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall (Sensitivity): {recall:.4f}\")\nprint(f\"Specificity:  {specificity:.4f}\")\nprint(f\"Average Rate: {average_rate:.4f}\")\nprint(f\"Enrichment:   {enrichment:.4f} (Relative Precision)\")"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-7",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-7",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nTrade-off between recall and precision/enrichment\n\nThere is a trade-off between recall and precision/enrichment.\n\nWhat would be the optimal threshold?\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_recall_curve\n\npdf = dtest_1.select(\"prediction\", \"atRisk\").toPandas()\n\n# Extract predictions and true labels\ny_true = pdf[\"atRisk\"]  # True labels\ny_scores = pdf[\"prediction\"]  # Predicted probabilities\n\n# Compute precision, recall, and thresholds\nprecision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n\n# Compute enrichment: precision divided by average at-risk rate\naverage_rate = np.mean(y_true)\nenrichment = precision / average_rate\n\n# Define optimal threshold (example: threshold where recall ≈ enrichment balance)\noptimal_threshold = 0.02  # Adjust based on the plot\n\n# Plot Enrichment vs. Recall vs. Threshold\nplt.figure(figsize=(8, 6))\nplt.plot(thresholds, enrichment[:-1], label=\"Enrichment\", color=\"blue\", linestyle=\"--\")\nplt.plot(thresholds, recall[:-1], label=\"Recall\", color=\"red\", linestyle=\"-\")\n\n# Add vertical line for chosen threshold\nplt.axvline(x=optimal_threshold, color=\"black\", linestyle=\"dashed\", label=f\"Optimal Threshold = {optimal_threshold}\")\n\n# Labels and legend\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Score\")\nplt.title(\"Enrichment vs. Recall\")\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-8",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-8",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nTrade-off between sensitivity and specificity\n\nThere is also a trade-off between sensitivity and specificity.\nThe receiver operating characteristic curve (or ROC curve) plot both the true positive rate (recall) and the false positive rate (or 1 - specificity) for all threshold levels.\n\nArea under the curve (or AUC) can be another measure of the quality of the model.\n\n\nfrom sklearn.metrics import roc_curve\n\n# Convert to Pandas\npdf = dtest_1.select(\"prediction\", \"atRisk\").toPandas()\n\n# Compute ROC curve\nfpr, tpr, _ = roc_curve(pdf[\"atRisk\"], pdf[\"prediction\"])\n\n# Plot ROC curve\nplt.figure(figsize=(8,6))\nplt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc:.4f})\")\nplt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-9",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-9",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nAUC\n\n\n(0,0)—Corresponding to a classifier defined by the threshold \\(\\text{Prob}(y_{i} = 1) = 1\\):\n\nNothing gets classified as at-risk."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-10",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-10",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nAUC\n\n\n(1,1)—Corresponding to a classifier defined by the threshold \\(\\text{Prob}(y_{i} = 1) = 0\\):\n\nEverything gets classified as at-risk."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-11",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-11",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nAUC\n\n\n(0,1)—Corresponding to any classifier defined by a threshold between 0 and 1:\n\nEverything is classified perfectly!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-12",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-12",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nAUC\n\nThe AUC for the random model is 0.5.\n\nYou want a classifier whose AUC is close to 1, and greater than 0.5.\n\nWhen comparing multiple classifiers, you generally want to prefer classifiers that have a higher AUC.\nYou also want to examine the shape of the ROC to explore possible trade-offs."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-13",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-13",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\n\nSuppose that you have successfully trained a classifier with acceptable precision and recall using NY hospital data.\nNow you want to apply the same classifier on all hospital data in MA.\n\nWill the classifier perform as well?\n\nThe proportion of at-risk babies in MA hospitals may differ from that in NY.\n\nCan this difference impact classifier performance on MA data?\n\nLet’s examine how classifier performance changes with varying at-risk rates.\n\ndtrain, dtest = df.randomSplit([0.5, 0.5], seed = 1234)\n\npd_dtrain = dtrain.toPandas()\npd_dtest = dtest.toPandas()\n\n# Set seed for reproducibility\nnp.random.seed(23464)\n\n# Sample 1000 random indices from the test dataset without replacement\nsample_indices = np.random.choice(pd_dtest.index, size=1000, replace=False)\n\n# Separate the selected observations from testing data\nseparated = pd_dtest.loc[sample_indices]\n\n# Remove the selected observations from the testing data\n# Consider this as data from NY hospitals\npd_dtest_NY = pd_dtest.drop(sample_indices)\n\n# Split the separated sample into at-risk and not-at-risk groups\nat_risk_sample = separated[separated[\"atRisk\"] == 1]  # Only at-risk cases\nnot_at_risk_sample = separated[separated[\"atRisk\"] == 0]  # Only not-at-risk cases\n\n# Create test sets for MA hospitals with different at-risk rates\npd_dtest_MA_moreRisk = pd.concat([pd_dtest_NY, at_risk_sample])  # Adds back only at-risk cases\npd_dtest_MA_lessRisk = pd.concat([pd_dtest_NY, not_at_risk_sample])  # Adds back only not-at-risk cases\n\n# Show counts to verify results\nprint(\"Original Test Set Size:\", pd_dtest.shape[0])\nprint(\"Sampled Separated Size:\", separated.shape[0])\nprint(\"NY Hospitals Data Size:\", pd_dtest_NY.shape[0])\nprint(\"MA More Risk Data Size:\", pd_dtest_MA_moreRisk.shape[0])\nprint(\"MA Less Risk Data Size:\", pd_dtest_MA_lessRisk.shape[0])"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-14",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-14",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-15",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-15",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\n\nWhich classifier do you prefer for identifying at-risk babies?\n\nHigh accuracy, low recall, other things being equal;\nLow accuracy, high recall, other things being equal.\n\nAccuracy may not be a good measure for the classes that have unbalanced distribution of predicted probabilities (e.g., rare event)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#accuracy-can-be-misleading-in-imbalanced-data",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#accuracy-can-be-misleading-in-imbalanced-data",
    "title": "Lecture 8",
    "section": "Accuracy Can Be Misleading in Imbalanced Data",
    "text": "Accuracy Can Be Misleading in Imbalanced Data\n\nRare events (e.g., severe childbirth complications) occur in a very small percentage of cases (e.g., 1% of the population).\nA simple model that always predicts “not-at-risk” would be 99% accurate, as it correctly classifies 99% of cases where no complications occur.\nHowever, this does not mean the simple model is better—accuracy alone does not capture the effectiveness of a model when class distributions are skewed.\nA better model that identifies 5% of cases as “at-risk” and catches all true at-risk cases may appear to have lower overall accuracy than the simple model.\nMissing a severe complication (false negative) can be more costly than mistakenly flagging a healthy case as at risk (false positive)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#what-is-separation",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#what-is-separation",
    "title": "Lecture 8",
    "section": "What is Separation?",
    "text": "What is Separation?\n\nOne of the model variables or some combination of the model variables predicts the outcome perfectly for at least a subset of the training data.\n\nYou’d think this would be a good thing; but, ironically, logistic regression fails when the variables are too powerful.\n\nSeparation occurs when a predictor (or combination of predictors) perfectly separates the outcome classes.\nFor example, if:\n\nAll fail = TRUE when safety = low, and\nAll fail = FALSE when safety ≠ low,\nthen the model can predict the outcome with 100% accuracy based on safety.\n\n\n\n\n\n➡️ This leads to infinite (non-estimable) coefficients and convergence problems."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#what-is-quasi-separation",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#what-is-quasi-separation",
    "title": "Lecture 8",
    "section": "What is Quasi-Separation?",
    "text": "What is Quasi-Separation?\n\nQuasi-separation occurs when:\n\nSome, but not all, values of a predictor perfectly predict the outcome.\n\nExample:\n\nfail = TRUE for all cars with safety = low,\n\nBut fail is mixed for safety = med or high.\n\n\n➡️ Model still suffers from unstable coefficient estimates or high standard errors."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#example-of-quasi-separation",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#example-of-quasi-separation",
    "title": "Lecture 8",
    "section": "Example of Quasi-separation",
    "text": "Example of Quasi-separation\n\nSuppose a car review site rates cars on several characteristics, including affordability and safety rating.\nCar ratings can be “very good,” “good,” “acceptable,” or “unacceptable.”\nYour goal is to predict whether a car will fail the review: that is, get an unacceptable rating.\n\nLet’s do Classwork 11."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#building-a-personal-website-on-github",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#building-a-personal-website-on-github",
    "title": "Lecture 2",
    "section": "Building a Personal Website on GitHub",
    "text": "Building a Personal Website on GitHub\n\nFollow steps described in Classwork 1."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#lets-practice-markdown",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#lets-practice-markdown",
    "title": "Lecture 2",
    "section": "Let’s Practice Markdown!",
    "text": "Let’s Practice Markdown!\n\nJupyter Notebook, Quarto, and GitHub-based Discussion Boards use markdown as its underlying document syntax.\nLet’s do Classwork 2."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#getting-started-with-jupyter-notebook-and-html",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#getting-started-with-jupyter-notebook-and-html",
    "title": "Lecture 2",
    "section": "Getting Started with Jupyter Notebook and HTML",
    "text": "Getting Started with Jupyter Notebook and HTML\nYAML\n\n\n\n\n\n\nAn YAML (yet another markup language) header surrounded by ---.\n\nIt is commonly used for document configuration (e.g., title, author, date, style, …).\n\nIn YAML, indentation really matters!\n\ntab (or four spaces) defines a level in YAML."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#getting-started-with-jupyter-notebook-and-html-1",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#getting-started-with-jupyter-notebook-and-html-1",
    "title": "Lecture 2",
    "section": "Getting Started with Jupyter Notebook and HTML",
    "text": "Getting Started with Jupyter Notebook and HTML\nKnitting / Rendering\n\n\nWhen we knit the document, Quarto sends the .qmd file to jupyter/knitr, which executes all of the code chunks and creates a new markdown (.md) document which includes the code and its output.\nThe markdown file (*.md) generated by jupyter/knitr is then processed by pandoc, which is responsible for creating the output file."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#getting-started-with-jupyter-notebook-and-html-2",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#getting-started-with-jupyter-notebook-and-html-2",
    "title": "Lecture 2",
    "section": "Getting Started with Jupyter Notebook and HTML",
    "text": "Getting Started with Jupyter Notebook and HTML\nMarkdown, Jupyter Notebook, and HTML\n\nThe very original version of Markdown was invented mainly to write HTML content more easily.\n\nFor example, - SOME_TEXT in “.md” is equivalent to &lt;ul&gt;&lt;li&gt; SOME_TEXT &lt;/li&gt; in ”.html”\n\nPandoc makes it possible to convert a Markdown document to a large variety of output formats, such as HTML."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#getting-started-with-jupyter-notebook-and-html-3",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#getting-started-with-jupyter-notebook-and-html-3",
    "title": "Lecture 2",
    "section": "Getting Started with Jupyter Notebook and HTML",
    "text": "Getting Started with Jupyter Notebook and HTML\nMarkdown, Jupyter Notebook, and HTML\n---\ntitle: \"Habits\"\nauthor: YOUR_NAME\ndate: January 27, 2025\nformat: \n  html\n---\n\nTo create an HTML document from Jupyter Notebook, we specify the html output format in the YAML metadata of our document.\n\nBy default, format: html is set.\n\nOpen an empty Jupyter Notebook file from Google Colab (or VSCode).\n\nCreate the first cell that is Text.\nType the above YAML metadata to the first Text cell."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#getting-started-with-jupyter-notebook-and-html-4",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#getting-started-with-jupyter-notebook-and-html-4",
    "title": "Lecture 2",
    "section": "Getting Started with Jupyter Notebook and HTML",
    "text": "Getting Started with Jupyter Notebook and HTML\nMarkdown, Jupyter Notebook, and HTML\n---\ntitle: \"Python Basics\"\nauthor: YOUR_NAME\ndate: \"2025-01-27\"\n---\n\nDownload the Jupyter Notebook file, danl-320-python-basic.ipynb from Brightspace, and open it from Google Colab (or VSCode if you prefer).\nThe above syntax is part of YAML metadata in danl-320-python-basic.ipynb.\n\nYAML should be always in the first cell, and the first cell should be text, not code."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-_quarto.yml",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-_quarto.yml",
    "title": "Lecture 2",
    "section": "Quarto Website: _quarto.yml",
    "text": "Quarto Website: _quarto.yml\n\n\n---\nproject:\n  type: website\n\nwebsite:\n  title: \"YOUR NAME\"\n  navbar:\n    left:\n      - text: Project\n        href: danl_proj_nba.ipynb\n      - text: Blog\n        href: blog-listing.qmd\n\nformat:\n  html:\n    theme: cosmo\n    css: styles.css\n    toc: false\n---\n\nThe _quarto.yml file configures the website settings.\nIndentation matters!\n\n\n\nIn RStudio, open the project USERNAME.github.io.Rporj.\n\nClick Project: (None) at the top-right corner.\nClick USERNAME.github.io.Rproj.\n\n_quarto.yml configures a website, and provides various options for HTML documents within the website."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website",
    "title": "Lecture 2",
    "section": "Quarto Website",
    "text": "Quarto Website\nCustom CSS\n\nCascading Style Sheets (CSS) is used to format the layout of a webpage (color, font, text size, background, display, etc.).\n\nHTML will format the architecture of the house.\nCSS will be the carpet and walls to decorate the house.\nJavaScript adds interactive elements in the house, such as opening doors and lighting.\n\nWe are not front-end web developers.\n\nWe will not cover the use of CSS and JavaScript."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-1",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-1",
    "title": "Lecture 2",
    "section": "Quarto Website",
    "text": "Quarto Website\nRendering\n\nThe Render button (command/Ctrl + shift + K) renders a single Quarto document file (e.g., index.qmd) to create an output document.\nquarto render from Terminal renders ALL Quarto documents and Jupyter Notebook files in your local working directory:\n\nquarto render\n\nquarto render should be used if there is any change in _quarto.yml.\n\n\n\n\nTip\n\n\n\nEdit _quarto.yml, *.qmd, or *.ipynb files ONLY from your local laptop or Google Colab.\n\nDo not edit them from your GitHub repo for the website."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-2",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-2",
    "title": "Lecture 2",
    "section": "Quarto Website",
    "text": "Quarto Website\nAdding *.ipynb to a Quarto website\n\nBy default, quarto render doesn’t execute any code in .ipynb notebooks.\nquarto render renders .ipynb notebooks, so that corresponding html files are rendered.\n\nIf you need to update cell outputs in *.ipynb, run that *.ipynb on Google Colab, save the notebook, and download it to your local working directory."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-3",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-3",
    "title": "Lecture 2",
    "section": "Quarto Website",
    "text": "Quarto Website\nAppearance and Style\n\ntheme specifies the Bootstrap theme to use for the page (themes are drawn from the Bootswatch theme library).\n\nValid themes include default, bootstrap, cerulean, cosmo, darkly, flatly, journal, lumen, paper, readable, sandstone, simplex, spacelab, united, and yeti.\n\nhighlight-style specifies the code highlighting style.\n\nSupported styles include default, tango, pygments, kate, monochrome, espresso, zenburn, haddock, breezedark, and textmate."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-4",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-4",
    "title": "Lecture 2",
    "section": "Quarto Website",
    "text": "Quarto Website\nAbout\n\nYour index.qmd sets a front page about you.\n\nDetails in about pages are available here:\nhttps://quarto.org/docs/websites/website-about.html.\n\nQuarto includes 5 built-in templates:\n\njolla\ntrestles\nsolana\nmarquee\nbroadside"
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-5",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-5",
    "title": "Lecture 2",
    "section": "Quarto Website",
    "text": "Quarto Website\nIcons and Emojis\n\nA ton of Bootstrap icons are available here:\n\nhttps://icons.getbootstrap.com.\n\nA ton of markdown emojis are available here 😄:\n\nhttps://github.com/ikatyang/emoji-cheat-sheet\nhttps://gist.github.com/rxaviers/7360908"
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-6",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-6",
    "title": "Lecture 2",
    "section": "Quarto Website",
    "text": "Quarto Website\nNaviation and Adding Pages\nleft:\n- text: Project\n  href: project.ipynb\n- text: Blog\n  href: blog-listing.qmd\n- text: Homeowrk\n  href: hw.ipynb\n\nWe can add a new page to the website through navbar in _quarto.yml"
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-7",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-7",
    "title": "Lecture 2",
    "section": "Quarto Website",
    "text": "Quarto Website\nNaviation and Adding Pages\nleft:\n  - text: \"Python Data Analysis\"\n    menu:\n      - pandas_basic.ipynb\n      - seaborn_basic.ipynb\n\nWe can also create a drop-down menu by including a menu\nMore details about navbar are available here:\n\nhttps://quarto.org/docs/websites/website-navigation.html"
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-8",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-8",
    "title": "Lecture 2",
    "section": "Quarto Website",
    "text": "Quarto Website\nColors\n\nA ton of hex codes for colors are available here:\n\nhttps://www.color-hex.com"
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-basics",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-basics",
    "title": "Lecture 2",
    "section": "Quarto Website Basics",
    "text": "Quarto Website Basics\n\nLet’s do Classwork 3."
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#decision-tree",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#decision-tree",
    "title": "Lecture 10",
    "section": "Decision Tree",
    "text": "Decision Tree\n\n\n\n\nTree-logic uses a series of steps to come to a conclusion.\nThe trick is to have mini-decisions combine for good choices.\nEach decision is a node, and the final prediction is a leaf node."
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#decision-tree-1",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#decision-tree-1",
    "title": "Lecture 10",
    "section": "Decision Tree",
    "text": "Decision Tree\n\nDecision trees partition training data into homogenous nodes/subgroups with similar response values\n\nDecision trees take any type of data, numerical or categorical.\n\nThe subgroups are found recursively using binary partitions\n\ni.e. asking a series of yes-no questions about the predictor variables\n\nWe stop splitting the tree once a stopping criteria has been reached (e.g. maximum depth allowed)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#decision-tree-2",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#decision-tree-2",
    "title": "Lecture 10",
    "section": "Decision Tree",
    "text": "Decision Tree\n\nFor each subgroup/node, predictions are made with:\n\nClassification tree: the most popular class in the node\nRegression tree: the average of the outcome values in the node\n\nClassification trees have class probabilities at the leaves.\n\nProbability I’ll be in heavy rain is 0.9 (so take an umbrella).\n\nRegression trees have a mean outcome at the leaves.\n\nThe expected amount of rain is 2.2 inches (so take an umbrella).\n\nDecision trees make fewer assumptions about the relationship between x and y.\n\nE.g., linear model assumes the linear relationship between x and y.\n\nDecision trees naturally express certain kinds of interactions among the predictor variables: those of the form: “IF x is true AND y is true, THEN….”"
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#decision-tree-3",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#decision-tree-3",
    "title": "Lecture 10",
    "section": "Decision Tree",
    "text": "Decision Tree\n\nWe need a way to estimate the sequence of decisions.\n\nHow many are they?\nWhat is the order?\n\nCART grows the tree through a sequence of splits:\n\nGiven any set (node) of data, you can find the optimal split (the error minimizing split) and divide into two child sets.\nWe then look at each child set, and again find the optimal split to divide it into two homogeneous subsets.\nThe children become parents, and we look again for the optimal split on their new children (the grandchildren!).\n\nYou stop splitting and growing when the size of the leaf nodes hits some minimum threshold (e.g., say no less than 10 observations per leaf)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#nbc-shows",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#nbc-shows",
    "title": "Lecture 10",
    "section": "NBC Shows",
    "text": "NBC Shows\n\nData from NBC on response to TV pilots\n\nGross Ratings Points (GRP): estimated total viewership, which measures broadcast marketability.\nProjected Engagement (PE): a more subtle measure of audience.\n\nAfter watching a show, viewer is quizzed on order and detail.\nThis measures their engagement with the show (and ads!)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#nbc-shows-1",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#nbc-shows-1",
    "title": "Lecture 10",
    "section": "NBC Shows",
    "text": "NBC Shows\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nnbc = pd.read_csv(\"https://bcdanl.github.io/data/nbc_show.csv\")\nnbc_demog = pd.read_csv(\"https://bcdanl.github.io/data/nbc_demog.csv\")\n\nplt.figure(figsize=(8, 6))\nsns.lmplot(data=nbc, x=\"GRP\", y=\"PE\", hue=\"Genre\", ci=None, aspect=1.2, height=6,\n           markers=\"o\", scatter_kws={\"s\": 50}, line_kws={\"linewidth\": 2})\nplt.title(\"Scatter Plot with Linear Fit of GRP vs PE by Genre\")\nplt.xlabel(\"GRP\")\nplt.ylabel(\"PE\")\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#regression-tree-with-nbc-shows",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#regression-tree-with-nbc-shows",
    "title": "Lecture 10",
    "section": "Regression Tree with NBC Shows",
    "text": "Regression Tree with NBC Shows\n\nConsider predicting engagement from GRP and genre.\n\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n\n# Prepare the predictor set and target variable.\n# We want to model: PE ~ Genre + GRP using all columns except the first.\n# Here, we select the 'Genre' and 'GRP' columns as predictors and 'PE' as the target.\nX = nbc[['Genre', 'GRP']]\ny = nbc['PE']\n\n# If 'Genre' is categorical, convert it to dummy variables.\n# This is necessary because scikit-learn models require numerical inputs.\nX = pd.get_dummies(X, columns=['Genre'], drop_first=True)\n\n# Build and fit the regression tree.\nreg_tree = DecisionTreeRegressor(max_depth=3, min_samples_split=2, random_state=42)\nreg_tree.fit(X, y)\n\n# Generate predictions for PE and store them in the DataFrame.\nnbc['PEpred'] = reg_tree.predict(X)\n\n# Plot the regression tree.\nplt.figure(figsize=(12, 8))\nplot_tree(reg_tree, feature_names=X.columns, filled=True, rounded=True)\nplt.title(\"Regression Tree for PE\")\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#regression-tree-with-nbc-shows-1",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#regression-tree-with-nbc-shows-1",
    "title": "Lecture 10",
    "section": "Regression Tree with NBC Shows",
    "text": "Regression Tree with NBC Shows\nreg_tree = DecisionTreeRegressor(max_depth=3, min_samples_split=2, random_state=42)\nreg_tree.fit(X, y)\n\nmax_depth=3: Limits the tree to 3 levels.\n\nHelps simplify the model and makes the tree easier to interpret, especially with small datasets.\n\nmin_samples_split=2\n\nRequires at least 2 samples to consider splitting a node.\nEnsures that the tree can grow even with very few samples in a node.\n\nreg_tree.fit(X, y)\n\nTrains the regression tree model using predictors X and outcome variable y.\nThe model learns to predict the outcome by finding the best splits based on reducing the sum of squared errors."
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#regression-tree-outcome-explained",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#regression-tree-outcome-explained",
    "title": "Lecture 10",
    "section": "Regression Tree Outcome Explained",
    "text": "Regression Tree Outcome Explained\n\nSquared Error:\n\nThe sum of squared differences between the actual outcome values and the predicted value (i.e., the mean of those target values) for all samples in the node.\n\nIt quantifies the “impurity” or error of the node—the lower the value, the more homogeneous the node is with respect to the outcome variable.\n\nSamples:\n\nThe number of observations in the node.\n\nThe root node starts with all samples, decreasing with each split."
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#regression-tree-outcome-explained-1",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#regression-tree-outcome-explained-1",
    "title": "Lecture 10",
    "section": "Regression Tree Outcome Explained",
    "text": "Regression Tree Outcome Explained\n\nValue:\n\nIn a regression tree, the predicted value for a node is the average of the outcome values for the samples in that node.\n\nThe value shown in the node represents this average.\n\nLeaf Node:\n\nA terminal node where no further splitting occurs.\n\nWhen a new data point falls into that leaf node, the regression tree will predict its output as this value."
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#regression-tree-with-nbc-shows-2",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#regression-tree-with-nbc-shows-2",
    "title": "Lecture 10",
    "section": "Regression Tree with NBC Shows",
    "text": "Regression Tree with NBC Shows\n\n\n\n  \n\n\n\n  \n\n\n\nGreen is comedy, blue is drama, red is reality\nNonlinear: PE increases with GRP, but in jumps\n\nTrees automatically learn non-linear response functions and will discover interactions between variables.\n\nFollow how the tree translates into changing the predicted PE."
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#classification-tree-with-nbc-shows",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#classification-tree-with-nbc-shows",
    "title": "Lecture 10",
    "section": "Classification Tree with NBC Shows",
    "text": "Classification Tree with NBC Shows\n\nConsider a classification tree to predict genre from demographics.\nOutput from tree shows a series of decision nodes and the proportion in each genre at these nodes, down to the leaves.\n\n# Use the demographic variables (excluding the first column) as predictors\nX = demog.iloc[:, 1:]\n\n# Outcome\ny = nbc[\"Genre\"]\n\n# Build the classification tree. \nclf = DecisionTreeClassifier(min_samples_split=2, random_state=42)\nclf.fit(X, y)\n\n# Generate predictions for the 'Genre' and store them in the nbc DataFrame\nnbc[\"genrepred\"] = clf.predict(X)\n\n# Plot the decision tree\nplt.figure(figsize=(12, 8))\nplot_tree(clf, feature_names=X.columns, class_names=clf.classes_, filled=True, rounded=True)\nplt.title(\"Classification Tree for Genre\")\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#classification-tree-outcome-explained",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#classification-tree-outcome-explained",
    "title": "Lecture 10",
    "section": "Classification Tree Outcome Explained",
    "text": "Classification Tree Outcome Explained\nEach node in the decision tree plot displays key information:\n\nGini:\n\nA measure of impurity in the node.\nLower values indicate more homogeneous groups.\nThe algorithm tries to minimize this value with each split.\n\nValue:\n\nThe distribution of classes (counts) in the node.\nHelps determine the majority class for prediction.\n\nLeaf Node:\n\nThe prediction is made based on the majority class of samples in the node."
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#prunning-1",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#prunning-1",
    "title": "Lecture 10",
    "section": "Prunning",
    "text": "Prunning\n\nThe biggest challenge with CART models is avoiding overfit.\nFor CART, the usual solution is to rely on cross validation (CV).\nThe way to cross-validate the fully fitted tree is to prune it by removing split rules from the bottom up:\n\nAt each step, remove the split that contributes least to deviance reduction.\nThis is a reverse to CART’s growth process.\n\nPruning yields candidate tree.\nEach prune step produces a candidate tree model, and we can compare their out-of-sample prediction performance through CV."
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#boston-housing-data",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#boston-housing-data",
    "title": "Lecture 10",
    "section": "Boston Housing Data",
    "text": "Boston Housing Data\nboston = pd.read_csv(\"https://bcdanl.github.io/data/boston.csv\")\nThe boston DataFrame has 506 observations and 14 variables. - per capita income, - environmental factors, - educational facilities, - property size, - crime rate, - etc.\n\nThe goal is to predict housing values (medv in $1,000)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#boston-housing-data-1",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#boston-housing-data-1",
    "title": "Lecture 10",
    "section": "Boston Housing Data",
    "text": "Boston Housing Data\n\nmin_impurity_decrease=0.005: The minimum reduction in impurity required for a split to occur.\nPurpose:\n\nEnsures that each split meaningfully improves the homogeneity of the node.\nPrevents splitting when the improvement is too small.\n\nEffect:\n\nOnly splits that reduce the impurity by at least 0.005 are allowed.\n\nDo we need all the splits? Is the tree just fitting noise?\n\n# Set a random seed for reproducibility\nnp.random.seed(42120532)\ntrain, test = train_test_split(boston, test_size=0.20, random_state=42120532)\nX_train = train.drop(columns=[\"medv\"])\ny_train = train[\"medv\"]\n\n# Without max_depth=3\ntree_model = DecisionTreeRegressor(min_impurity_decrease=0.005, random_state=42)\ntree_model.fit(X_train, y_train)\n\n# Plot the initial regression tree\nplt.figure(figsize=(16, 12), dpi=300)\nplot_tree(tree_model, feature_names=X_train.columns, filled=True, rounded=True)\nplt.title(\"Regression Tree for medv (Initial Fit)\")\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#cost-complexity-pruning-path",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#cost-complexity-pruning-path",
    "title": "Lecture 10",
    "section": "Cost-Complexity Pruning Path",
    "text": "Cost-Complexity Pruning Path\n# Obtain the cost-complexity pruning path from the initial tree\npath = tree_model.cost_complexity_pruning_path(X_train, y_train)  # Get candidate ccp_alpha values and corresponding impurities\nccp_alphas = path.ccp_alphas  # Candidate pruning parameters (alpha values)\nimpurities = path.impurities  # Impurity values at each candidate alpha\n\n# Exclude the maximum alpha value to avoid the trivial tree (a tree with only the root)\nccp_alphas = ccp_alphas[:-1]  # Remove the last alpha value which would prune the tree to a single node\n\n# Set up 10-fold cross-validation\nkf = KFold(n_splits=10, shuffle=True, random_state=42)  # Initialize 10-fold CV with shuffling and fixed random state\ncv_scores = []  # List to store mean cross-validated scores (negative MSE)\nleaf_nodes = []  # List to record the number of leaves for each pruned tree\n\n# Loop over each candidate alpha value to evaluate its performance\nfor ccp_alpha in ccp_alphas:\n    # Create a DecisionTreeRegressor with the current ccp_alpha and other specified parameters\n    clf = DecisionTreeRegressor(random_state=42,\n                                ccp_alpha=ccp_alpha,\n                                min_impurity_decrease=0.005)\n    \n    # Perform 10-fold cross-validation and compute negative mean squared error (MSE)\n    scores = cross_val_score(clf, X_train, y_train,\n                             cv=kf, scoring=\"neg_mean_squared_error\")\n    cv_scores.append(np.mean(scores))  # Append the mean CV score for the current alpha\n    \n    # Fit the tree on the training data to record additional metrics\n    clf.fit(X_train, y_train)\n    leaf_nodes.append(clf.get_n_leaves())  # Record the number of leaf nodes in the tree\n\n# Select the best alpha based on the highest (least negative) mean CV score\nbest_alpha = ccp_alphas[np.argmax(cv_scores)]  # Identify the alpha with the best CV performance\nprint(\"Best alpha:\", best_alpha)  # Print the best alpha value\n\n# Train the final pruned tree using the best alpha found\nfinal_tree = DecisionTreeRegressor(random_state=42,\n                                   ccp_alpha=best_alpha,\n                                   min_impurity_decrease=0.005)\nfinal_tree.fit(X_train, y_train)  # Fit the final model on the training data"
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#cost-complexity-pruning-path-1",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#cost-complexity-pruning-path-1",
    "title": "Lecture 10",
    "section": "Cost-Complexity Pruning Path",
    "text": "Cost-Complexity Pruning Path\npath = tree_model.cost_complexity_pruning_path(X_train, y_train)  # Get candidate ccp_alpha values and corresponding impurities\nccp_alphas = path.ccp_alphas  # Candidate pruning parameters (alpha values)\nimpurities = path.impurities  # Impurity values at each candidate alpha\n\n# Exclude the maximum alpha value to avoid the trivial tree (a tree with only the root)\n  # Remove the last alpha value which would prune the tree to a single node\nccp_alphas = ccp_alphas[:-1]  \n\nThe cost_complexity_pruning_path method computes a series of effective alpha values (ccp_alphas) and the corresponding impurities.\nThese alpha values control the amount of pruning: higher alphas result in simpler (smaller) trees."
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#cross-validation-and-metrics-collection",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#cross-validation-and-metrics-collection",
    "title": "Lecture 10",
    "section": "Cross-Validation and Metrics Collection",
    "text": "Cross-Validation and Metrics Collection\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\ncv_scores = []  # mean CV scores (negative MSE)\nleaf_nodes = []\nsse = []\n\nA 10-fold cross-validation is set up to evaluate each candidate ccp_alpha.\nWe also prepare lists to store:\n\nMean CV scores\nThe number of leaf nodes for each pruned tree\nSSE on training data"
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#loop-over-alpha-values",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#loop-over-alpha-values",
    "title": "Lecture 10",
    "section": "Loop Over Alpha Values",
    "text": "Loop Over Alpha Values\n\nFor each ccp_alpha, a new tree is built:\nCross-validation: cross_val_score computes negative MSE over 10 folds.\nLeaf Nodes: After fitting, clf.get_n_leaves() records the number of terminal nodes.\nSSE on Training: SSE is computed by summing squared errors on training data.\n\n# Loop over each candidate alpha value to evaluate its performance\nfor ccp_alpha in ccp_alphas:\n    # Create a DecisionTreeRegressor with the current ccp_alpha and other specified parameters\n    clf = DecisionTreeRegressor(random_state=42,\n                                ccp_alpha=ccp_alpha,\n                                min_impurity_decrease=0.005)\n    \n    # Perform 10-fold cross-validation and compute negative mean squared error (MSE)\n    scores = cross_val_score(clf, X_train, y_train,\n                             cv=kf, scoring=\"neg_mean_squared_error\")\n    cv_scores.append(np.mean(scores))  # Append the mean CV score for the current alpha\n    \n    # Fit the tree on the training data to record additional metrics\n    clf.fit(X_train, y_train)\n    leaf_nodes.append(clf.get_n_leaves())  # Record the number of leaf nodes in the tree\n\n    # Compute SSE (sum of squared errors) on the training set\n    preds = clf.predict(X_train)  # Predict target values on training data\n    sse.append(np.sum((y_train - preds) ** 2))  # Calculate and record SSE for training set"
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#selecting-and-training-the-final-tree",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#selecting-and-training-the-final-tree",
    "title": "Lecture 10",
    "section": "Selecting and Training the Final Tree",
    "text": "Selecting and Training the Final Tree\n# Select the best alpha based on the highest (least negative) mean CV score\nbest_alpha = ccp_alphas[np.argmax(cv_scores)]  # Identify the alpha with the best CV performance\nprint(\"Best alpha:\", best_alpha)  # Print the best alpha value\n\n# Train the final pruned tree using the best alpha found\nfinal_tree = DecisionTreeRegressor(random_state=42,\n                                   ccp_alpha=best_alpha,\n                                   min_impurity_decrease=0.005)\nfinal_tree.fit(X_train, y_train)  # Fit the final model on the training data\n\nThe best alpha is chosen as the one with the highest mean CV score (remember: higher negative MSE means lower error).\nA final tree is trained using the optimal best_alpha, which prunes the tree for better generalization."
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#cross-validated-tree-pruning",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#cross-validated-tree-pruning",
    "title": "Lecture 10",
    "section": "Cross-Validated Tree Pruning",
    "text": "Cross-Validated Tree Pruning\nThe algorithm does cross-validation across pruning levels.\n# Plot the average cross-validated MSE against the number of leaf nodes\nnegative_cv_scores = -np.array(cv_scores)\n\nplt.figure(figsize=(8, 6), dpi=150)\nplt.plot(leaf_nodes, negative_cv_scores, marker='o', linestyle='-')\nplt.axvline(x=final_tree.get_n_leaves(), color='red', linestyle='--', label='Leaf Nodes = 21')  # Add vertical line at 21 leaf nodes\nplt.xlabel(\"Number of Leaf Nodes\")\nplt.ylabel(\"Mean CV MSE\")\nplt.title(\"CV Error vs. Number of Leaf Nodes\")\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#overfitting",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#overfitting",
    "title": "Lecture 10",
    "section": "Overfitting",
    "text": "Overfitting\nThe larger the number of leaf nodes, the smaller SSE on the training data.\n# Plot the SSE on the training against the number of leaf nodes\nplt.figure(figsize=(8, 6), dpi=150)\nplt.plot(leaf_nodes, sse, marker='o', linestyle='-')\nplt.xlabel(\"Number of Leaf Nodes\")\nplt.ylabel(\"SSE\")\nplt.title(\"SSE vs. Number of Leaf Nodes\")\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#limitation-of-cart",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#limitation-of-cart",
    "title": "Lecture 10",
    "section": "Limitation of CART",
    "text": "Limitation of CART\n\nCART automatically learns non-linear response functions and will discover interactions between variables.\nUnfortunately, it is tough to avoid overfit with CART.\nReal structure of the tree is not easily chosen via cross validation.\nOne way to mitigate the shortcomings of CART is bootstrap aggregation, or bagging."
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#bootstrap",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#bootstrap",
    "title": "Lecture 10",
    "section": "Bootstrap",
    "text": "Bootstrap\n\n\n\n\nBootstrap is random sampling with replacement.\nBootstrap is a reliable tool for uncertainty quantification."
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#bagging",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#bagging",
    "title": "Lecture 10",
    "section": "Bagging",
    "text": "Bagging\n\n\n\n\nReal structure that persists across datasets shows up in the average.\nA bagged ensemble of trees is also less likely to overfit the data."
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#random-forest-1",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#random-forest-1",
    "title": "Lecture 10",
    "section": "Random Forest",
    "text": "Random Forest\n\n\n\n\nPredictions are made the same way as bagging:\n\nRegression: take the average across the trees\nClassification: take the majority vote across the trees\n\nSplit-variable randomization adds more randomness to make each tree more independent of each other\nRandom forest introduces \\(\\texttt{max_features}\\) as a tuning parameter:\n\nIt controls the diversity of trees in the ensemble:\n\nSmaller \\(\\texttt{max_features}\\): More randomness → more diverse trees → potentially better generalization.\nLarger \\(\\texttt{max_features}\\): Less randomness → trees are more similar → can lead to overfitting/underfitting depending on data.\n\nTypically use \\(p / 3\\) (regression) or \\(\\sqrt{p}\\) (classification)\n\\(\\texttt{max_features} = p\\) is bagging (use all predictors at every split.)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#random-forest-2",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#random-forest-2",
    "title": "Lecture 10",
    "section": "Random Forest",
    "text": "Random Forest\n\n\n\n\n\n\n\nThe final ensemble of trees is bagged to make the random forest predictions."
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#accuracy-of-the-tree",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#accuracy-of-the-tree",
    "title": "Lecture 10",
    "section": "Accuracy of the Tree",
    "text": "Accuracy of the Tree\n\nFor classification, accuracy = \\(\\frac{\\text{Number of Correct Prediction}}{\\text{Total Prediction}}\\).\nFor regression, accuracy means \\(R^{2}\\).\n\n\\(R^2 = 1\\): model explains all variability\n\\(R^2 = 0\\): model explains none of it"
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#out-of-bag-samples-for-estimating-the-accuracy",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#out-of-bag-samples-for-estimating-the-accuracy",
    "title": "Lecture 10",
    "section": "Out-of-bag Samples for Estimating the Accuracy",
    "text": "Out-of-bag Samples for Estimating the Accuracy\n\n  Out-of-bag samples for observation x1\n\n\nSince random forest uses a large number of bootstrap samples, each data point has a corresponding set of out-of-bag samples."
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#examining-variable-importance",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#examining-variable-importance",
    "title": "Lecture 10",
    "section": "Examining Variable Importance",
    "text": "Examining Variable Importance\n\n  Calculating variable importance of variable v1"
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#gradient-boosted-trees-1",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#gradient-boosted-trees-1",
    "title": "Lecture 10",
    "section": "Gradient-boosted Trees",
    "text": "Gradient-boosted Trees\n\nGradient boosting tries to improve prediction performance by sequentially adding trees to an existing ensemble:\n\n\nUse the current ensemble \\(TE\\) to make predictions on the training data.\nMeasure the residuals between the true outcomes and the predictions on the training data.\nFit a new tree \\(T_i\\) to the residuals. Add \\(T_i\\) to the ensemble \\(TE\\).\nContinue until some stopping criteria to reach final model as a sum of trees.\n\n\nEach model in the sequence slightly improves upon the predictions of the previous models by focusing on the observations with the largest errors / residuals"
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#building-up-a-gradient-boosted-tree-model",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#building-up-a-gradient-boosted-tree-model",
    "title": "Lecture 10",
    "section": "Building Up a Gradient-Boosted Tree Model",
    "text": "Building Up a Gradient-Boosted Tree Model\n\n  Building up a gradient-boosted tree model"
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#visual-example-of-boosting-in-action",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#visual-example-of-boosting-in-action",
    "title": "Lecture 10",
    "section": "Visual Example of Boosting in Action",
    "text": "Visual Example of Boosting in Action\n\n  Boosted regression trees as 0-1024 successive trees are added."
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#gradient-boosted-trees-2",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#gradient-boosted-trees-2",
    "title": "Lecture 10",
    "section": "Gradient-Boosted Trees",
    "text": "Gradient-Boosted Trees\nUpdate the model parameters in the direction of the loss function (e.g., MSE, deviance)’s descending gradient"
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#tune-the-learning-rate-in-gradient-descent",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#tune-the-learning-rate-in-gradient-descent",
    "title": "Lecture 10",
    "section": "Tune the Learning Rate in Gradient Descent",
    "text": "Tune the Learning Rate in Gradient Descent\nWe need to control how much we update by in each step - the learning rate"
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#extreme-gradient-boosting-with-xgboost",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#extreme-gradient-boosting-with-xgboost",
    "title": "Lecture 10",
    "section": "eXtreme Gradient Boosting with XGBoost",
    "text": "eXtreme Gradient Boosting with XGBoost\n\n\n\n\nXGBoost is one of the most popular open-source library for the gradient boosting algorithm."
  },
  {
    "objectID": "danl-lec/danl-320-lec-10-2025-0407.html#tuning-hyperparameters-with-xgboost",
    "href": "danl-lec/danl-320-lec-10-2025-0407.html#tuning-hyperparameters-with-xgboost",
    "title": "Lecture 10",
    "section": "Tuning hyperparameters with XGBoost",
    "text": "Tuning hyperparameters with XGBoost\n\nWhat we have to consider tuning (our hyperparameters):\n\nnumber of trees (n_estimators)\nlearning rate (learning_rate), i.e. how much we update in each step\nthese two really have to be tuned together\ncomplexity of the trees (max_depth, number of observations in nodes)\nXGBoost also provides more regularization (via gamma) and early stopping\n\nMore work to tune properly as compared to random forests"
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-1",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-1",
    "title": "Lecture 4",
    "section": "Big Data",
    "text": "Big Data\n\nBig data and analytics are likely to be significant components of future careers across various fields.\nBig data refers to enormous and complex data collections that traditional data management tools can’t handle effectively.\nFive key characteristics of big data (5 V’s):\n\nVolume\nVelocity\nValue\nVeracity\nVariety"
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-2",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-2",
    "title": "Lecture 4",
    "section": "Big Data",
    "text": "Big Data\n1. Volume\n\nIn 2017, the digital universe contained an estimated 16.1 zettabytes of data.\nExpected to grow to 163 zettabytes by 2025.\nMuch new data will come from embedded systems in smart devices."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-3",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-3",
    "title": "Lecture 4",
    "section": "Big Data",
    "text": "Big Data\n1. Volume\n\n\n\n\n\nName\nSymbol\nValue\n\n\n\n\nKilobyte\nkB\n10³\n\n\nMegabyte\nMB\n10⁶\n\n\nGigabyte\nGB\n10⁹\n\n\nTerabyte\nTB\n10¹²\n\n\nPetabyte\nPB\n10¹⁵\n\n\nExabyte\nEB\n10¹⁸\n\n\nZettabyte\nZB\n10²¹\n\n\nYottabyte\nYB\n10²⁴\n\n\nBrontobyte*\nBB\n10²⁷\n\n\nGegobyte*\nGeB\n10³⁰\n\n\n\nNote: The asterisks (*) next to Brontobyte and Gegobyte in the original image have been preserved in this table. These likely indicate that these units are less commonly used or are proposed extensions to the standard system of byte units.\n\n\n\n\n\n\nIncrease in size of the global datasphere"
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-4",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-4",
    "title": "Lecture 4",
    "section": "Big Data",
    "text": "Big Data\n2. Velocity\n\nRefers to the rate at which new data is generated.\nEstimated at 0.33 zetabytes each day (120 zetabytes annually).\n90% of the world’s data was generated in just the past two years."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-5",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-5",
    "title": "Lecture 4",
    "section": "Big Data",
    "text": "Big Data\n3. Value\n\nRefers to the worth of data in decision-making.\nEmphasizes the need to quickly identify and process relevant data.\nUsers may be able to find more patterns and interesting anomalies from “big” data than from “small” data."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-6",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-6",
    "title": "Lecture 4",
    "section": "Big Data",
    "text": "Big Data\n4. Veracity\n\nMeasures the quality of the data.\nConsiders accuracy, completeness, and currency of data.\nDetermines if the data can be trusted for good decision-making."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-7",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-7",
    "title": "Lecture 4",
    "section": "Big Data",
    "text": "Big Data\n5. Variety\n\n\n\n\n\n\nData comes in various formats.\nStructured data: Has a predefined format, fits into traditional databases.\nUnstructured data: Not organized in a predefined manner, comes from sources like documents, social media, emails, photos, videos, etc."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#sources-of-an-organizations-data",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#sources-of-an-organizations-data",
    "title": "Lecture 4",
    "section": "Sources of an Organization’s Data",
    "text": "Sources of an Organization’s Data"
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#free-sources-of-useful-big-data",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#free-sources-of-useful-big-data",
    "title": "Lecture 4",
    "section": "Free Sources of Useful (Big) Data",
    "text": "Free Sources of Useful (Big) Data\nEconomics/Finance\n\n\n\n\nData Source\nDescription\nURL\n\n\n\n\nBureau of Labor Statistics (BLS)\nProvides access to data on inflation and prices, wages and benefits, employment, spending and time use, productivity, and workplace injuries\nBLS\n\n\nFRED (Federal Reserve Economic Data)\nProvides access to a vast collection of U.S. economic data, including interest rates, GDP, inflation, employment, and more\nFRED\n\n\nYahoo Finance\nProvides comprehensive financial news, data, and analysis, including stock quotes, market data, and financial reports\nYahoo Finance\n\n\nIMF (International Monetary Fund)\nProvides access to a range of economic data and reports on countries’ economies\nIMF Data\n\n\nWorld Bank Open Data\nFree and open access to global development data, including world development indicators\nWorld Bank Open Data\n\n\nOECD Data\nProvides access to economic, environmental, and social data and indicators from OECD member countries\nOECD Data"
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#free-sources-of-useful-big-data-1",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#free-sources-of-useful-big-data-1",
    "title": "Lecture 4",
    "section": "Free Sources of Useful (Big) Data",
    "text": "Free Sources of Useful (Big) Data\nGovernment/Public Data\n\n\n\n\nData Source\nDescription\nURL\n\n\n\n\nData.gov\nPortal providing access to over 186,000 government data sets, related to topics such as agriculture, education, health, and public safety\nData.gov\n\n\nCIA World Factbook\nPortal to information on the economy, government, history, infrastructure, military, and population of 267 countries\nCIA World Factbook\n\n\nU.S. Census Bureau\nPortal to a huge variety of government statistics and data relating to the U.S. economy and its population\nU.S. Census Bureau\n\n\nEuropean Union Open Data Portal\nProvides access to public data from EU institutions\nEU Open Data Portal\n\n\nNew York City Open Data\nProvides access to datasets from New York City, covering a wide range of topics such as public safety, transportation, and health\nNYC Open Data\n\n\nLos Angeles Open Data\nPortal for accessing public data from the City of Los Angeles, including transportation, public safety, and city services\nLA Open Data\n\n\nChicago Data Portal\nOffers access to datasets from the City of Chicago, including crime data, transportation, and health statistics\nChicago Data Portal"
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#free-sources-of-useful-big-data-2",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#free-sources-of-useful-big-data-2",
    "title": "Lecture 4",
    "section": "Free Sources of Useful (Big) Data",
    "text": "Free Sources of Useful (Big) Data\nHealth, Climate/Environment, and Social Data\n\n\n\n\nData Source\nDescription\nURL\n\n\n\n\nHealthdata.gov\nPortal to 125 years of U.S. health care data, including national health care expenditures, claim-level Medicare data, and other topics\nHealthdata.gov\n\n\nCMS Open Payments\nCenters for Medicare & Medicaid Services (CMS) Open Payments houses a publicly accessible database of payments that reporting entities, including drug and medical device companies, make to covered recipients like physicians.\nCMS\n\n\nWorld Health Organization (WHO)\nPortal to data and statistics on global health issues\nWHO Data\n\n\nNational Centers for Environmental Information (NOAA)\nPortal for accessing a variety of climate and weather data sets\nNCEI\n\n\nNOAA National Weather Service\nProvides weather, water, and climate data, forecasts and warnings\nNOAA NWS\n\n\nFAO (Food and Agriculture Organization)\nProvides access to data on food and agriculture, including data on production, trade, food security, and sustainability\nFAOSTAT\n\n\nPew Research Center Internet & Technology\nPortal to research on U.S. politics, media and news, social trends, religion, Internet and technology, science, Hispanic, and global topics\nPew Research\n\n\nData for Good from Facebook\nProvides access to anonymized data from Facebook to help non-profits and research communities with insights on crises, health, and well-being\nFacebook Data for Good\n\n\nData for Good from Canada\nProvides open access to datasets that address pressing social challenges across Canada\nData for Good Canada"
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#free-sources-of-useful-big-data-3",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#free-sources-of-useful-big-data-3",
    "title": "Lecture 4",
    "section": "Free Sources of Useful (Big) Data",
    "text": "Free Sources of Useful (Big) Data\nGeneral Data Repositories\n\n\n\n\nData Source\nDescription\nURL\n\n\n\n\nAmazon Web Services (AWS) public data sets\nPortal to a huge repository of public data, including climate data, the million song dataset, and data from the 1000 Genomes project\nAWS Datasets\n\n\nOpportunity Insights\nHarvard-based team offers big data to solve economic and social problems\nOpportunity Insights\n\n\nGapminder\nPortal to data from the World Health Organization and World Bank on economic, medical, and social issues\nGapminder\n\n\nGoogle Dataset Search\nHelps find datasets stored across the web\nGoogle Dataset Search\n\n\nKaggle Datasets\nA community-driven platform with datasets from various fields, useful for machine learning and data science projects\nKaggle Datasets\n\n\nUCI Machine Learning Repository\nA collection of databases, domain theories, and datasets used for machine learning research\nUCI ML Repository\n\n\nUnited Nations Data\nProvides access to global statistical data compiled by the United Nations\nUN Data\n\n\nHumanitarian Data Exchange (HDX)\nProvides humanitarian data from the United Nations, NGOs, and other organizations\nHDX\n\n\nDemocratizing Data from data.org\nA platform providing access to high-impact datasets, tools, and resources aimed at solving critical global challenges\nDemocratizing Data\n\n\nJustia Federal District Court Opinions and Orders database\nA free searchable database of full-text opinions and orders from civil cases heard in U.S. Federal District Courts\nJustia"
  },
  {
    "objectID": "danl-lec/danl-320-lec-11-2025-0430.html#unsupervised-learning-1",
    "href": "danl-lec/danl-320-lec-11-2025-0430.html#unsupervised-learning-1",
    "title": "Lecture 11",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nUnsupervised learning methods discover unknown relationships in data.\nWith unsupervised methods, there’s no outcome that we’re trying to predict.\n\nInstead, we want to discover patterns in the data that perhaps we hadn’t previously suspected.\n\nUnsupervised learning: ML + Data Visualization\n\n\nWe look at two classes of unsupervised methods:\n\nCluster analysis finds groups with similar characteristics.\nPrincipal component analysis (PCA) turns many variables into a few new ones, called principal components, that still capture most of our data.\n\nPCA is a popular dimensionality reduction method!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-11-2025-0430.html#clustering",
    "href": "danl-lec/danl-320-lec-11-2025-0430.html#clustering",
    "title": "Lecture 11",
    "section": "Clustering",
    "text": "Clustering\n\nIn cluster analysis, the goal is to group the observations in our data into clusters such that every observation in a cluster is more similar to other observations in the same cluster than observations in other clusters.\n\nE.g., a company that offers guided tours might want to cluster its clients by behavior and tastes:\n\nWhich countries they like to visit\nWhether they prefer adventure tours, luxury tours, or educational tours\nWhat kinds of activities they participate in\nWhat sorts of sites they like to visit"
  },
  {
    "objectID": "danl-lec/danl-320-lec-11-2025-0430.html#protein-consumption-data",
    "href": "danl-lec/danl-320-lec-11-2025-0430.html#protein-consumption-data",
    "title": "Lecture 11",
    "section": "Protein Consumption Data",
    "text": "Protein Consumption Data\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import pairwise_distances, calinski_harabasz_score, silhouette_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n\nprotein = pd.read_csv(\"https://bcdanl.github.io/data/protein.csv\")\n\nTo demonstrate clustering, we will use a small dataset from 1973 on protein consumption from 9 different food variables in 25 countries in Europe.\nThe goal is to group the countries based on patterns in their protein consumption."
  },
  {
    "objectID": "danl-lec/danl-320-lec-11-2025-0430.html#units-and-scaling",
    "href": "danl-lec/danl-320-lec-11-2025-0430.html#units-and-scaling",
    "title": "Lecture 11",
    "section": "Units and Scaling",
    "text": "Units and Scaling\n\nThe units we choose to measure our data can significantly influence the clusters that an algorithm uncovers.\nOne way to try to make the units of each variable more compatible is to standardize all the variables to have a mean value of 0 and a standard deviation of 1.\nWe can scale numeric data using the function StandardScaler().\nThe StandardScaler() function annotates its output with two attributes:\n\nmean_ returns the mean values of all the columns;\nscale_ returns the standard deviations.\n\n\nvars_to_use = protein.columns[1:]\nscaler = StandardScaler()\n\n# computes those means and standard deviations on your selected columns \n  # and returns a NumPy array (`pmatrix`) \n  # where each column now has mean 0 and variance 1.\npmatrix = scaler.fit_transform(protein[vars_to_use])\n\npcenter, pscale = scaler.mean_, scaler.scale_"
  },
  {
    "objectID": "danl-lec/danl-320-lec-11-2025-0430.html#density-plots---raw-vs.-scaled",
    "href": "danl-lec/danl-320-lec-11-2025-0430.html#density-plots---raw-vs.-scaled",
    "title": "Lecture 11",
    "section": "Density Plots - Raw vs. Scaled",
    "text": "Density Plots - Raw vs. Scaled\nfig, ax = plt.subplots()\nsns.kdeplot(data=protein, x=\"FrVeg\", ax=ax, label=\"FrVeg\")\nsns.kdeplot(data=protein, x=\"RedMeat\", ax=ax, color=\"red\", label=\"RedMeat\")\nax.set_title(\"Original scale\"); ax.legend(); plt.show()\n\nscaled_df = pd.DataFrame(pmatrix, columns=vars_to_use)\nfig, ax = plt.subplots()\nsns.kdeplot(data=scaled_df, x=\"FrVeg\", ax=ax, linestyle=\"--\", label=\"FrVeg (scaled)\")\nsns.kdeplot(data=scaled_df, x=\"RedMeat\", ax=ax, color=\"red\", linestyle=\"--\",\n            label=\"RedMeat (scaled)\")\nax.set_title(\"After StandardScaler\"); ax.legend(); plt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-11-2025-0430.html#hierarchical-clustering",
    "href": "danl-lec/danl-320-lec-11-2025-0430.html#hierarchical-clustering",
    "title": "Lecture 11",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nHierarchical clustering builds a tree of nested groupings, called a dendrogram.\nlink = linkage(pmatrix, method=\"ward\")\nplt.figure(figsize=(10, 6))\ndendrogram(link, labels=protein[\"Country\"].values, leaf_font_size=8)\nplt.title(\"Ward dendrogram\"); plt.tight_layout(); plt.show()\n\nThe linkage() uses one of a variety of clustering methods to produce a tree that records the nested cluster structure.\n\nHere we choose the Ward’s method.\nThe linkage() function takes an Numpy array, pmatrix, as an input.\nThe linkage() compute the distance matrix that records the distances between all pairs of points in the data.."
  },
  {
    "objectID": "danl-lec/danl-320-lec-11-2025-0430.html#wards-method-and-within-sum-of-squares-wss",
    "href": "danl-lec/danl-320-lec-11-2025-0430.html#wards-method-and-within-sum-of-squares-wss",
    "title": "Lecture 11",
    "section": "Ward’s method and Within Sum of Squares (WSS)",
    "text": "Ward’s method and Within Sum of Squares (WSS)\n\n\n\n\nWard’s method starts out with each data point as an individual cluster and merges clusters iteratively so as to minimize the total within sum of squares (WSS) of the clustering"
  },
  {
    "objectID": "danl-lec/danl-320-lec-11-2025-0430.html#hierarchical-clustering-1",
    "href": "danl-lec/danl-320-lec-11-2025-0430.html#hierarchical-clustering-1",
    "title": "Lecture 11",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nfcluster() extract the members of each cluster from the linkage() object.\ngroups_hc = fcluster(link, t=5, criterion=\"maxclust\")      # k = 5\n\n# Convenience: print selected cols by cluster\ndef print_clusters(df: pd.DataFrame, labels, cols):\n    for k, sub in df.assign(cluster=labels).groupby(\"cluster\"):\n        print(f\"\\nCluster {k} ({len(sub)} obs)\")\n        print(sub[cols].to_string(index=False))\n\ncols_to_print = [\"Country\", \"RedMeat\", \"Fish\", \"FrVeg\"]\nprint_clusters(protein, groups_hc, cols_to_print)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-11-2025-0430.html#principal-component-analysis-pca",
    "href": "danl-lec/danl-320-lec-11-2025-0430.html#principal-component-analysis-pca",
    "title": "Lecture 11",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\n\n\n\n\nWe can visualize the clustering by projecting the data onto the first two principal components of the data.\nEllipsoid is described by three principal components.\nThe ellipsoid roughly bounds the data.\nThe first two principal components, \\(\\text{PC}_{1}\\) & \\(\\text{PC}_{2}\\), describe the best 2-D projection of the data.\n\nNotice that the principal components are orthogonal.\n\\(Cor(\\text{PC}_{1}, \\text{PC}_{2}) = 0\\)!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-11-2025-0430.html#principal-component-analysis",
    "href": "danl-lec/danl-320-lec-11-2025-0430.html#principal-component-analysis",
    "title": "Lecture 11",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\nRotation and interpretation\n\nPrincipal components are unknown but associated with variables in the data.\n\nA principal component can be interpreted in terms of how much each variable in the data is associated with each principal component.\n\nPrincipal components describe the data by projecting the data into the space of the (independent) principal components, which is called rotations.\nThe maximum number of principal components of the data is the number of numeric variables in the data."
  },
  {
    "objectID": "danl-lec/danl-320-lec-11-2025-0430.html#principal-component-analysis-1",
    "href": "danl-lec/danl-320-lec-11-2025-0430.html#principal-component-analysis-1",
    "title": "Lecture 11",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\nThe PCA() function does the principal components decomposition.\npca = PCA().fit(pmatrix)\nproj = pca.transform(pmatrix)[:, :2]                # first 2 PCs\n\nproj_df = (\n    pd.DataFrame(proj, columns=[\"PC1\", \"PC2\"])\n      .assign(cluster=groups_hc.astype(str), \n              country=protein[\"Country\"])\n)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-11-2025-0430.html#principal-component---interpretation",
    "href": "danl-lec/danl-320-lec-11-2025-0430.html#principal-component---interpretation",
    "title": "Lecture 11",
    "section": "Principal Component - Interpretation",
    "text": "Principal Component - Interpretation\n# print first 3 loading vectors \nloadings_protein = pd.DataFrame(pca.components_.T, index = protein.drop(columns=\"Country\").columns,\n                                columns=[f\"PC{i}\" for i in range(1, pca.n_components_+1)])\nprint(loadings_protein.iloc[:, :3].round(2))\n\n\n\n\n\n\n\n\\(PC_{1}\\) is high nut/grain and low meat/dairy:\n\n\\(PC_{1}\\) increases by 0.42 with 1 S.D. increase in nuts.\n\\(PC_{1}\\) decreases by 0.30 with 1 S.D. increase in red meat.\n\n\\(PC_{2}\\) is Iberian:\n\n\\(PC_{2}\\) increases by 0.65 with 1 S.D. extra fish."
  },
  {
    "objectID": "danl-lec/danl-320-lec-11-2025-0430.html#pca-projection-by-ward-clusters",
    "href": "danl-lec/danl-320-lec-11-2025-0430.html#pca-projection-by-ward-clusters",
    "title": "Lecture 11",
    "section": "PCA projection by Ward clusters",
    "text": "PCA projection by Ward clusters\n# Global axis limits so every facet is identical\nxlim = proj_df[\"PC1\"].min() - .5, proj_df[\"PC1\"].max() + .5\nylim = proj_df[\"PC2\"].min() - .5, proj_df[\"PC2\"].max() + .5\n\n# Create FacetGrid with shared axes\ng = sns.FacetGrid(\n        proj_df, col=\"cluster\", col_wrap=3, height=3.5,\n        sharex=True, sharey=True, hue=\"cluster\", palette=\"tab10\"\n    )\n# --------------------------------------------------\n# Helper that draws the full background cloud\n# --------------------------------------------------\ndef background_scatter(x, y, **kwargs):\n    \"\"\"Ignore facet-specific x & y — plot all points instead.\"\"\"\n    plt.scatter(\n        proj_df[\"PC1\"], proj_df[\"PC2\"],\n        color=\"lightgray\", alpha=.25, s=15, zorder=1\n    )\n\n# Plot the background in every facet\ng.map(background_scatter, \"PC1\", \"PC2\")\n\n# Foreground: cluster-specific points\ng.map_dataframe(\n    sns.scatterplot, x=\"PC1\", y=\"PC2\",\n    edgecolor=\"black\", linewidth=.3, s=35, zorder=2\n)\n\n# Text labels for the cluster’s own points\ndef annotate(data, **k):\n    for x, y, label in zip(data.PC1, data.PC2, data.country):\n        plt.text(x, y, label, fontsize=7, ha=\"left\", va=\"top\")\ng.map_dataframe(annotate)\n\n# Apply identical limits to every axis\nfor ax in g.axes.flat:\n    ax.set_xlim(xlim)\n    ax.set_ylim(ylim)\n\ng.fig.subplots_adjust(top=.9)\ng.fig.suptitle(\"PCA projection by Ward clusters\")\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-11-2025-0430.html#k-means-algorithm",
    "href": "danl-lec/danl-320-lec-11-2025-0430.html#k-means-algorithm",
    "title": "Lecture 11",
    "section": "K-means Algorithm",
    "text": "K-means Algorithm\n\nK-Means partitions data into \\(k\\) groups by assigning each point to the nearest cluster center and iteratively updating the centers to minimize within-cluster variance."
  },
  {
    "objectID": "danl-lec/danl-320-lec-11-2025-0430.html#k-means-algorithm-1",
    "href": "danl-lec/danl-320-lec-11-2025-0430.html#k-means-algorithm-1",
    "title": "Lecture 11",
    "section": "K-means Algorithm",
    "text": "K-means Algorithm\nKMeans() function implements the k-means algorithm.\nk = 5\nkmeans = KMeans(n_clusters= k, n_init=100, max_iter=100, random_state=1)\ngroups_km = kmeans.fit_predict(pmatrix)\n\nprint(\"k-means cluster sizes:\", np.bincount(groups_km))\nprint_clusters(protein, groups_km + 1, cols_to_print)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-11-2025-0430.html#best-number-of-clusters",
    "href": "danl-lec/danl-320-lec-11-2025-0430.html#best-number-of-clusters",
    "title": "Lecture 11",
    "section": "Best Number of Clusters",
    "text": "Best Number of Clusters\n\n\n\n\n\n\n\n\n\n\nCalinski-Harabasz Index: the adjusted ratio of between-sum-of-square (BSS) to within-sum-of-square (WSS).\n\n\\(\\text{BSS} = \\text{TSS} - \\text{WSS}\\), where \\(\\text{TSS}\\) is total-sum-of-square.\nGood clustering: a small average WSS & a large average BSS.\n\nAverage Silhouette Width (ASW) Index: the mean of individual silhouette coefficients\n\nSilhouette coefficient quantifies how well a point sits in its cluster by comparing cohesion and separation metrics."
  },
  {
    "objectID": "danl-lec/danl-320-lec-11-2025-0430.html#best-number-of-clusters-1",
    "href": "danl-lec/danl-320-lec-11-2025-0430.html#best-number-of-clusters-1",
    "title": "Lecture 11",
    "section": "Best Number of Clusters",
    "text": "Best Number of Clusters\nkrange = range(2, 11)      # k = 2…10  (k=1 undefined for CH/ASW)\nch_scores, asw_scores = [], []\n\nfor k in krange:\n    lbls = KMeans(n_clusters=k, n_init=20, random_state=0).fit_predict(pmatrix)\n    ch_scores.append(calinski_harabasz_score(pmatrix, lbls))\n    asw_scores.append(silhouette_score(pmatrix, lbls))\n\nbest_k_ch  = krange[np.argmax(ch_scores)]\nbest_k_asw = krange[np.argmax(asw_scores)]\nprint(f\"Best k by CH : {best_k_ch}\")\nprint(f\"Best k by ASW: {best_k_asw}\")\n\nThe \\(k^{*} = 2\\) clustering corresponds to the first split of the protein data dendrogram.\n\nClustering with \\(k^{*} = 2\\) might not be so informative."
  },
  {
    "objectID": "danl-lec/danl-320-lec-11-2025-0430.html#nbc-show",
    "href": "danl-lec/danl-320-lec-11-2025-0430.html#nbc-show",
    "title": "Lecture 11",
    "section": "NBC Show",
    "text": "NBC Show\n\nData from NBC on response to TV pilots\n\nGross Ratings Points (GRP): estimated total viewership, which measures broadcast marketability.\nProjected Engagement (PE): a more subtle measure of audience.\n\nAfter watching a show, viewer is quizzed on order and detail.\nThis measures their engagement with the show (and ads!).\n\n\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nimport statsmodels.api as sm\n\n# ---------------------------------------------------------\n# 1.  TV-show metadata\n# ---------------------------------------------------------\nurl_shows   = \"https://bcdanl.github.io/data/nbc_show.csv\"\nshows = (\n    pd.read_csv(url_shows, index_col=0)\n      .assign(Genre=lambda d: d[\"Genre\"].astype(\"category\"))\n)\n# quick glance\nsns.lmplot(\n    data=shows, x=\"GRP\", y=\"PE\",\n    hue=\"Genre\", height=5, aspect=1.2, scatter_kws=dict(s=40, alpha=.8),\n    line_kws=dict(linewidth=1.2), ci=None\n)\nplt.title(\"Gross Rating Points vs. Projected Engagement\")\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-11-2025-0430.html#nbc-show-survey",
    "href": "danl-lec/danl-320-lec-11-2025-0430.html#nbc-show-survey",
    "title": "Lecture 11",
    "section": "NBC Show Survey",
    "text": "NBC Show Survey\n\nAfter watching a show, viewer is quizzed on order and detail.\n\nThe survey data include 6241 views and 20 questions for 40 shows.\n\nThere are two types of questions in the survey.\n\nFor Q1, this statement takes the form of “This show makes me feel …”\nFor Q2, the statement is “I find this show feel …”\n\nTo relate survey results to show performance, we first calculate the average survey response by show.\n\nurl_survey  = \"https://bcdanl.github.io/data/nbc_survey.csv\"\nsurvey = pd.read_csv(url_survey)\n\n# lock survey Show order to match *shows* index\nsurvey[\"Show\"] = pd.Categorical(\n    survey[\"Show\"], categories=shows.index, ordered=True\n)\n\n# average each question by show (drops first two cols = Show, Viewer)\npilot_avg = (\n    survey\n      .drop(columns=survey.columns[:2])\n      .join(survey[\"Show\"])\n      .groupby(\"Show\")\n      .mean()\n      .reindex(shows.index)              # make absolutely sure the order matches\n)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-11-2025-0430.html#scree-plot",
    "href": "danl-lec/danl-320-lec-11-2025-0430.html#scree-plot",
    "title": "Lecture 11",
    "section": "Scree Plot",
    "text": "Scree Plot\nThe scree plot is simply showing us, for each principal component \\(j\\) (on the \\(x\\)-axis), what fraction of the total variance in our pilot-survey data that component captures (on the \\(y\\)-axis).\nscaler = StandardScaler()\nX      = scaler.fit_transform(pilot_avg)\npca    = PCA().fit(X)\n\n# scree plot\nplt.figure(figsize=(5,3))\nplt.plot(np.arange(1, pca.n_components_+1), pca.explained_variance_ratio_, \"o-\")\nplt.xlabel(\"Principal component\")\nplt.ylabel(\"Variance explained\")\nplt.title(\"Pilot-Survey PCs\")\nplt.show()\n\\[\nVar(PC_{1}) \\geq Var(PC_{2}) \\geq Var(PC_{3}) \\geq \\cdots\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-11-2025-0430.html#pc-interpretation",
    "href": "danl-lec/danl-320-lec-11-2025-0430.html#pc-interpretation",
    "title": "Lecture 11",
    "section": "PC Interpretation",
    "text": "PC Interpretation\n# print first 3 loading vectors \nloadings = pd.DataFrame(pca.components_.T,\n                        index=pilot_avg.columns,\n                        columns=[f\"PC{i}\" for i in range(1, pca.n_components_+1)])\nprint(loadings.iloc[:, :3].round(1))\n\n\\(\\text{PC}_{1}\\) (“Overall Engagement”)\n\\(\\text{PC}_{2}\\) (“Passive/Comfort vs. Active Drama”)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-11-2025-0430.html#pca-projection-of-the-nbc-show",
    "href": "danl-lec/danl-320-lec-11-2025-0430.html#pca-projection-of-the-nbc-show",
    "title": "Lecture 11",
    "section": "PCA Projection of the NBC Show",
    "text": "PCA Projection of the NBC Show\n# project data into PC space\nZ = pca.transform(X)\nzpilot_df = (\n    pd.DataFrame(Z, columns=[f\"PC{i}\" for i in range(1, pca.n_components_+1)])\n      .assign(Shows=shows.index)\n      .join(shows.reset_index(drop=True))\n)\n\nzpilot_df[\"PE_norm\"] = (zpilot_df[\"PE\"] - zpilot_df[\"PE\"].min()) \\\n                       / (zpilot_df[\"PE\"].max() - zpilot_df[\"PE\"].min())\n\n# ---------- STATIC ----------------------------------------------------------\nplt.figure(figsize=(6, 5))\nsns.scatterplot(\n    data=zpilot_df,\n    x=\"PC1\", y=\"PC2\",\n    hue=\"Genre\",\n    size=\"PE_norm\",          # bigger bubble == bigger PE\n    sizes=(20, 250),         # min & max dot size in points²\n    alpha=0.8,               # one scalar alpha for *all* points\n    legend=\"brief\"\n)\n\n# add labels\nfor _, r in zpilot_df.iterrows():\n    plt.text(r[\"PC1\"] + .05, r[\"PC2\"] + .05, r[\"Shows\"], fontsize=7)\n\nplt.title(\"NBC pilots in survey PC space\")\nplt.show()\n\n# ---------- INTERACTIVE -----------------------------------------------------\nfig = px.scatter(\n    zpilot_df, x=\"PC1\", y=\"PC2\",\n    color=\"Genre\",\n    size=\"PE_norm\",              # same visual cue as above\n    size_max=25,\n    hover_name=\"Shows\",\n    opacity=0.85                 # one scalar for the whole trace\n)\nfig.update_layout(title=\"NBC pilots in survey PC space (interactive)\")\nfig.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-11-2025-0430.html#principal-component-regression-pcr",
    "href": "danl-lec/danl-320-lec-11-2025-0430.html#principal-component-regression-pcr",
    "title": "Lecture 11",
    "section": "Principal Component Regression (PCR)",
    "text": "Principal Component Regression (PCR)\n\nPCR uses a lower-dimension set of principal components as predictors.\n\nPC analysis can reduce dimension, which is usually good.\nThe PCs are independent (\\(Cov(PC_{i}, PC_{j}) = 0\\)).\n\nPC analysis will be driven by the dominant sources of variation in \\(x\\).\n\nIf the outcome is connected to these dominant sources of variation, PCR works well.\n\nThe AIC (Akaike information criterion) is negatively associated to the probability that the data would be observed from the model.\n\nThe lower AIC is, the better model is.\n\n\n# ---------------------------------------------------------\n# 5a.  Principal-components regression (PE ~ first K PCs)\n# ---------------------------------------------------------\nPE        = shows[\"PE\"].to_numpy()\nmax_K     = min(20, Z.shape[1])           # never exceed available PCs\naic_vals  = []\n\nfor k in range(1, max_K+1):\n    Xk   = sm.add_constant(Z[:, :k])\n    res  = sm.GLM(PE, Xk).fit()\n    aic_vals.append(res.aic)\n    # Uncomment for verbose summaries—\n    # print(f\"\\n### K = {k}\\n\", res.summary())\n\nbest_k = np.argmin(aic_vals) + 1\nprint(f\"Lowest AIC achieved with K = {best_k} PCs.\")\n\n# Optional: plot AIC vs. K\nplt.plot(range(1, max_K+1), aic_vals, \"o-\")\nplt.xticks(range(1, max_K+1))\nplt.xlabel(\"Number of principal components (K)\")\nplt.ylabel(\"AIC\")\nplt.title(\"PCR model of PE comparison\")\nplt.show()\n\n\n# ---------------------------------------------------------\n# 5b.  Principal-components regression (GRP ~ first K PCs)\n# ---------------------------------------------------------\nGRP        = shows[\"GRP\"].to_numpy()\nmax_K     = min(20, Z.shape[1])           # never exceed available PCs\naic_vals  = []\n\nfor k in range(1, max_K+1):\n    Xk   = sm.add_constant(Z[:, :k])\n    res  = sm.GLM(GRP, Xk).fit()\n    aic_vals.append(res.aic)\n    # Uncomment for verbose summaries—\n    # print(f\"\\n### K = {k}\\n\", res.summary())\n\nbest_k = np.argmin(aic_vals) + 1\nprint(f\"Lowest AIC achieved with K = {best_k} PCs.\")\n\n# Optional: plot AIC vs. K\nplt.plot(range(1, max_K+1), aic_vals, \"o-\")\nplt.xticks(range(1, max_K+1))\nplt.xlabel(\"Number of principal components (K)\")\nplt.ylabel(\"AIC\")\nplt.title(\"PCR model comparison\")\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#big-data-and-machine-learning-ml",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#big-data-and-machine-learning-ml",
    "title": "Lecture 7",
    "section": "Big Data and Machine Learning (ML)",
    "text": "Big Data and Machine Learning (ML)\n\nThe term “Big Data” originated from computer scientists working with datasets too large to fit on a single machine.\n\nAs aggregation evolved into analysis, Big Data became closely linked with statistics and machine learning (ML).\nScalability of algorithms is crucial for handling large datasets.\n\nWe will use ML to:\n✅ Identify patterns in big data\n✅ Make data-driven decisions"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#what-does-it-mean-to-be-big",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#what-does-it-mean-to-be-big",
    "title": "Lecture 7",
    "section": "What does it mean to be “big”?",
    "text": "What does it mean to be “big”?\n\nBig in both the number of observations (size n) and in the number of variables (dimension p).\nIn these settings, we cannot:\n\nLook at each individual variable and make a decision.\nChoose among a small set of candidate models.\nPlot every variable to look for interactions or transformations.\n\nSome ML tools are straight out of previous statistics classes (linear regression) and some are totally new (ensemble models, principal component analysis).\n\nAll require a different approach when n and p get really big."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#ml-topics",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#ml-topics",
    "title": "Lecture 7",
    "section": "ML topics",
    "text": "ML topics\n\nRegression: inference and prediction\nRegularization: cross-validation\nPrincipal Component Analysis: dimension reduction\nTree-based models: decision trees, random forest, XGBoost\nClassfication: kNN\nClustering: k-means, association rules\nText Mining: sentiment analysis; topic models"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions",
    "title": "Lecture 7",
    "section": "Models and Assumptions",
    "text": "Models and Assumptions\nLinear Model\n\nLinear regression assumes a linear relationship for \\(Y = f(X_{1})\\):\n\n\\[Y_{i} \\,=\\, \\beta_{0} \\,+\\, \\beta_{1} X_{1, i} \\,+\\, \\epsilon_{i}\\] for \\(i \\,=\\, 1, 2, \\dots, n\\), where \\(i\\) is the \\(i\\)-th observation in data.\n\n\\(Y_i\\) is the \\(i\\)-th value for the outcome/dependent/response/target variable \\(Y\\).\n\\(X_{1, i}\\) is the \\(i\\)-th value for the explanatory/independent/predictor/input variable or feature \\(X_{1}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions-1",
    "title": "Lecture 7",
    "section": "Models and Assumptions",
    "text": "Models and Assumptions\nBeta coefficients\n\nLinear regression assumes a linear relationship for \\(Y = f(X_{1})\\):\n\n\\[Y_{i} \\,=\\, \\beta_{0} \\,+\\, \\beta_{1} X_{1, i} \\,+\\, \\epsilon_{i}\\] for \\(i \\,=\\, 1, 2, \\dots, n\\), where \\(i\\) is the \\(i\\)-th observation in data.\n\n\\(\\beta_0\\) is an unknown true value of an intercept: average value for \\(Y\\) if \\(X_{1} = 0\\)\n\\(\\beta_1\\) is an unknown true value of a slope: increase in average value for \\(Y\\) for each one-unit increase in \\(X_{1}\\)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions-2",
    "title": "Lecture 7",
    "section": "Models and Assumptions",
    "text": "Models and Assumptions\nRandom Noises\n\nLinear regression assumes a linear relationship for \\(Y = f(X_{1})\\):\n\n\\[Y_{i} \\,=\\, \\beta_{0} \\,+\\, \\beta_{1} X_{1, i} \\,+\\, \\epsilon_{i}\\] for \\(i \\,=\\, 1, 2, \\dots, n\\), where \\(i\\) is the \\(i\\)-th observation in data.\n\n\\(\\epsilon_i\\) is a random noise, or a statistical error:\n\n\\[\n\\epsilon_i \\sim N(0, \\sigma^2)\n\\]\n\nErrors have a mean value of 0 with constant variance \\(\\sigma^2\\).\nErrors are uncorrelated with \\(X_{1, i}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#what-is-linear-regression-doing",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#what-is-linear-regression-doing",
    "title": "Lecture 7",
    "section": "What Is Linear Regression Doing?",
    "text": "What Is Linear Regression Doing?\nBest Fitting Line\n\nLinear regression finds the beta estimates \\(( \\hat{\\beta_{0}}, \\hat{\\beta_{1}} )\\) such that:\n– The linear function \\(f(X_{1}) = \\hat{\\beta_{0}} + \\hat{\\beta_{1}}X_{1}\\) is as near as possible to \\(Y\\) for all \\((X_{1, i}\\,,\\, Y_{i})\\) pairs in the data.\n\nIt is the best fitting line, or the predicted outcome, \\(\\hat{Y_{\\,}} = \\hat{\\beta_{0}} + \\hat{\\beta_{1}}X_{1}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#what-is-linear-regression-doing-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#what-is-linear-regression-doing-1",
    "title": "Lecture 7",
    "section": "What Is Linear Regression Doing?",
    "text": "What Is Linear Regression Doing?\nResidual errors\n\nThe estimated beta coefficients are chosen to minimize the sum of squares of the residual errors \\((SSR)\\): \\[\n\\begin{align}\nSSR &\\,=\\, (\\texttt{Residual_Error}_{1})^{2}\\\\\n&\\quad \\,+\\, (\\texttt{Residual_Error}_{2})^{2}\\\\\n&\\quad\\,+\\, \\cdots + (\\texttt{Residual_Error}_{n})^{2}\\\\\n\\text{where}\\qquad\\qquad\\qquad\\qquad&\\\\\n\\texttt{Residual_Error}_{i} &\\,=\\, Y_{i} \\,-\\, \\hat{Y_{i}},\\\\\n\\texttt{Predicted_Outcome}_{i}: \\hat{Y_{i}} &\\,=\\, \\hat{\\beta_{0}} \\,+\\, \\hat{\\beta_{1}}X_{1, i}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#what-is-linear-regression-doing-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#what-is-linear-regression-doing-2",
    "title": "Lecture 7",
    "section": "What Is Linear Regression Doing?",
    "text": "What Is Linear Regression Doing?\nHat Notation\n\nWe use the hat notation \\((\\,\\hat{\\texttt{ }_{\\,}}\\,)\\) to distinguish true values and estimated/predicted values.\nThe value of true beta coefficient is denoted by \\(\\beta_{1}\\).\nThe value of estimated beta coefficient is denoted by \\(\\hat{\\beta_{1}}\\).\nThe \\(i\\)-th value of true outcome variable is denoted by \\(Y_{i}\\).\nThe \\(i\\)-th value of predicted outcome variable is denoted by \\(\\hat{Y_{i}}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#what-is-linear-regression-doing-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#what-is-linear-regression-doing-3",
    "title": "Lecture 7",
    "section": "What Is Linear Regression Doing?",
    "text": "What Is Linear Regression Doing?\nRelationship\n1. Finding the relationship between \\(X_{1}\\) and \\(Y\\) \\[\\hat{\\beta_{1}}\\]: How is an increase in \\(X_1\\) by one unit associated with a change in \\(Y\\) on average? - Positive? Negative? Independent? - How strong?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#statistical-significance-in-estimated-beta-coefficients",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#statistical-significance-in-estimated-beta-coefficients",
    "title": "Lecture 7",
    "section": "Statistical Significance in Estimated Beta Coefficients",
    "text": "Statistical Significance in Estimated Beta Coefficients\n\nWhat does it mean for a beta estimate \\(\\hat{\\beta_{\\,}}\\) to be statistically significant at 5% level?\n\nIt means that the null hypothesis \\(H_{0}: \\beta = 0\\) is rejected for a given significance level 5%.\n“2 standard error rule” of thumb: The true value of \\(\\beta\\) is 95% likely to be in the confidence interval \\((\\, \\hat{\\beta_{\\,}} - 2 * \\texttt{Std. Error}\\;,\\; \\hat{\\beta_{\\,}} + 2 * \\texttt{Std. Error} \\,)\\).\nThe standard error tells us how uncertain our beta estimate is.\nWe should look for the stars!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#what-is-linear-regression-doing-4",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#what-is-linear-regression-doing-4",
    "title": "Lecture 7",
    "section": "What Is Linear Regression Doing?",
    "text": "What Is Linear Regression Doing?\nPrediction\n2. Making a prediction on \\(Y\\): \\[\\hat{Y_{\\,}}\\] For unseen data point of \\(X_1\\), what is the predicted value of outcome, \\(\\hat{Y_{\\,}}\\)?\n\nE.g., For \\(X_{1} = 2\\), the predicted outcome is \\(\\hat{Y_{\\,}} = \\hat{\\beta_{0}} + \\hat{\\beta_{1}} \\times 2\\).\nE.g., For \\(X_{1} = 3\\), the predicted outcome is \\(\\hat{Y_{\\,}} = \\hat{\\beta_{0}} + \\hat{\\beta_{1}} \\times 3\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression---example",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression---example",
    "title": "Lecture 7",
    "section": "Linear Regression - Example",
    "text": "Linear Regression - Example\n\nSuppose we want to predict a property’s sales price based on the property size.\n\nIn other words, for some house sale i, we want to predict sale_price[i] based on gross_square_feet[i].\n\nWe also want to focus on the relationship between a property’s sales price and a property size.\n\nIn other words, we estimate how an increase in gross_square_feet[i] is associated with sale_price[i]."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-relationship",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-relationship",
    "title": "Lecture 7",
    "section": "Linear Relationship",
    "text": "Linear Relationship\n\nLinear regression assumes that:\n\nThe outcome sale_price[i] is linearly related to the input gross_square_feet[i]:\n\n\n\\[\\texttt{sale_price[i]} \\;=\\quad \\texttt{b0} \\,+\\, \\texttt{b1*gross_square_feet[i]} \\,+\\, \\texttt{e[i]}\\] where e[i] is a statistical error term."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#the-linear-relationship-between-sale_price-and-gross_square_feet",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#the-linear-relationship-between-sale_price-and-gross_square_feet",
    "title": "Lecture 7",
    "section": "The Linear Relationship between sale_price and gross_square_feet",
    "text": "The Linear Relationship between sale_price and gross_square_feet"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#best-fitting-line-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#best-fitting-line-1",
    "title": "Lecture 7",
    "section": "Best Fitting Line",
    "text": "Best Fitting Line\n\n\nWhat do the vertical lines visualize?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation",
    "title": "Lecture 7",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nMean squared error (MSE)\n\nOne of the most common metrics used to measure the prediction accuracy of a linear regression model is MSE, which stands for mean squared error.\n\n\\(MSE\\) is \\(SSR\\) divided by \\(n\\) (the number of observations in the data that are used in making predictions).\n\n\n\n\n\n\\[\nMSE = SSR / n\n\\] \\[\n\\begin{align}\nSSR &\\,=\\, (\\texttt{Residual_Error}_{1})^{2}\\\\\n&\\quad \\,+\\, (\\texttt{Residual_Error}_{2})^{2}\\\\\n&\\quad\\,+\\, \\cdots + (\\texttt{Residual_Error}_{n})^{2}\n\\end{align}\n\\]\n\n\n\n\nThe lower MSE, the higher accuracy of the model.\n\nThe root MSE (RMSE) is the square root of MSE."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-1",
    "title": "Lecture 7",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nMean squared error (MSE)\n\nThe root MSE (RMSE) represents the overall deviation of \\(Y_{i}\\) from the best fitting regression line."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-2",
    "title": "Lecture 7",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nR-squared\n\nR-squared is a measure of how well the model “fits” the data, or its “goodness of fit.”\n\nR-squared can be thought of as what fraction of the y’s variation is explained by the explanatory variables.\n\nWe want R-squared to be fairly large and R-squareds that are similar on testing and training.\nCaution: R-squared will be higher for models with more explanatory variables, regardless of whether the additional explanatory variables actually improve the model or not."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#goals-of-linear-regression",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#goals-of-linear-regression",
    "title": "Lecture 7",
    "section": "Goals of Linear Regression",
    "text": "Goals of Linear Regression\n\nThe goals of linear regression here are to:\n\n\nModeling for explanation: Find the relationship between gross_square_feet and sale_price by estimating a true value of b1.\n\n\nThe estimated value of b1 is denoted by \\(\\hat{\\texttt{b1}}\\).\n\n\nModeling for prediction: Make a prediction on sale_price[i] for new property i\n\n\nThe predicted value of sale_price[i] is denoted by \\(\\widehat{\\texttt{sale_price}}\\texttt{[i]}\\), where\n\n\\[\\widehat{\\texttt{sale_price}}\\texttt{[i]} \\;=\\quad \\hat{\\texttt{b0}} \\,+\\, \\hat{\\texttt{b1}}\\texttt{*gross_square_feet[i]}\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#training-and-test-data",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#training-and-test-data",
    "title": "Lecture 7",
    "section": "Training and Test Data",
    "text": "Training and Test Data\n\nTraining data: When we’re building a linear regression model, we need data to train the model.\nTest data: We also need data to test whether the model works well on new data.\n\n\n\n\nSo, we start with splitting a given data.frame into training and test data.frames when building a linear regression model."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#training-and-test-data-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#training-and-test-data-1",
    "title": "Lecture 7",
    "section": "Training and Test Data",
    "text": "Training and Test Data\nTraining vs. Test\n\nWe use training data to train/fit the linear regression model.\n\nWe then make a prediction using test data, which are unseen/new from the viewpoint of the trained linear regression model.\n\nIn this way, we can test whether our model performs well in the real world, where unseen data points exist."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#training-and-test-data-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#training-and-test-data-2",
    "title": "Lecture 7",
    "section": "Training and Test Data",
    "text": "Training and Test Data\nOverfitting"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#training-and-test-data-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#training-and-test-data-3",
    "title": "Lecture 7",
    "section": "Training and Test Data",
    "text": "Training and Test Data\nModel Construction and Evaluation"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#splitting-data-into-training-and-testing-data",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#splitting-data-into-training-and-testing-data",
    "title": "Lecture 7",
    "section": "Splitting Data into Training and Testing Data",
    "text": "Splitting Data into Training and Testing Data\nA Little Bit of Statistics for the Uniform Distribution\n\n\n\nThe probability density function for the uniform distribution looks like:\nWith the uniform distribution, any values of \\(x\\) between 0 and 1 is equally likely drawn.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will use the uniform distribution when splitting data into training and testing data sets."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#splitting-data-into-training-and-testing-data-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#splitting-data-into-training-and-testing-data-1",
    "title": "Lecture 7",
    "section": "Splitting Data into Training and Testing Data",
    "text": "Splitting Data into Training and Testing Data\nRandomization in the Sampling Process\n\nWhy do we randomize when splitting given data into training and test data?\n\nRandomizing the sampling process ensures that the training and test data sets are representative for the population data.\nIf the sample does not properly represent the entire population, the model result is biased toward the sample.\n\nSuppose the splitting process is not randomized, so that the observations with sale_price &gt; 10^6 are in the training data and the observations with sale_price &lt;= 10^6 are in the test data.\n\nWhat would be the model result then?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#example-of-linear-regression-using-pyspark",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#example-of-linear-regression-using-pyspark",
    "title": "Lecture 7",
    "section": "Example of Linear Regression using PySpark",
    "text": "Example of Linear Regression using PySpark\n\nWe will use the data for residential property sales from September 2017 and August 2018 in NYC.\nEach sales data recorded contains a number of interesting variables, but here we focus on the followings:\n\nsale_price: a property’s sales price;\ngross_square_feet: a property’s size;\nage: a property’s age;\nborough_name: a borough where a property is located.\n\nUse summary statistics and visualization to explore the data."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#splitting-data-into-training-and-testing-data-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#splitting-data-into-training-and-testing-data-2",
    "title": "Lecture 7",
    "section": "Splitting Data into Training and Testing Data",
    "text": "Splitting Data into Training and Testing Data\nStep 1. Importing Modules and Reading DataFrames\nimport pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n# 1. Read CSV data from URL\ndf_pd = pd.read_csv('https://bcdanl.github.io/data/home_sales_nyc.csv')\nsale_df = spark.createDataFrame(df_pd)\nsale_df.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#splitting-data-into-training-and-testing-data-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#splitting-data-into-training-and-testing-data-3",
    "title": "Lecture 7",
    "section": "Splitting Data into Training and Testing Data",
    "text": "Splitting Data into Training and Testing Data\nStep 2. rand(seed = ANY_NUMBER)\n# 2. Split data into training and testing sets by creating a random column \"gp\"\nsale_df = sale_df.withColumn(\"gp\", rand(seed=123)) # seed is set for replication\n\n# Splits 60-40 into training and test sets \ndtrain = sale_df.filter(col(\"gp\") &gt;= 0.4) \ndtest = sale_df.filter(col(\"gp\") &lt; 0.4)\n\n# Or simply,\ndtrain, dtest = sale_df.randomSplit([0.6, 0.4], seed = 123)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#building-an-ml-dataframe-using-vectorassembler",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#building-an-ml-dataframe-using-vectorassembler",
    "title": "Lecture 7",
    "section": "Building an ML DataFrame using VectorAssembler()",
    "text": "Building an ML DataFrame using VectorAssembler()\n# Now assemble predictors using the renamed column\nassembler1 = VectorAssembler(\n    inputCols=[\"gross_square_feet\"], \n    outputCol=\"predictors\")\n\nVectorAssembler is a transformer in PySpark’s ML library that is used to combine multiple columns into a single vector column.\n\nMany ML algorithms in Spark require the predictors to be represented as a single vector.\nVectorAssembler is often one of the first steps in a Spark ML pipeline."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#building-an-ml-dataframe-using-vectorassembler-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#building-an-ml-dataframe-using-vectorassembler-1",
    "title": "Lecture 7",
    "section": "Building an ML DataFrame using VectorAssembler()",
    "text": "Building an ML DataFrame using VectorAssembler()\ndtrain1 = assembler1.transform(dtrain) # training data\ndtest1  = assembler1.transform(dtest)  # test data\n\ndtrain1.select(\"predictors\", \"sale_price\").show()\ndtest1.select(\"predictors\", \"sale_price\").show()\n\nVectorAssembler.transform() returns a DataFrame with a new column, specified in outputCol in VectorAssembler()."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#building-a-linear-regression-model-using-linearregression.fit",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#building-a-linear-regression-model-using-linearregression.fit",
    "title": "Lecture 7",
    "section": "Building a Linear Regression Model using LinearRegression().fit()",
    "text": "Building a Linear Regression Model using LinearRegression().fit()\n# Fit linear regression model using the new label column \"sale_price\"\nmodel1 = (\n    LinearRegression(\n        featuresCol = \"predictors\", \n        labelCol = \"sale_price\")\n    .fit(dtrain1)\n)\n\nLinearRegression(featuresCol=\"predictors\", labelCol=\"sale_price\")\n\nThis creates an instance of the LinearRegression class.\nThe features (independent variables) are in a column named “predictors”.\nThe label (dependent variable) is in a column named “sale_price”.\n\n.fit() trains (fits) the linear regression model using the training DataFrame dtrain1.\n\nThis training process estimates the beta coefficients that best predict the label from the features."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#building-a-linear-regression-model-using-linearregression.fit-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#building-a-linear-regression-model-using-linearregression.fit-1",
    "title": "Lecture 7",
    "section": "Building a Linear Regression Model using LinearRegression().fit()",
    "text": "Building a Linear Regression Model using LinearRegression().fit()\n# Adding a \"prediction\" coulmn to dtest1 DataFrame\ndtest1  = model1.transform(dtest1)\n\ndtest1 = model1.transform(dtest1): Adds a new column prediction to dtest1 DataFrame.\n\nThis new column contains the predicted outcome based on the trained model1 to predict an outcome for the test dataset dtest1."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#summary-of-the-regression-result",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#summary-of-the-regression-result",
    "title": "Lecture 7",
    "section": "Summary of the Regression Result",
    "text": "Summary of the Regression Result\nmodel1.intercept\nmodel1.coefficients\nmodel1.summary.coefficientStandardErrors\nmodel1.summary.rootMeanSquaredError\nmodel1.summary.r2"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#summary-of-the-regression-result-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#summary-of-the-regression-result-1",
    "title": "Lecture 7",
    "section": "Summary of the Regression Result",
    "text": "Summary of the Regression Result\nExample of a Regression Table"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#summary-of-the-regression-result-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#summary-of-the-regression-result-2",
    "title": "Lecture 7",
    "section": "Summary of the Regression Result",
    "text": "Summary of the Regression Result\nMake the Summary Pretty\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                ci_upper,\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: Exp(95% CI Lower), 8: 95% CI Upper, 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"Exp(95% CI Lower)\", \"95% CI Upper\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#summary-of-the-regression-result-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#summary-of-the-regression-result-3",
    "title": "Lecture 7",
    "section": "Summary of the Regression Result",
    "text": "Summary of the Regression Result\nMake the Summary Pretty\n# Using the UDF, regression_table(model, assembler)\nprint(regression_table(model1, assembler1))"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression",
    "title": "Lecture 7",
    "section": "Multiple Regression",
    "text": "Multiple Regression\n\nWhat if the regression were missing something?\n\nMaybe prices are not just about size, but maybe there are certain parts of NYC that are categorically more expensive than other parts of NYC.\nMaybe Manhattan is just more expensive than Bronx.\nMaybe apartments are different than non-apartments.\nMaybe old houses are different than new houses.\n\nIt is often helpful to bring in multiple explanatory variables—a Multivariate Regression."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions-3",
    "title": "Lecture 7",
    "section": "Models and Assumptions",
    "text": "Models and Assumptions\n\nLinear regression assumes a linear relationship for \\(Y = f(X_{1}, X_{2})\\):\n\n\\[Y_{i} \\,=\\, \\beta_{0} \\,+\\, \\beta_{1} X_{1, i} \\,+\\,\\beta_{2} X_{2, i} \\,+\\, \\epsilon_{i}\\] for \\(i \\,=\\, 1, 2, \\dots, n\\), where \\(i\\) is the \\(i\\)-th observation in data.\n\n\\(\\beta_0\\) is an unknown true value of an intercept: average value for \\(Y\\) if \\(X_{1} = 0\\) and \\(X_{2} = 0\\)\n\\(\\beta_1\\) is an unknown true value of a slope: increase in average value for \\(Y\\) for each one-unit increase in \\(X_{1}\\)\n\\(\\beta_2\\) is an unknown true value of a slope: increase in average value for \\(Y\\) for each one-unit increase in \\(X_{2}\\)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions-4",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions-4",
    "title": "Lecture 7",
    "section": "Models and Assumptions",
    "text": "Models and Assumptions\nRandom Noises\n\nLinear regression assumes a linear relationship for \\(Y = f(X_{1}, X_{2})\\):\n\n\\[Y_{i} \\,=\\, \\beta_{0} \\,+\\, \\beta_{1} X_{1, i}\\,+\\, \\beta_{1} X_{2, i} \\,+\\, \\epsilon_{i}\\] for \\(i \\,=\\, 1, 2, \\dots, n\\), where \\(i\\) is the \\(i\\)-th observation in data.\n\n\\(\\epsilon_i\\) is a random noise, or a statistical error:\n\n\\[\n\\epsilon_i \\sim N(0, \\sigma^2)\n\\]\n\nErrors have a mean value of 0 with constant variance \\(\\sigma^2\\).\nErrors are uncorrelated with \\(X_{1, i}\\) and with \\(X_{2, i}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions-5",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions-5",
    "title": "Lecture 7",
    "section": "Models and Assumptions",
    "text": "Models and Assumptions\nBest Fitting Plane\n\nLinear regression finds the beta estimates \\(( \\hat{\\beta_{0}}, \\hat{\\beta_{1}}, \\hat{\\beta_{2}} )\\) such that:\n– The linear function \\(f(X_{1}, X_{2}) = \\hat{\\beta_{0}} + \\hat{\\beta_{1}}X_{1} + \\hat{\\beta_{2}}X_{2}\\) is as near as possible to \\(Y\\) for all \\((X_{1, i}\\,,\\,X_{2, i}\\,,\\, Y_{i})\\) pairs in the data.\n\nIt is the best fitting plane, or the predicted outcome, \\(\\hat{Y_{\\,}} = \\hat{\\beta_{0}} + \\hat{\\beta_{1}}X_{1} + \\hat{\\beta_{2}}X_{2}\\)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions-6",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions-6",
    "title": "Lecture 7",
    "section": "Models and Assumptions",
    "text": "Models and Assumptions\nResidual Errors\n\nThe estimated beta coefficients are chosen to minimize the sum of squares of the residual errors \\((SSR)\\): \\[\n\\begin{align}\nSSR &\\,=\\, (\\texttt{Residual_Error}_{1})^{2}\\\\\n&\\quad \\,+\\, (\\texttt{Residual_Error}_{2})^{2}\\\\\n&\\quad\\,+\\, \\cdots + (\\texttt{Residual_Error}_{n})^{2}\\\\\n\\text{where}\\qquad\\qquad\\qquad\\qquad&\\\\\n\\texttt{Residual_Error}_{i} &\\,=\\, Y_{i} \\,-\\, \\hat{Y_{i}},\\\\\n\\texttt{Predicted_Outcome}_{i}: \\hat{Y_{i}} &\\,=\\, \\hat{\\beta_{0}} \\,+\\, \\hat{\\beta_{1}}X_{1, i} \\,+\\, \\hat{\\beta_{2}}X_{2, i}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression---best-fitting-plane",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression---best-fitting-plane",
    "title": "Lecture 7",
    "section": "Multiple Regression - Best Fitting Plane",
    "text": "Multiple Regression - Best Fitting Plane\n\n\n\n\n\n\n\n\n\n\nAll else being equal, an increase in gross_square_feet by one unit is associated with an increase in sale_price by \\(\\hat{\\beta_{1}}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression-using-pyspark",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression-using-pyspark",
    "title": "Lecture 7",
    "section": "Multiple Regression using PySpark",
    "text": "Multiple Regression using PySpark\n\nLet’s add a new predictor, age, to the model.\n\nassembler2 = VectorAssembler(\n                    inputCols=[\"gross_square_feet\", \"age\"], \n                    outputCol=\"predictors\")\ndtrain2 = assembler2.transform(dtrain)\ndtest2  = assembler2.transform(dtest)\n\nmodel2 = LinearRegression(\n                featuresCol=\"predictors\",\n                labelCol=\"sale_price\")\n        .fit(dtrain2)\ndtest2 = model2.transform(dtest2) \n\nprint(regression_table(model2, assembler2))"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#motivation-treating-categorical-variables-in-linear-regression",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#motivation-treating-categorical-variables-in-linear-regression",
    "title": "Lecture 7",
    "section": "Motivation: Treating Categorical Variables in Linear Regression",
    "text": "Motivation: Treating Categorical Variables in Linear Regression\n\nLinear regression models require numerical predictors, but many variables are categorical.\n\ne.g., Consider the two equivalent houses, except for its location—one in Manhattan and the other in Bronx.\n\nThe Approach:\nConvert categorical variables into numerical format using dummy variables.\nWhy Do This?\n\nAllows the model to compare different categories.\nEach dummy variable indicates the presence (1) or absence (0) of a category."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#what-are-dummy-variables",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#what-are-dummy-variables",
    "title": "Lecture 7",
    "section": "What are Dummy Variables?",
    "text": "What are Dummy Variables?\n\nDefinition: Binary indicators (0 or 1) representing categorical data.\nPurpose: Transform qualitative data into a quantitative form for regression analysis.\n\nExample:\n\\[\nD_i = \\begin{cases}\n1, & \\text{if the observation belongs to the category} \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#dummy-variables-in-regression-models",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#dummy-variables-in-regression-models",
    "title": "Lecture 7",
    "section": "Dummy Variables in Regression Models",
    "text": "Dummy Variables in Regression Models\nConsider a regression model including a dummy variable:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 D_i + \\epsilon_i\n\\]\n\n\\(x_i\\): A continuous predictor.\n\\(D_i\\): Dummy variable (e.g., political party affiliation, type of car).\n\nInterpretation:\n\\(\\beta_2\\) captures the difference in the response \\(y\\) when the category is present (i.e., \\(D_i=1\\)) versus absent."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#the-dummy-variable-trap",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#the-dummy-variable-trap",
    "title": "Lecture 7",
    "section": "The Dummy Variable Trap",
    "text": "The Dummy Variable Trap\n\nProblem: Including a dummy for every category introduces redundancy.\nFor a categorical variable with \\(k\\) levels:\n\nIf you include all \\(k\\) dummy variables in the model, their values always sum to 1:\n\n\\[\nD_{1i} + D_{2i} + \\cdots + D_{ki} = 1 \\quad \\text{(for each observation)}\n\\]\nThis is problematic, because one dummy is completely predictable from the others.\nThe intercept already captures the constant part (1), making one of the dummy variables redundant."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#avoiding-the-dummy-variable-trap",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#avoiding-the-dummy-variable-trap",
    "title": "Lecture 7",
    "section": "Avoiding the Dummy Variable Trap",
    "text": "Avoiding the Dummy Variable Trap\n\nSolution: Drop one dummy (often called the reference category)\nThe reference category is represented by a combination of \\(\\texttt{borough}\\) variables.\n\nDummy for the reference category is 1 if all the rest of the dummies is 0.\nDummy for the reference category is 0 otherwise.\n\n\nProper model:\n\\[\ny_i = \\beta_0 + \\beta_1 D_{1, i} + \\beta_2 D_{2, i} + \\cdots + \\beta_{k-1} D_{(k-1), i} + \\epsilon_i\n\\]\n\nInterpretation:\n\nEach \\(\\beta_j\\) (for \\(j=1,2,\\ldots,k-1\\)) represents the difference from the reference category."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#dummy-variable-regression-using-pyspark",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#dummy-variable-regression-using-pyspark",
    "title": "Lecture 7",
    "section": "Dummy Variable Regression using PySpark",
    "text": "Dummy Variable Regression using PySpark\n\nThe UDF, add_dummy_variables(var_name, reference_level) convert a categorical variable into its dummy variables:\n\nvar_name: a string of categorical variable name\nreference_level: index position of alphabetically sorted categories\nThis function requires dtrain and dtest DataFrames.\n\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n    \n  \n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#dummy-variable-regression-using-pyspark-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#dummy-variable-regression-using-pyspark-1",
    "title": "Lecture 7",
    "section": "Dummy Variable Regression using PySpark",
    "text": "Dummy Variable Regression using PySpark\n\nLet’s check what categories are in “borough_name” and how many categories are:\n\n\n\n# Distinct categories:\n(\n    dtrain\n    .select(\"borough_name\")\n    .distinct()\n    .orderBy(\"borough_name\")\n    .show()\n)\n\n# Number of categories\n(\n    dtrain\n    .select(\"borough_name\")\n    .distinct()\n    .count()\n)\n\n\nLet’s convert the borough_name variable into its dummies using the UDF, add_dummy_variables(var_name, reference_level):\n\n# 0: Bronx, 1: Brooklyn, 2: Manhattan, 3: Queens, 4: Staten Island\ndummy_cols, ref_category = add_dummy_variables(\"borough_name\", 2)\n# To see dummy variables\ndtrain.select(['borough_name'] + dummy_cols).show()\ndtrain.select(['borough_name'] + dummy_cols).filter(col('borough_name') == \"Manhattan\").show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#dummy-variable-regression-using-pyspark-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#dummy-variable-regression-using-pyspark-2",
    "title": "Lecture 7",
    "section": "Dummy Variable Regression using PySpark",
    "text": "Dummy Variable Regression using PySpark\n\nNow we’re ready to run a linear regression with dummy variables:\n\n# Define the list of predictor columns:\n# Two numeric predictors plus the dummy variables (excluding the reference dummy)\nconti_cols = [\"gross_square_feet\", \"age\"]\nassembler_predictors = conti_cols + dummy_cols\n\nassembler3 = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\ndtrain3 = assembler3.transform(dtrain)\ndtest3  = assembler3.transform(dtest)\nmodel3 = LinearRegression(featuresCol=\"predictors\", labelCol=\"sale_price\").fit(dtrain3)\ndtest3 = model3.transform(dtest3)\n\n# For model3 and assembler3:\nprint(regression_table(model3, assembler3))"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#residuals",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#residuals",
    "title": "Lecture 7",
    "section": "Residuals",
    "text": "Residuals\n\nModel equation: \\(Y_{i} \\,=\\, \\beta_{0} \\,+\\, \\beta_{1}X_{1,i} \\,+\\, \\beta_{2}X_{2,i}\\)\n\n\\(\\epsilon_i\\) is a random noise, or a statistical error:\n\n\n\\[\n\\epsilon_i \\sim N(0, \\sigma^2)\n\\] - Errors have a mean value of 0 with constant variance \\(\\sigma^2\\).\n\nErrors are uncorrelated with \\(X_{1,i}\\) and with \\(X_{2, i}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#residuals-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#residuals-1",
    "title": "Lecture 7",
    "section": "Residuals",
    "text": "Residuals\n\nIf we re-arrange the simple regression equation, \\[\\begin{align}\n{\\epsilon}_{i} \\,=\\, Y_{i} \\,-\\, (\\, {\\beta}_{0} \\,+\\, {\\beta}_{1}X_{1,i} \\,).\n\\end{align}\\]\n\\(\\texttt{residual_error}_{i}\\) can be thought of as the expected value of \\(\\epsilon_{i}\\), denoted by \\(\\hat{\\epsilon_{i}}\\). \\[\\begin{align}\n\\hat{\\epsilon_{i}} \\,=\\, Y_{i} \\,-\\, (\\, \\hat{\\beta_{0}} \\,+\\, \\hat{\\beta_{1}}X_{1,i} \\,)\n\\end{align}\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-1",
    "title": "Lecture 7",
    "section": "Residual Plots",
    "text": "Residual Plots\n\nResidual plot is a scatterplot of fitted values and residuals.\n\nA variable of fitted values on x-axis\nA variable of residuals on y-axis\n\nA residual plot can be used to diagnose the quality of model results.\nBecause we assume that \\(\\epsilon_{i}\\) have a mean value of 0 with constant variance \\(\\sigma^2\\), a well-behaved residual plot should bounce randomly and form a cloud roughly at the level of zero residual, the perfect prediction line.\nFrom the residual plot, we should ask the following the two questions ourselves:\n\nOn average, are the predictions correct?\nAre there systematic errors?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-2",
    "title": "Lecture 7",
    "section": "Residual Plots",
    "text": "Residual Plots\n\n\nOn average, are the predictions correct?\nAre there systematic errors?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-3",
    "title": "Lecture 7",
    "section": "Residual Plots",
    "text": "Residual Plots\nUnbiasedness and Homoskedasticity\n\nWe would like have a residual plot to be\n\nUnbiased: have an average value of zero in any thin vertical strip;\nHomoskedastic, which means “same stretch”: it is ideal to have the same spread of the residuals in any thin vertical strip.\nWhen the variance of residuals changes across predicted values (e.g., residuals get larger as predicted values increase), the model suffers from heteroscedasticity."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-4",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-4",
    "title": "Lecture 7",
    "section": "Residual Plots",
    "text": "Residual Plots\nExamples"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-5",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-5",
    "title": "Lecture 7",
    "section": "Residual Plots",
    "text": "Residual Plots\nWhat happens if biased?\n\nThe model consistently overpredicts or underpredicts for certain values.\n\nIndicates that the model might be misspecified—perhaps missing important variables or using an incorrect functional form.\nLeads to biased parameter estimates, meaning the coefficients are systematically off, reducing the validity of predictions and inferences."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-6",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-6",
    "title": "Lecture 7",
    "section": "Residual Plots",
    "text": "Residual Plots\nWhat happens if heteroscedasticity is present?\n\nConsequences of heteroscedasticity:\n\n📉 Inefficient coefficient estimates: Estimates remain unbiased but are no longer efficient (i.e., they don’t have the smallest variance).\n❌ Biased standard errors: Leads to unreliable p-values and confidence intervals, potentially resulting in invalid hypothesis tests.\n⚠️ Misleading inferences: Predictors may appear statistically significant or insignificant incorrectly.\n🎯 Poor predictive performance: The model might perform poorly on future data, especially if the residual variance grows with higher predicted values."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-in-python",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-in-python",
    "title": "Lecture 7",
    "section": "Residual Plots in Python",
    "text": "Residual Plots in Python\n\nPySpark itself does not have built-in visualization capabilities.\n\nFirstly, we convert PySpark DataFrame into Pandas DataFrame by using .toPandas()\n\n\n# Residual plot for Model 2:\n# Convert test predictions to Pandas\nrdf = dtest2.select([\"prediction\", \"sale_price\"]).toPandas()\nrdf[\"residual\"] = rdf[\"sale_price\"] - rdf[\"prediction\"]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-in-python-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-in-python-1",
    "title": "Lecture 7",
    "section": "Residual Plots in Python",
    "text": "Residual Plots in Python\n\nWe then use matplotlib.pyplot to draw a residual plot:\n\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm  # for lowess smoothing\n\nplt.scatter(rdf[\"prediction\"], rdf[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n# Use lowess smoothing for the trend line\nsmoothed = sm.nonparametric.lowess(rdf[\"residual\"], rdf[\"prediction\"])\nplt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\n\n# Perfect prediction line\n#   A horizontal line at residual = 0\nplt.axhline(y=0, color=\"red\", linestyle=\"--\")\n\n# Labeling\nplt.xlabel(\"Predicted sale.price (Model 2)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residual Plot for Model 2\")\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#hypothesis-testing-on-beta-coefficient-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#hypothesis-testing-on-beta-coefficient-1",
    "title": "Lecture 7",
    "section": "Hypothesis Testing on Beta Coefficient",
    "text": "Hypothesis Testing on Beta Coefficient\n\nTo determine whether an independent variable has a statistically significant effect on the dependent variable in a linear regression model.\nConsider the following linear regression model: \\[\ny_{i} = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\dots + \\beta_k x_{k,i} + \\epsilon_{i}\n\\]\n\\(y\\): Outcome\n\\(x_1, x_2, \\dots, x_k\\): Predictors\n\\(\\beta_0\\): Intercept\n\n\\(\\beta_1, \\beta_2, \\dots, \\beta_k\\): Coefficients\n\n\\(\\epsilon_{i}\\): Error term"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#hypotheses-and-test-statistic",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#hypotheses-and-test-statistic",
    "title": "Lecture 7",
    "section": "Hypotheses and Test Statistic",
    "text": "Hypotheses and Test Statistic\n\nWe test whether a specific coefficient \\(\\beta_j\\) significantly differs from zero:\n\nNull Hypothesis (\\(H_0\\)): \\(\\beta_j = 0\\) (No relationship)\n\nAlternative Hypothesis (\\(H_A\\)): \\(\\beta_j \\neq 0\\) (Significant relationship)\n\nThe t-statistic is used to test each coefficient:\n\n\\[\nt = \\frac{\\hat{\\beta_j} - 0}{SE(\\hat{\\beta_j})}\n\\]\n\n\\(\\hat{\\beta_j}\\): Estimated coefficient\n\n\\(SE(\\hat{\\beta_j})\\): Standard error of the estimate"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#hypothesis-testing---decision-rule-and-interpretation",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#hypothesis-testing---decision-rule-and-interpretation",
    "title": "Lecture 7",
    "section": "Hypothesis Testing - Decision Rule and Interpretation",
    "text": "Hypothesis Testing - Decision Rule and Interpretation\n\nCalculate the p-value based on the t-distribution with \\(n - k - 1\\) degrees of freedom.\nCompare p-value with significance level \\(\\alpha\\) (e.g., 0.05):\n\nIf \\(p \\leq \\alpha\\): Reject \\(H_0\\) (Significant)\nIf \\(p &gt; \\alpha\\): Fail to reject \\(H_0\\) (Not significant)\n\n\n\nIn our course, stars in a regression table mean a significance level:\n\n* (10%); ** (5%); *** (1%)\n\nReject \\(H_0\\): There is sufficient evidence to suggest a statistically significant relationship between \\(x_j\\) and \\(y\\).\nFail to reject \\(H_0\\): No statistically significant evidence of a relationship between \\(x_j\\) and \\(y\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-estimated-beta-coefficients",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-estimated-beta-coefficients",
    "title": "Lecture 7",
    "section": "Interpreting Estimated Beta Coefficients",
    "text": "Interpreting Estimated Beta Coefficients\nExample\nThe model equation is \\[\\begin{align}\n\\texttt{sale_price[i]} \\;=\\;\\, &\\texttt{b0} \\,+\\,\\\\ &\\texttt{b1*gross_square_feet[i]} \\,+\\,\\texttt{b2*age[i]}\\,+\\,\\\\ &\\texttt{b3*Bronx[i]} \\,+\\,\\texttt{b4*Brooklyn[i]} \\,+\\,\\\\&\\texttt{b5*Queens[i]} \\,+\\,\\texttt{b6*Staten Island[i]}\\,+\\,\\\\ &\\texttt{e[i]}\n\\end{align}\\] - The reference level of borough_name variables is Manhattan."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-estimated-beta-coefficients-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-estimated-beta-coefficients-1",
    "title": "Lecture 7",
    "section": "Interpreting Estimated Beta Coefficients",
    "text": "Interpreting Estimated Beta Coefficients\n1. gross_square_feet\n\nConsider the predicted sales prices of the two houses, A and B.\n\nBoth A and B are in Bronx and with the same age.\ngross_square_feet of house A is 2001, while that of house B is 2000.\n\nAll else being equal, an increase in gross_square_feet by one unit is associated with an increase in sale_price by \\(\\hat{\\beta_{1}}\\).\n\nWhy?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-estimated-beta-coefficients-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-estimated-beta-coefficients-2",
    "title": "Lecture 7",
    "section": "Interpreting Estimated Beta Coefficients",
    "text": "Interpreting Estimated Beta Coefficients\n1. gross_square_feet\n\\[\n\\begin{align}\\widehat{\\texttt{sale_price[A]}} \\;=\\quad& \\hat{\\texttt{b0}} \\,+\\, \\hat{\\texttt{b1}}\\texttt{*gross_square_feet[A]} \\,+\\, \\hat{\\texttt{b2}}\\texttt{*age[A]} \\,+\\,\\\\\n&\\hat{\\texttt{b3}}\\texttt{*Bronx[A]}\\,+\\,\\hat{\\texttt{b4}}\\texttt{*Brooklyn[A]} \\,+\\,\\\\ &\\hat{\\texttt{b5}}\\texttt{*Queens[A]}\\,+\\, \\hat{\\texttt{b6}}\\texttt{*Staten Island[A]}\\\\\n\\widehat{\\texttt{sale_price[B]}} \\;=\\quad& \\hat{\\texttt{b0}} \\,+\\, \\hat{\\texttt{b1}}\\texttt{*gross_square_feet[B]} \\,+\\, \\hat{\\texttt{b2}}\\texttt{*age[B]}\\,+\\,\\\\\n&\\hat{\\texttt{b3}}\\texttt{*Bronx[B]}\\,+\\,\n\\hat{\\texttt{b4}}\\texttt{*Brooklyn[B]} \\,+\\,\\\\ &\\hat{\\texttt{b5}}\\texttt{*Queens[B]}\\,+\\, \\hat{\\texttt{b6}}\\texttt{*Staten Island[B]} \\end{align}\n\\]\n\\[\n\\begin{align}\\Leftrightarrow\\qquad&\\widehat{\\texttt{sale_price[A]}} \\,-\\, \\widehat{\\texttt{sale_price[B]}}\\qquad  \\\\\n\\;=\\quad &\\hat{\\texttt{b1}}\\texttt{*}(\\texttt{gross_square_feet[A]} - \\texttt{gross_square_feet[B]})\\\\\n\\;=\\quad &\\hat{\\texttt{b1}}\\texttt{*}\\texttt{(2001 - 2000)} \\,=\\, \\hat{\\texttt{b1}}\\qquad\\qquad\\quad\\;\\;\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-estimated-beta-coefficients-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-estimated-beta-coefficients-3",
    "title": "Lecture 7",
    "section": "Interpreting Estimated Beta Coefficients",
    "text": "Interpreting Estimated Beta Coefficients\n2. borough_nameBronx\n\nConsider the predicted sales prices of the two houses, A and C.\n\nBoth A and C are with the same age and the same gross_square_feet.\nA is in Bronx, and C is in Manhattan.\n\nAll else being equal, an increase in borough_nameBronx by one unit is associated with an increase in sale_price by b3.\nEquivalently, all else being equal, being in Bronx relative to being a in Manhattan is associated with a decrease in sale_price by |b3|.\n\nWhy?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-estimated-beta-coefficients-4",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-estimated-beta-coefficients-4",
    "title": "Lecture 7",
    "section": "Interpreting Estimated Beta Coefficients",
    "text": "Interpreting Estimated Beta Coefficients\n2. borough_nameBronx\n\\[\n\\begin{align}\\widehat{\\texttt{sale_price[A]}} \\;=\\quad& \\hat{\\texttt{b0}} \\,+\\, \\hat{\\texttt{b1}}\\texttt{*gross_square_feet[A]} \\,+\\, \\hat{\\texttt{b2}}\\texttt{*age[A]} \\,+\\,\\\\ &\\hat{\\texttt{b3}}\\texttt{*Bronx[A]}\\,+\\,\n\\hat{\\texttt{b4}}\\texttt{*Brooklyn[A]} \\,+\\,\\\\ &\\hat{\\texttt{b5}}\\texttt{*Queens[A]}\\,+\\, \\hat{\\texttt{b6}}\\texttt{*Staten Island[A]}\\\\\n\\widehat{\\texttt{sale_price[C]}} \\;=\\quad& \\hat{\\texttt{b0}} \\,+\\, \\hat{\\texttt{b1}}\\texttt{*gross_square_feet[C]} \\,+\\, \\hat{\\texttt{b2}}\\texttt{*age[C]}\\,+\\,\\\\\n&\\hat{\\texttt{b3}}\\texttt{*Bronx[C]}\\,+\\,\n\\hat{\\texttt{b4}}\\texttt{*Brooklyn[C]} \\,+\\,\\\\ &\\hat{\\texttt{b5}}\\texttt{*Queens[C]}\\,+\\, \\hat{\\texttt{b6}}\\texttt{*Staten Island[C]} \\end{align}\n\\]\n\\[\n\\begin{align}\\Leftrightarrow\\qquad&\\widehat{\\texttt{sale_price[A]}} \\,-\\, \\widehat{\\texttt{sale_price[C]}}\\qquad  \\\\\n\\;=\\quad &\\hat{\\texttt{b3}}\\texttt{*}\\texttt{Bronx[A]} \\\\\n\\;=\\quad &\\hat{\\texttt{b3}}\\qquad\\qquad\\qquad\\qquad\\quad\\;\\;\\;\\,\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#coefficient-plots-in-python",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#coefficient-plots-in-python",
    "title": "Lecture 7",
    "section": "Coefficient Plots in Python",
    "text": "Coefficient Plots in Python\n\nA coefficient plot shows beta estimates and their confidence intervals.\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Create a Pandas DataFrame from model3's summary information\nterms = assembler3.getInputCols()\ncoefs = model3.coefficients.toArray()[:len(terms)]\nstdErrs = model3.summary.coefficientStandardErrors[:len(terms)]\n\ndf_summary = pd.DataFrame({\n    \"term\": terms,\n    \"estimate\": coefs,\n    \"std_error\": stdErrs\n})\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary[\"term\"], df_summary[\"estimate\"],\n             yerr = 1.96 * df_summary[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates (Model 2)\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-log-transformation-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-log-transformation-1",
    "title": "Lecture 7",
    "section": "Linear Regression with Log Transformation",
    "text": "Linear Regression with Log Transformation\nThe model equation with log-transformed \\(\\texttt{sale.price[i]}\\) is \\[\\begin{align}\n\\log(\\texttt{sale.price[i]}) \\;=\\;\\, &\\texttt{b0} \\,+\\,\\\\ &\\texttt{b1*gross.square.feet[i]} \\,+\\,\\texttt{b2*age[i]}\\,+\\,\\\\ &\\texttt{b3*Bronx[i]} \\,+\\,\\texttt{b4*Brooklyn[i]} \\,+\\,\\\\&\\texttt{b5*Queens[i]} \\,+\\,\\texttt{b6*Staten Island[i]}\\,+\\,\\\\ &\\texttt{e[i]}.\n\\end{align}\\]\n\nNote that the reference level for borough_name is Manhattan."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables",
    "title": "Lecture 7",
    "section": "Beta Estimates for Log-transformed Variables",
    "text": "Beta Estimates for Log-transformed Variables\n1. gross_square_feet\n\nLet’s re-consider the two properties \\(\\texttt{A}\\) and \\(\\texttt{B}\\).\n\n\\(\\texttt{gross.square.feet[A]} = 2001\\) and \\(\\texttt{gross.square.feet[B]} = 2000\\).\nBoth are in the same borough.\nBoth properties’ ages are the same."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables-1",
    "title": "Lecture 7",
    "section": "Beta Estimates for Log-transformed Variables",
    "text": "Beta Estimates for Log-transformed Variables\n1. gross_square_feet\n\nIf we apply the rule above for \\(\\widehat{\\texttt{sale.price}}\\texttt{[A]}\\) and \\(\\widehat{\\texttt{sale.price}}\\texttt{[B]}\\),\n\n\n\n\n\\[\\begin{align}&\\log(\\widehat{\\texttt{sale.price}}\\texttt{[A]}) - \\log(\\widehat{\\texttt{sale.price}}\\texttt{[B]}) \\\\\n\\,=\\, &\\hat{\\texttt{b1}}\\,*\\,(\\texttt{gross.square.feet[A]} \\,-\\, \\texttt{gross.square.feet[B]})\\\\\n\\,=\\, &\\hat{\\texttt{b1}}\\end{align}\\]\nSo we can have the following: \\[\n\\begin{align}\n&\\Leftrightarrow\\qquad\\frac{\\widehat{\\texttt{sale.price[A]}}}{ \\widehat{\\texttt{sale.price[B]}}} \\;=\\; \\texttt{exp(}\\hat{\\texttt{b1}}\\texttt{)}\\\\ \\quad&\\Leftrightarrow\\qquad\\widehat{\\texttt{sale.price[A]}} \\;=\\; \\widehat{\\texttt{sale.price[B]}} * \\texttt{exp(}\\hat{\\texttt{b1}}\\texttt{)}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables-2",
    "title": "Lecture 7",
    "section": "Beta Estimates for Log-transformed Variables",
    "text": "Beta Estimates for Log-transformed Variables\n1. gross_square_feet\n\nSuppose \\(\\texttt{exp(}\\hat{\\texttt{b2}}\\texttt{)} = 1.000431\\).\n\nThen \\(\\widehat{\\texttt{sale.price[A]}}\\) is \\(1.000431\\times\\widehat{\\texttt{sale.price[B]}}\\).\nAll else being equal, an increase in \\(\\texttt{gross.square.feet}\\) by one unit is associated with an increase in \\(\\texttt{sale.price}\\) by 0.0431%."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables-3",
    "title": "Lecture 7",
    "section": "Beta Estimates for Log-transformed Variables",
    "text": "Beta Estimates for Log-transformed Variables\n2. borough_nameBronx\n\nLet’s re-consider the two properties \\(\\texttt{A}\\) and \\(\\texttt{C}\\).\n\nA is in Bronx, and C is in Manhattan.\nBoth A and C’s age are the same.\nBoth A and C’s gross.square.feet are the same."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables-4",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables-4",
    "title": "Lecture 7",
    "section": "Beta Estimates for Log-transformed Variables",
    "text": "Beta Estimates for Log-transformed Variables\n2. borough_nameBronx\n\nIf we apply the log()-exp() rules for \\(\\widehat{\\texttt{sale.price}}\\texttt{[A]}\\) and \\(\\widehat{\\texttt{sale.price}}\\texttt{[C]}\\),\n\n\n\n\n\\[\\begin{align}&\\log(\\widehat{\\texttt{sale.price}}\\texttt{[A]}) - \\log(\\widehat{\\texttt{sale.price}}\\texttt{[C]}) \\\\\n\\,=\\, &\\hat{\\texttt{b3}}\\,*\\,(\\texttt{borough_Bronx[A]} \\,-\\, \\texttt{borough_Bronx[C]})\\,=\\, \\hat{\\texttt{b3}}\\end{align}\\]\nSo we can have the following: \\[\n\\begin{align}&\\Leftrightarrow\\qquad\\frac{\\widehat{\\texttt{sale.price[A]}}}{ \\widehat{\\texttt{sale.price[C]}}} \\;=\\; \\texttt{exp(}\\hat{\\texttt{b3}}\\texttt{)}\\\\ \\quad&\\Leftrightarrow\\qquad\\,\\widehat{\\texttt{sale.price[A]}} \\;=\\; \\widehat{\\texttt{sale.price[C]}} * \\texttt{exp(}\\hat{\\texttt{b3}}\\texttt{)}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables-5",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables-5",
    "title": "Lecture 7",
    "section": "Beta Estimates for Log-transformed Variables",
    "text": "Beta Estimates for Log-transformed Variables\n2. borough_nameBronx\n\nSuppose \\(\\texttt{exp(}\\hat{\\texttt{b3}}\\texttt{)} = 0.2831691\\).\n\nThen \\(\\widehat{\\texttt{sale.price[A]}}\\) is \\(0.2831691\\times\\widehat{\\texttt{sale.price[B]}}\\).\nAll else being equal, an increase in \\(\\texttt{borough_Bronx}\\) by one unit is associated with a decrease in \\(\\texttt{sale.price}\\) by (1 - 0.2831691) = 71.78%.\nAll else being equal, being in Bronx relative to being in Manhattan is associated with a decrease in \\(\\texttt{sale.price}\\) by 71.78%."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-transformed-variables-in-pyspark",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-transformed-variables-in-pyspark",
    "title": "Lecture 7",
    "section": "Log-transformed Variables in PySpark",
    "text": "Log-transformed Variables in PySpark\n\nWe use the log() function to add a log-transformed variable:\n\nfrom pyspark.sql.functions import log\nsale_df = sale_df.withColumn(\"log_sale_price\", \n                              log( sale_df['sale_price'] ) ) \n\nTo use a exponential function, we can use an exp() function from numpy:\n\nimport numpy as np\n\nnp.exp(1)\nnp.exp([1, 2, 3])"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-interaction-terms-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-interaction-terms-1",
    "title": "Lecture 7",
    "section": "Linear Regression with Interaction Terms",
    "text": "Linear Regression with Interaction Terms\nMotivation\n\nDoes the relationship between sale.price and gross.square.feet vary by borough_name?\n\nHow can linear regression address the question above?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-interaction-terms-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-interaction-terms-2",
    "title": "Lecture 7",
    "section": "Linear Regression with Interaction Terms",
    "text": "Linear Regression with Interaction Terms\nModel\n\nThe linear regression with an interaction between predictors \\(X_{1}\\) and \\(X_{2}\\) are:\n\n\\[Y_{\\texttt{i}} \\,=\\, b_{0} \\,+\\, b_{1}\\,X_{1,\\texttt{i}} \\,+\\, b_{2}\\,X_{2,\\texttt{i}} \\,+\\, b_{3}\\,X_{1,\\texttt{i}}\\times \\color{Red}{X_{2,\\texttt{i}}} \\,+\\, e_{\\texttt{i}},\\]\n\nwhere\n\n\\(i\\;\\): \\(\\;\\;i\\)-th observation in the training DataFrame, \\(i = 1, 2, 3, \\cdots\\).\n\\(Y_{i}\\,\\): \\(\\;i\\)-th observation of outcome \\(Y\\).\n\\(X_{p, i}\\,\\): \\(i\\)-th observation of the \\(p\\)-th predictor \\(X_{p}\\).\n\\(e_{i}\\;\\): \\(\\;i\\)-th observation of statistical error."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-interaction-terms-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-interaction-terms-3",
    "title": "Lecture 7",
    "section": "Linear Regression with Interaction Terms",
    "text": "Linear Regression with Interaction Terms\nModel\n\nThe linear regression with an interaction between predictors \\(X_{1}\\) and \\(X_{2}\\) are:\n\n\\[Y_{\\texttt{i}} \\,=\\, b_{0} \\,+\\, b_{1}\\,X_{1,\\texttt{i}} \\,+\\, b_{2}\\,X_{2,\\texttt{i}} \\,+\\, b_{3}\\,X_{1,\\texttt{i}}\\times \\color{Red}{X_{2,\\texttt{i}}} \\,+\\, e_{\\texttt{i}}\\;.\\]\n\n\nInteraction\n\nThe relationship between \\(X_{1}\\) and \\(Y\\) varies by values of \\(b_{3}\\, X_{2}\\):\n\n\\[\\frac{\\Delta Y}{\\Delta X_{1}} \\,=\\, b_{1} + b_{3}\\, X_{2}\\].\n\nExample\n\n\\(X_{2}\\) is often a dummy variable. If \\(b_{3} \\neq 0\\) and \\(X_{2, \\texttt{i}} = 1\\),\n\n\\[\\frac{\\Delta Y}{\\Delta X_{1}} \\,=\\, b_{1} + b_{3}\\]."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-interaction-terms-4",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-interaction-terms-4",
    "title": "Lecture 7",
    "section": "Linear Regression with Interaction Terms",
    "text": "Linear Regression with Interaction Terms\nInteraction with a Dummy Variable\n\nThe linear regression with an interaction between predictors \\(X_{1}\\) and \\(X_{2}\\in\\{\\,0, 1\\,\\}\\) are:\n\n\\[Y_{\\texttt{i}} \\,=\\, b_{0} \\,+\\, b_{1}\\,X_{1,\\texttt{i}} \\,+\\, b_{2}\\,X_{2,\\texttt{i}} \\,+\\, b_{3}\\,X_{1,\\texttt{i}}\\times \\color{Red}{X_{2,\\texttt{i}}} \\,+\\, e_{\\texttt{i}},\\] where \\(X_{\\,2, \\texttt{i}}\\) is either 0 or 1.\n\nFor \\(\\texttt{i}\\) such that \\(X_{\\,2, \\texttt{i}} = 0\\), the model is \\[Y_{\\texttt{i}} \\,=\\, b_{0} \\,+\\, b_{1}\\,X_{1,\\texttt{i}} \\,+\\, e_{\\texttt{i}}\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\;\\;\\]\nFor \\(\\texttt{i}\\) such that \\(X_{\\,2, \\texttt{i}} = 1\\), the model is\n\n\\[Y_{\\texttt{i}} \\,=\\, (\\,b_{0} \\,+\\, b_{2}\\,) \\,+\\, (\\,b_{1}\\,+\\, b_{3}\\,)\\,X_{1,\\texttt{i}} \\,+\\, e_{\\texttt{i}}\\qquad\\qquad\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-interaction-terms-5",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-interaction-terms-5",
    "title": "Lecture 7",
    "section": "Linear Regression with Interaction Terms",
    "text": "Linear Regression with Interaction Terms\nMotivation\n\nIs sale.price related with gross.square.feet?\n\n\\[\n\\begin{align}\n\\texttt{sale_price[i]} \\;=\\;\\, &\\texttt{b0} \\,+\\,\\\\\n&\\texttt{b1*Bronx[i]} \\,+\\,\\texttt{b2*Brooklyn[i]} \\,+\\,\\\\&\\texttt{b3*Queens[i]} \\,+\\,\\texttt{b4*Staten Island[i]}\\,+\\,\\\\\n&\\texttt{b5*age[i]}\\,+\\,\\\\\n&\\texttt{b6*gross_square_feet[i]} \\,+\\,\\texttt{e[i]}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-interaction-terms-6",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-interaction-terms-6",
    "title": "Lecture 7",
    "section": "Linear Regression with Interaction Terms",
    "text": "Linear Regression with Interaction Terms\nMotivation\n\nDoes the relationship between sale.price and gross.square.feet vary by borough_name?\n\n\n\n\n\\[\n\\begin{align}\n\\texttt{sale_price[i]} \\;=\\;\\, &\\texttt{b0} \\,+\\,\\\\\n&\\texttt{b1*Bronx[i]} \\,+\\,\\texttt{b2*Brooklyn[i]} \\,+\\,\\\\\n&\\texttt{b3*Queens[i]} \\,+\\,\\texttt{b4*Staten Island[i]}\\,+\\, \\\\\n&\\texttt{b5*age[i]}\\,+\\,\\\\\n&\\texttt{b6*gross_square_feet[i]} \\,+\\,\\\\\n&\\texttt{b7*gross_square_feet[i]*Bronx[i]} \\,+\\,  \\\\\n&\\texttt{b8*gross_square_feet[i]*Brooklyn[i]} \\,+\\,  \\\\\n&\\texttt{b9*gross_square_feet[i]*Queens[i]} \\,+\\,  \\\\\n&\\texttt{b10*gross_square_feet[i]*Staten Island[i]} \\,+\\, \\texttt{e[i]} \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-1",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\n\nTo estimate the price elasticity of orange juice (OJ), we will use sales data for OJ from Dominick’s grocery stores in the 1990s.\n\nWeekly price and sales (in number of cartons “sold”) for three OJ brandsTropicana, Minute Maid, Dominick’s\nA dummy, ad, showing whether each brand was advertised (in store or flyer) that week.\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nsales\nQuantity of OJ cartons sold\n\n\nprice\nPrice of OJ\n\n\nbrand\nBrand of OJ\n\n\nad\nAdvertisement status"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-2",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\nEstimating Price Elasticity\n\nLet’s prepare the OJ data:\n\noj = pd.read_csv('https://bcdanl.github.io/data/dominick_oj_feat.csv')"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-3",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\nEstimating Price Elasticity\n\nThe following model estimates the price elasticity of demand for a carton of OJ:\n\n\n\n\n\\[\\begin{align}\n\\log(\\texttt{sales}_{\\texttt{i}}) &\\,=\\, \\;\\; b_{\\texttt{intercept}} \\,+\\, b_{\\,\\texttt{mm}}\\,\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}} \\,+\\, b_{\\,\\texttt{tr}}\\,\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}\\\\\n&\\quad\\,+\\, b_{\\texttt{price}}\\,\\log(\\texttt{price}_{\\texttt{i}}) \\,+\\, e_{\\texttt{i}},\\\\\n\\text{where}\\qquad\\qquad&\\\\\n\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}\n&\\,=\\, \\begin{cases}\n\\texttt{1} & \\text{ if an orange juice } \\texttt{i} \\text{ is } \\texttt{Tropicana};\\\\\\\\\n\\texttt{0} & \\text{otherwise}.\\qquad\\qquad\\quad\\,\n\\end{cases}\\\\\n\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}} &\\,=\\, \\begin{cases}\n\\texttt{1} & \\text{ if an orange juice } \\texttt{i} \\text{ is } \\texttt{Minute Maid};\\\\\\\\\n\\texttt{0} & \\text{otherwise}.\\qquad\\qquad\\quad\\,\n\\end{cases}\n\\end{align}\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-4",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-4",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\nEstimating Price Elasticity\n\nThe following model estimates the price elasticity of demand for a carton of OJ:\n\n\\[\\log(\\texttt{sales}_{\\texttt{i}}) \\,=\\, \\quad\\;\\; b_{\\texttt{intercept}} \\,+\\, b_{\\,\\texttt{mm}}\\,\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}} \\,+\\, b_{\\,\\texttt{tr}}\\,\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}\\\\\n\\,+\\, b_{\\texttt{price}}\\,\\log(\\texttt{price}_{\\texttt{i}}) \\,+\\, e_{\\texttt{i}}\\]\n\nWhen \\(\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}\\,=\\,0\\) and \\(\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}}\\,=\\,0\\), the beta coefficient for the intercept \\(b_{\\texttt{intercept}}\\) gives the value of Dominick’s log sales at \\(\\log(\\,\\texttt{price[i]}\\,) = 0\\).\nThe beta coefficient \\(b_{\\texttt{price}}\\) is the price elasticity of demand.\n\nIt measures how sensitive the quantity demanded is to its price."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-5",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-5",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\nEstimating Price Elasticity\n\nFor small changes in variable \\(x\\) from \\(x_{0}\\) to \\(x_{1}\\), the following equation holds:\n\n\n\n\n\\[\\Delta \\log(x) \\,= \\, \\log(x_{1}) \\,-\\, \\log(x_{0})\n\\approx\\, \\frac{x_{1} \\,-\\, x_{0}}{x_{0}}\n\\,=\\, \\frac{\\Delta\\, x}{x_{0}}.\\]\n\nThe coefficient on \\(\\log(\\texttt{price}_{\\texttt{i}})\\), \\(b_{\\texttt{price}}\\), is therefore\n\n\n\n\n\\[b_{\\texttt{price}} \\,=\\, \\frac{\\Delta \\log(\\texttt{sales}_{\\texttt{i}})}{\\Delta \\log(\\texttt{price}_{\\texttt{i}})}\\,=\\, \\frac{\\frac{\\Delta \\texttt{sales}_{\\texttt{i}}}{\\texttt{sales}_{\\texttt{i}}}}{\\frac{\\Delta \\texttt{price}_{\\texttt{i}}}{\\texttt{price}_{\\texttt{i}}}}.\\]\n\nAll else being equal, an increase in \\(\\texttt{price}\\) by 1% is associated with a decrease in \\(\\texttt{sales}\\) by \\(b_{\\texttt{price}}\\)%."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-6",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-6",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\nEDA\n\nDescribe the relationship between log(price) and log(sales) by brand.\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\noj['log_price'] = oj['price'].apply(lambda x: np.log(x))\noj['log_sales'] = oj['sales'].apply(lambda x: np.log(x))\n\nsns.lmplot(\n    x='log_price', \n    y='log_sales', \n    hue='brand',     # color by brand\n    data=oj, \n    aspect=1.2, \n    height=5,\n    markers=[\"o\", \"s\", \"D\"],  # optional: different markers per brand\n    scatter_kws={'alpha': 0.025},  # Pass alpha to the scatter plot\n    legend=True\n)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-7",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-7",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\nEDA\n\nDescribe the relationship between log(price) and log(sales) by brand and ad.\n\nsns.lmplot(\n    x='log_price', \n    y='log_sales', \n    hue='brand',     # color by brand\n    col='ad',         # facet by feat\n    data=oj, \n    aspect=1.2, \n    height=5,\n    markers=[\"o\", \"s\", \"D\"],  # optional: different markers per brand\n    # Pass alpha to the scatter plot\n    scatter_kws={'alpha': 0.025},\n    col_wrap=2,         # how many facets per row\n    legend=True\n)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-8",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-8",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\nEstimating Price Elasticity\n\nLet’s train the first model, model_1:\n\n\\[\n\\begin{align}\n\\log(\\texttt{sales}_{\\texttt{i}}) \\,=\\, &\\quad\\;\\; b_{\\texttt{intercept}} \\\\ &\\,+\\,b_{\\,\\texttt{mm}}\\,\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}} \\,+\\, b_{\\,\\texttt{tr}}\\,\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}\\\\\n&\\,+\\, b_{\\texttt{price}}\\,\\log(\\texttt{price}_{\\texttt{i}}) \\,+\\, e_{\\texttt{i}}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-9",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-9",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\nEstimating Price Elasticity\n\nHow does the relationship between log(sales) and log(price) vary by brand?\n\nLet’s train the second model, model_2, that addresses the above question:\n\n\n\\[\n\\begin{align}\n\\log(\\texttt{sales}_{\\texttt{i}}) \\,=\\,&\\;\\; \\quad b_{\\texttt{intercept}} \\,+\\, \\color{Green}{b_{\\,\\texttt{mm}}\\,\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}}} \\,+\\, \\color{Blue}{b_{\\,\\texttt{tr}}\\,\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}}\\\\\n&\\,+\\, b_{\\texttt{price}}\\,\\log(\\texttt{price}_{\\texttt{i}})  \\\\\n&\\, +\\, b_{\\texttt{price*mm}}\\,\\log(\\texttt{price}_{\\texttt{i}})\\,\\times\\,\\color{Green} {\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}}} \\\\\n&\\,+\\, b_{\\texttt{price*tr}}\\,\\log(\\texttt{price}_{\\texttt{i}})\\,\\times\\,\\color{Blue} {\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}} \\,+\\, e_{\\texttt{i}}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-10",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-10",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\n\nFor \\(\\texttt{i}\\) such that \\(\\color{Green}{\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}}} = 0\\) and \\(\\color{Blue}{\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}} = 0\\), the model equation is: \\[\\log(\\texttt{sales}_{\\texttt{i}}) \\,=\\, \\; \\,b_{\\texttt{intercept}}\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\\\\n\\qquad\\,+\\, b_{\\texttt{price}} \\,\\log(\\texttt{price}_{\\texttt{i}}) \\,+\\, e_{\\texttt{i}}\\,.\\qquad\\qquad\\;\\]\nFor \\(\\texttt{i}\\) such that \\(\\color{Green}{\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}}} = 1\\) and \\(\\color{Blue}{\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}} = 0\\), the model equation is: \\[\\log(\\texttt{sales}_{\\texttt{i}}) \\,=\\, \\; (\\,b_{\\texttt{intercept}} \\,+\\, b_{\\,\\texttt{mm}}\\,)\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\\\\n\\qquad\\!\\,+\\,(\\, b_{\\texttt{price}} \\,+\\, \\color{Green}{b_{\\texttt{price*mm}}}\\,)\\,\\log(\\texttt{price}_{\\texttt{i}}) \\,+\\, e_{\\texttt{i}}\\,.\\]\nFor \\(\\texttt{i}\\) such that \\(\\color{Green}{\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}}} = 0\\) and \\(\\color{Blue}{\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}} = 1\\), the model equation is: \\[\\log(\\texttt{sales}_{\\texttt{i}}) \\,=\\, \\; (\\,b_{\\texttt{intercept}} \\,+\\, b_{\\,\\texttt{tr}}\\,)\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\\\\n\\qquad\\!\\,+\\,(\\, b_{\\texttt{price}} \\,+\\, \\color{Blue}{b_{\\texttt{price*tr}}}\\,)\\,\\log(\\texttt{price}_{\\texttt{i}}) \\,+\\, e_{\\texttt{i}}\\,.\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#adding-interaction-terms-in-pyspark",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#adding-interaction-terms-in-pyspark",
    "title": "Lecture 7",
    "section": "Adding Interaction Terms in PySpark",
    "text": "Adding Interaction Terms in PySpark\n\nThe UDF add_interaction_terms requires at least two lists of variable names.\nvar_list3 is optional.\n\nIf var_list3 is provided, the add_interaction_terms function adds all possible two-way interactions and three-way interaction to the dtrain and dtest DataFrames.\n\n\ndef add_interaction_terms(var_list1, var_list2, var_list3=None):\n    \"\"\"\n    Creates interaction term columns in the global DataFrames dtrain and dtest.\n\n    For two sets of variable names (which may represent categorical (dummy) or continuous variables),\n    this function creates two-way interactions by multiplying each variable in var_list1 with each\n    variable in var_list2.\n\n    Optionally, if a third list of variable names (var_list3) is provided, the function also creates\n    three-way interactions among each variable in var_list1, each variable in var_list2, and each variable\n    in var_list3.\n\n    Parameters:\n        var_list1 (list): List of column names for the first set of variables.\n        var_list2 (list): List of column names for the second set of variables.\n        var_list3 (list, optional): List of column names for the third set of variables for three-way interactions.\n\n    Returns:\n        A flat list of new interaction column names.\n    \"\"\"\n    global dtrain, dtest\n\n    interaction_cols = []\n\n    # Create two-way interactions between var_list1 and var_list2.\n    for var1 in var_list1:\n        for var2 in var_list2:\n            col_name = f\"{var1}_*_{var2}\"\n            dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list1 and var_list3.\n    if var_list3 is not None:\n      for var1 in var_list1:\n          for var3 in var_list3:\n              col_name = f\"{var1}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list2 and var_list3.\n    if var_list3 is not None:\n      for var2 in var_list2:\n          for var3 in var_list3:\n              col_name = f\"{var2}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # If a third list is provided, create three-way interactions.\n    if var_list3 is not None:\n        for var1 in var_list1:\n            for var2 in var_list2:\n                for var3 in var_list3:\n                    col_name = f\"{var1}_*_{var2}_*_{var3}\"\n                    dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    interaction_cols.append(col_name)\n\n    return interaction_cols\n\n # Example\n # interaction_cols_brand_price = add_interaction_terms(dummy_cols_brand, ['log_price'])\n # interaction_cols_brand_ad_price = add_interaction_terms(dummy_cols_brand, dummy_cols_ad, ['log_price'])"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-11",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-11",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\n\nHow does advertisement play a role in the relationship between sales and prices in the OJ market?\n\nThe ads can increase sales at all prices.\nThey can change price sensitivity.\nThey can do both of these things in a brand-specific manner.\n\nWhat would be the formula that address the question above?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-12",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-12",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\n\nLet’s train the third model, model_3, that addresses the question in the previous page:\n\n\n\n\n\\[\n\\begin{align}\n\\log(\\texttt{sales}_{\\texttt{i}}) \\,=\\,\\quad\\;\\;& b_{\\texttt{intercept}} \\,+\\, \\color{Green}{b_{\\,\\texttt{mm}}\\,\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}}} \\,+\\, \\color{Blue}{b_{\\,\\texttt{tr}}\\,\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}}  \\\\\n&\\,+\\; b_{\\,\\texttt{ad}}\\,\\color{Orange}{\\texttt{ad}_{\\,\\texttt{i}}} \\qquad\\qquad\\qquad\\qquad\\quad   \\\\\n&\\,+\\, b_{\\texttt{mm*ad}}\\,\\color{Green} {\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}}}\\,\\times\\, \\color{Orange}{\\texttt{ad}_{\\,\\texttt{i}}}\\,+\\, b_{\\texttt{tr*ad}}\\,\\color{Blue} {\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}}\\,\\times\\, \\color{Orange}{\\texttt{ad}_{\\,\\texttt{i}}} \\\\\n&\\,+\\;  b_{\\texttt{price}}\\,\\log(\\texttt{price}_{\\texttt{i}}) \\qquad\\qquad\\qquad\\;\\;\\;\\;\\,  \\\\\n&\\,+\\, b_{\\texttt{price*mm}}\\,\\log(\\texttt{price}_{\\texttt{i}})\\,\\times\\,\\color{Green} {\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}}}\\qquad\\qquad\\qquad\\;\\, \\\\\n&\\,+\\, b_{\\texttt{price*tr}}\\,\\log(\\texttt{price}_{\\texttt{i}})\\,\\times\\,\\color{Blue} {\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}}\\qquad\\qquad\\qquad\\;\\, \\\\\n& \\,+\\, b_{\\texttt{price*ad}}\\,\\log(\\texttt{price}_{\\texttt{i}})\\,\\times\\,\\color{Orange}{\\texttt{ad}_{\\,\\texttt{i}}}\\qquad\\qquad\\qquad\\;\\;\\, \\\\\n&\\,+\\, b_{\\texttt{price*mm*ad}}\\,\\log(\\texttt{price}_{\\texttt{i}}) \\,\\times\\,\\,\\color{Green} {\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}}}\\,\\times\\, \\color{Orange}{\\texttt{ad}_{\\,\\texttt{i}}} \\\\\n&\\,+\\, b_{\\texttt{price*tr*ad}}\\,\\log(\\texttt{price}_{\\texttt{i}}) \\,\\times\\,\\,\\color{Blue} {\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}}\\,\\times\\, \\color{Orange}{\\texttt{ad}_{\\,\\texttt{i}}}  \\,+\\, e_{\\texttt{i}}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-13",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-13",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\nEstimating Price Elasticity\n\nDescribe the relationship between ad and brand using stacked bar charts.\n\nsns.histplot(\n    data=oj,\n    x='ad_status',   # categorical variable on the x-axis\n    hue='brand',     # fill color by brand\n    multiple='fill'  # 100% stacked bars\n)\nplt.ylabel('Proportion')\n\nModel 3 assumes that the relationship between price and sales can vary by ad."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-14",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-14",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\nEstimating Price Elasticity\n\n\nHow would you explain different estimation results across different models?\nWhich model do you prefer? Why?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#instructor-1",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#instructor-1",
    "title": "Lecture 1",
    "section": "Instructor",
    "text": "Instructor\nCurrent Appointment & Education\n\nName: Byeong-Hak Choe.\nAssistant Professor of Data Analytics and Economics, School of Business at SUNY Geneseo.\nPh.D. in Economics from University of Wyoming.\nM.S. in Economics from Arizona State University.\nM.A. in Economics from SUNY Stony Brook.\nB.A. in Economics & B.S. in Applied Mathematics from Hanyang University at Ansan, South Korea.\n\nMinor in Business Administration.\nConcentration in Finance."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#instructor-2",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#instructor-2",
    "title": "Lecture 1",
    "section": "Instructor",
    "text": "Instructor\nEconomics and Data Science\n\nChoe, B.H., Newbold, S. and James, A., “Estimating the Value of Statistical Life through Big Data”\n\nQuestion: How much is the society willing to pay to reduce the likelihood of fatality?\n\nChoe, B.H., “Social Media Campaigns, Lobbying and Legislation: Evidence from #climatechange and Energy Lobbies.”\n\nQuestion: To what extent do social media campaigns compete with fossil fuel lobbying on climate change legislation?\n\nChoe, B.H. and Ore-Monago, T., 2024. “Governance and Climate Finance in the Developing World”\n\nQuestion: In what ways and through what forms does poor governance act as a significant barrier to reducing greenhouse gas emissions in developing countries?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-1",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-1",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nEmail, Class & Office Hours\n\nEmail: bchoe@geneseo.edu\nClass Homepage:\n\nhttps://brightspace.geneseo.edu/\nhttp://bcdanl.github.io/320/\n\nOffice: South Hall 227B\nOffice Hours:\n\nMondays 5:00 P.M. – 6:30 P.M.\n\nWednesdays 5:00 P.M. – 6:30 P.M."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-2",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-2",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nCourse Description\n\nThis course teaches you how to analyze big data sets, and this course is specifically designed to bring you up to speed on one of the best technologies for this task, Apache Spark!\nThe top technology companies like Google, Facebook, Netflix, Airbnb, 3 Amazon, and many more are all using Spark to solve their big data problems!\nWith the Spark 3.0 DataFrame framework, it can perform up to 100x faster than Hadoop MapReduce."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-3",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-3",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nCourse Description\n\nThis course will review the basics in Python, continuing on to learning how to use Spark DataFrame API with the latest Spark 3.0 syntax!\nIn addition, you will learn how to perform supervised an unsupervised machine learning on massive datasets using the Machine Learning Library (MLlib).\nYou will also gain hands-on experience using PySpark within the Jupyter Notebook environment. This course also covers the latest Spark technologies, like Spark SQL, Spark Streaming, and advanced data analytics modeling methodologies."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-4",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-4",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nRequired Materials\n\nIntroduction to pyspark by Pedro Duarte Faria\n\nA free online version of this book is available."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-5",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-5",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nReference Materials - PySpark\n\nSpark by Examples\nApache PySpark - PySpark Documentation"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-6",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-6",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nReference Materials - Sports Data\n\nIntroduction to Sports Analytics Using R by Ryan Elmore and Andrew Urbaczewski\n\nAn eBook version of this book is available at:\n\nVitalSource\nRedShelf"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-7",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-7",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nReference Materials - Python\n\nPython for Data Analysis (3rd Edition) by Wes McKinney\nIPython Interactive Computing and Visualization Cookbook\nPython Programming for Data Science by Tomas Beuzen\nCoding for Economists by Arthur Turrell\nPython for Econometrics in Economics by Fabian H. C. Raters\nQuantEcon DataScience - Python Fundamentals by Chase Coleman, Spencer Lyon, and Jesse Perla\nQuantEcon DataScience - pandas by Chase Coleman, Spencer Lyon, and Jesse Perla"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-8",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-8",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nReference Materials - Website\n\nGuide for Quarto"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-9",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-9",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nCourse Requirements\n\nLaptop: You should bring your own laptop (Mac or Windows) to the classroom.\n\nThe minimum specification for your laptop in this course is 2+ core CPU, 4+ GB RAM, and 500+ GB disk storage.\n\nHomework: There will be six homework assignments.\nProject: There will be one project on a personal website.\nExams: There will be one Midterm Exam.\nParticipation: You are encouraged to participate in GitHub-based online discussions and class discussion, and office hours.\n\nCheckout the netiquette policy in the syllabus."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-10",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-10",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nPersonal Website\n\nYou will create your own website using Quarto, R Studio, and Git.\nYou will publish your homework assignments and team project on your website.\nYour website will be hosted in GitHub.\nThe basics in Markdown will be discussed.\nReferences:\n\nQuarto Guide"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-11",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-11",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nTeam Project\n\nTeam formation is scheduled for early April.\n\nEach team must have one to two students.\n\nFor the team project, a team must choose data related to business or socioeconomic issues.\nThe project report should include both (1) exploratory data analysis using summary statistics, visual representations, and data wrangling, and (2) machine learning analysis.\nThe document for the team project must be published in each member’s website.\nAny changes to team composition require approval from Byeong-Hak Choe."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-12",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-12",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nClass Schedule and Exams\n\nThere will be tentatively 28 class sessions.\nThe Midterm Exam is scheduled on March 31, 2025, Wednesday, during the class time.\nThe Project Presentation is scheduled on May 9, 2025, Friday, 3:30 P.M.-5:30 P.M.\nThe due for the Project write-up is May 16, 2024, Friday."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-13",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-13",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nCourse Contents"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-14",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-14",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nCourse Contents"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-15",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-15",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nGrading\n\\[\n\\begin{align}\n(\\text{Total Percentage Grade}) =&\\quad\\;\\, 0.05\\times(\\text{Total Attendance Score})\\notag\\\\\n&\\,+\\, 0.05\\times(\\text{Total Participation Score})\\notag\\\\\n&\\,+\\, 0.10\\times(\\text{Website Score})\\notag\\\\\n&\\,+\\, 0.30\\times(\\text{Total Homework Score})\\notag\\\\\n&\\,+\\, 0.50\\times(\\text{Total Exam and Project Score}).\\notag\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-16",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-16",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nGrading\n\nYou are allowed up to 2 absences without penalty.\n\nSend me an email if you have standard excused reasons (illness, family emergency, transportation problems, etc.).\n\nFor each absence beyond the initial two, there will be a deduction of 1% from the Total Percentage Grade.\nParticipation will be evaluated by quantity and quality of GitHub-based online discussions and in-person discussion.\nThe single lowest homework score will be dropped when calculating the total homework score."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-17",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-17",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nMake-up Policy\n\nMake-up exams will not be given unless you have either a medically verified excuse or an absence excused by the University.\nIf you cannot take exams because of religious obligations, notify me by email at least two weeks in advance so that an alternative exam time may be set.\nA missed exam without an excused absence earns a grade of zero.\nLate submissions for homework assignment will be accepted with a penalty.\nA zero will be recorded for a missed assignment.\n\n:::"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-1",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-1",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nIntroduction: Share Your Background and Experience\nMarcie Hogan, Class of 2023\nJaehyung Lee (Andy), Class of 2022\nJason Rappazzo, Class of 2023 - Part 1\nJason Rappazzo, Class of 2023 - Part 2"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-2",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-2",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nQ. What challenges have you encountered in your role at the workplace?\nMarcie Hogan, Class of 2023\nJason Rappazzo, Class of 2023\nJaehyung Lee (Andy), Class of 2022"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-3",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-3",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nQ. How do you envision the future of AI?\nJaehyung Lee (Andy), Class of 2022\nMarcie Hogan, Class of 2023"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-4",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-4",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nQ. How does working at a large company like Meta compare to your previous experiences?\nMarcie Hogan, Class of 2023"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-5",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-5",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nQ. What has been the most fulfilling aspect of your work?\nJason Rappazzo, Class of 2023\nJaehyung Lee (Andy), Class of 2022\nMarcie Hogan, Class of 2023"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-6",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-6",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nQ. Can you share advice for students who are preparing for internships or job applications?\nMarcie Hogan, Class of 2023\nJaehyung Lee (Andy), Class of 2022"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---1.-marcie-hogan",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---1.-marcie-hogan",
    "title": "Lecture 1",
    "section": "DANL Career Session - 1. Marcie Hogan",
    "text": "DANL Career Session - 1. Marcie Hogan\n\nBackground\n\nGraduated from SUNY Geneseo in 2023 with a degree in Geography and minors in Mathematics and Physics.\nTook several data analytics courses and was a tutor in the Data Analytics Lab.\nInitially worked at Crown Castle as a Geospatial Data Analyst, including an internship, totaling about two years.\n\nCareer Path\n\nAt Crown Castle, Marcie managed database operations for spatial data, frequently working with Python and spatial packages.\nIn July 2024, she began a new role at Meta as a Community Project Manager within the Maps Organization, working on a contract basis."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---1.-marcie-hogan-1",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---1.-marcie-hogan-1",
    "title": "Lecture 1",
    "section": "DANL Career Session - 1. Marcie Hogan",
    "text": "DANL Career Session - 1. Marcie Hogan\n\nRole at Meta\n\nWorks extensively on Mapillary, an open-source street-level imagery hosting platform acquired by Meta.\nConducts analytics to understand the user community and guide product development.\nWhile her job title doesn’t explicitly mention data, data analytics is heavily integrated into her role."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---2.-jaehyung-lee-andy",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---2.-jaehyung-lee-andy",
    "title": "Lecture 1",
    "section": "DANL Career Session - 2. Jaehyung Lee (Andy)",
    "text": "DANL Career Session - 2. Jaehyung Lee (Andy)\n\nBackground\n\nGraduated from SUNY Geneseo in 2022 with a major in Mathematics and a concentration in Data Analytics.\nServed in the South Korean military for two years before resuming his academic and professional pursuits.\nTook several data analytics courses and was a tutor in the Data Analytics Lab."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---2.-jaehyung-lee-andy-1",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---2.-jaehyung-lee-andy-1",
    "title": "Lecture 1",
    "section": "DANL Career Session - 2. Jaehyung Lee (Andy)",
    "text": "DANL Career Session - 2. Jaehyung Lee (Andy)\n\nInvisible Technologies\n\nWorks as Analytics Associate at Invisible Technologies, a fast-growing startup that partners with leading AI platforms like OpenAI, Cohere, Google, and Character.AI.\nThe company specializes in refining AI models through techniques like Reinforcement Learning from Human Feedback (RLHF) and Supervised Fine-Tuning (SFT)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---2.-jaehyung-lee-andy-2",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---2.-jaehyung-lee-andy-2",
    "title": "Lecture 1",
    "section": "DANL Career Session - 2. Jaehyung Lee (Andy)",
    "text": "DANL Career Session - 2. Jaehyung Lee (Andy)\n\nRole at Invisible Technologies\n\nHolds monthly business reviews for executives.\nProvides visibility on departmental targets for hiring, people operations, and client services.\nAnalyzes client data to ensure targets are met, particularly for clients like OpenAI.\nCreates numerous data pipelines using Python and develops data models with SQL."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---2.-jaehyung-lee-andy-3",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---2.-jaehyung-lee-andy-3",
    "title": "Lecture 1",
    "section": "DANL Career Session - 2. Jaehyung Lee (Andy)",
    "text": "DANL Career Session - 2. Jaehyung Lee (Andy)\n\nProjects\n\nSelf-Evaluation Project: Tracked over 20 personal metrics daily for two years to improve his lifestyle and productivity, such as wake-up times, tasks completed, reading, and meditation.\nUsed this project to enhance his coding skills and had valuable discussions during job interviews.\nEmphasized the importance of personal projects in learning and showcasing skills."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---2.-jaehyung-lee-andy-4",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---2.-jaehyung-lee-andy-4",
    "title": "Lecture 1",
    "section": "DANL Career Session - 2. Jaehyung Lee (Andy)",
    "text": "DANL Career Session - 2. Jaehyung Lee (Andy)\n\nImpact of Academic Courses\n\nTook courses like Data Analytics 100, 200, 210, and 310, which helped him build a meaningful relationship with data.\nLearned programming languages like R and Python, and developed skills in data visualization and presentation.\nCreated interactive dashboards using Shiny, which impressed interviewers and aided in job acquisition.\nStressed the applicability of classroom knowledge to real-world scenarios."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---3.-jason-rappazzo",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---3.-jason-rappazzo",
    "title": "Lecture 1",
    "section": "DANL Career Session - 3. Jason Rappazzo",
    "text": "DANL Career Session - 3. Jason Rappazzo\n\nBackground\n\nGraduated from SUNY Geneseo in 2023 with a major in Economics and a minor in Data Analytics.\nServed as a tutor in the Data Analytics Lab and was part of the track team alongside Marcie.\n\nRole at Momentive\n\nWorks as a data analyst in the Global IT Automation and Reporting at Momentive, a chemical manufacturing company.\nThe company produces a wide array of products found in everyday items, from tires to mattresses and aerospace components.\nResponsible for automating manual business processes and turning complex data into actionable insights."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---3.-jason-rappazzo-1",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---3.-jason-rappazzo-1",
    "title": "Lecture 1",
    "section": "DANL Career Session - 3. Jason Rappazzo",
    "text": "DANL Career Session - 3. Jason Rappazzo\n\nTechnologies Used\n\nAlteryx: For data extraction and consolidating data from various company sources.\nSnowflake: Serves as the data warehouse where data is transformed and prepared.\nTableau: Used to create dashboards and visualizations for different departments."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---3.-jason-rappazzo-2",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---3.-jason-rappazzo-2",
    "title": "Lecture 1",
    "section": "DANL Career Session - 3. Jason Rappazzo",
    "text": "DANL Career Session - 3. Jason Rappazzo\n\nInsights on Learning\n\nEmphasized the importance of adaptability and the ability to learn new technologies in a rapidly changing tech landscape.\nEncouraged persistence in coding, highlighting that overcoming errors and frustrations is part of the learning process.\nBelieves that proficiency in one programming language can facilitate learning others."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---3.-jason-rappazzo-3",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---3.-jason-rappazzo-3",
    "title": "Lecture 1",
    "section": "DANL Career Session - 3. Jason Rappazzo",
    "text": "DANL Career Session - 3. Jason Rappazzo\n\nProjects\n\nSG&A Dashboard: Acted as the project manager for a dashboard visualizing the company’s Selling, General, and Administrative expenses.\nDigital Growth Dashboard: Visualizes online sales data and tracks the company’s shift toward automated online product sales.\nRegularly combines data from various ERP-related tables into coherent views for analysis and presentation."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-8",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-8",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nChallenges Faced\n\nMarcie\n\nExperienced corporate instability at Crown Castle, witnessing four rounds of layoffs, a CEO change, and office closures.\nHad to make an earlier-than-expected career move due to the company’s uncertain future.\nAdjusted to Meta’s faster-paced environment and the need to learn new internal tools and processes.\nNavigated strict data privacy and legal considerations at Meta, a shift from her previous role."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-9",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-9",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nChallenges Faced\n\nAndy\n\nInitially believed data analytics was primarily technical but realized the importance of interpersonal skills.\nLearned to communicate effectively with stakeholders to meet their needs and deliver valuable insights.\nContinues to work on improving communication and presentation skills."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-10",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-10",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nChallenges Faced\n\nJason\n\nFaced a learning curve due to a limited background in finance, necessitating on-the-job learning of financial data analysis.\nRecognized the importance of understanding data context to create meaningful visualizations for stakeholders.\nLearned that real-world data is often messy and inconsistent, requiring significant data preparation and cleaning."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-11",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-11",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nInsights on AI\n\nMarcie\n\nObserved Meta’s significant investment in AI technologies, such as open-source models like Llama 3.\nBelieves AI applications will expand across various sectors, including geospatial data and map-making.\nEmphasized the importance of having industry-specific knowledge to optimize AI models effectively."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-12",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-12",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nInsights on AI\n\nAndy\n\nAcknowledged the hype surrounding AI and the possibility of an AI bubble.\nMentioned that his company is preparing by diversifying into other industries beyond AI.\nBelieves AI will become more prominent across all businesses, requiring adaptability and broader skill sets."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-13",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-13",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nMost Fulfilling Aspects\n\nMarcie\n\nFinds fulfillment when her work aligns with her passions, particularly in geospatial data and map-making.\nEnjoys seeing the direct impact of her insights on public-facing products and contributing to an interconnected world.\nPrefers roles where she can witness the end results and user impact of her work."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-14",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-14",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nMost Fulfilling Aspects\n\nAndy\n\nDerives satisfaction from building solutions from scratch and seeing them positively impact stakeholders.\nValues moments when his work translates into tangible benefits for the company and receives appreciation."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-15",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-15",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nMost Fulfilling Aspects\n\nJason\n\nFinds it rewarding when code runs flawlessly and data is accurately prepared.\nEnjoys impressing stakeholders with the final product, especially when it simplifies their work.\nValues making a tangible impact on colleagues’ efficiency and effectiveness."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-16",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-16",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nAdvice to Students Who Are Looking for a Job\n\nMarcie\n\nEncouraged students to leverage opportunities at Geneseo to build their resumes, such as tutoring, research, and extracurricular activities.\nHighlighted the competitive job market and the necessity of relevant experience to secure internships and jobs.\nAdvised students to combine their data analytics skills with industries they are passionate about for a more fulfilling career."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-17",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-17",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nAdvice to Students Who Are Looking for a Job\n\nAndy\n\nEmphasized the significance of working on personal projects to deepen coding and data science expertise.\nRecommended mastering Python, R, and SQL as essential tools for data-related jobs.\nAdvised students to have one or two significant projects to discuss during interviews.\nEncouraged networking with professionals in the field to gain insights and collaboration opportunities.\nHighlighted that exploring various projects builds confidence and aids in transitioning from academia to industry."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-18",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-18",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nAdvice to Students Who Are Looking for a Job\n\nJason\n\nRecommended learning basic finance and accounting concepts to better understand and meet business needs.\nAdvised developing strong interpersonal and communication skills to effectively collaborate with non-technical stakeholders.\nEncouraged being open to learning new tools and technologies to stay relevant in the field."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-19",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-19",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nSummary\n\nThe speakers highlighted the importance of:\n\nTechnical Skills: Proficiency in programming languages like Python, R, and SQL is crucial.\nContinuous Learning: Staying adaptable and willing to learn new technologies and methodologies.\nInterpersonal Skills: Effective communication with stakeholders and team members is essential.\nPassion Alignment: Combining data analytics skills with personal interests leads to a more fulfilling career.\nPractical Experience: Engaging in projects, internships, and leveraging academic opportunities enhances employability."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career",
    "title": "Lecture 1",
    "section": "DANL Career",
    "text": "DANL Career\nUseful Skills for DANL Students\n\nData Analysis with Python and R\nData Science with Python and R\nProgramming Languages for databases such as SQL\nBusiness Intelligence Software such as Power BI and/or Tableau\nVersion Control with Git and GitHub\nData Analytics/Science Portfolio with a Capstone Project on GitHub\nPrompt Engineering in Generative AI"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-1",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-1",
    "title": "Lecture 1",
    "section": "DANL Career",
    "text": "DANL Career\nData Analytics-related Certificates on Coursera\n\nGeneral Data Analytics, SQL, and Business Intelligence\n\nGoogle Data Analytics Professional Certificate\nIBM Databases and SQL for Data Science with Python\nTableau Business Intelligence Analyst Professional Certificate\nMicrosoft Power BI Data Analyst Professional Certificate\n\nData Science\n\nIBM Data Science Professional Certificate\n\nGenerative AI\n\nGoogle AI Essentials\nIBM Generative AI: Prompt Engineering Basics"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-1",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-1",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nVariables Are Names, Not Places\n\n\n\n\nA value is datum (literal) such as a number or text.\nThere are different types of values:\n\n352.3 is known as a float or double;\n22 is an integer;\n“Hello World!” is a string."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-2",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-2",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nValues, Variables, and Types\na = 10\nprint(a)\n\n\n\n\n\n\n\nA variable is a name that refers to a value.\n\nWe can think of a variable as a box that has a value, or multiple values, packed inside it.\n\nA variable is just a name!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-3",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-3",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nValues, Variables, and Types\n\n\nSometimes you will hear variables referred to as objects.\nEverything that is not a literal value, such as 10, is an object."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-4",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-4",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#variable-in-data.frame",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#variable-in-data.frame",
    "title": "Lecture 3",
    "section": "Variable in data.frame",
    "text": "Variable in data.frame\n\n\n\n\n\n\nDefinition: A data.frame is a table-like data structure used for storing data in a tabular format with rows and columns.\nStructure: Consists of:\n\nVariables (Columns)\nObservations (Rows)\nValues (Cells): Individual data points within each cell of the data.frame."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-5",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-5",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nAssignment ( = )\n# Here we assign the integer value 5 to the variable x.\nx = 5   \n\n# Now we can use the variable x in the next line.\ny = x + 12  \ny\n\nIn Python, we use = to assign a value to a variable.\nIn math, = means equality of both sides.\nIn programs, = means assignment: assign the value on the right side to the variable on the left side."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-6",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-6",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nCode and comment style\n\nThe two main principles for coding and managing data are:\n\nMake things easier for your future self.\nDon’t trust your future self.\n\nThe # mark is Google Colab’s comment character.\n\nThe # character has many names: hash, sharp, pound, or octothorpe.\n# indicates that the rest of the line is to be ignored.\nWrite comments before the line that you want the comment to apply to.\n\nConsider adding more comments on code cells and their results using text cells."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-7",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-7",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nAssignment\n\nIn programming code, everything on the right side needs to have a value.\n\nThe right side can be a literal value, or a variable that has already been assigned a value, or a combination.\n\nWhen Python reads y = x + 12, it does the following:\n\nSees the = in the middle.\nKnows that this is an assignment.\nCalculates the right side (gets the value of the object referred to by x and adds it to 12).\nAssigns the result to the left-side variable, y."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-8",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-8",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nVariables Are Names, Not Places\nlist_example = [10, 1.23, \"like this\", True, None]\nprint(list_example)\ntype(list_example)\n\nThe most basic built-in data types that we’ll need to know about are:\n\nintegers 10\nfloats 1.23\nstrings \"like this\"\nbooleans True\nnothing None\n\nPython also has a built-in type of data container called a list (e.g., [10, 15, 20]) that can contain anything, even different types"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-9",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-9",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nTypes\n\n\n\n\nThe second column (Type) contains the Python name of that type.\nThe third column (Mutable?) indicates whether the value can be changed after creation."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-10",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-10",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nBrackets\n\nThere are several kinds of brackets in Python, including [], {}, and ().\n\n\n[]{}()\n\n\nvector = ['a', 'b']\nvector[0]\n\n[] is used to denote a list or to signify accessing a position using an index.\n\n\n\n{'a', 'b'}  # set\n{'first_letter': 'a', 'second_letter': 'b'}  # dictionary\n\n{} is used to denote a set or a dictionary (with key-value pairs).\n\n\n\nnum_tup = (1, 2, 3)\nsum(num_tup)\n\n() is used to denote\n\na tuple, or\nthe arguments to a function, e.g., function(x) where x is the input passed to the function."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-11",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-11",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nOperators\nstring_one = \"This is an example \"\nstring_two = \"of string concatenation\"\nstring_full = string_one + string_two\nprint(string_full)\n\nAll of the basic operators we see in mathematics are available to use:\n\n\n\n\n+ for addition\n- for subtraction\n\n\n\n* for multiplication\n** for powers\n\n\n\n/ for division\n// for integer division\n\n\n\nThese work as you’d expect on numbers.\nThese operators are sometimes defined for other built-in data types too.\n\nWe can ‘sum’ strings (which really concatenates them)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-12",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-12",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nOperators\n\n\nlist_one = [\"apples\", \"oranges\"]\nlist_two = [\"pears\", \"satsumas\"]\nlist_full = list_one + list_two\nprint(list_full)\n\nIt works for lists too:\n\n\nstring = \"apples, \"\nprint(string * 3)\n\nWe can multiply strings!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-13",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-13",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nOperators\nQ. Classwork 4.1\nUsing Python operations only, calculate below: \\[\\frac{2^5}{7 \\cdot (4 - 2^3)}\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-14",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-14",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nCasting Variables\n\n\norig_number = 4.39898498\ntype(orig_number)\n\nmod_number = int(orig_number)\nmod_number\ntype(mod_number)\n\n\nSometimes we need to explicitly cast a value from one type to another.\n\nWe can do this using built-in functions like str(), int(), and float().\nIf we try these, Python will do its best to interpret the input and convert it to the output type we’d like and, if they can’t, the code will throw a great big error."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-15",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-15",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nTuples and (im)mutability\n\n\nA tuple is an object that is defined by parentheses and entries that are separated by commas, for example (15, 20, 32). (They are of type tuple.)\nTuples are immutable, while lists are mutable.\nImmutable objects, such as tuples and strings, can’t have their elements changed, appended, extended, or removed.\n\nMutable objects, such as lists, can do all of these things.\n\nIn everyday programming, we use lists and dictionaries more than tuples."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-16",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-16",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nDictionaries\ncities_to_temps = {\"Paris\": 28, \"London\": 22, \"New York\": 36, \"Seoul\": 29}\n\ncities_to_temps.keys()\ncities_to_temps.values()\ncities_to_temps.items()\n\nAnother built-in Python type that is enormously useful is the dictionary.\n\nThis provides a mapping one set of variables to another (either one-to-one or many-to-one).\nIf you need to create associations between objects, use a dictionary.\n\nWe can obtain keys, values, or key-value paris from dictionaries."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-17",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-17",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nRunning on Empty\n\nBeing able to create empty containers is sometimes useful, especially when using loops.\nThe commands to create empty lists, tuples, dictionaries, and sets are lst = [], tup=(), dic={}, and st = set() respectively.\nQ. What is the type of an empty list?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-1",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-1",
    "title": "Lecture 3",
    "section": "Booleans, Conditions, and if Statements",
    "text": "Booleans, Conditions, and if Statements\n10 == 20\n10 == '10'\n\nBoolean data have either True or False value."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-2",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-2",
    "title": "Lecture 3",
    "section": "Booleans, Conditions, and if Statements",
    "text": "Booleans, Conditions, and if Statements\n\n\n\n\n\n\n\nExisting booleans can be combined, which create a boolean when executed."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-3",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-3",
    "title": "Lecture 3",
    "section": "Booleans, Conditions, and if Statements",
    "text": "Booleans, Conditions, and if Statements\nConditions are expressions that evaluate as booleans."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-4",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-4",
    "title": "Lecture 3",
    "section": "Booleans, Conditions, and if Statements",
    "text": "Booleans, Conditions, and if Statements\nboolean_condition1 = 10 == 20\nprint(boolean_condition1)\n\nboolean_condition2 = 10 == '10'\nprint(boolean_condition2)\n\nThe == is an operator that compares the objects on either side and returns True if they have the same values\nQ. What does not (not True) evaluate to?\nQ. Classwork 4.2"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-5",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-5",
    "title": "Lecture 3",
    "section": "Booleans, Conditions, and if Statements",
    "text": "Booleans, Conditions, and if Statements\nname = \"Geneseo\"\nscore = 99\n\nif name == \"Geneseo\" and score &gt; 90:\n    print(\"Geneseo, you achieved a high score.\")\n\nif name == \"Geneseo\" or score &gt; 90:\n    print(\"You could be called Geneseo or have a high score\")\n\nif name != \"Geneseo\" and score &gt; 90:\n    print(\"You are not called Geneseo and you have a high score\")\n\nThe real power of conditions comes when we start to use them in more complex examples, such as if statements."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-6",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-6",
    "title": "Lecture 3",
    "section": "Booleans, Conditions, and if Statements",
    "text": "Booleans, Conditions, and if Statements\nname_list = [\"Lovelace\", \"Smith\", \"Hopper\", \"Babbage\"]\n\nprint(\"Lovelace\" in name_list)\n\nprint(\"Bob\" in name_list)\n\nOne of the most useful conditional keywords is in.\n\nThis one must pop up ten times a day in most coders’ lives because it can pick out a variable or make sure something is where it’s supposed to be.\n\nQ. Check if “a” is in the string “Sun Devil Arena” using in. Is “a” in “Anyone”?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-7",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-7",
    "title": "Lecture 3",
    "section": "Booleans, Conditions, and if Statements",
    "text": "Booleans, Conditions, and if Statements\nscore = 98\n\nif score == 100:\n    print(\"Top marks!\")\nelif score &gt; 90 and score &lt; 100:\n    print(\"High score!\")\nelif score &gt; 10 and score &lt;= 90:\n    pass\nelse:\n    print(\"Better luck next time.\")\n\nOne conditional construct we’re bound to use at some point, is the if-else chain:"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-8",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-8",
    "title": "Lecture 3",
    "section": "Booleans, Conditions, and if Statements",
    "text": "Booleans, Conditions, and if Statements\nIndentation\n\nWe have seen that certain parts of the code examples are indented.\nCode that is part of a function, a conditional clause, or loop is indented.\nIndention is actually what tells the Python interpreter that some code is to be executed as part of, say, a loop and not to executed after the loop is finished."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-9",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-9",
    "title": "Lecture 3",
    "section": "Booleans, Conditions, and if Statements",
    "text": "Booleans, Conditions, and if Statements\nIndentation\nx = 10\n\nif x &gt; 2:\n    print(\"x is greater than 2\")\n\nHere’s a basic example of indentation as part of an if statement.\nThe standard practice for indentation is that each sub-statement should be indented by 4 spaces."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods",
    "title": "Lecture 3",
    "section": "Slicing Methods",
    "text": "Slicing Methods\n\n\n\n\nWith slicing methods, we can get subset of the data object.\nSlicing methods can apply for strings, lists, arrays, and DataFrames.\nThe above example describes indexing in Python"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-1",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-1",
    "title": "Lecture 3",
    "section": "Slicing Methods",
    "text": "Slicing Methods\nStrings\nstring = \"cheesecake\"\nprint( string[-4:] )\n\nFrom strings, we can access the individual characters via slicing and indexing.\n\n\n\nstring = \"cheesecake\"\nprint(\"String has length:\")\nprint( len(string) )\n\nlist_of_numbers = range(1, 20)\nprint(\"List of numbers has length:\")\nprint( len(list_of_numbers) )\n\n\nBoth lists and strings will allow us to use the len() command to get their length:"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-2",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-2",
    "title": "Lecture 3",
    "section": "Slicing Methods",
    "text": "Slicing Methods\nString-related Functions\nDot operation\n\nIn Python, we can access attributes by using a dot notation (.).\nUnlike len(), some functions use a dot to access to strings.\nTo use those string functions, type (1) the name of the string, (2) a dot, (3) the name of the function, and (4) any arguments that the function needs:\n\nstring_name.some_function(arguments)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-3",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-3",
    "title": "Lecture 3",
    "section": "Slicing Methods",
    "text": "Slicing Methods\nString-related Functions\nSplit with split()\n\nWe can use the built-in string split() function to break a string into a list of smaller strings based on some separator.\n\nIf we don’t specify a separator, split() uses any sequence of white space characters—newlines, spaces, and tabs:\n\ntasks = 'get gloves,get mask,give cat vitamins,call ambulance'\ntasks.split(',')\ntasks.split()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-4",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-4",
    "title": "Lecture 3",
    "section": "Slicing Methods",
    "text": "Slicing Methods\nString-related Functions\nCombine by Using join()\n\njoin() collapses a list of strings into a single string.\n\ncrypto_list = ['Yeti', 'Bigfoot', 'Loch Ness Monster']\ncrypto_string = ', '.join(crypto_list)\nprint('Found and signing book deals:', crypto_string)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-5",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-5",
    "title": "Lecture 3",
    "section": "Slicing Methods",
    "text": "Slicing Methods\nStrings and Slicing\n\nWe can extract a substring (a part of a string) from a string by using a slice.\nWe define a slice by using square brackets ([]), a start index, an end index, and an optional step count between them.\n\nWe can omit some of these.\n\nThe slice will include characters from index start to one before end:"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-6",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-6",
    "title": "Lecture 3",
    "section": "Slicing Methods",
    "text": "Slicing Methods\nGet a Substring with a Slice\n\n[:][ start :][: end ][ start : end ][ start : end : step ]\n\n\nletters = 'abcdefghij'\nletters[:]\n\n[:] extracts the entire sequence from start to end.\n\n\n\nletters = 'abcdefghij'\nletters[4:]\nletters[2:]\nletters[-3:]\nletters[-50:]\n\n[ start :] specifies from the start index to the end.\n\n\n\nletters = 'abcdefghij'\nletters[:3]\nletters[:-3]\nletters[:70]\n\n[: end ] specifies from the beginning to the end index minus 1.\n\n\n\nletters = 'abcdefghij'\nletters[2:5]\nletters[-26:-24]\nletters[35:37]\n\n[ start : end ] indicates from the start index to the end index minus 1.\n\n\n\nletters = 'abcdefghij'\nletters[2 : 6 : 2]   # From index 2 to 5, by steps of 2 characters\nletters[ : : 3]     # From the start to the end, in steps of 3 characters\nletters[ 6 : : 4 ]    # From index 19 to the end, by 4\nletters[ : 7 : 5 ]    # From the start to index 6 by 5:\nletters[-1 : : -1 ]   # Starts at the end and ends at the start\nletters[: : -1 ]\n\n[ start : end : step ] extracts from the start index to the end index minus 1, skipping characters by step."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-7",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-7",
    "title": "Lecture 3",
    "section": "Slicing Methods",
    "text": "Slicing Methods\nLists\n\nPython is\n\na zero-indexed language (things start counting from zero);\nleft inclusive;\nright exclusive when we are specifying a range of values."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-8",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-8",
    "title": "Lecture 3",
    "section": "Slicing Methods",
    "text": "Slicing Methods\nLists\nlist_example = ['one', 'two', 'three']\nlist_example[ 0 : 1 ]\nlist_example[ 1 : 3 ]\n\n\n\n\nWe can think of items in a list-like object as being fenced in.\n\nThe index represents the fence post."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-9",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-9",
    "title": "Lecture 3",
    "section": "Slicing Methods",
    "text": "Slicing Methods\nLists\n\n[index]Slicing Methods\n\n\nGet an Item by [index]\nsuny = ['Geneseo', 'Brockport', 'Oswego', 'Binghamton', \n        'Stony Brook', 'New Paltz'] \n\nWe can extract a single value from a list by specifying its index:\n\n\n\nsuny[0]\nsuny[1]\nsuny[2]\nsuny[7]\n\nsuny[-1]\nsuny[-2]\nsuny[-3]\nsuny[-7]\n\n\n\nGet an Item with a Slice\n\nWe can extract a subsequence of a list by using a slice:\n\nsuny = ['Geneseo', 'Brockport', 'Oswego', 'Binghamton', \n        'Stony Brook', 'New Paltz'] \nsuny[0:2]    # A slice of a list is also a list.\n\n\nsuny[ : : 2]\nsuny[ : : -2]\nsuny[ : : -1]\n\nsuny[4 : ]\nsuny[-6 : ]\nsuny[-6 : -2]\nsuny[-6 : -4]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-11",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-11",
    "title": "Lecture 3",
    "section": "Slicing Methods",
    "text": "Slicing Methods\n\nQ. Classwork 4.3"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#functions-arguments-and-parameters-1",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#functions-arguments-and-parameters-1",
    "title": "Lecture 3",
    "section": "Functions, Arguments, and Parameters",
    "text": "Functions, Arguments, and Parameters\nFunctions\nint(\"20\") \nfloat(\"14.3\")\nstr(5)\nint(\"xyz\")\n\nA function can take any number and type of input parameters and return any number and type of output results.\nPython ships with more than 65 built-in functions.\nPython also allows a user to define a new function.\nWe will mostly use built-in functions."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#functions-arguments-and-parameters-2",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#functions-arguments-and-parameters-2",
    "title": "Lecture 3",
    "section": "Functions, Arguments, and Parameters",
    "text": "Functions, Arguments, and Parameters\nprint(\"Cherry\", \"Strawberry\", \"Key Lime\")\nprint(\"Cherry\", \"Strawberry\", \"Key Lime\", sep = \"!\")\nprint(\"Cherry\", \"Strawberry\", \"Key Lime\", sep=\" \")\n\nWe invoke a function by entering its name and a pair of opening and closing parentheses.\nMuch as a cooking recipe can accept ingredients, a function invocation can accept inputs called arguments.\nWe pass arguments sequentially inside the parentheses (, separated by commas).\nA parameter is a name given to an expected function argument.\nA default argument is a fallback value that Python passes to a parameter if the function invocation does not explicitly provide one."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#functions-arguments-and-parameters-3",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#functions-arguments-and-parameters-3",
    "title": "Lecture 3",
    "section": "Functions, Arguments, and Parameters",
    "text": "Functions, Arguments, and Parameters\n\nQ. Classwork 4.4"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#repeat-with-while",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#repeat-with-while",
    "title": "Lecture 3",
    "section": "Repeat with while",
    "text": "Repeat with while\n\n\ncount = 1        \nwhile count &lt;= 5:\n    print(count)\n    count += 1\n\n\nWe first assigned the value 1 to count.\nThe while loop compared the value of count to 5 and continued if count was less than or equal to 5.\nInside the loop, we printed the value of count and then incremented its value by one with the statement count += 1.\nPython goes back to the top of the loop, and again compares count with 5.\nThe value of count is now 2, so the contents of the while loop are again executed, and count is incremented to 3.\nThis continues until count is incremented from 5 to 6 at the bottom of the loop.\nOn the next trip to the top, count &lt;= 5 is now False, and the while loop ends."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#repeat-with-while-1",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#repeat-with-while-1",
    "title": "Lecture 3",
    "section": "Repeat with while",
    "text": "Repeat with while\nAsking the user for input\n\n\nstuff = input()\n# Type something and press Return/Enter on Console \n# before running print(stuff)\nprint(stuff)\n\n\nSometimes we would like to take the value for a variable from the user via their keyboard.\n\nThe input() function gets input from the keyboard.\nWhen the input() is called, the program stops and waits for the user to type something on Console (interactive Python interpreter).\nWhen the user presses Return or Enter on Console, the program resumes and input returns what the user typed as a string."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#repeat-with-while-2",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#repeat-with-while-2",
    "title": "Lecture 3",
    "section": "Repeat with while",
    "text": "Repeat with while\nCancel with break\n\n\nwhile True:\n    user_input = input(\"Enter 'yes' to continue or 'no' to stop: \")\n    if user_input.lower() == 'no':\n        print(\"Exiting the loop. Goodbye!\")\n        break\n    elif user_input.lower() == 'yes':\n        print(\"You chose to continue.\")\n    else:\n        print(\"Invalid input, please enter 'yes' or 'no'.\")\n\n\nWhile loop is used to execute a block of code repeatedly until given boolean condition evaluated to False.\n\nwhile True loop will run forever unless we write it with a break statement.\n\nIf we want to loop until something occurs, but we’re not sure when that might happen, we can use an infinite loop with a break statement."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#repeat-with-while-3",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#repeat-with-while-3",
    "title": "Lecture 3",
    "section": "Repeat with while",
    "text": "Repeat with while\nSkip Ahead with continue\n\n\nwhile True:\n    value = input(\"Integer, please [q to quit]: \")\n    if value == 'q': # quit\n        break\n    number = int(value)\n    if number % 2 == 0: # an even number\n        continue\n    print(number, \"squared is\", number*number)\n\n\nSometimes, we don’t want to break out of a loop but just want to skip ahead to the next iteration for some reason.\nThe continue statement is used to skip the rest of the code inside a loop for the current iteration only."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#repeat-with-while-4",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#repeat-with-while-4",
    "title": "Lecture 3",
    "section": "Repeat with while",
    "text": "Repeat with while\nCheck break Use with else\n\nWe can consider using while with else when we’ve coded a while loop to check for something, and breaking as soon as it’s found. \n\nnumbers = [1, 3, 5]\nposition = 0\n\nwhile position &lt; len(numbers):\n    number = numbers[position]\n    if number &gt; 4:  # Condition changed to checking if the number is greater than 4\n        print('Found a number greater than 4:', number)\n        break\n    position += 1\nelse:  # break not called\n    print('No number greater than 4 found')\n\nConsider it a break checker."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in",
    "title": "Lecture 3",
    "section": "Iterate with for and in",
    "text": "Iterate with for and in\n\nSometimes we want to loop through a set of things such as a string of text, a list of words or a list of numbers.\n\nWhen we have a list of things to loop through, we can construct a for loop.\nA for loop makes it possible for you to traverse data structures without knowing how large they are or how they are implemented."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in-1",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in-1",
    "title": "Lecture 3",
    "section": "Iterate with for and in",
    "text": "Iterate with for and in\n\nLet’s see two ways to walk through a string here:\n\n\n\nword = 'thud'\noffset = 0\nwhile offset &lt; len(word):\n    print(word[offset])\n    offset += 1\n\nword = 'thud'\nfor letter in word:\n    print(letter)\n\n\nWhich one do you prefer?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in-2",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in-2",
    "title": "Lecture 3",
    "section": "Iterate with for and in",
    "text": "Iterate with for and in\nCancel with break\nword = 'thud'\nfor letter in word:\n    if letter == 'u':\n        break\n    print(letter)\n\nA break in a for loop breaks out of the loop, as it does for a while loop:"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in-3",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in-3",
    "title": "Lecture 3",
    "section": "Iterate with for and in",
    "text": "Iterate with for and in\nSkip with continue\nword = 'thud'\nfor letter in word:\n    if letter == 'u':\n        continue\n    print(letter)\n\nInserting a continue in a for loop jumps to the next iteration of the loop, as it does for a while loop."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in-4",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in-4",
    "title": "Lecture 3",
    "section": "Iterate with for and in",
    "text": "Iterate with for and in\nGenerate Number Sequences with range()\n\nThe range() function returns a stream of numbers within a specified range, without first having to create and store a large data structure such as a list or tuple.\n\nThis lets us create huge ranges without using all the memory in our computers and crashing our program.\nrange() returns an iterable object, so we need to step through the values with for … in, or convert the object to a sequence like a list."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in-5",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in-5",
    "title": "Lecture 3",
    "section": "Iterate with for and in",
    "text": "Iterate with for and in\nfor … in range()\n\n\nfor x in range(0, 3):\n    print(x)\nlist( range(0, 3) )\n\n\nWe use range() similar to how we use slices: range( start, stop, step ).\n\nIf we omit start, the range begins at 0.\nThe only required value is stop; as with slices, the last value created will be just before stop.\nThe default value of step is 1, but we can change it."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in-6",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in-6",
    "title": "Lecture 3",
    "section": "Iterate with for and in",
    "text": "Iterate with for and in\nCheck break Use with else\n\nSimilar to while, for has an optional else that checks whether the for completed normally.\n\nIf break was not called, the else statement is run.\n\n\nword = 'thud'\nfor letter in word:\n    if letter == 'x':\n        print(\"Eek! An 'x'!\")\n        break\n    print(letter)\nelse:\n    print(\"No 'x' in there.\")"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#loop-with-while-and-for-1",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#loop-with-while-and-for-1",
    "title": "Lecture 3",
    "section": "Loop with while and for",
    "text": "Loop with while and for\nClass Exercises\n\nQ. Classwork 4.5\nQ. Classwork 4.6"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#list-comprehension",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#list-comprehension",
    "title": "Lecture 3",
    "section": "List Comprehension",
    "text": "List Comprehension\nWhat is List Comprehension?\n\nA concise way to create or modify lists.\nSyntax: [expression for item in iterable if condition]\n\n\nCreating a List of Squares:\n\nsquares = [x**2 for x in range(5)]\n\nFiltering Items:\n\nnumbers = [1, 2, 3, 4, 5, 6]\nevens = [x for x in numbers if x != 2]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#dictionary-comprehension",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#dictionary-comprehension",
    "title": "Lecture 3",
    "section": "Dictionary Comprehension",
    "text": "Dictionary Comprehension\nWhat is Dictionary Comprehension?\n\nA concise way to create or modify dictionaries.\nSyntax: {key_expression: value_expression for item in iterable if condition}\n\n\nCreating a Dictionary of Squares:\n\nsquares_dict = {x: x**2 for x in range(5)}\n\nFiltering Dictionary Items:\n\n   my_dict = {'a': 1, 'b': 2, 'c': 3, 'd': 4}\n   filtered_dict = {k: v for k, v in my_dict.items() if v != 2}\n\nSwapping Keys and Values:\n\noriginal_dict = {'a': 1, 'b': 2, 'c': 3}\nswapped_dict = {v: k for k, v in original_dict.items()}"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#modifying-a-list",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#modifying-a-list",
    "title": "Lecture 3",
    "section": "Modifying a List",
    "text": "Modifying a List\nAdding Items\n\nappend(): Adds an item to the end of the list.\n\nmy_list = [1, 2, 3]\nmy_list.append(4)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#modifying-a-list-1",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#modifying-a-list-1",
    "title": "Lecture 3",
    "section": "Modifying a List",
    "text": "Modifying a List\nDeleting Items\n\nremove(): Deletes the first occurrence of value in the list.\n\nmy_list = [1, 2, 3, 4, 2]\nmy_list.remove(2)\n\nList Comprehension: Removes items based on a condition.\n\nmy_list = [1, 2, 3, 4, 2]\nmy_list = [x for x in my_list if x != 2]  \n\ndel statement: Deletes an item by index or a slice of items.\n\nmy_list = [1, 2, 3, 4]\ndel my_list[1] \ndel my_list[1:3]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#modifying-a-dictionary",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#modifying-a-dictionary",
    "title": "Lecture 3",
    "section": "Modifying a Dictionary",
    "text": "Modifying a Dictionary\nAdding/Updating Items\n\nupdate(): Adds new key-value pairs or updates existing ones.\n\nmy_dict = {'a': 1, 'b': 2}\nmy_dict.update({'c': 3})  \nmy_dict.update({'a': 10})"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#modifying-a-dictionary-1",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#modifying-a-dictionary-1",
    "title": "Lecture 3",
    "section": "Modifying a Dictionary",
    "text": "Modifying a Dictionary\nDeleting Items\n\nDictionary Comprehension: Removes items based on a condition.\n\nmy_dict = {'a': 1, 'b': 2, 'c': 3}\nmy_dict = {k: v for k, v in my_dict.items() if v != 2}  \n\ndel statement: Deletes an item by key.\n\nmy_dict = {'a': 1, 'b': 2, 'c': 3}\ndel my_dict['b']"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-1",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-1",
    "title": "Lecture 3",
    "section": "Handle Errors with try and except",
    "text": "Handle Errors with try and except\nException handlers"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-2",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-2",
    "title": "Lecture 3",
    "section": "Handle Errors with try and except",
    "text": "Handle Errors with try and except\nException handlers\n\nIn some languages, errors are indicated by special function return values.\n\nPython uses exceptions: code that is executed when an associated error occurs.\n\nWhen we run code that might fail under some circumstances, we also need appropriate exception handlers to intercept any potential errors.\n\nAccessing a list or tuple with an out-of-range position, or a dictionary with a nonexistent key."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-3",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-3",
    "title": "Lecture 3",
    "section": "Handle Errors with try and except",
    "text": "Handle Errors with try and except\nErrors\nshort_list = [1, 2, 3]\nposition = 5\nshort_list[position]\n\nIf we don’t provide your own exception handler, Python prints an error message and some information about where the error occurred and then terminates the program:"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-4",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-4",
    "title": "Lecture 3",
    "section": "Handle Errors with try and except",
    "text": "Handle Errors with try and except\nshort_list = [1, 2, 3]\nposition = 5\n\ntry:\n    short_list[position]\nexcept:\n    print('Need a position between 0 and', len(short_list)-1, ' but got',\n    position)\n\nRather than leaving things to chance, use try to wrap your code, and except to provide the error handling:"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-5",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-5",
    "title": "Lecture 3",
    "section": "Handle Errors with try and except",
    "text": "Handle Errors with try and except\nshort_list = [1, 2, 3]\nposition = 5\ntry:\n    short_list[position]\nexcept:\n    print('Need a position between 0 and', len(short_list)-1, ' but got',\n    position)\n\nThe code inside the try block is run.\n\nIf there is an error, an exception is raised and the code inside the except block runs.\n\nIf there are no errors, the except block is skipped."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-6",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-6",
    "title": "Lecture 3",
    "section": "Handle Errors with try and except",
    "text": "Handle Errors with try and except\nexcept type\n\nSpecifying a plain except with no arguments, as we did here, is a catchall for any exception type.\nIf more than one type of exception could occur, it’s best to provide a separate exception handler for each.\nWe get the full exception object in the variable name if we use the form:"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-7",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-7",
    "title": "Lecture 3",
    "section": "Handle Errors with try and except",
    "text": "Handle Errors with try and except\nexcept type\nshort_list = [1, 2, 3]\nwhile True:\n    value = input('Position [q to quit]? ')\n    if value == 'q':\n        break\n    try:\n        position = int(value)\n        print(short_list[position])\n    except IndexError as err:\n        print('Bad index:', position, '-', err)\n    except Exception as other:\n        print('Something else broke:', other)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-8",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-8",
    "title": "Lecture 3",
    "section": "Handle Errors with try and except",
    "text": "Handle Errors with try and except\nexcept type\n\nThe example looks for an IndexError first, because that’s the exception type raised when we provide an illegal position to a sequence.\nIt saves an IndexError exception in the variable err, and any other exception in the variable other.\nThe example prints everything stored in other to show what you get in that object.\n\nInputting position 3 raised an IndexError as expected.\nEntering two annoyed the int() function, which we handled in our second, catchall except code."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-9",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-9",
    "title": "Lecture 3",
    "section": "Handle Errors with try and except",
    "text": "Handle Errors with try and except\nClass Exercises\n\nQ. Classwork 4.7"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#importing-modules-packages-and-libraries",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#importing-modules-packages-and-libraries",
    "title": "Lecture 3",
    "section": "Importing Modules, Packages, and Libraries",
    "text": "Importing Modules, Packages, and Libraries\n\nPython is a general-purpose programming language and is not specialized for numerical or statistical computation.\nThe core libraries that enable Python to store and analyze data efficiently are:\n\npandas\nnumpy\nmatplotlib and seaborn"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#importing-modules-packages-and-libraries-1",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#importing-modules-packages-and-libraries-1",
    "title": "Lecture 3",
    "section": "Importing Modules, Packages, and Libraries",
    "text": "Importing Modules, Packages, and Libraries\npandas\n\n\n\n\npandas provides Series and DataFrames which are used to store data in an easy-to-use format."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#importing-modules-packages-and-libraries-2",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#importing-modules-packages-and-libraries-2",
    "title": "Lecture 3",
    "section": "Importing Modules, Packages, and Libraries",
    "text": "Importing Modules, Packages, and Libraries\nnumpy\n\n\n\n\nnumpy, numerical Python, provides the array block (np.array()) for doing fast and efficient computations;"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#importing-modules-packages-and-libraries-3",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#importing-modules-packages-and-libraries-3",
    "title": "Lecture 3",
    "section": "Importing Modules, Packages, and Libraries",
    "text": "Importing Modules, Packages, and Libraries\nmatplotlib and seaborn\n\n\n\n\nmatplotlib provides graphics. The most important submodule would be matplotlib.pyplot.\nseaborn provides a general improvement in the default appearance of matplotlib-produced plots."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#importing-modules-packages-and-libraries-4",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#importing-modules-packages-and-libraries-4",
    "title": "Lecture 3",
    "section": "Importing Modules, Packages, and Libraries",
    "text": "Importing Modules, Packages, and Libraries\nimport statement\n\nA module is basically a bunch of related codes saved in a file with the extension .py.\nA package is basically a directory of a collection of modules.\nA library is a collection of packages\nWe refer to code of other module/package/library by using the Python import statement.\n\nimport LIBRARY_NAME\n\nThis makes the code and variables in the imported module available to our programming codes."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#importing-modules-packages-and-libraries-5",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#importing-modules-packages-and-libraries-5",
    "title": "Lecture 3",
    "section": "Importing Modules, Packages, and Libraries",
    "text": "Importing Modules, Packages, and Libraries\nimport statement\n\n\nas\n\nWe can use the as keyword when importing the module/package/library using its canonical names.\n\nimport LIBRARY as SOMETHING_SHORT\n\nfrom\n\nWe can use the from keyword when specifying Python module/package/library from which we want to import something.\n\nfrom LIBRARY import FUNCTION, PACKAGE, MODULE"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#installing-modules-packages-and-libraries",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#installing-modules-packages-and-libraries",
    "title": "Lecture 3",
    "section": "Installing Modules, Packages, and Libraries",
    "text": "Installing Modules, Packages, and Libraries\npip tool\n\nTo install a library LIBRARY on your Google Colab or Anaconda Python, run the following:\n\npip install LIBRARY\n\nQ. Classwork 4.8"
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf",
    "title": "Lecture 5",
    "section": "Distributed Computing Framework (DCF)",
    "text": "Distributed Computing Framework (DCF)\n\n\n\n\n\n\n\n\nMassive Jigsaw Puzzle:\n\nSolving alone takes forever.\nInvite friends to work on different sections simultaneously.\n\n\n\n\nDCF Role:\n\nActs like a team manager.\nSplits large problems into manageable tasks.\nCoordinates parallel work across multiple computers (nodes)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#how-dcf-works-the-process",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#how-dcf-works-the-process",
    "title": "Lecture 5",
    "section": "How DCF Works: The Process",
    "text": "How DCF Works: The Process\n\nTask Distribution:\n\nBreaks problems into smaller tasks.\nAssigns tasks to different nodes in the network.\n\nParallel Processing:\n\nMultiple nodes work at the same time.\nLike an assembly line where different parts are built simultaneously.\n\nCommunication & Aggregation:\n\nEnsures nodes communicate effectively.\nGathers individual results into a final solution."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf-1",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf-1",
    "title": "Lecture 5",
    "section": "Distributed Computing Framework (DCF)",
    "text": "Distributed Computing Framework (DCF)\nReal-World Analogies\n\nFactory Manager:\n\nEach worker builds a part of a toy (arms, legs, wheels).\nThe manager (DCF) ensures all parts come together to form a complete toy.\n\nRace Organizer:\n\nDifferent computers have varying speeds and capabilities.\nTougher tasks are assigned to faster or more capable nodes."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf-2",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf-2",
    "title": "Lecture 5",
    "section": "Distributed Computing Framework (DCF)",
    "text": "Distributed Computing Framework (DCF)\nRobustness and Scalability\n\nFault Tolerance:\n\nHandles failures gracefully.\nIf a node fails, tasks are reassigned or retried (like a substitute worker in a factory).\n\nResource Allocation:\n\nDistributes tasks based on node capability.\nOptimizes efficiency across the network.\n\nScalability:\n\nEasily adds more computers to the network.\nMore helpers = faster puzzle solving."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf-3",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf-3",
    "title": "Lecture 5",
    "section": "Distributed Computing Framework (DCF)",
    "text": "Distributed Computing Framework (DCF)\n\nDCF is the Conductor of the Orchestra:\n\nEvery musician (node) plays their part.\nThe conductor (DCF) synchronizes the performance to create a harmonious final result.\n\nKey Benefits:\n\nFaster problem-solving through parallelism.\nEfficient management of tasks and resources.\nResilient to failures and scalable for growing problems."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf-4",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf-4",
    "title": "Lecture 5",
    "section": "Distributed Computing Framework (DCF)",
    "text": "Distributed Computing Framework (DCF)\nReal-World Examples\n\nHadoop\nApache Spark"
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop",
    "title": "Lecture 5",
    "section": "Hadoop",
    "text": "Hadoop\nIntroduction to Hadoop\n\n\n\n\n\n\nDefinition\n\nAn open-source software framework for storing and processing large data sets.\n\nComponents\n\nHadoop Distributed File System (HDFS): Distributed data storage.\nMapReduce: Data processing model."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop-1",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop-1",
    "title": "Lecture 5",
    "section": "Hadoop",
    "text": "Hadoop\nIntroduction to Hadoop\n\n\n\n\n\n\nPurpose\n\nEnables distributed processing of large data sets across clusters of computers."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop-2",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop-2",
    "title": "Lecture 5",
    "section": "Hadoop",
    "text": "Hadoop\nHadoop Architecture - HDFS\n\n\n\n\n\n\n\n\n\nHDFS\n\nDivides data into blocks and distributes them across different servers for processing.\nProvides a highly redundant computing environment\n\nAllows the application to keep running even if individual servers fail."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop-3",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop-3",
    "title": "Lecture 5",
    "section": "Hadoop",
    "text": "Hadoop\nHadoop Architecture - MapReduce\n\nMapReduce: Distributes the processing of big data files across a large cluster of machines.\n\nHigh performance is achieved by breaking the processing into small units of work that can be run in parallel across nodes in the cluster.\n\nMap Phase: Filters and sorts data.\n\ne.g., Sorting customer orders based on their product IDs, with each group corresponding to a specific product ID.\n\nReduce Phase: Summarizes and aggregates results.\n\ne.g., Counting the number of orders within each group, thereby determining the frequency of each product ID."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop-4",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop-4",
    "title": "Lecture 5",
    "section": "Hadoop",
    "text": "Hadoop\nHadoop Architecture - MapReduce"
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop-5",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop-5",
    "title": "Lecture 5",
    "section": "Hadoop",
    "text": "Hadoop\nHow Hadoop Works\n\nData Distribution\n\nLarge data sets are split into smaller blocks.\n\nData Storage\n\nBlocks are stored across multiple servers in the cluster.\n\nProcessing with MapReduce\n\nMap Tasks: Executed on servers where data resides, minimizing data movement.\nReduce Tasks: Combine results from map tasks to produce final output.\n\nFault Tolerance\n\nData replication ensures processing continues even if servers fail."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop-6",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop-6",
    "title": "Lecture 5",
    "section": "Hadoop",
    "text": "Hadoop\nExtending Hadoop for Real-Time Processing\n\nLimitation of Hadoop\n\nHadoop is originally designed for batch processing.\n\nBatch Processing: Data or tasks are collected over a period of time and then processed all at once, typically at scheduled times or during periods of low activity.\nResults come after the entire dataset is analyzed.\n\n\nReal-Time Processing Limitation:\n\nHadoop cannot natively process real-time streaming data (e.g., stock prices flowing into stock exchanges, live sensor data)\n\nExtending Hadoop’s Capabilities\n\nBoth Apache Storm and Apache Spark can run on top of Hadoop clusters, utilizing HDFS for storage."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#spark",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#spark",
    "title": "Lecture 5",
    "section": "Spark",
    "text": "Spark\n\n\n\n\n\n\nApache Spark: distributed processing system used for big data workloads. a unified computing engine and computer clusters\n\nIt contains a set of libraries for parallel processing for data analysis, machine learning, graph analysis, and streaming live data."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#spark-application-structure-on-a-cluster-of-computers",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#spark-application-structure-on-a-cluster-of-computers",
    "title": "Lecture 5",
    "section": "Spark Application Structure on a Cluster of Computers",
    "text": "Spark Application Structure on a Cluster of Computers\n\n\n\n\n\n\nDriver Process\n\nCommunicates with the cluster manager to acquire worker nodes.\nBreaks the application into smaller tasks if resources are allocated."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#spark-application-structure-on-a-cluster-of-computers-1",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#spark-application-structure-on-a-cluster-of-computers-1",
    "title": "Lecture 5",
    "section": "Spark Application Structure on a Cluster of Computers",
    "text": "Spark Application Structure on a Cluster of Computers\n\n\n\n\n\n\nCluster Manager\n\nDecides if Spark can use cluster resources (machines/nodes).\nAllocates necessary nodes to Spark applications."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#spark-application-structure-on-a-cluster-of-computers-2",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#spark-application-structure-on-a-cluster-of-computers-2",
    "title": "Lecture 5",
    "section": "Spark Application Structure on a Cluster of Computers",
    "text": "Spark Application Structure on a Cluster of Computers\n\n\n\n\n\n\nWorker Nodes \n\nExecute tasks assigned by the driver program.\nSend results back to the driver after execution.\nCan communicate with each other if needed during task execution."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#spark-vs.-hadoop",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#spark-vs.-hadoop",
    "title": "Lecture 5",
    "section": "Spark vs. Hadoop",
    "text": "Spark vs. Hadoop\nHadoop MapReduce: The Challenge\n\nSequential Multi-Step Process:\n\nReads data from the cluster.\nProcesses data.\nWrites results back to HDFS.\n\nDisk Input/Output Latency:\n\nEach step requires disk read/write.\nResults in slower performance due to latency."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#spark-vs.-hadoop-1",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#spark-vs.-hadoop-1",
    "title": "Lecture 5",
    "section": "Spark vs. Hadoop",
    "text": "Spark vs. Hadoop\nApache Spark: The Solution\n\nIn-Memory Processing:\n\nLoads data into memory once.\nPerforms all operations in-memory.\n\nData Reuse:\n\nCaches data for reuse in multiple operations (ideal for iterative tasks like machine learning).\n\nFaster Execution:\n\nEliminates multiple disk I/O steps.\nDramatically reduces latency for interactive analytics and real-time processing."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#spark-vs.-hadoop-2",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#spark-vs.-hadoop-2",
    "title": "Lecture 5",
    "section": "Spark vs. Hadoop",
    "text": "Spark vs. Hadoop\n\n\nApache Hadoop\n\nFramework Components:\n\nHDFS: Distributed storage system.\nMapReduce: Programming model for parallel processing.\n\nEcosystem:\n\nTypically integrates multiple execution engines (e.g., Spark) within a single deployment.\n\n\n\nSpark\n\nFocus Areas:\n\nInteractive queries, machine learning, and real-time analytics.\n\nStorage Agnostic:\n\nDoes not have its own storage system.\nOperates on data stored in systems like HDFS, etc.\n\nIntegration:\n\nCan run alongside Hadoop"
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#spark-vs.-hadoop-3",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#spark-vs.-hadoop-3",
    "title": "Lecture 5",
    "section": "Spark vs. Hadoop",
    "text": "Spark vs. Hadoop\nComplementary Use\n\nMany organizations store massive datasets in HDFS and utilize Spark for fast, interactive data processing.\n\nSpark can read data directly from HDFS, enabling seamless integration between storage and computation.\n\nHadoop provides robust storage and processing capabilities.\nSpark brings speed and versatility to data analytics, making them a powerful combination for solving complex business challenges.\nUse Case Example: An e-commerce company might store historical sales data in HDFS while using Spark to analyze customer behavior in real time to recommend products or detect fraudulent transactions."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#apache-spark-1",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#apache-spark-1",
    "title": "Lecture 5",
    "section": "Apache Spark",
    "text": "Apache Spark\nMedscape: Real-Time Medical News for Healthcare Professionals\n\nA medical news app for smartphones and tablets designed to keep healthcare professionals informed.\n\nProvides up-to-date medical news and expert perspectives.\n\n\n\n\n\n\n\n\n\n\n\nReal-Time Updates:\n\nUses Apache Storm/Spark to process about 500 million tweets per day.\nAutomatic Twitter feed integration helps users track important medical trends shared by physicians and medical commentators."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#pyspark-on-google-colab-1",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#pyspark-on-google-colab-1",
    "title": "Lecture 5",
    "section": "PySpark on Google Colab",
    "text": "PySpark on Google Colab\nPySpark = Spark + Python\n\npyspark is a Python API to Apache Spark.\n\nAPI: application programming interface, the set of functions, classes, and variables provided for you to interact with.\nSpark itself is coded in a different programming language, called Scala.\n\nWe can combine Python, pandas, and PySpark in one program.\n\nKoalas (now called pyspark.pandas) provides a pandas-like porcelain on top of PySpark."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#google-drive-on-google-colab",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#google-drive-on-google-colab",
    "title": "Lecture 5",
    "section": "Google Drive on Google Colab",
    "text": "Google Drive on Google Colab\n\n\nfrom google.colab import drive, files\ndrive.mount('/content/drive')\nfiles.upload()\n\ndrive.mount('/content/drive')\n\nTo mount your Google Drive on Google colab:\n\nfiles.upload()\n\nTo initiate uploading a file on Google Drive:\n\n\n\n\nTo find a pathname of a CSV file in Google Drive:\n\nClick 📁 from the sidebar menu\ndrive ➡️ MyDrive …\nHover a mouse cursor on the CSV file\nClick the vertical dots\nClick “Copy path”"
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#spark-dataframe-vs.-pandas-dataframe",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#spark-dataframe-vs.-pandas-dataframe",
    "title": "Lecture 5",
    "section": "Spark DataFrame vs. Pandas DataFrame",
    "text": "Spark DataFrame vs. Pandas DataFrame\n\nWhat makes a Spark DataFrame different from other DataFrame?\n\nSpark DataFrames are designed for big data and distributed computing.\n\nSpark DataFrame:\n\nData is distributed across a cluster of machines.\nOperations are executed in parallel on multiple nodes.\nCan process datasets that exceed the memory of a single machine.\n\nOther DataFrames (e.g., Pandas):\n\nOperate on a single machine.\nEntire dataset must fit into memory.\nLimited by local system resources."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#spark-dataframe-vs.-pandas-dataframe-1",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#spark-dataframe-vs.-pandas-dataframe-1",
    "title": "Lecture 5",
    "section": "Spark DataFrame vs. Pandas DataFrame",
    "text": "Spark DataFrame vs. Pandas DataFrame\nLazy Evaluation and Optimization\n\nSpark DataFrame:\n\nUses lazy evaluation: transformations are not computed until an action is called.\nOptimize query execution.\n\nOther DataFrames:\n\nOperations are evaluated eagerly (immediately).\nNo built-in query optimization across multiple operations."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#spark-dataframe-vs.-pandas-dataframe-2",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#spark-dataframe-vs.-pandas-dataframe-2",
    "title": "Lecture 5",
    "section": "Spark DataFrame vs. Pandas DataFrame",
    "text": "Spark DataFrame vs. Pandas DataFrame\nScalability\n\nSpark DataFrame:\n\nDesigned to scale to petabytes of data.\nUtilizes distributed storage and computing resources.\nIdeal for large-scale data processing and analytics.\n\nOther DataFrames:\n\nBest suited for small to medium datasets.\nLimited by the hardware of a single computer."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#spark-dataframe-vs.-pandas-dataframe-3",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#spark-dataframe-vs.-pandas-dataframe-3",
    "title": "Lecture 5",
    "section": "Spark DataFrame vs. Pandas DataFrame",
    "text": "Spark DataFrame vs. Pandas DataFrame\nFault Tolerance\n\nSpark DataFrame:\n\nBuilt on Resilient Distributed Datasets (RDDs).\nAutomatically recovers lost data if a node fails.\nEnsures high reliability in distributed environments.\n\nOther DataFrames:\n\nTypically lack built-in fault tolerance.\nFailures on a single machine can result in data loss."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#rdd-and-pyspark-dataframe",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#rdd-and-pyspark-dataframe",
    "title": "Lecture 5",
    "section": "RDD and PySpark DataFrame",
    "text": "RDD and PySpark DataFrame\n\n\n\n\n\n\nIn the RDD, we think of each row as an independent entity.\nWith the DataFrame, we mostly interact with columns, performing functions on them.\n\nWe still can access the rows of a DataFrame via RDD if necessary."
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#k-fold-cross-validation-1",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#k-fold-cross-validation-1",
    "title": "Lecture 9",
    "section": "K-fold Cross-Validation",
    "text": "K-fold Cross-Validation\n\n\n\n  Partitioning Data for 3-fold Cross-Validation\n\n\n\nA single train–test split uses each subset only once—either for training or for evaluation.\nK‑fold cross-validation divides the training data into K equal parts (folds).\n\nFor each fold \\(k=1, \\dots, K\\):\n\nStep 1: Train the model on \\(K‑1\\) folds.\nStep 2: Evaluate the model on the held‑out fold.\n\n\nThe average error (e.g., deviance) across folds provides a robust estimate of model performance."
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#training-validation-and-test-datasets",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#training-validation-and-test-datasets",
    "title": "Lecture 9",
    "section": "Training, Validation, and Test Datasets",
    "text": "Training, Validation, and Test Datasets\n\nTraining Data:\nThe portion of data used to fit the model.\nWithin this set, k‑fold cross-validation is applied.\nValidation Data:\nTemporary splits within the training set during cross-validation, used to tune hyperparameters and assess performance.\nTest Data:\nA held-out dataset that is never used during model tuning.\nProvides an unbiased evaluation of the final model’s performance.\nWorkflow:\n\nSplit: Divide the dataset into training and test sets.\n\nCross-Validate: Apply k‑fold CV on the training set for model tuning and selection.\nEvaluate: Use the test set for final performance assessment."
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#regularization-1",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#regularization-1",
    "title": "Lecture 9",
    "section": "Regularization",
    "text": "Regularization\n\nRegularized regression can resolve the following problems:\n\nQuasi-separation in logistic regression\nMulticolinearity in linear regression\n\ne.g., Variables \\(\\texttt{age}\\) and \\(\\texttt{years_of_workforce}\\) in linear regression of \\(\\texttt{income}\\).\n\nOverfitting\n\nThe above situations usually happen when the model is too complex (e.g., has large or many beta variables).\nWe will discuss three regularized regression methods:\n\nLasso or LASSO (least absolute shrinkage and selection operator) (L1)\nRidge (L2)\nElastic net"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#what-is-linear-regression-doing",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#what-is-linear-regression-doing",
    "title": "Lecture 9",
    "section": "What is Linear Regression Doing?",
    "text": "What is Linear Regression Doing?\n\nRegular linear regression tries to find the beta parameters \\(\\beta_0, \\beta_1, \\beta_2, \\,\\cdots\\, \\beta_{p}\\) such that\n\n\\[\nf(x_i) = b_0 + b_1 x_{1,i} + b_2 x_{2,i} + \\,\\cdots\\, +  b_p x_{p,i}\n\\] is as close as possible to \\(y_i\\) for all the training data by minimizing the sum of the squared error (SSE) between \\(y\\) and \\(f(x)\\) with observations \\(i = 1, \\cdots, N\\), where the SSE is\n\\[\n(y_1 - f(x_1))^2 + (y_2 - f(x_2))^2 + \\,\\cdots\\, + (y_N - f(x_N))^2\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#what-is-lasso-regression-doing",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#what-is-lasso-regression-doing",
    "title": "Lecture 9",
    "section": "What is Lasso Regression Doing?",
    "text": "What is Lasso Regression Doing?\n\nLasso regression tries to find the beta parameters \\(\\beta_0, \\beta_1, \\beta_2, \\,\\cdots\\, \\beta_{p}\\) and \\(\\alpha\\) such that\n\n\\[\nf(x_i) = b_0 + b_1 x_{1,i} + b_2 x_{2,i} + \\,\\cdots\\, +  b_p x_{p,i}\n\\] is as close as possible to \\(y_i\\) for all the training data by minimizing the sum of the squared error (SSE) plus the sum of the absolute value of the beta parameters multiplied by the alpha parameter:\n\\[\n\\begin{align}\n&(y_1 - f(x_1))^2 + (y_2 - f(x_2))^2 + \\,\\cdots\\, + (y_N - f(x_N))^2 \\\\\n&+ \\alpha \\times(| \\beta_1 | + |\\beta_2 | + \\,\\cdots\\, + |\\beta_{p}|)\n\\end{align}\n\\] - When \\(\\alpha = 0\\), this reduces to regular linear regression."
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#what-is-lasso-regression-doing-1",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#what-is-lasso-regression-doing-1",
    "title": "Lecture 9",
    "section": "What is Lasso Regression Doing?",
    "text": "What is Lasso Regression Doing?\n\nWhen variables are nearly collinear, lasso regression tends to drive one or more of them to zero.\nIn the regression of \\(\\text{income}\\), lasso regression might give zero credit to one of the two variables, \\(\\texttt{age}\\) and \\(\\texttt{years_of_workforce}\\).\nFor this reason, lasso regression is often used as a form of model/variable selection."
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#what-is-ridge-regression-doing",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#what-is-ridge-regression-doing",
    "title": "Lecture 9",
    "section": "What is Ridge Regression Doing?",
    "text": "What is Ridge Regression Doing?\n\nRidge regression tries to find the beta parameters \\(\\beta_0, \\beta_1, \\beta_2, \\,\\cdots\\, \\beta_{p}\\) and \\(\\alpha\\) such that\n\n\\[\nf(x_i) = b_0 + b_1 x_{1,i} + b_2 x_{2,i} + \\,\\cdots\\, +  b_p x_{p,i}\n\\] is as close as possible to \\(y_i\\) for all the training data by minimizing the sum of the squared error (SSE) plus the sum of the squared beta parameters multiplied by the alpha parameter:\n\\[\n\\begin{align}\n&(y_1 - f(x_1))^2 + (y_2 - f(x_2))^2 + \\,\\cdots\\, + (y_N - f(x_N))^2 \\\\\n&+ \\alpha \\times (\\beta_1^2 + \\beta_2^2 + \\,\\cdots\\, + \\beta_{p}^2)\n\\end{align}\n\\] - When \\(\\alpha = 0\\), this reduces to regular linear regression."
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#what-is-ridge-regression-doing-1",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#what-is-ridge-regression-doing-1",
    "title": "Lecture 9",
    "section": "What is Ridge Regression Doing?",
    "text": "What is Ridge Regression Doing?\n\nWhen variables are nearly collinear, ridge regression tends to average the collinear variables together.\nYou can think of this as “ridge regression shares the credit.”\n\nImagine that being one year older/one year longer in the workforce increases \\(\\texttt{income}\\) in the training data.\nIn this situation, ridge regression might give a half credit to each variable of \\(\\texttt{age}\\) and \\(\\texttt{years_of_workforce}\\), which adds up to the appropriate effect."
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#what-is-elastic-net-regression-doing",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#what-is-elastic-net-regression-doing",
    "title": "Lecture 9",
    "section": "What is Elastic Net Regression Doing?",
    "text": "What is Elastic Net Regression Doing?\n\nElastic net regression tries to find the beta parameters \\(\\beta_0, \\beta_1, \\beta_2, \\,\\cdots\\, \\beta_{p}\\) and \\(\\lambda\\) (L1 parameter) such that\n\n\\[\nf(x_i) = b_0 + b_1 x_{1,i} + b_2 x_{2,i} + \\,\\cdots\\, +  b_p x_{p,i}\n\\] is as close as possible to \\(y_i\\) for all the training data by minimizing the sum of the squared error (SSE) plus a linear combination of the ridge and the lasso penalties with the \\(\\lambda\\) parameter:\n\\[\n\\begin{align}\n&(y_1 - f(x_1))^2 + (y_2 - f(x_2))^2 + \\,\\cdots\\, + (y_N - f(x_N))^2 \\\\\n&+ \\lambda \\times(| \\beta_1 | + |\\beta_2 | + \\,\\cdots\\, + |\\beta_{p}|)\\\\\n&+ (1-\\lambda) \\times(\\beta_1^2 + \\beta_2^2 + \\,\\cdots\\, + \\beta_{p}^2)\n\\end{align}\n\\]\nwhere \\(0 \\leq \\lambda \\leq 1\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#choosing-between-lasso-ridge-and-elastic-net",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#choosing-between-lasso-ridge-and-elastic-net",
    "title": "Lecture 9",
    "section": "Choosing Between Lasso, Ridge, and Elastic Net",
    "text": "Choosing Between Lasso, Ridge, and Elastic Net\n\nIn some situations, such as when you have a very large number of variables, many of which are correlated to each other, the lasso may be preferred.\nIn other situations, like quasi-separability, the ridge solution may be preferred.\nWhen you are not sure which is the best approach, you can combine the two by using elastic net regression.\n\nDifferent values of \\(\\lambda\\) between 0 and 1 give different trade-offs between sharing the credit among correlated variables, and only keeping a subset of them."
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#intuition-on-different-penalties",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#intuition-on-different-penalties",
    "title": "Lecture 9",
    "section": "Intuition on Different Penalties",
    "text": "Intuition on Different Penalties\n\n\n\n\n\n\n\n\nLasso (L1) Penalty (\\(|\\beta|\\)):\n\n\nEach unit increase in \\(\\beta\\) adds a constant penalty, regardless of \\(\\beta\\) ’s size.\nDrives some coefficients exactly to zero, acting as a predictor selection mechanism.\n\n\n\n\nRidge (L2) Penalty (\\(\\beta^2\\)):\n\n\nGently penalizes small-to-moderate deviations from zero, but penalty increases quickly for large \\(\\beta\\).\nShrinks coefficients but does not set them exactly to zero."
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#intuition-on-different-penalties-1",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#intuition-on-different-penalties-1",
    "title": "Lecture 9",
    "section": "Intuition on Different Penalties",
    "text": "Intuition on Different Penalties\n\n\n\n\nLasso induces corner solutions!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#regularization-affects-the-interpretation-of-beta",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#regularization-affects-the-interpretation-of-beta",
    "title": "Lecture 9",
    "section": "Regularization Affects the Interpretation of Beta",
    "text": "Regularization Affects the Interpretation of Beta\n\n\n\n\nNo Need to Omit a Reference Category:\n\nIn standard regression, one dummy is typically omitted to avoid perfect multicollinearity.\n\nEven though including all dummies creates perfect collinearity with an intercept, regularization resolves this by penalizing the beta parameters.\n\nIn regularized regression, the penalty term handles multicollinearity by shrinking all coefficients, so you can include all dummy variables.\nEach coefficient then represents the deviation from a shared baseline (an implicit average effect).\n\nInterpretation:\n\nWith this approach, we interpret a dummy’s beta as how much that category’s association with the outcome, without worrying about the reference level (intercept)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#regularized-regression-with-cross-validation-with-scikit-learn",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#regularized-regression-with-cross-validation-with-scikit-learn",
    "title": "Lecture 9",
    "section": "Regularized Regression with Cross-Validation with scikit-learn",
    "text": "Regularized Regression with Cross-Validation with scikit-learn\n\n\n\n\n\n\n\n\\(\\alpha_{min}\\): the \\(\\alpha\\) for the model with the minimum cross-validation (CV) error"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#python-libraries-and-modules-for-regularization",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#python-libraries-and-modules-for-regularization",
    "title": "Lecture 9",
    "section": "Python Libraries and Modules for Regularization",
    "text": "Python Libraries and Modules for Regularization\nHere are the required libraries and modules for logistic regression with regularization:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.metrics import (confusion_matrix, accuracy_score, precision_score,\n                             recall_score, roc_curve, roc_auc_score)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#data-preparation-with-train_test_split-and-pd.get_dummies",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#data-preparation-with-train_test_split-and-pd.get_dummies",
    "title": "Lecture 9",
    "section": "Data Preparation with train_test_split() and pd.get_dummies()",
    "text": "Data Preparation with train_test_split() and pd.get_dummies()\ncars = pd.read_csv('https://bcdanl.github.io/data/car-data.csv')\n\n# Split into train and test (70% train, 30% test)\n# Using a fixed random state for reproducibility (seed = 24351)\ncars_train, cars_test = train_test_split(cars, test_size=0.3, random_state=24351)\n\n# Define predictors: all columns except \"rating\" and \"fail\"\npredictors = [col for col in cars.columns if col not in ['rating', 'fail']]\n\n# One-hot encode categorical predictors.\nX_train = pd.get_dummies(cars_train[predictors])\nX_test = pd.get_dummies(cars_test[predictors])\n# Ensure that the test set has the same dummy columns as the training set\nX_test = X_test.reindex(columns=X_train.columns)\n\n# Outcome variable\ny_train = cars_train['fail'].astype(int)\ny_test = cars_test['fail'].astype(int)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#lasso---logisticregressioncvpenaltyl1-solversaga",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#lasso---logisticregressioncvpenaltyl1-solversaga",
    "title": "Lecture 9",
    "section": "Lasso - LogisticRegressionCV(penalty='l1', solver='saga')",
    "text": "Lasso - LogisticRegressionCV(penalty='l1', solver='saga')\nlasso_cv = LogisticRegressionCV(Cs=100, cv=5, \n  penalty='l1', solver='saga', max_iter=1000, scoring='neg_log_loss')\nlasso_cv.fit(X_train, y_train)\n\nCs=100: Tries 100 different values of regularization strength (1/C)\ncv=5: Uses 5-fold cross-validation\npenalty='l1': Applies L1 regularization (Lasso)\nsolver='saga' Supports L1 regularization\nmax_iter=1000: Sets the maximum number of iterations for solver to converge\nscoring='neg_log_loss': Uses deviance as the CV performance metric\n.fit(X_train, y_train): Fits the model; Selects the best \\(\\alpha = \\frac{1}{C}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#lasso-logistic-regression-l1---full-code",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#lasso-logistic-regression-l1---full-code",
    "title": "Lecture 9",
    "section": "Lasso Logistic Regression (L1) - Full Code",
    "text": "Lasso Logistic Regression (L1) - Full Code\n# Note: solver='saga' supports L1 regularization.\nlasso_cv = LogisticRegressionCV(\n    Cs=100, cv=5, penalty='l1', solver='saga', max_iter=1000, scoring='neg_log_loss'\n)\nlasso_cv.fit(X_train, y_train)\n\nintercept = float(lasso_cv.intercept_)\ncoef_lasso = pd.DataFrame({\n    'predictor': list(X_train.columns),\n    'coefficient': list(lasso_cv.coef_[0])\n})\n\nprint(\"Lasso Regression Coefficients:\")\nprint(coef_lasso)\n\n# Force an order for the y-axis (using the feature names as they appear in coef_lasso)\norder = coef_lasso['predictor'].tolist()\n\nplt.figure(figsize=(8,6))\nax = sns.pointplot(x=\"coefficient\", y=\"predictor\", data=coef_lasso, order=order, join=False)\nplt.title(\"Coefficients of Lasso Logistic Regression Model\")\nplt.xlabel(\"Coefficient value\")\nplt.ylabel(\"Predictor\")\n\n# Draw horizontal lines from 0 to each coefficient.\nfor _, row in coef_lasso.iterrows():\n    # Get the y-axis position from the order list.\n    y_pos = order.index(row['predictor'])\n    plt.hlines(y=y_pos, xmin=0, xmax=row['coefficient'], color='gray', linestyle='--')\n\n# Draw a vertical line at 0.\nplt.axvline(0, color='black', linestyle='--')\n\nplt.show()\n\n# Prediction and evaluation for lasso model\ny_pred_prob_lasso = lasso_cv.predict_proba(X_test)[:, 1]\ny_pred_lasso = (y_pred_prob_lasso &gt; 0.5).astype(int)\nctab_lasso = confusion_matrix(y_test, y_pred_lasso)\naccuracy_lasso = accuracy_score(y_test, y_pred_lasso)\nprecision_lasso = precision_score(y_test, y_pred_lasso)\nrecall_lasso = recall_score(y_test, y_pred_lasso)\nauc_lasso = roc_auc_score(y_test, y_pred_prob_lasso)\n\nprint(\"Confusion Matrix (Lasso):\\n\", ctab_lasso)\nprint(\"Lasso Accuracy:\", accuracy_lasso)\nprint(\"Lasso Precision:\", precision_lasso)\nprint(\"Lasso Recall:\", recall_lasso)\n\n\n# Plot ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_lasso)\nplt.figure(figsize=(8,6))\nplt.plot(fpr, tpr, label=f'Lasso (AUC = {auc_ridge:.2f})')\nplt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Lasso Logistic Regression Model')\nplt.legend(loc='best')\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#ridge-l2---logisticregressioncvpenaltyl2",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#ridge-l2---logisticregressioncvpenaltyl2",
    "title": "Lecture 9",
    "section": "Ridge (L2) - LogisticRegressionCV(penalty='l2')",
    "text": "Ridge (L2) - LogisticRegressionCV(penalty='l2')\nridge_cv = LogisticRegressionCV(Cs=100, cv=5, \n  penalty='l2', solver='lbfgs', max_iter=1000, scoring='neg_log_loss'\n)\nridge_cv.fit(X_train, y_train)\n\npenalty='l2': Applies L2 regularization (Ridge)\nsolver='lbfgs' Supports L2 regularization (optional)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#ridge-logistic-regression-l2---full-code",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#ridge-logistic-regression-l2---full-code",
    "title": "Lecture 9",
    "section": "Ridge Logistic Regression (L2) - Full Code",
    "text": "Ridge Logistic Regression (L2) - Full Code\n# LogisticRegressionCV automatically selects the best regularization strength.\nridge_cv = LogisticRegressionCV(\n    Cs=100, cv=5, penalty='l2', solver='lbfgs', max_iter=1000, scoring='neg_log_loss'\n)\nridge_cv.fit(X_train, y_train)\n\nprint(\"Ridge Regression - Best C (inverse of regularization strength):\", ridge_cv.C_[0])\nintercept = float(ridge_cv.intercept_)\ncoef_ridge = pd.DataFrame({\n    'predictor': list(X_train.columns),\n    'coefficient': list(ridge_cv.coef_[0])\n})\nprint(\"Ridge Regression Coefficients:\")\nprint(coef_ridge)\n\n# Force an order for the y-axis (using the feature names as they appear in coef_ridge)\norder = coef_ridge['predictor'].tolist()\n\nplt.figure(figsize=(8,6))\nax = sns.pointplot(x=\"coefficient\", y=\"predictor\", data=coef_ridge, order=order, join=False)\nplt.title(\"Coefficients of Ridge Logistic Regression Model\")\nplt.xlabel(\"Coefficient value\")\nplt.ylabel(\"Predictor\")\n\n# Draw horizontal lines from 0 to each coefficient.\nfor _, row in coef_ridge.iterrows():\n    # Get the y-axis position from the order list.\n    y_pos = order.index(row['predictor'])\n    plt.hlines(y=y_pos, xmin=0, xmax=row['coefficient'], color='gray', linestyle='--')\n\n# Draw a vertical line at 0.\nplt.axvline(0, color='black', linestyle='--')\nplt.show()\n\n# Prediction and evaluation for ridge model\ny_pred_prob_ridge = ridge_cv.predict_proba(X_test)[:, 1]\ny_pred_ridge = (y_pred_prob_ridge &gt; 0.5).astype(int)\nctab_ridge = confusion_matrix(y_test, y_pred_ridge)\naccuracy_ridge = accuracy_score(y_test, y_pred_ridge)\nprecision_ridge = precision_score(y_test, y_pred_ridge)\nrecall_ridge = recall_score(y_test, y_pred_ridge)\nauc_ridge = roc_auc_score(y_test, y_pred_prob_ridge)\n\nprint(\"Confusion Matrix (Ridge):\\n\", ctab_ridge)\nprint(\"Ridge Accuracy:\", accuracy_ridge)\nprint(\"Ridge Precision:\", precision_ridge)\nprint(\"Ridge Recall:\", recall_ridge)\n\n\n# Plot ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_ridge)\nplt.figure(figsize=(8,6))\nplt.plot(fpr, tpr, label=f'Ridge (AUC = {auc_ridge:.2f})')\nplt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Ridge Logistic Regression Model')\nplt.legend(loc='best')\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#elastic-net---logisticregressioncvpenaltyl2",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#elastic-net---logisticregressioncvpenaltyl2",
    "title": "Lecture 9",
    "section": "Elastic Net - LogisticRegressionCV(penalty='l2')",
    "text": "Elastic Net - LogisticRegressionCV(penalty='l2')\nenet_cv = LogisticRegressionCV(\n    Cs=10, cv=5, penalty='elasticnet', solver='saga',\n    l1_ratios=[0.5, 0.7, 0.9], max_iter=1000, scoring='neg_log_loss'\n)\nenet_cv.fit(X_train, y_train)\n\nCs=10: Tries 10 different values of regularization strength (1/C)\ncv=5: Uses 5-fold cross-validation\npenalty='elasticnet': Applies Elastic Net regularization\nl1_ratios = [0.5, 0.7, 0.9] In this model, we try 0.5, 0.7, 0.9\n\nl1_ratio = 1 → Lasso\n\nl1_ratio = 0 → Ridge"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#elastic-net-logistic-regression---full-code",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#elastic-net-logistic-regression---full-code",
    "title": "Lecture 9",
    "section": "Elastic Net Logistic Regression - Full Code",
    "text": "Elastic Net Logistic Regression - Full Code\n# LogisticRegressionCV supports elastic net penalty with solver 'saga'.\n# l1_ratio specifies the mix between L1 and L2 (0 = ridge, 1 = lasso).\nenet_cv = LogisticRegressionCV(\n    Cs=10, cv=5, penalty='elasticnet', solver='saga',\n    l1_ratios=[0.5, 0.7, 0.9], max_iter=1000, scoring='neg_log_loss'\n)\nenet_cv.fit(X_train, y_train)\n\nprint(\"Elastic Net Regression - Best C:\", enet_cv.C_[0])\nprint(\"Elastic Net Regression - Best l1 ratio:\", enet_cv.l1_ratio_[0])\n\nintercept = float(enet_cv.intercept_)\ncoef_enet = pd.DataFrame({\n    'predictor': list(X_train.columns),\n    'coefficient': list(enet_cv.coef_[0])\n})\nprint(\"Elastic Net Regression Coefficients:\")\nprint(coef_enet)\n\n\n# Force an order for the y-axis (using the feature names as they appear in coef_lasso)\norder = coef_enet['predictor'].tolist()\n\nplt.figure(figsize=(8,6))\nax = sns.pointplot(x=\"coefficient\", y=\"predictor\", data=coef_enet, order=order, join=False)\nplt.title(\"Coefficients of Elastic Net Logistic Regression Model\")\nplt.xlabel(\"Coefficient value\")\nplt.ylabel(\"Predictor\")\n\n# Draw horizontal lines from 0 to each coefficient.\nfor _, row in coef_enet.iterrows():\n    # Get the y-axis position from the order list.\n    y_pos = order.index(row['predictor'])\n    plt.hlines(y=y_pos, xmin=0, xmax=row['coefficient'], color='gray', linestyle='--')\n\n# Draw a vertical line at 0.\nplt.axvline(0, color='black', linestyle='--')\n\nplt.show()\n\n# Prediction and evaluation for elastic net model\ny_pred_prob_enet = enet_cv.predict_proba(X_test)[:, 1]\ny_pred_enet = (y_pred_prob_enet &gt; 0.5).astype(int)\nctab_enet = confusion_matrix(y_test, y_pred_enet)\naccuracy_enet = accuracy_score(y_test, y_pred_enet)\nprecision_enet = precision_score(y_test, y_pred_enet)\nrecall_enet = recall_score(y_test, y_pred_enet)\n\nprint(\"Confusion Matrix (Elastic Net):\\n\", ctab_enet)\nprint(\"Elastic Net Accuracy:\", accuracy_enet)\nprint(\"Elastic Net Precision:\", precision_enet)\nprint(\"Elastic Net Recall:\", recall_enet)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#lasso-linear-regression",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#lasso-linear-regression",
    "title": "Lecture 9",
    "section": "Lasso Linear Regression",
    "text": "Lasso Linear Regression\n\nThe browser dataset contains web browsing logs for 10,000 households.\nThe browser dataset include a year’s worth of their browser logs for the 1,000 most heavily trafficked websites\nEach browser in the sample spent at least $1 online in the same year.\n\n\\[\n\\log(\\text{spend}_{i}) = \\beta_0 + \\beta_1 X_{1,i} +\\,\\cdots\\,+ \\beta_{1000} X_{1000,i} + \\epsilon_i\n\\] - \\(\\text{spend}_{i}\\): household \\(i\\)’s amount of dollars spent on online shopping - \\(X_{p, i}\\) household \\(i\\)’s percentage of visiting the \\(p\\) website"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#lasso-linear-regression---libraries-and-modules",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#lasso-linear-regression---libraries-and-modules",
    "title": "Lecture 9",
    "section": "Lasso Linear Regression - Libraries and Modules",
    "text": "Lasso Linear Regression - Libraries and Modules\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()  # Enabling an interactive DataFrame display\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import scale   # zero mean & one s.d.\nfrom sklearn.linear_model import LassoCV, lasso_path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#lasso-linear-regression---data-prep",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#lasso-linear-regression---data-prep",
    "title": "Lecture 9",
    "section": "Lasso Linear Regression - Data Prep",
    "text": "Lasso Linear Regression - Data Prep\ndf = pd.read_csv(\"https://bcdanl.github.io/data/browser-online-shopping.zip\")\n\nX = df.drop('spend', axis = 1)\ny = df['spend']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX_train_np = X_train.values\nX_test_np = X_test.values\ny_train_np = y_train.values\ny_test_np = y_test.values"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#lasso-linear-regression---fitting-the-model",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#lasso-linear-regression---fitting-the-model",
    "title": "Lecture 9",
    "section": "Lasso Linear Regression - Fitting the Model",
    "text": "Lasso Linear Regression - Fitting the Model\n# LassoCV with a range of alpha values\nlasso_cv = LassoCV(n_alphas = 100, # default is 100\n                   alphas = None, # alphas=None automatically generate 100 candidate alpha values\n                   cv = 5, \n                   random_state=42,\n                   max_iter=100000)\nlasso_cv.fit(X_train, np.log(y_train))"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#lasso-linear-regression---results",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#lasso-linear-regression---results",
    "title": "Lecture 9",
    "section": "Lasso Linear Regression - Results",
    "text": "Lasso Linear Regression - Results\n# Best alpha\nprint(\"LassoCV - Best alpha:\", lasso_cv.alpha_)\n\n# Create a DataFrame including the intercept and the coefficients:\ncoef_lasso = pd.DataFrame({\n    'predictor': list(X_train.columns),\n    'coefficient':  list(lasso_cv.coef_),\n    'exp_coefficient': np.exp(  list(lasso_cv.coef_) )\n})\n\n# Evaluate\ny_pred_lasso = lasso_cv.predict(X_test)\nmse_lasso = mean_squared_error(y_test, y_pred_lasso)\nprint(\"LassoCV - MSE:\", mse_lasso)\n\ncoef_lasso = coef_lasso.query('coefficient != 0')\ncoef_lasso.shape[0]\ncoef_lasso.sort_values('coefficient', ascending = False)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#lasso-path---cv-errors-as-as-a-function-of-alpha",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#lasso-path---cv-errors-as-as-a-function-of-alpha",
    "title": "Lecture 9",
    "section": "Lasso Path - CV Errors as as a Function of Alpha",
    "text": "Lasso Path - CV Errors as as a Function of Alpha\n# Compute the mean and standard deviation of the CV errors for each alpha.\nmean_cv_errors = np.mean(lasso_cv.mse_path_, axis=1)\nstd_cv_errors = np.std(lasso_cv.mse_path_, axis=1)\n\nplt.figure(figsize=(8, 6))\nplt.errorbar(lasso_cv.alphas_, mean_cv_errors, yerr=std_cv_errors, marker='o', linestyle='-', capsize=5)\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Mean CV Error (MSE)')\nplt.title('Cross-Validation Error vs. Alpha')\nplt.axvline(x=lasso_cv.alpha_, color='red', linestyle='--', label='Best alpha')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#lasso-path---beta-estimates-as-a-function-of-alpha",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#lasso-path---beta-estimates-as-a-function-of-alpha",
    "title": "Lecture 9",
    "section": "Lasso Path - Beta Estimates as a Function of Alpha",
    "text": "Lasso Path - Beta Estimates as a Function of Alpha\n# Compute the lasso path. Note: we use np.log(y_train) because that's what you used in LassoCV.\nalphas, coefs, _ = lasso_path(X_train, np.log(y_train), alphas=lasso_cv.alphas_, max_iter=100000)\n\nplt.figure(figsize=(8, 6))\n# Iterate over each predictor and plot its coefficient path.\nfor i, col in enumerate(X_train.columns):\n    plt.plot(alphas, coefs[i, :], label=col)\n\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Coefficient value')\nplt.title('Lasso Coefficient Paths')\nplt.axvline(x=lasso_cv.alpha_, color='red', linestyle='--', label='Best alpha')\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#lasso-path---the-number-of-nonzero-betas-as-a-function-of-alpha",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#lasso-path---the-number-of-nonzero-betas-as-a-function-of-alpha",
    "title": "Lecture 9",
    "section": "Lasso Path - The Number of Nonzero Betas as a Function of Alpha",
    "text": "Lasso Path - The Number of Nonzero Betas as a Function of Alpha\n# Compute the coefficient path over the alpha grid that LassoCV used\nalphas, coefs, _ = lasso_path(X_train, np.log(y_train),\n                              alphas=lasso_cv.alphas_,\n                              max_iter=100000)\n\n# Count nonzero coefficients for each alpha (coefs shape: (n_features, n_alphas))\nnonzero_counts = np.sum(coefs != 0, axis=0)\n\n# Plot the number of nonzero coefficients versus alpha\nplt.figure(figsize=(8,6))\nplt.plot(alphas, nonzero_counts, marker='o', linestyle='-')\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Number of nonzero coefficients')\nplt.title('Nonzero Coefficients vs. Alpha')\n#plt.gca().invert_xaxis()  # Lower alphas (less regularization) on the right\nplt.axvline(x=lasso_cv.alpha_, color='red', linestyle='--', label='Best alpha')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#background-and-motivation",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#background-and-motivation",
    "title": "Lecture 9",
    "section": "Background and Motivation",
    "text": "Background and Motivation\n\nThe player “plus-minus” (PM) is a common hockey performance metric.\nThe classic PM is a function of goals scored while that player is on the ice:\n\nthe number of goals for his team minus the number against.\n\nThe limits of this approach are obvious: there is no accounting for teammates or opponents.\nIn hockey, where players tend to be grouped together on “lines” and coaches will “line match” against opponents, a player’s PM can be artificially inflated or deflated by the play of his opponents and peers."
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#data",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#data",
    "title": "Lecture 9",
    "section": "Data",
    "text": "Data\n\nThe data comprise of play-by-play NHL game data for regular and playoff games during 11 seasons of 2002-2003 through 2013-2014.\nThere were p = 2,439 players involved in n = 69,449 goals.\nThe data contains information that indicates seasons, home & away teams, team configuration such as 5 on 4 powerplay, and which players are on & off the ice when a goal is made, etc.\nUnfortunately, Python scikit-learn is not optimized to handle all season’s data.\n\nWhat we can do with Python scikit-learn is handling one season each.\nI highly recommend R’s glmnet for regression with cross-validation."
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#data-1",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#data-1",
    "title": "Lecture 9",
    "section": "Data",
    "text": "Data\nnhl = pd.read_csv('https://bcdanl.github.io/data/NHL_data_2002_2003.csv')\n\nhomegoal: an indicator (0 or 1) for the home team scoring\nplayer_name: entries for who was on the ice for each goal\nteam: indicators for each team\nconfig: Special teams info. E.g., S5v4 is a 5 on 4 powerplay\nThe value of config, team, and player_name are:\n\n1 if it is for the home-team\n-1 for the away team\n0 otherwise"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#model",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#model",
    "title": "Lecture 9",
    "section": "Model",
    "text": "Model\n\nConsider constructing a binary response for every goal, equal to 1 for home-team goals and 0 for away-team goals:\n\n\\[\n\\begin{align}\n\\log\\left(\\frac{\\text{Prob}(\\text{home-goal}_{i})}{\\text{Prob}(\\text{away-goal}_{i})}\\right) &= \\beta_0 + \\sum_{j=1}^{J} \\beta_{\\text{team}_{j}}\\text{team}_{j, i} + \\sum_{k=1}^{K} \\beta_{\\text{config}_{k}}\\text{config}_{k, i}  \\\\\n&\\qquad + \\sum_{m=1}^{M} \\beta_{\\text{home-player}_{m}}\\text{home-player}_{m, i}\\\\\n&\\qquad - \\sum_{n=1}^{N} \\beta_{\\text{away-player}_{n}}\\text{away-player}_{n, i}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#home-ice-advantage",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#home-ice-advantage",
    "title": "Lecture 9",
    "section": "Home-ice Advantage",
    "text": "Home-ice Advantage\n\n\n\n\n\n\n\nHow can we estimate the home-team effect?\n\nnp.exp(lasso_cv.intercept_)\n\nThis is the effect on odds that a goal is home rather than away, regardless of any information about what teams are playing or who is on ice."
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#players-impact",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#players-impact",
    "title": "Lecture 9",
    "section": "Player’s Impact",
    "text": "Player’s Impact\n\n\n\n\n\n\n\nHow can we estimate the player effect?\n\nnp.exp(lasso_cv.coef_[0])\n\nWhenever a goal is scored in the season 2002-2003, …\n\nColorado’s odds of having scored (rather than having been scored on) increase by 160% if Peter Forsberg is on the ice."
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#traditional-plusminus-vs.-expected-plusminus",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#traditional-plusminus-vs.-expected-plusminus",
    "title": "Lecture 9",
    "section": "Traditional Plus‑Minus vs. Expected Plus‑Minus",
    "text": "Traditional Plus‑Minus vs. Expected Plus‑Minus\n\nTraditional Plus‑Minus (PM)\n\nDefinition: Sum of on-ice contributions; +1 for a goal scored for the player’s team and -1 for a goal against.\nLimitations: Can be noisy due to team context, limited ice time, or random variation.\n\nExpected Plus‑Minus (ppm)\n\nDefinition: A model-based estimate of a player’s impact.\nCalculation:\n\nConvert a player’s effect (\\(\\beta\\)) to a probability: \\(p = \\frac{e^{\\beta}}{1 + e^{\\beta}}\\)\nCompute expected PM as: \\(\\text{ppm} = ng \\times p − ng \\times (1−p )\\)\n\\(ng\\): the total number of goals the player was on the ice.\n\nBenefits: Smooths out noise and adjusts for context."
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#traditional-plusminus-vs.-expected-plusminus-1",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#traditional-plusminus-vs.-expected-plusminus-1",
    "title": "Lecture 9",
    "section": "Traditional Plus‑Minus vs. Expected Plus‑Minus",
    "text": "Traditional Plus‑Minus vs. Expected Plus‑Minus\n\nObserved vs. Modeled:\n\nPM is a raw, observed measure.\n\nExpected PM leverages model estimates to predict performance.\n\nVariability:\n\nPM may fluctuate due to external factors.\n\nExpected PM attempts to isolate a player’s true impact.\n\nApplications:\n\nExpected PM can help identify under- or over-performing players and guide strategic decisions."
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#scaling",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#scaling",
    "title": "Lecture 9",
    "section": "Scaling?",
    "text": "Scaling?\n\nWe do not want to standardize variables here.\n\nThe penalty with standardization is \\(\\alpha \\times SD(X_{player})\\)\nPlayers with small SD are those who play little (almost all zeros).\nPlayers with large SD are those who play a lot.\nStandardization would up-weight the influence of players who rarely play relative to those who have a lot of ice time.\n\nRe-do everything with\n\nfrom sklearn.preprocessing import scale # zero mean & one s.d.\nXpred = scale(nhl_train)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#omitted-variable-bias-1",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#omitted-variable-bias-1",
    "title": "Lecture 9",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\n\nOmitted variable bias (OVB): bias in the model because of omitting an important predictor that is correlated with existing predictor(s).\nLet’s use an orange juice (OJ) example to demonstrate the OVB.\n\nOJ price elasticity estimates vary with models, whether or not taking into account brand or ad_status"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#short--and-long--regressions",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#short--and-long--regressions",
    "title": "Lecture 9",
    "section": "Short- and Long- Regressions",
    "text": "Short- and Long- Regressions\n\nOVB is the difference in beta estimates between short- and long-form regressions.\nShort regression: The regression model with less predictors\n\n\n\n\n\\[\n\\begin{align}\n\\log(\\text{sales}_i) = \\beta_0 + \\beta_1\\log(\\text{price}_i) + \\epsilon_i\n\\end{align}\n\\]\n\nLong regression: The regression model that adds additional predictor(s) to the short one.\n\n\n\n\n\\[\n\\begin{align}\n\\log(\\text{sales}_i) =& \\beta_0 + \\beta_{1}\\log(\\text{price}_i) \\\\\n&+ \\beta_{2}\\text{minute.maid}_i + \\beta_{3}\\text{tropicana}_i + \\epsilon_i\n\\end{align}\n\\] - OVB for \\(\\beta_1\\) is:\n\n\n\n\\[\n\\text{OVB} = \\widehat{\\beta_{1}^{short}} - \\widehat{\\beta_{1}^{long}}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#ovb-formula",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#ovb-formula",
    "title": "Lecture 9",
    "section": "OVB formula",
    "text": "OVB formula\n\nConsider the following short- and long- regressions:\n\nShort: \\(Y_i = \\beta_0 + \\beta_{1}^{short}X_1 + \\epsilon_{short}\\)\n\nLong: \\(Y_i = \\beta_0 + \\beta_{1}^{long}X_1 +\\beta_{2}X_2 + \\epsilon_{long}\\)\n\nError in short form can be represented as: \\[\n{\\epsilon_{short}} = \\beta_{2}X_2 + \\epsilon_{long}\n\\]\nIf variable \\(X_1\\) is correlated with \\(X_2\\), the following assumptions are violated in the short regression model:\n\nErrors are not correlated with predictors.\nErrors have a mean value of 0."
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#how-does-an-ovb-happen-in-regression",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#how-does-an-ovb-happen-in-regression",
    "title": "Lecture 9",
    "section": "How does an OVB happen in regression?",
    "text": "How does an OVB happen in regression?\n\nIn the first stage, consider the relationship between price and brand:\n\n\\[\n\\log(\\text{price}) = \\beta_0 + \\beta_1\\text{minute_maid} + \\beta_2\\text{tropicana} + \\epsilon_{1st}\n\\]\n\nThen, calculate the residual: \\[\n\\widehat{\\epsilon_{1st}} = \\log(\\text{price}) - \\widehat{\\log(\\text{price})}\n\\]\nThe residual represents the log of OJ price after its correlation with brand has been removed!\nIn the second stage, regress \\(\\log(\\text{sales})\\) on residual \\(\\widehat{\\epsilon_{1st}}\\):\n\n\\[\n\\log(\\text{sales}) = \\beta_0 + \\beta_1\\widehat{\\epsilon_{1st}}  + \\epsilon_{2nd}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-09-2025-0326.html#regression-sensitivity-analysis",
    "href": "danl-lec/danl-320-lec-09-2025-0326.html#regression-sensitivity-analysis",
    "title": "Lecture 9",
    "section": "Regression Sensitivity Analysis",
    "text": "Regression Sensitivity Analysis\n\nRegression finds the coefficients on the part of each predictor that is independent from the other predictors.\nWhat can we do to deal with OVB problems?\n\nBecause we can never be sure whether a given set of controls is enough to eliminate OVB, it’s important to ask how sensitive regression results are to changes in the list of controls."
  },
  {
    "objectID": "posts/nba/nba.html#salary-distribution-among-teams",
    "href": "posts/nba/nba.html#salary-distribution-among-teams",
    "title": "NBA",
    "section": "Salary Distribution Among Teams",
    "text": "Salary Distribution Among Teams\nLet’s start with the salary distribution among teams using seaborn for visualization. ​​\n\n\nCode\n\n# Handle missing values in 'Salary' by replacing them with the median salary\nmedian_salary = nba['Salary'].median()\nnba['Salary'].fillna(median_salary, inplace=True)\n\n\n\n\nCode\n# Set the aesthetic style of the plots\nsns.set_style(\"whitegrid\")\n\n# Calculate total salary by team\nteam_salary = (\n    nba\n    .groupby('Team')['Salary']\n    .sum()\n    .reset_index()\n    .sort_values(by='Salary', ascending=False)\n)\n\n# Plot total salary by team\nplt.figure(figsize=(10, 8))\nsns.barplot(data = team_salary,\n            x = 'Salary', y = 'Team',\n            palette = 'coolwarm')\nplt.title('Total Salary Distribution Among NBA Teams')\nplt.xlabel('Total Salary')\nplt.ylabel('Team')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe visualization above displays the total salary distribution among NBA teams, with teams sorted by their total salary expenditure. This bar plot reveals which teams are the biggest spenders on player salaries and which are more conservative. The color gradient provides a visual cue to easily distinguish between the higher and lower spending teams. Portland Trail Blazers spent most in their players’ salary, followed by Golden State Warriors and Philadelphia 76ers."
  },
  {
    "objectID": "posts/nba/nba.html#player-age-distribution",
    "href": "posts/nba/nba.html#player-age-distribution",
    "title": "NBA",
    "section": "Player Age Distribution",
    "text": "Player Age Distribution\nNext, let’s explore the Player Age Distribution across the NBA. We’ll create a histogram to visualize how player ages are distributed, which will help us understand if the league trends younger, older, or has a balanced age mix. ​​\n\n\nCode\n# Convert 'Birthday' column to datetime format\nfrom dateutil import parser\nnba['Birthday'] = nba['Birthday'].apply(lambda x: parser.parse(x))\n\n# Now, let's calculate the age of each player\nnba['Age'] = (datetime.now() - nba['Birthday']).dt.days // 365\n\n# Plot the age distribution of NBA players\nplt.figure(figsize=(10, 6))\nsns.histplot(nba['Age'],\n             bins = 15,\n             kde = True,\n             color = 'skyblue')\nplt.title('Age Distribution of NBA Players')\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe histogram above shows the age distribution of NBA players, with a kernel density estimate (KDE) overlay to indicate the distribution shape. The majority of players fall within a certain age range from 25 to 35, illustrating the league’s age dynamics. The plot helps identify the common ages for NBA players and whether there are significant numbers of very young or older players."
  },
  {
    "objectID": "posts/nba/nba.html#position-wise-salary-insights",
    "href": "posts/nba/nba.html#position-wise-salary-insights",
    "title": "NBA",
    "section": "Position-wise Salary Insights",
    "text": "Position-wise Salary Insights\nMoving on to Position-wise Salary Insights, we’ll examine how average salaries differ across player positions. This analysis could reveal which positions are typically higher-paid, potentially reflecting their value on the basketball court. Let’s create a box plot to visualize the salary distribution for each position. ​​\n\n\nCode\n# Plot salary distribution by player position\nplt.figure(figsize=(10, 6))\nsns.boxplot(data = nba,\n            x = 'Position', y = 'Salary',\n            palette = 'Set2')\nplt.title('Salary Distribution by Position')\nplt.xlabel('Position')\nplt.ylabel('Salary')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe box plot above illustrates the salary distribution by player position, showcasing the variation in salaries among different positions within the NBA. This visualization helps us understand which positions tend to have higher median salaries and the spread of salaries within each position, including outliers that represent exceptionally high or low salaries. While the positions of C and PG have the widest interquantiles of salaries, the positions of FC, F, G, and GF have the narrowest interquantiles of them."
  },
  {
    "objectID": "posts/nba/nba.html#top-10-highest-paid-players",
    "href": "posts/nba/nba.html#top-10-highest-paid-players",
    "title": "NBA",
    "section": "Top 10 Highest Paid Players",
    "text": "Top 10 Highest Paid Players\nLastly, we’ll identify the Top 10 Highest Paid Players in the NBA. This analysis highlights the star earners of the league, providing insights into which players command the highest salaries and potentially why. Let’s extract and visualize this information. ​​\n\n\nCode\n# Identify the top 10 highest paid players\ntop_10_salaries = nba.sort_values(by='Salary', ascending=False).head(10)\n\n# Plot the top 10 highest paid players\nplt.figure(figsize=(12, 8))\nsns.barplot(data = top_10_salaries,\n            x = 'Salary', y = 'Name',\n            palette = 'viridis')\nplt.title('Top 10 Highest Paid NBA Players')\nplt.xlabel('Salary')\nplt.ylabel('Player')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe bar chart above reveals the top 10 highest-paid NBA players, showcasing those who stand at the pinnacle of the league in terms of salary. Stephen Curry is the highest-paid NBA player, followed by Russel Westbrook and Chris Paul. This visualization not only highlights the star players who command the highest salaries but also may reflect their marketability, performance, and contribution to their respective teams."
  },
  {
    "objectID": "posts/py-basic/blog-python-basics.html",
    "href": "posts/py-basic/blog-python-basics.html",
    "title": "Python Basics",
    "section": "",
    "text": "Python is a high-level, interpreted programming language. This is a simple Python code:\n\n\nCode\nprint('Hello, World!')\n\n\n\n\n\nIn Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\n\nCode\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n\n10.5\n\n\n\n\n\nPython supports the usual logical conditions from mathematics:\n\n\nCode\n# Equals: a == b\n# Not Equals: a != b\n# Less than: a &lt; b\n# Less than or equal to: a &lt;= b\n# Greater than: a &gt; b\n# Greater than or equal to: a &gt;= b\n\n\nThese conditions can be used in several ways, most commonly in ‘if statements’ and loops.\n\n\nCode\n# if statement:\nif 5 &gt; 2:\n    print('Five is greater than two!')\n\n\n\n\n\nA function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\n\nCode\n# Defining a function:\ndef my_function():\n    print('Hello from a function')\n\n# Calling a function:\nmy_function()\n\n\n\n\n\nA list is a collection which is ordered and changeable.\nA dictionary is a collection which is unordered, changeable and indexed.\n\n\nCode\n# List example:\nmy_list = ['apple', 'banana', 'cherry']\n\n# Dictionary example:\nmy_dict = {'name': 'John', 'age': 36}"
  },
  {
    "objectID": "posts/py-basic/blog-python-basics.html#what-is-python",
    "href": "posts/py-basic/blog-python-basics.html#what-is-python",
    "title": "Python Basics",
    "section": "",
    "text": "Python is a high-level, interpreted programming language. This is a simple Python code:\n\n\nCode\nprint('Hello, World!')"
  },
  {
    "objectID": "posts/py-basic/blog-python-basics.html#variables-and-data-types",
    "href": "posts/py-basic/blog-python-basics.html#variables-and-data-types",
    "title": "Python Basics",
    "section": "",
    "text": "In Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\n\nCode\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n\n10.5"
  },
  {
    "objectID": "posts/py-basic/blog-python-basics.html#control-structures",
    "href": "posts/py-basic/blog-python-basics.html#control-structures",
    "title": "Python Basics",
    "section": "",
    "text": "Python supports the usual logical conditions from mathematics:\n\n\nCode\n# Equals: a == b\n# Not Equals: a != b\n# Less than: a &lt; b\n# Less than or equal to: a &lt;= b\n# Greater than: a &gt; b\n# Greater than or equal to: a &gt;= b\n\n\nThese conditions can be used in several ways, most commonly in ‘if statements’ and loops.\n\n\nCode\n# if statement:\nif 5 &gt; 2:\n    print('Five is greater than two!')"
  },
  {
    "objectID": "posts/py-basic/blog-python-basics.html#functions",
    "href": "posts/py-basic/blog-python-basics.html#functions",
    "title": "Python Basics",
    "section": "",
    "text": "A function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\n\nCode\n# Defining a function:\ndef my_function():\n    print('Hello from a function')\n\n# Calling a function:\nmy_function()"
  },
  {
    "objectID": "posts/py-basic/blog-python-basics.html#lists-and-dictionaries",
    "href": "posts/py-basic/blog-python-basics.html#lists-and-dictionaries",
    "title": "Python Basics",
    "section": "",
    "text": "A list is a collection which is ordered and changeable.\nA dictionary is a collection which is unordered, changeable and indexed.\n\n\nCode\n# List example:\nmy_list = ['apple', 'banana', 'cherry']\n\n# Dictionary example:\nmy_dict = {'name': 'John', 'age': 36}"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html",
    "href": "danl-cw/danl-320-cw-09.html",
    "title": "Classwork 9",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n\n\n\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                ci_upper,\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: Exp(95% CI Lower), 8: 95% CI Upper, 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"Exp(95% CI Lower)\", \"95% CI Upper\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))\n\n\n\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\n\ndef add_interaction_terms(var_list1, var_list2, var_list3=None):\n    \"\"\"\n    Creates interaction term columns in the global DataFrames dtrain and dtest.\n\n    For two sets of variable names (which may represent categorical (dummy) or continuous variables),\n    this function creates two-way interactions by multiplying each variable in var_list1 with each\n    variable in var_list2.\n\n    Optionally, if a third list of variable names (var_list3) is provided, the function also creates\n    three-way interactions among each variable in var_list1, each variable in var_list2, and each variable\n    in var_list3.\n\n    Parameters:\n        var_list1 (list): List of column names for the first set of variables.\n        var_list2 (list): List of column names for the second set of variables.\n        var_list3 (list, optional): List of column names for the third set of variables for three-way interactions.\n\n    Returns:\n        A flat list of new interaction column names.\n    \"\"\"\n    global dtrain, dtest\n\n    interaction_cols = []\n\n    # Create two-way interactions between var_list1 and var_list2.\n    for var1 in var_list1:\n        for var2 in var_list2:\n            col_name = f\"{var1}_*_{var2}\"\n            dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list1 and var_list3.\n    if var_list3 is not None:\n      for var1 in var_list1:\n          for var3 in var_list3:\n              col_name = f\"{var1}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list2 and var_list3.\n    if var_list3 is not None:\n      for var2 in var_list2:\n          for var3 in var_list3:\n              col_name = f\"{var2}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # If a third list is provided, create three-way interactions.\n    if var_list3 is not None:\n        for var1 in var_list1:\n            for var2 in var_list2:\n                for var3 in var_list3:\n                    col_name = f\"{var1}_*_{var2}_*_{var3}\"\n                    dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    interaction_cols.append(col_name)\n\n    return interaction_cols\n\n # Example\n # interaction_cols_brand_price = add_interaction_terms(dummy_cols_brand, ['log_price'])\n # interaction_cols_brand_ad_price = add_interaction_terms(dummy_cols_brand, dummy_cols_ad, ['log_price'])\n\n\n\n\ndef compare_reg_models(models, assemblers, names=None):\n    \"\"\"\n    Produces a single formatted table comparing multiple regression models.\n\n    For each predictor (the union across models, ordered by first appearance), the table shows\n    the beta estimate (with significance stars) from each model (blank if not used).\n    For a predictor, if a model's outcome (model.getLabelCol()) starts with \"log\", the cell displays\n    both the beta and its exponential (separated by \" / \"), except when the predictor's name includes \"log_\".\n    (The intercept row does not display exp(.))\n\n    Additional rows for Intercept, Observations, R², and RMSE are appended.\n\n    The header's first column is labeled \"Predictor\", and subsequent columns are\n    \"y: [outcome] ([name])\" for each model.\n\n    The table is produced in grid format (with vertical lines). A dashed line (using '-' characters)\n    is inserted at the top, immediately after the header, and at the bottom.\n    Additionally, immediately after the Intercept row, the border line is replaced with one using '='\n    (to appear as, for example, \"+==============================================+==========================+...\").\n\n    Parameters:\n        models (list): List of fitted LinearRegression models.\n        assemblers (list): List of corresponding VectorAssembler objects.\n        names (list, optional): List of model names; defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing the combined regression table.\n    \"\"\"\n    # Default model names.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(models))]\n\n    # For each model, get outcome and determine if that model is log-transformed.\n    outcomes = [m.getLabelCol() for m in models]\n    is_log_flags = [out.lower().startswith(\"log\") for out in outcomes]\n\n    # Build an ordered union of predictors based on first appearance.\n    ordered_predictors = []\n    for assembler in assemblers:\n        for feat in assembler.getInputCols():\n            if feat not in ordered_predictors:\n                ordered_predictors.append(feat)\n\n    # Helper for significance stars.\n    def significance_stars(p):\n        if p is None:\n            return \"\"\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build rows for each predictor.\n    rows = []\n    for feat in ordered_predictors:\n        row = [feat]\n        for m, a, is_log in zip(models, assemblers, is_log_flags):\n            feats_model = a.getInputCols()\n            if feat in feats_model:\n                idx = feats_model.index(feat)\n                beta = m.coefficients.toArray()[idx]\n                p_val = m.summary.pValues[idx] if m.summary.pValues is not None else None\n                stars = significance_stars(p_val)\n                cell = f\"{beta:.3f}{stars}\"\n                # Only add exp(beta) if model is log and predictor name does NOT include \"log_\"\n                if is_log and (\"log_\" not in feat.lower()):\n                    cell += f\" / {np.exp(beta):,.3f}\"\n                row.append(cell)\n            else:\n                row.append(\"\")\n        rows.append(row)\n\n    # Build intercept row (do NOT compute exp(intercept)).\n    intercept_row = [\"Intercept\"]\n    for m in models:\n        std_all = np.array(m.summary.coefficientStandardErrors)\n        coeffs = m.coefficients.toArray()\n        if len(std_all) == len(coeffs) + 1:\n            intercept_p = m.summary.pValues[0] if m.summary.pValues is not None else None\n        else:\n            intercept_p = None\n        sig = significance_stars(intercept_p)\n        cell = f\"{m.intercept:.3f}{sig}\"\n        intercept_row.append(cell)\n    rows.append(intercept_row)\n\n    # Add Observations row.\n    obs_row = [\"Observations\"]\n    for m in models:\n        obs = m.summary.numInstances\n        obs_row.append(f\"{int(obs):,}\")\n    rows.append(obs_row)\n\n    # Add R² row.\n    r2_row = [\"R²\"]\n    for m in models:\n        r2_row.append(f\"{m.summary.r2:.3f}\")\n    rows.append(r2_row)\n\n    # Add RMSE row.\n    rmse_row = [\"RMSE\"]\n    for m in models:\n        rmse_row.append(f\"{m.summary.rootMeanSquaredError:.3f}\")\n    rows.append(rmse_row)\n\n    # Build header: first column \"Predictor\", then for each model: \"y: [outcome] ([name])\"\n    header = [\"Predictor\"]\n    for out, name in zip(outcomes, names):\n        header.append(f\"y: {out} ({name})\")\n\n    # Create table string using grid format.\n    table_str = tabulate(rows, headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(models))\n\n    # Split into lines.\n    lines = table_str.split(\"\\n\")\n\n    # Create a dashed line spanning the full width.\n    full_width = len(lines[0])\n    dash_line = '-' * full_width\n    # Create an equals line by replacing '-' with '='.\n    eq_line = dash_line.replace('-', '=')\n\n    # Insert a dashed line after the header row.\n    lines = table_str.split(\"\\n\")\n    # In grid format, header and separator are usually the first two lines.\n    lines.insert(2, dash_line)\n\n    # Insert an equals line after the Intercept row.\n    for i, line in enumerate(lines):\n        if line.startswith(\"|\") and \"Intercept\" in line:\n            if i+1 &lt; len(lines):\n                lines[i+1] = eq_line\n            break\n\n    # Add dashed lines at the very top and bottom.\n    final_table = dash_line + \"\\n\" + \"\\n\".join(lines) + \"\\n\" + dash_line\n\n    return final_table\n\n# Example usage:\n# print(compare_regression_models([model_1, model_2, model_3],\n#                                 [assembler_1, assembler_2, assembler_3],\n#                                 [\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\n\ndef compare_rmse(test_dfs, label_col, pred_col=\"prediction\", names=None):\n    \"\"\"\n    Computes and compares RMSE values for a list of test DataFrames.\n\n    For each DataFrame in test_dfs, this function calculates the RMSE between the actual outcome\n    (given by label_col) and the predicted value (given by pred_col, default \"prediction\"). It then\n    produces a formatted table where the first column header is empty and the first row's first cell is\n    \"RMSE\", with each model's RMSE in its own column.\n\n    Parameters:\n        test_dfs (list): List of test DataFrames.\n        label_col (str): The name of the outcome column.\n        pred_col (str, optional): The name of the prediction column (default \"prediction\").\n        names (list, optional): List of model names corresponding to the test DataFrames.\n                                Defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing a table that compares RMSE values for each test DataFrame,\n        with one model per column.\n    \"\"\"\n    # Set default model names if none provided.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(test_dfs))]\n\n    rmse_values = []\n    for df in test_dfs:\n        # Create a column for squared error.\n        df = df.withColumn(\"error_sq\", pow(col(label_col) - col(pred_col), 2))\n        # Calculate RMSE: square root of the mean squared error.\n        rmse = df.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n        rmse_values.append(rmse)\n\n    # Build a single row table: first cell \"RMSE\", then one cell per model with the RMSE value.\n    row = [\"RMSE\"] + [f\"{rmse:.3f}\" for rmse in rmse_values]\n\n    # Build header: first column header is empty, then model names.\n    header = [\"\"] + names\n\n    table_str = tabulate([row], headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(names))\n    return table_str\n\n# Example usage:\n# print(compare_rmse([dtest_1, dtest_2, dtest_3], \"log_sales\", names=[\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\n\n\ndef residual_plot(df, label_col, model_name):\n    \"\"\"\n    Generates a residual plot for a given test dataframe.\n\n    Parameters:\n        df (DataFrame): Spark DataFrame containing the test set with predictions.\n        label_col (str): The column name of the actual outcome variable.\n        title (str): The title for the residual plot.\n\n    Returns:\n        None (displays the plot)\n    \"\"\"\n    # Convert to Pandas DataFrame\n    df_pd = df.select([\"prediction\", label_col]).toPandas()\n    df_pd[\"residual\"] = df_pd[label_col] - df_pd[\"prediction\"]\n\n    # Scatter plot of residuals vs. predicted values\n    plt.scatter(df_pd[\"prediction\"], df_pd[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n    # Use LOWESS smoothing for trend line\n    smoothed = sm.nonparametric.lowess(df_pd[\"residual\"], df_pd[\"prediction\"])\n    plt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\n\n    # Add reference line at y=0\n    plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n\n    # Labels and title (model_name)\n    plt.xlabel(\"Predicted Values\")\n    plt.ylabel(\"Residuals\")\n    model_name = \"Residual Plot for \" + model_name\n    plt.title(model_name)\n\n    # Show plot\n    plt.show()\n\n# Example usage:\n# residual_plot(dtest_1, \"log_sales\", \"Model 1\")\n\n\n\nterms = assembler3.getInputCols()\ncoefs = model3.coefficients.toArray()[:len(terms)]\nstdErrs = model3.summary.coefficientStandardErrors[:len(terms)]\n\ndf_summary = pd.DataFrame({\n    \"term\": terms,\n    \"estimate\": coefs,\n    \"std_error\": stdErrs\n})\n\n# Filter df_summary if needed\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary[\"term\"], df_summary[\"estimate\"],\n             yerr = 1.96 * df_summary[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates (Model 2)\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n# Create a histogram\ndfpd = DATAFRAME.select([\"Y_VARIABLE\"]).toPandas()\nsns.histplot(dfpd[\"Y_VARIABLE\"], bins=10, kde=True)"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#required-libraries-and-sparksession-entry-point",
    "href": "danl-cw/danl-320-cw-09.html#required-libraries-and-sparksession-entry-point",
    "title": "Classwork 9",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#udf-for-regression-tables",
    "href": "danl-cw/danl-320-cw-09.html#udf-for-regression-tables",
    "title": "Classwork 9",
    "section": "",
    "text": "def regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                ci_upper,\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: Exp(95% CI Lower), 8: 95% CI Upper, 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"Exp(95% CI Lower)\", \"95% CI Upper\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#udf-for-adding-dummy-variables",
    "href": "danl-cw/danl-320-cw-09.html#udf-for-adding-dummy-variables",
    "title": "Classwork 9",
    "section": "",
    "text": "def add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#udf-for-adding-interaction-terms",
    "href": "danl-cw/danl-320-cw-09.html#udf-for-adding-interaction-terms",
    "title": "Classwork 9",
    "section": "",
    "text": "def add_interaction_terms(var_list1, var_list2, var_list3=None):\n    \"\"\"\n    Creates interaction term columns in the global DataFrames dtrain and dtest.\n\n    For two sets of variable names (which may represent categorical (dummy) or continuous variables),\n    this function creates two-way interactions by multiplying each variable in var_list1 with each\n    variable in var_list2.\n\n    Optionally, if a third list of variable names (var_list3) is provided, the function also creates\n    three-way interactions among each variable in var_list1, each variable in var_list2, and each variable\n    in var_list3.\n\n    Parameters:\n        var_list1 (list): List of column names for the first set of variables.\n        var_list2 (list): List of column names for the second set of variables.\n        var_list3 (list, optional): List of column names for the third set of variables for three-way interactions.\n\n    Returns:\n        A flat list of new interaction column names.\n    \"\"\"\n    global dtrain, dtest\n\n    interaction_cols = []\n\n    # Create two-way interactions between var_list1 and var_list2.\n    for var1 in var_list1:\n        for var2 in var_list2:\n            col_name = f\"{var1}_*_{var2}\"\n            dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list1 and var_list3.\n    if var_list3 is not None:\n      for var1 in var_list1:\n          for var3 in var_list3:\n              col_name = f\"{var1}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list2 and var_list3.\n    if var_list3 is not None:\n      for var2 in var_list2:\n          for var3 in var_list3:\n              col_name = f\"{var2}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # If a third list is provided, create three-way interactions.\n    if var_list3 is not None:\n        for var1 in var_list1:\n            for var2 in var_list2:\n                for var3 in var_list3:\n                    col_name = f\"{var1}_*_{var2}_*_{var3}\"\n                    dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    interaction_cols.append(col_name)\n\n    return interaction_cols\n\n # Example\n # interaction_cols_brand_price = add_interaction_terms(dummy_cols_brand, ['log_price'])\n # interaction_cols_brand_ad_price = add_interaction_terms(dummy_cols_brand, dummy_cols_ad, ['log_price'])"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#udf-for-model-comparison",
    "href": "danl-cw/danl-320-cw-09.html#udf-for-model-comparison",
    "title": "Classwork 9",
    "section": "",
    "text": "def compare_reg_models(models, assemblers, names=None):\n    \"\"\"\n    Produces a single formatted table comparing multiple regression models.\n\n    For each predictor (the union across models, ordered by first appearance), the table shows\n    the beta estimate (with significance stars) from each model (blank if not used).\n    For a predictor, if a model's outcome (model.getLabelCol()) starts with \"log\", the cell displays\n    both the beta and its exponential (separated by \" / \"), except when the predictor's name includes \"log_\".\n    (The intercept row does not display exp(.))\n\n    Additional rows for Intercept, Observations, R², and RMSE are appended.\n\n    The header's first column is labeled \"Predictor\", and subsequent columns are\n    \"y: [outcome] ([name])\" for each model.\n\n    The table is produced in grid format (with vertical lines). A dashed line (using '-' characters)\n    is inserted at the top, immediately after the header, and at the bottom.\n    Additionally, immediately after the Intercept row, the border line is replaced with one using '='\n    (to appear as, for example, \"+==============================================+==========================+...\").\n\n    Parameters:\n        models (list): List of fitted LinearRegression models.\n        assemblers (list): List of corresponding VectorAssembler objects.\n        names (list, optional): List of model names; defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing the combined regression table.\n    \"\"\"\n    # Default model names.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(models))]\n\n    # For each model, get outcome and determine if that model is log-transformed.\n    outcomes = [m.getLabelCol() for m in models]\n    is_log_flags = [out.lower().startswith(\"log\") for out in outcomes]\n\n    # Build an ordered union of predictors based on first appearance.\n    ordered_predictors = []\n    for assembler in assemblers:\n        for feat in assembler.getInputCols():\n            if feat not in ordered_predictors:\n                ordered_predictors.append(feat)\n\n    # Helper for significance stars.\n    def significance_stars(p):\n        if p is None:\n            return \"\"\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build rows for each predictor.\n    rows = []\n    for feat in ordered_predictors:\n        row = [feat]\n        for m, a, is_log in zip(models, assemblers, is_log_flags):\n            feats_model = a.getInputCols()\n            if feat in feats_model:\n                idx = feats_model.index(feat)\n                beta = m.coefficients.toArray()[idx]\n                p_val = m.summary.pValues[idx] if m.summary.pValues is not None else None\n                stars = significance_stars(p_val)\n                cell = f\"{beta:.3f}{stars}\"\n                # Only add exp(beta) if model is log and predictor name does NOT include \"log_\"\n                if is_log and (\"log_\" not in feat.lower()):\n                    cell += f\" / {np.exp(beta):,.3f}\"\n                row.append(cell)\n            else:\n                row.append(\"\")\n        rows.append(row)\n\n    # Build intercept row (do NOT compute exp(intercept)).\n    intercept_row = [\"Intercept\"]\n    for m in models:\n        std_all = np.array(m.summary.coefficientStandardErrors)\n        coeffs = m.coefficients.toArray()\n        if len(std_all) == len(coeffs) + 1:\n            intercept_p = m.summary.pValues[0] if m.summary.pValues is not None else None\n        else:\n            intercept_p = None\n        sig = significance_stars(intercept_p)\n        cell = f\"{m.intercept:.3f}{sig}\"\n        intercept_row.append(cell)\n    rows.append(intercept_row)\n\n    # Add Observations row.\n    obs_row = [\"Observations\"]\n    for m in models:\n        obs = m.summary.numInstances\n        obs_row.append(f\"{int(obs):,}\")\n    rows.append(obs_row)\n\n    # Add R² row.\n    r2_row = [\"R²\"]\n    for m in models:\n        r2_row.append(f\"{m.summary.r2:.3f}\")\n    rows.append(r2_row)\n\n    # Add RMSE row.\n    rmse_row = [\"RMSE\"]\n    for m in models:\n        rmse_row.append(f\"{m.summary.rootMeanSquaredError:.3f}\")\n    rows.append(rmse_row)\n\n    # Build header: first column \"Predictor\", then for each model: \"y: [outcome] ([name])\"\n    header = [\"Predictor\"]\n    for out, name in zip(outcomes, names):\n        header.append(f\"y: {out} ({name})\")\n\n    # Create table string using grid format.\n    table_str = tabulate(rows, headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(models))\n\n    # Split into lines.\n    lines = table_str.split(\"\\n\")\n\n    # Create a dashed line spanning the full width.\n    full_width = len(lines[0])\n    dash_line = '-' * full_width\n    # Create an equals line by replacing '-' with '='.\n    eq_line = dash_line.replace('-', '=')\n\n    # Insert a dashed line after the header row.\n    lines = table_str.split(\"\\n\")\n    # In grid format, header and separator are usually the first two lines.\n    lines.insert(2, dash_line)\n\n    # Insert an equals line after the Intercept row.\n    for i, line in enumerate(lines):\n        if line.startswith(\"|\") and \"Intercept\" in line:\n            if i+1 &lt; len(lines):\n                lines[i+1] = eq_line\n            break\n\n    # Add dashed lines at the very top and bottom.\n    final_table = dash_line + \"\\n\" + \"\\n\".join(lines) + \"\\n\" + dash_line\n\n    return final_table\n\n# Example usage:\n# print(compare_regression_models([model_1, model_2, model_3],\n#                                 [assembler_1, assembler_2, assembler_3],\n#                                 [\"Model 1\", \"Model 2\", \"Model 3\"]))"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#udf-for-rmses",
    "href": "danl-cw/danl-320-cw-09.html#udf-for-rmses",
    "title": "Classwork 9",
    "section": "",
    "text": "def compare_rmse(test_dfs, label_col, pred_col=\"prediction\", names=None):\n    \"\"\"\n    Computes and compares RMSE values for a list of test DataFrames.\n\n    For each DataFrame in test_dfs, this function calculates the RMSE between the actual outcome\n    (given by label_col) and the predicted value (given by pred_col, default \"prediction\"). It then\n    produces a formatted table where the first column header is empty and the first row's first cell is\n    \"RMSE\", with each model's RMSE in its own column.\n\n    Parameters:\n        test_dfs (list): List of test DataFrames.\n        label_col (str): The name of the outcome column.\n        pred_col (str, optional): The name of the prediction column (default \"prediction\").\n        names (list, optional): List of model names corresponding to the test DataFrames.\n                                Defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing a table that compares RMSE values for each test DataFrame,\n        with one model per column.\n    \"\"\"\n    # Set default model names if none provided.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(test_dfs))]\n\n    rmse_values = []\n    for df in test_dfs:\n        # Create a column for squared error.\n        df = df.withColumn(\"error_sq\", pow(col(label_col) - col(pred_col), 2))\n        # Calculate RMSE: square root of the mean squared error.\n        rmse = df.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n        rmse_values.append(rmse)\n\n    # Build a single row table: first cell \"RMSE\", then one cell per model with the RMSE value.\n    row = [\"RMSE\"] + [f\"{rmse:.3f}\" for rmse in rmse_values]\n\n    # Build header: first column header is empty, then model names.\n    header = [\"\"] + names\n\n    table_str = tabulate([row], headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(names))\n    return table_str\n\n# Example usage:\n# print(compare_rmse([dtest_1, dtest_2, dtest_3], \"log_sales\", names=[\"Model 1\", \"Model 2\", \"Model 3\"]))"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#udf-for-a-residual-plot",
    "href": "danl-cw/danl-320-cw-09.html#udf-for-a-residual-plot",
    "title": "Classwork 9",
    "section": "",
    "text": "def residual_plot(df, label_col, model_name):\n    \"\"\"\n    Generates a residual plot for a given test dataframe.\n\n    Parameters:\n        df (DataFrame): Spark DataFrame containing the test set with predictions.\n        label_col (str): The column name of the actual outcome variable.\n        title (str): The title for the residual plot.\n\n    Returns:\n        None (displays the plot)\n    \"\"\"\n    # Convert to Pandas DataFrame\n    df_pd = df.select([\"prediction\", label_col]).toPandas()\n    df_pd[\"residual\"] = df_pd[label_col] - df_pd[\"prediction\"]\n\n    # Scatter plot of residuals vs. predicted values\n    plt.scatter(df_pd[\"prediction\"], df_pd[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n    # Use LOWESS smoothing for trend line\n    smoothed = sm.nonparametric.lowess(df_pd[\"residual\"], df_pd[\"prediction\"])\n    plt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\n\n    # Add reference line at y=0\n    plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n\n    # Labels and title (model_name)\n    plt.xlabel(\"Predicted Values\")\n    plt.ylabel(\"Residuals\")\n    model_name = \"Residual Plot for \" + model_name\n    plt.title(model_name)\n\n    # Show plot\n    plt.show()\n\n# Example usage:\n# residual_plot(dtest_1, \"log_sales\", \"Model 1\")"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#coefficient-plots",
    "href": "danl-cw/danl-320-cw-09.html#coefficient-plots",
    "title": "Classwork 9",
    "section": "",
    "text": "terms = assembler3.getInputCols()\ncoefs = model3.coefficients.toArray()[:len(terms)]\nstdErrs = model3.summary.coefficientStandardErrors[:len(terms)]\n\ndf_summary = pd.DataFrame({\n    \"term\": terms,\n    \"estimate\": coefs,\n    \"std_error\": stdErrs\n})\n\n# Filter df_summary if needed\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary[\"term\"], df_summary[\"estimate\"],\n             yerr = 1.96 * df_summary[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates (Model 2)\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#histogram",
    "href": "danl-cw/danl-320-cw-09.html#histogram",
    "title": "Classwork 9",
    "section": "",
    "text": "# Create a histogram\ndfpd = DATAFRAME.select([\"Y_VARIABLE\"]).toPandas()\nsns.histplot(dfpd[\"Y_VARIABLE\"], bins=10, kde=True)"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#variable-description",
    "href": "danl-cw/danl-320-cw-09.html#variable-description",
    "title": "Classwork 9",
    "section": "Variable description",
    "text": "Variable description\n\n\n\nVariable\nDescription\n\n\n\n\nsales\nQuantity of OJ cartons sold\n\n\nprice\nPrice of OJ\n\n\nbrand\nBrand of OJ\n\n\nad\nAdvertisement status"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#model-1",
    "href": "danl-cw/danl-320-cw-09.html#model-1",
    "title": "Classwork 9",
    "section": "Model 1",
    "text": "Model 1\n\\[\n\\begin{align}\n\\log(\\text{sales}_{\\text{i}}) &\\,=\\, \\;\\; b_{\\text{intercept}} \\,+\\, b_{\\,\\text{mm}}\\,\\text{brand}_{\\,\\text{mm}, \\text{i}} \\,+\\, b_{\\,\\text{tr}}\\,\\text{brand}_{\\,\\text{tr}, \\text{i}}\\\\\n&\\quad\\,+\\, b_{\\text{price}}\\,\\log(\\text{price}_{\\text{i}}) \\,+\\, e_{\\text{i}},\\\\\n\\text{where}\\qquad\\qquad&\\\\\n\\text{brand}_{\\,\\text{tr}, \\text{i}}\n&\\,=\\, \\begin{cases}\n\\text{1} & \\text{ if an orange juice } \\text{i} \\text{ is } \\text{Tropicana};\\\\\\\\\n\\text{0} & \\text{otherwise}.\\qquad\\qquad\\quad\\,\n\\end{cases}\\\\\n\\text{brand}_{\\,\\text{mm}, \\text{i}} &\\,=\\, \\begin{cases}\n\\text{1} & \\text{ if an orange juice } \\text{i} \\text{ is } \\text{Minute Maid};\\\\\\\\\n\\text{0} & \\text{otherwise}.\\qquad\\qquad\\quad\\,\n\\end{cases}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#model-2",
    "href": "danl-cw/danl-320-cw-09.html#model-2",
    "title": "Classwork 9",
    "section": "Model 2",
    "text": "Model 2\n\\[\n\\begin{align}\n\\log(\\text{sales}_{\\text{i}}) \\,=\\,&\\;\\; \\quad b_{\\text{intercept}} \\,+\\, \\color{Green}{b_{\\,\\text{mm}}\\,\\text{brand}_{\\,\\text{mm}, \\text{i}}} \\,+\\, \\color{Blue}{b_{\\,\\text{tr}}\\,\\text{brand}_{\\,\\text{tr}, \\text{i}}}\\\\\n&\\,+\\, b_{\\text{price}}\\,\\log(\\text{price}_{\\text{i}})  \\\\\n&\\, +\\, b_{\\text{price*mm}}\\,\\log(\\text{price}_{\\text{i}})\\,\\times\\,\\color{Green} {\\text{brand}_{\\,\\text{mm}, \\text{i}}} \\\\\n&\\,+\\, b_{\\text{price*tr}}\\,\\log(\\text{price}_{\\text{i}})\\,\\times\\,\\color{Blue} {\\text{brand}_{\\,\\text{tr}, \\text{i}}} \\,+\\, e_{\\text{i}}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#model-3",
    "href": "danl-cw/danl-320-cw-09.html#model-3",
    "title": "Classwork 9",
    "section": "Model 3",
    "text": "Model 3\n\\[\n\\begin{align}\n\\log(\\text{sales}_{\\text{i}}) \\,=\\,\\quad\\;\\;& b_{\\text{intercept}} \\,+\\, \\color{Green}{b_{\\,\\text{mm}}\\,\\text{brand}_{\\,\\text{mm}, \\text{i}}} \\,+\\, \\color{Blue}{b_{\\,\\text{tr}}\\,\\text{brand}_{\\,\\text{tr}, \\text{i}}}  \\\\\n&\\,+\\; b_{\\,\\text{ad}}\\,\\color{Orange}{\\text{ad}_{\\,\\text{i}}} \\qquad\\qquad\\qquad\\qquad\\quad   \\\\\n&\\,+\\, b_{\\text{mm*ad}}\\,\\color{Green} {\\text{brand}_{\\,\\text{mm}, \\text{i}}}\\,\\times\\, \\color{Orange}{\\text{ad}_{\\,\\text{i}}}\\,+\\, b_{\\text{tr*ad}}\\,\\color{Blue} {\\text{brand}_{\\,\\text{tr}, \\text{i}}}\\,\\times\\, \\color{Orange}{\\text{ad}_{\\,\\text{i}}} \\\\\n&\\,+\\;  b_{\\text{price}}\\,\\log(\\text{price}_{\\text{i}}) \\qquad\\qquad\\qquad\\;\\;\\;\\;\\,  \\\\\n&\\,+\\, b_{\\text{price*mm}}\\,\\log(\\text{price}_{\\text{i}})\\,\\times\\,\\color{Green} {\\text{brand}_{\\,\\text{mm}, \\text{i}}}\\qquad\\qquad\\qquad\\;\\, \\\\\n&\\,+\\, b_{\\text{price*tr}}\\,\\log(\\text{price}_{\\text{i}})\\,\\times\\,\\color{Blue} {\\text{brand}_{\\,\\text{tr}, \\text{i}}}\\qquad\\qquad\\qquad\\;\\, \\\\\n& \\,+\\, b_{\\text{price*ad}}\\,\\log(\\text{price}_{\\text{i}})\\,\\times\\,\\color{Orange}{\\text{ad}_{\\,\\text{i}}}\\qquad\\qquad\\qquad\\;\\;\\, \\\\\n&\\,+\\, b_{\\text{price*mm*ad}}\\,\\log(\\text{price}_{\\text{i}}) \\,\\times\\,\\,\\color{Green} {\\text{brand}_{\\,\\text{mm}, \\text{i}}}\\,\\times\\, \\color{Orange}{\\text{ad}_{\\,\\text{i}}} \\\\\n&\\,+\\, b_{\\text{price*tr*ad}}\\,\\log(\\text{price}_{\\text{i}}) \\,\\times\\,\\,\\color{Blue} {\\text{brand}_{\\,\\text{tr}, \\text{i}}}\\,\\times\\, \\color{Orange}{\\text{ad}_{\\,\\text{i}}}  \\,+\\, e_{\\text{i}}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-cw/danl-320-cw-12.html",
    "href": "danl-cw/danl-320-cw-12.html",
    "title": "Classwork 12",
    "section": "",
    "text": "# Below is for an interactive display of Pandas DataFrame in Colab\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\nfrom tabulate import tabulate  # for table summary\n\n# For basic libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport statsmodels.api as sm  # for lowess smoothing\n\n# `scikit-learn`\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import (confusion_matrix, accuracy_score, precision_score, recall_score, roc_curve, roc_auc_score)\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.inspection import PartialDependenceDisplay, plot_partial_dependence\n\nfrom sklearn.preprocessing import scale # zero mean & one s.d.\nfrom sklearn.linear_model import LassoCV, lasso_path\nfrom sklearn.linear_model import RidgeCV, Ridge\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor, plot_importance\n\n# PySpark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n\n\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\n\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                ci_upper,\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: Exp(95% CI Lower), 8: 95% CI Upper, 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"Exp(95% CI Lower)\", \"95% CI Upper\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))\n\n # Data\ndf = pd.read_csv('https://bcdanl.github.io/data/california_housing_cleaned.csv')\n\n\n\n  \n\n\n\n\n\nThe housing data at census tract-level in California include:\n\nLatitude/Longitude of tract centers\nMedian Home age.\nMedian Income\nAverage room/bedroom numbers\nAverage occupancy\nMedian home values\n\nThe goal is to predict the log of median housing value for census tracts."
  },
  {
    "objectID": "danl-cw/danl-320-cw-12.html#required-libraries-and-sparksession-entry-point",
    "href": "danl-cw/danl-320-cw-12.html#required-libraries-and-sparksession-entry-point",
    "title": "Classwork 12",
    "section": "",
    "text": "# Below is for an interactive display of Pandas DataFrame in Colab\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\nfrom tabulate import tabulate  # for table summary\n\n# For basic libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport statsmodels.api as sm  # for lowess smoothing\n\n# `scikit-learn`\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import (confusion_matrix, accuracy_score, precision_score, recall_score, roc_curve, roc_auc_score)\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.inspection import PartialDependenceDisplay, plot_partial_dependence\n\nfrom sklearn.preprocessing import scale # zero mean & one s.d.\nfrom sklearn.linear_model import LassoCV, lasso_path\nfrom sklearn.linear_model import RidgeCV, Ridge\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor, plot_importance\n\n# PySpark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
  },
  {
    "objectID": "danl-cw/danl-320-cw-12.html#udf-for-adding-dummy-variables",
    "href": "danl-cw/danl-320-cw-12.html#udf-for-adding-dummy-variables",
    "title": "Classwork 12",
    "section": "",
    "text": "def add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)"
  },
  {
    "objectID": "danl-cw/danl-320-cw-12.html#udf-for-regression-tables",
    "href": "danl-cw/danl-320-cw-12.html#udf-for-regression-tables",
    "title": "Classwork 12",
    "section": "",
    "text": "def regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                ci_upper,\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: Exp(95% CI Lower), 8: 95% CI Upper, 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"Exp(95% CI Lower)\", \"95% CI Upper\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))\n\n # Data\ndf = pd.read_csv('https://bcdanl.github.io/data/california_housing_cleaned.csv')\n\n\n\n  \n\n\n\n\n\nThe housing data at census tract-level in California include:\n\nLatitude/Longitude of tract centers\nMedian Home age.\nMedian Income\nAverage room/bedroom numbers\nAverage occupancy\nMedian home values\n\nThe goal is to predict the log of median housing value for census tracts."
  },
  {
    "objectID": "danl-cw/danl-320-cw-12.html#questions-7-10",
    "href": "danl-cw/danl-320-cw-12.html#questions-7-10",
    "title": "Classwork 12",
    "section": "Questions 7-10",
    "text": "Questions 7-10\nConsider the tree-based model described below:\n\\[\n\\begin{align}\n\\log(\\text{medianHouseValue})_{i} = f(&\\text{housingMedianAge}_{i}, \\text{medianIncome}_{i},\\\\\n&\\text{AveBedrms}_{i}, \\text{AveRooms}_{i}, \\text{AveOccupancy}_{i},\\\\\n&\\text{latitude}_{i}, \\text{longitude}_{i})\n\\end{align}\n\\]\n\nQuestion 7\nFit a regression tree model.\n\n\n\nQuestion 8\nFit a random forest model.\n\n\n\nQuestion 9\nFit a gradient boosting tree model."
  },
  {
    "objectID": "danl-cw/danl-320-cw-12.html#question-10",
    "href": "danl-cw/danl-320-cw-12.html#question-10",
    "title": "Classwork 12",
    "section": "Question 10",
    "text": "Question 10\nCompare the prediction performance across the models - Linear regression - Lasso regression - Ridge regression - Regression tree - Random forest - Gradient boosting tree"
  },
  {
    "objectID": "danl-cw/danl-320-cw-07.html",
    "href": "danl-cw/danl-320-cw-07.html",
    "title": "Classwork 7",
    "section": "",
    "text": "The dataset ,cereals_oatmeal.csv,(with its pathname https://bcdanl.github.io/data/cereal_oatmeal.csv) is a listing of 76 popular breakfast cereals and oatmeal.\n\ncereal = pd.read_csv('https://bcdanl.github.io/data/cereal_oatmeal.csv')\n\n\n\n\n  \n\n\n\nUse PySpark to solve this classwork."
  },
  {
    "objectID": "danl-cw/danl-320-cw-07.html#direction",
    "href": "danl-cw/danl-320-cw-07.html#direction",
    "title": "Classwork 7",
    "section": "",
    "text": "The dataset ,cereals_oatmeal.csv,(with its pathname https://bcdanl.github.io/data/cereal_oatmeal.csv) is a listing of 76 popular breakfast cereals and oatmeal.\n\ncereal = pd.read_csv('https://bcdanl.github.io/data/cereal_oatmeal.csv')\n\n\n\n\n  \n\n\n\nUse PySpark to solve this classwork."
  },
  {
    "objectID": "danl-cw/danl-320-cw-07.html#question-1",
    "href": "danl-cw/danl-320-cw-07.html#question-1",
    "title": "Classwork 7",
    "section": "Question 1",
    "text": "Question 1\nGroup the cereal DataFrame, using the Manufacturer variable.\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-07.html#question-2",
    "href": "danl-cw/danl-320-cw-07.html#question-2",
    "title": "Classwork 7",
    "section": "Question 2",
    "text": "Question 2\nDetermine the total number of groups, and the number of cereals per group.\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-07.html#question-3",
    "href": "danl-cw/danl-320-cw-07.html#question-3",
    "title": "Classwork 7",
    "section": "Question 3",
    "text": "Question 3\nExtract the cereals that belong to the manufacturer \"Kellogg's\".\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-07.html#question-4",
    "href": "danl-cw/danl-320-cw-07.html#question-4",
    "title": "Classwork 7",
    "section": "Question 4",
    "text": "Question 4\nCalculate the average of values in the Calories, Fiber, and Sugars variables for each manufacturer.\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-07.html#question-5",
    "href": "danl-cw/danl-320-cw-07.html#question-5",
    "title": "Classwork 7",
    "section": "Question 5",
    "text": "Question 5\nFind the maximum value in the Sugars variable for each manufacturer.\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-07.html#question-6",
    "href": "danl-cw/danl-320-cw-07.html#question-6",
    "title": "Classwork 7",
    "section": "Question 6",
    "text": "Question 6\nFind the minimum value in the Fiber variable for each manufacturer.\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-07.html#question-7",
    "href": "danl-cw/danl-320-cw-07.html#question-7",
    "title": "Classwork 7",
    "section": "Question 7",
    "text": "Question 7\n\nCalculate a ‘Normalized_Sugars’ variable for each product by Manufacturer, where the normalization formula is\n\n\\[\n\\text{Normalized\\_Sugars} = \\frac{\\text{Sugars} - \\text{mean(Sugars)}}{\\text{std(Sugars)}}\n\\]\nfor each Manufacturer group. This formula adjusts the sugar content of each product by subtracting the mean sugar content of its manufacturer and then dividing by the standard deviation of the sugar content within its manufacturer.\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-11.html",
    "href": "danl-cw/danl-320-cw-11.html",
    "title": "Classwork 11",
    "section": "",
    "text": "# Below is for an interactive display of Pandas DataFrame in Colab\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\nimport pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_curve\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n\n\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)"
  },
  {
    "objectID": "danl-cw/danl-320-cw-11.html#required-libraries-and-sparksession-entry-point",
    "href": "danl-cw/danl-320-cw-11.html#required-libraries-and-sparksession-entry-point",
    "title": "Classwork 11",
    "section": "",
    "text": "# Below is for an interactive display of Pandas DataFrame in Colab\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\nimport pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_curve\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
  },
  {
    "objectID": "danl-cw/danl-320-cw-11.html#udf-for-adding-dummy-variables",
    "href": "danl-cw/danl-320-cw-11.html#udf-for-adding-dummy-variables",
    "title": "Classwork 11",
    "section": "",
    "text": "def add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)"
  },
  {
    "objectID": "danl-cw/danl-320-cw-11.html#variable-description",
    "href": "danl-cw/danl-320-cw-11.html#variable-description",
    "title": "Classwork 11",
    "section": "Variable description",
    "text": "Variable description\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nbuying\nBuying price of the car (vhigh, high, med, low)\n\n\nmaint\nMaintenance cost (vhigh, high, med, low)\n\n\ndoors\nNumber of doors (2, 3, 4, 5more)\n\n\npersons\nCapacity in terms of persons to carry (2, 4, more)\n\n\nlug_boot\nSize of luggage boot (small, med, big)\n\n\nsafety\nEstimated safety of the car (low, med, high)\n\n\nrating\nCar acceptability (unacc, acc, good, vgood)\n\n\nfail\nTRUE if the car is unacceptable (unacc), otherwise FALSE"
  },
  {
    "objectID": "danl-cw/danl-320-cw-04.html",
    "href": "danl-cw/danl-320-cw-04.html",
    "title": "Classwork 4",
    "section": "",
    "text": "Using Python operations only, calculate below: \\[\\frac{2^5}{7 \\cdot (4 - 2^3)}\\]\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-320-cw-04.html#question-1",
    "href": "danl-cw/danl-320-cw-04.html#question-1",
    "title": "Classwork 4",
    "section": "",
    "text": "Using Python operations only, calculate below: \\[\\frac{2^5}{7 \\cdot (4 - 2^3)}\\]\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-320-cw-04.html#question-2",
    "href": "danl-cw/danl-320-cw-04.html#question-2",
    "title": "Classwork 4",
    "section": "Question 2",
    "text": "Question 2\nFor each expression below, what is the value of the expression? Explain thoroughly.\n\n20 == '20'\n\n\nx = 4.0\ny = .5\n\nx &lt; y or 3*y &lt; x\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-320-cw-04.html#question-3",
    "href": "danl-cw/danl-320-cw-04.html#question-3",
    "title": "Classwork 4",
    "section": "Question 3",
    "text": "Question 3\n\nfare = \"$10.00\"\ntip = \"2.00$\"\ntax = \"$ 0.80\"\n\nWrite a Python code that uses slicing and the print() function to print out the following message:\n\nThe total trip cost is: $12.80\n\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-320-cw-04.html#question-4",
    "href": "danl-cw/danl-320-cw-04.html#question-4",
    "title": "Classwork 4",
    "section": "Question 4",
    "text": "Question 4\n\nlist_variable = [100, 144, 169, 1000, 8]\n\nWrite a Python code that uses print() and max() functions to print out the largest value in the list, list_variable, as follows:\n\nThe largest value in the list is: 1000\n\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-320-cw-04.html#question-5",
    "href": "danl-cw/danl-320-cw-04.html#question-5",
    "title": "Classwork 4",
    "section": "Question 5",
    "text": "Question 5\n\nvals = [3, 2, 1, 0]\n\n\nUse a while loop to print each value of the list [3, 2, 1, 0], one at a time.\nUse a for loop to print each value of the list [3, 2, 1, 0], one at a time.\n\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-320-cw-04.html#question-6",
    "href": "danl-cw/danl-320-cw-04.html#question-6",
    "title": "Classwork 4",
    "section": "Question 6",
    "text": "Question 6\n\nAssign the value 7 to the variable guess_me, and the value 1 to the variable number.\nWrite a while loop that compares number with guess_me.\n\nPrint ‘too low’ if number is less than guess me.\nIf number equals guess_me, print ‘found it!’ and then exit the loop.\nIf number is greater than guess_me, print ‘oops’ and then exit the loop.\nIncrement number at the end of the loop.\n\nWrite a for loop that compares number with guess_me.\n\nPrint ‘too low’ if number is less than guess me.\nIf number equals guess_me, print ‘found it!’ and then exit the loop.\nIf number is greater than guess_me, print ‘oops’ and then exit the loop.\nIncrement number at the end of the loop.\n\n\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-320-cw-03.html",
    "href": "danl-cw/danl-320-cw-03.html",
    "title": "Classwork 3",
    "section": "",
    "text": "_quarto.yml configures the website:\n\nIt determines the structure of the website.\n\ne.g., Navigation bar, themes, HTML options, etc.\n\nIf _quarto.yml is edited, use quarto render to render all qmd and ipynb files.\n\nindex.qmd renders index.html, the front page of the website.\n\nDo not create Quarto files something like index2.qmd within the working directory.\n\nblog-listing.qmd configures the blog listing page.\nposts directory includes sub-directories of blog posts.\nimg directory can be used to store picture files.\n\n\n\n\nA file in the working directory can have its own web address.\n\nFor example, if you have resume-example.pdf in your working directory, it has the web address, https://USERNAME.github.io/resume-example.pdf.\n\nWhen naming a file in the website, do not have any space in a file name!\nBe systematic when naming a series of files in the website.\n\nE.g., danl-320-cw-01.ipynb, danl-320-cw-02.ipynb, danl-320-cw-03.ipynb.\n\n\n\n\n\n\n\nRules\n\nOne blog post corresponds to:\n\n\nOne sub-directory in the posts directory.\nOne *.ipynb (or *.qmd) file.\n\n\nPut all files for one blog post (e.g., *.ipynb (or *.qmd), *.png) in one corresponding sub-directory in the posts directory.\nWhen inserting an image file to a blog post, use a relative path, i.e., a file name of the image file.\n\n\n\n\n\n\n\nDecorate your website:\n\n\nReplace YOUR NAME with your name in _quarto.yml and index.qmd.\nDescribe yourself in index.qmd.\nAdd the picture (png) file of your profile photo to img directory. Then correct img/profile.png in index.qmd accordingly.\nCorrect links for your resumé, linkedin, email, and social media.\n\n\nAdd a menu of “Project” to the navigation bar using danl_proj_nba.ipynb.\nAdd a drop-down menu of “Python Data Analysis” to the navigation bar.\n\n\nUnder the menu of “Python Data Analysis”, add links for the following webpage:\n\nPandas Basics using pandas_basic.ipynb\nSeaborn Basics using seaborn_basic.ipynb\n\n\n\nUse the 3-step git commands (git add, git commit, and git push) to update your website.\n\n\n\n\n\n\nQuarto - Creating a Website\nQuarto - HTML Basics\nQuarto - HTML Code Blocks\nQuarto - HTML Theming\nQuarto - Creating a Blog"
  },
  {
    "objectID": "danl-cw/danl-320-cw-03.html#website-files",
    "href": "danl-cw/danl-320-cw-03.html#website-files",
    "title": "Classwork 3",
    "section": "",
    "text": "_quarto.yml configures the website:\n\nIt determines the structure of the website.\n\ne.g., Navigation bar, themes, HTML options, etc.\n\nIf _quarto.yml is edited, use quarto render to render all qmd and ipynb files.\n\nindex.qmd renders index.html, the front page of the website.\n\nDo not create Quarto files something like index2.qmd within the working directory.\n\nblog-listing.qmd configures the blog listing page.\nposts directory includes sub-directories of blog posts.\nimg directory can be used to store picture files.\n\n\n\n\nA file in the working directory can have its own web address.\n\nFor example, if you have resume-example.pdf in your working directory, it has the web address, https://USERNAME.github.io/resume-example.pdf.\n\nWhen naming a file in the website, do not have any space in a file name!\nBe systematic when naming a series of files in the website.\n\nE.g., danl-320-cw-01.ipynb, danl-320-cw-02.ipynb, danl-320-cw-03.ipynb."
  },
  {
    "objectID": "danl-cw/danl-320-cw-03.html#blogging",
    "href": "danl-cw/danl-320-cw-03.html#blogging",
    "title": "Classwork 3",
    "section": "",
    "text": "Rules\n\nOne blog post corresponds to:\n\n\nOne sub-directory in the posts directory.\nOne *.ipynb (or *.qmd) file.\n\n\nPut all files for one blog post (e.g., *.ipynb (or *.qmd), *.png) in one corresponding sub-directory in the posts directory.\nWhen inserting an image file to a blog post, use a relative path, i.e., a file name of the image file."
  },
  {
    "objectID": "danl-cw/danl-320-cw-03.html#practice-problems",
    "href": "danl-cw/danl-320-cw-03.html#practice-problems",
    "title": "Classwork 3",
    "section": "",
    "text": "Decorate your website:\n\n\nReplace YOUR NAME with your name in _quarto.yml and index.qmd.\nDescribe yourself in index.qmd.\nAdd the picture (png) file of your profile photo to img directory. Then correct img/profile.png in index.qmd accordingly.\nCorrect links for your resumé, linkedin, email, and social media.\n\n\nAdd a menu of “Project” to the navigation bar using danl_proj_nba.ipynb.\nAdd a drop-down menu of “Python Data Analysis” to the navigation bar.\n\n\nUnder the menu of “Python Data Analysis”, add links for the following webpage:\n\nPandas Basics using pandas_basic.ipynb\nSeaborn Basics using seaborn_basic.ipynb\n\n\n\nUse the 3-step git commands (git add, git commit, and git push) to update your website."
  },
  {
    "objectID": "danl-cw/danl-320-cw-03.html#references",
    "href": "danl-cw/danl-320-cw-03.html#references",
    "title": "Classwork 3",
    "section": "",
    "text": "Quarto - Creating a Website\nQuarto - HTML Basics\nQuarto - HTML Code Blocks\nQuarto - HTML Theming\nQuarto - Creating a Blog"
  },
  {
    "objectID": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html",
    "href": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html",
    "title": "Homework 3",
    "section": "",
    "text": "from google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\nimport pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_curve\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
  },
  {
    "objectID": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#regression_table",
    "href": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#regression_table",
    "title": "Homework 3",
    "section": "regression_table()",
    "text": "regression_table()\n\n\nCode\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper | Exp(95% CI Lower) | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: 95% CI Upper, 8: Exp(95% CI Lower), 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"95% CI Upper\", \"Exp(95% CI Lower)\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))"
  },
  {
    "objectID": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#add_dummy_variables",
    "href": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#add_dummy_variables",
    "title": "Homework 3",
    "section": "add_dummy_variables()",
    "text": "add_dummy_variables()\n\n\nCode\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)"
  },
  {
    "objectID": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#add_interaction_terms",
    "href": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#add_interaction_terms",
    "title": "Homework 3",
    "section": "add_interaction_terms()",
    "text": "add_interaction_terms()\n\n\nCode\ndef add_interaction_terms(var_list1, var_list2, var_list3=None):\n    \"\"\"\n    Creates interaction term columns in the global DataFrames dtrain and dtest.\n\n    For two sets of variable names (which may represent categorical (dummy) or continuous variables),\n    this function creates two-way interactions by multiplying each variable in var_list1 with each\n    variable in var_list2.\n\n    Optionally, if a third list of variable names (var_list3) is provided, the function also creates\n    three-way interactions among each variable in var_list1, each variable in var_list2, and each variable\n    in var_list3.\n\n    Parameters:\n        var_list1 (list): List of column names for the first set of variables.\n        var_list2 (list): List of column names for the second set of variables.\n        var_list3 (list, optional): List of column names for the third set of variables for three-way interactions.\n\n    Returns:\n        A flat list of new interaction column names.\n    \"\"\"\n    global dtrain, dtest\n\n    interaction_cols = []\n\n    # Create two-way interactions between var_list1 and var_list2.\n    for var1 in var_list1:\n        for var2 in var_list2:\n            col_name = f\"{var1}_*_{var2}\"\n            dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list1 and var_list3.\n    if var_list3 is not None:\n      for var1 in var_list1:\n          for var3 in var_list3:\n              col_name = f\"{var1}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list2 and var_list3.\n    if var_list3 is not None:\n      for var2 in var_list2:\n          for var3 in var_list3:\n              col_name = f\"{var2}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # If a third list is provided, create three-way interactions.\n    if var_list3 is not None:\n        for var1 in var_list1:\n            for var2 in var_list2:\n                for var3 in var_list3:\n                    col_name = f\"{var1}_*_{var2}_*_{var3}\"\n                    dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    interaction_cols.append(col_name)\n\n    return interaction_cols\n\n # Example\n # interaction_cols_brand_price = add_interaction_terms(dummy_cols_brand, ['log_price'])\n # interaction_cols_brand_ad_price = add_interaction_terms(dummy_cols_brand, dummy_cols_ad, ['log_price'])"
  },
  {
    "objectID": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#compare_reg_models",
    "href": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#compare_reg_models",
    "title": "Homework 3",
    "section": "compare_reg_models()",
    "text": "compare_reg_models()\n\n\nCode\ndef compare_reg_models(models, assemblers, names=None):\n    \"\"\"\n    Produces a single formatted table comparing multiple regression models.\n\n    For each predictor (the union across models, ordered by first appearance), the table shows\n    the beta estimate (with significance stars) from each model (blank if not used).\n    For a predictor, if a model's outcome (model.getLabelCol()) starts with \"log\", the cell displays\n    both the beta and its exponential (separated by \" / \"), except when the predictor's name includes \"log_\".\n    (The intercept row does not display exp(.))\n\n    Additional rows for Intercept, Observations, R², and RMSE are appended.\n\n    The header's first column is labeled \"Predictor\", and subsequent columns are\n    \"y: [outcome] ([name])\" for each model.\n\n    The table is produced in grid format (with vertical lines). A dashed line (using '-' characters)\n    is inserted at the top, immediately after the header, and at the bottom.\n    Additionally, immediately after the Intercept row, the border line is replaced with one using '='\n    (to appear as, for example, \"+==============================================+==========================+...\").\n\n    Parameters:\n        models (list): List of fitted LinearRegression models.\n        assemblers (list): List of corresponding VectorAssembler objects.\n        names (list, optional): List of model names; defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing the combined regression table.\n    \"\"\"\n    # Default model names.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(models))]\n\n    # For each model, get outcome and determine if that model is log-transformed.\n    outcomes = [m.getLabelCol() for m in models]\n    is_log_flags = [out.lower().startswith(\"log\") for out in outcomes]\n\n    # Build an ordered union of predictors based on first appearance.\n    ordered_predictors = []\n    for assembler in assemblers:\n        for feat in assembler.getInputCols():\n            if feat not in ordered_predictors:\n                ordered_predictors.append(feat)\n\n    # Helper for significance stars.\n    def significance_stars(p):\n        if p is None:\n            return \"\"\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build rows for each predictor.\n    rows = []\n    for feat in ordered_predictors:\n        row = [feat]\n        for m, a, is_log in zip(models, assemblers, is_log_flags):\n            feats_model = a.getInputCols()\n            if feat in feats_model:\n                idx = feats_model.index(feat)\n                beta = m.coefficients.toArray()[idx]\n                p_val = m.summary.pValues[idx] if m.summary.pValues is not None else None\n                stars = significance_stars(p_val)\n                cell = f\"{beta:.3f}{stars}\"\n                # Only add exp(beta) if model is log and predictor name does NOT include \"log_\"\n                if is_log and (\"log_\" not in feat.lower()):\n                    cell += f\" / {np.exp(beta):,.3f}\"\n                row.append(cell)\n            else:\n                row.append(\"\")\n        rows.append(row)\n\n    # Build intercept row (do NOT compute exp(intercept)).\n    intercept_row = [\"Intercept\"]\n    for m in models:\n        std_all = np.array(m.summary.coefficientStandardErrors)\n        coeffs = m.coefficients.toArray()\n        if len(std_all) == len(coeffs) + 1:\n            intercept_p = m.summary.pValues[0] if m.summary.pValues is not None else None\n        else:\n            intercept_p = None\n        sig = significance_stars(intercept_p)\n        cell = f\"{m.intercept:.3f}{sig}\"\n        intercept_row.append(cell)\n    rows.append(intercept_row)\n\n    # Add Observations row.\n    obs_row = [\"Observations\"]\n    for m in models:\n        obs = m.summary.numInstances\n        obs_row.append(f\"{int(obs):,}\")\n    rows.append(obs_row)\n\n    # Add R² row.\n    r2_row = [\"R²\"]\n    for m in models:\n        r2_row.append(f\"{m.summary.r2:.3f}\")\n    rows.append(r2_row)\n\n    # Add RMSE row.\n    rmse_row = [\"RMSE\"]\n    for m in models:\n        rmse_row.append(f\"{m.summary.rootMeanSquaredError:.3f}\")\n    rows.append(rmse_row)\n\n    # Build header: first column \"Predictor\", then for each model: \"y: [outcome] ([name])\"\n    header = [\"Predictor\"]\n    for out, name in zip(outcomes, names):\n        header.append(f\"y: {out} ({name})\")\n\n    # Create table string using grid format.\n    table_str = tabulate(rows, headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(models))\n\n    # Split into lines.\n    lines = table_str.split(\"\\n\")\n\n    # Create a dashed line spanning the full width.\n    full_width = len(lines[0])\n    dash_line = '-' * full_width\n    # Create an equals line by replacing '-' with '='.\n    eq_line = dash_line.replace('-', '=')\n\n    # Insert a dashed line after the header row.\n    lines = table_str.split(\"\\n\")\n    # In grid format, header and separator are usually the first two lines.\n    lines.insert(2, dash_line)\n\n    # Insert an equals line after the Intercept row.\n    for i, line in enumerate(lines):\n        if line.startswith(\"|\") and \"Intercept\" in line:\n            if i+1 &lt; len(lines):\n                lines[i+1] = eq_line\n            break\n\n    # Add dashed lines at the very top and bottom.\n    final_table = dash_line + \"\\n\" + \"\\n\".join(lines) + \"\\n\" + dash_line\n\n    return final_table\n\n# Example usage:\n# print(compare_reg_models([model_1, model_2, model_3],\n#                          [assembler_1, assembler_2, assembler_3],\n#                          [\"Model 1\", \"Model 2\", \"Model 3\"]))"
  },
  {
    "objectID": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#compare_rmse",
    "href": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#compare_rmse",
    "title": "Homework 3",
    "section": "compare_rmse()",
    "text": "compare_rmse()\n\n\nCode\ndef compare_rmse(test_dfs, label_col, pred_col=\"prediction\", names=None):\n    \"\"\"\n    Computes and compares RMSE values for a list of test DataFrames.\n\n    For each DataFrame in test_dfs, this function calculates the RMSE between the actual outcome\n    (given by label_col) and the predicted value (given by pred_col, default \"prediction\"). It then\n    produces a formatted table where the first column header is empty and the first row's first cell is\n    \"RMSE\", with each model's RMSE in its own column.\n\n    Parameters:\n        test_dfs (list): List of test DataFrames.\n        label_col (str): The name of the outcome column.\n        pred_col (str, optional): The name of the prediction column (default \"prediction\").\n        names (list, optional): List of model names corresponding to the test DataFrames.\n                                Defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing a table that compares RMSE values for each test DataFrame,\n        with one model per column.\n    \"\"\"\n    # Set default model names if none provided.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(test_dfs))]\n\n    rmse_values = []\n    for df in test_dfs:\n        # Create a column for squared error.\n        df = df.withColumn(\"error_sq\", pow(col(label_col) - col(pred_col), 2))\n        # Calculate RMSE: square root of the mean squared error.\n        rmse = df.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n        rmse_values.append(rmse)\n\n    # Build a single row table: first cell \"RMSE\", then one cell per model with the RMSE value.\n    row = [\"RMSE\"] + [f\"{rmse:.3f}\" for rmse in rmse_values]\n\n    # Build header: first column header is empty, then model names.\n    header = [\"\"] + names\n\n    table_str = tabulate([row], headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(names))\n    return table_str\n\n# Example usage:\n# print(compare_rmse([dtest_1, dtest_2, dtest_3], \"log_sales\", names=[\"Model 1\", \"Model 2\", \"Model 3\"]))"
  },
  {
    "objectID": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#residual_plot",
    "href": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#residual_plot",
    "title": "Homework 3",
    "section": "residual_plot()",
    "text": "residual_plot()\n\n\nCode\ndef residual_plot(df, label_col, model_name):\n    \"\"\"\n    Generates a residual plot for a given test dataframe.\n\n    Parameters:\n        df (DataFrame): Spark DataFrame containing the test set with predictions.\n        label_col (str): The column name of the actual outcome variable.\n        title (str): The title for the residual plot.\n\n    Returns:\n        None (displays the plot)\n    \"\"\"\n    # Convert to Pandas DataFrame\n    df_pd = df.select([\"prediction\", label_col]).toPandas()\n    df_pd[\"residual\"] = df_pd[label_col] - df_pd[\"prediction\"]\n\n    # Scatter plot of residuals vs. predicted values\n    plt.scatter(df_pd[\"prediction\"], df_pd[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n    # Use LOWESS smoothing for trend line\n    smoothed = sm.nonparametric.lowess(df_pd[\"residual\"], df_pd[\"prediction\"])\n    plt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\n\n    # Add reference line at y=0\n    plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n\n    # Labels and title (model_name)\n    plt.xlabel(\"Predicted Values\")\n    plt.ylabel(\"Residuals\")\n    model_name = \"Residual Plot for \" + model_name\n    plt.title(model_name)\n\n    # Show plot\n    plt.show()\n\n# Example usage:\n# residual_plot(dtest_1, \"log_sales\", \"Model 1\")"
  },
  {
    "objectID": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#marginal_effects",
    "href": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#marginal_effects",
    "title": "Homework 3",
    "section": "marginal_effects()",
    "text": "marginal_effects()\n\n\nCode\ndef marginal_effects(model, means):\n    \"\"\"\n    Compute marginal effects for all predictors in a PySpark GeneralizedLinearRegression model (logit)\n    and return a formatted table with statistical significance and standard errors.\n\n    Parameters:\n        model: Fitted GeneralizedLinearRegression model (with binomial family and logit link).\n        means: List of mean values for the predictor variables.\n\n    Returns:\n        - A formatted string containing the marginal effects table.\n        - A Pandas DataFrame with marginal effects, standard errors, confidence intervals, and significance stars.\n    \"\"\"\n    global assembler_predictors  # Use the global assembler_predictors list\n\n    # Extract model coefficients, standard errors, and intercept\n    coeffs = np.array(model.coefficients)\n    std_errors = np.array(model.summary.coefficientStandardErrors)\n    intercept = model.intercept\n\n    # Compute linear combination of means and coefficients (XB)\n    XB = np.dot(means, coeffs) + intercept\n\n    # Compute derivative of logistic function (G'(XB))\n    G_prime_XB = np.exp(XB) / ((1 + np.exp(XB)) ** 2)\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Create lists to store results\n    results = []\n    df_results = []  # For Pandas DataFrame\n\n    for i, predictor in enumerate(assembler_predictors):\n        # Compute marginal effect\n        marginal_effect = G_prime_XB * coeffs[i]\n\n        # Compute standard error of the marginal effect\n        std_error = G_prime_XB * std_errors[i]\n\n        # Compute z-score and p-value\n        z_score = marginal_effect / std_error if std_error != 0 else np.nan\n        p_value = 2 * (1 - norm.cdf(abs(z_score))) if not np.isnan(z_score) else np.nan\n\n        # Compute confidence interval (95%)\n        ci_lower = marginal_effect - 1.96 * std_error\n        ci_upper = marginal_effect + 1.96 * std_error\n\n        # Append results for table formatting\n        results.append([\n            predictor,\n            f\"{marginal_effect: .6f}\",\n            significance_stars(p_value),\n            f\"{std_error: .6f}\",\n            f\"{ci_lower: .6f}\",\n            f\"{ci_upper: .6f}\"\n        ])\n\n        # Append results for Pandas DataFrame\n        df_results.append({\n            \"Variable\": predictor,\n            \"Marginal Effect\": marginal_effect,\n            \"Significance\": significance_stars(p_value),\n            \"Std. Error\": std_error,\n            \"95% CI Lower\": ci_lower,\n            \"95% CI Upper\": ci_upper\n        })\n\n    # Convert results to formatted table\n    table_str = tabulate(results, headers=[\"Variable\", \"Marginal Effect\", \"Significance\", \"Std. Error\", \"95% CI Lower\", \"95% CI Upper\"],\n                         tablefmt=\"pretty\", colalign=(\"left\", \"decimal\", \"left\", \"decimal\", \"decimal\", \"decimal\"))\n\n    # Convert results to Pandas DataFrame\n    df_results = pd.DataFrame(df_results)\n\n    return table_str, df_results\n\n# Example usage:\n# means = [0.5, 30]  # Mean values for x1 and x2\n# assembler_predictors = ['x1', 'x2']  # Define globally before calling the function\n# table_output, df_output = marginal_effects(fitted_model, means)\n# print(table_output)\n# display(df_output)"
  },
  {
    "objectID": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#adding-variables-using-pandas",
    "href": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#adding-variables-using-pandas",
    "title": "Homework 3",
    "section": "Adding Variables using Pandas",
    "text": "Adding Variables using Pandas\n\nhomes['log_value'] = np.log(homes['VALUE'])\nhomes = homes[homes['ZINC2']&gt;0]\nhomes['log_zinc2'] = np.log(homes['ZINC2'])\nhomes['GT20DWN'] = np.where( (homes['LPRICE'] - homes['AMMORT'])/homes['LPRICE'] &gt; .2, 1, 0 )\n\n\nhomes.columns\n\nIndex(['LPRICE', 'VALUE', 'STATE', 'METRO', 'ZINC2', 'HHGRAD', 'BATHS',\n       'BEDRMS', 'PER', 'ZADULT', 'NUNITS', 'EAPTBL', 'ECOM1', 'ECOM2',\n       'EGREEN', 'EJUNK', 'ELOW1', 'ESFD', 'ETRANS', 'EABAN', 'HOWH', 'HOWN',\n       'ODORA', 'STRNA', 'AMMORT', 'INTW', 'MATBUY', 'DWNPAY', 'FRSTHO',\n       'log_value', 'log_zinc2', 'GT20DWN'],\n      dtype='object')\n\n\n\nhomes = homes[['LPRICE', 'VALUE', 'log_value', 'STATE', 'METRO', 'log_zinc2', 'HHGRAD', 'BATHS',\n       'BEDRMS', 'PER', 'ZADULT', 'NUNITS', 'EAPTBL', 'ECOM1', 'ECOM2',\n       'EGREEN', 'EJUNK', 'ELOW1', 'ESFD', 'ETRANS', 'EABAN', 'HOWH', 'HOWN',\n       'ODORA', 'STRNA', 'AMMORT', 'INTW', 'MATBUY', 'DWNPAY', 'FRSTHO',\n       'GT20DWN']]\n\n\nhomes[['LPRICE', 'AMMORT', 'GT20DWN']]\n\n\n    \n\n\n\n\n\n\nLPRICE\nAMMORT\nGT20DWN\n\n\n\n\n0\n85000\n50000\n1\n\n\n1\n76500\n70000\n0\n\n\n2\n93900\n117000\n0\n\n\n3\n100000\n100000\n0\n\n\n4\n100000\n100000\n0\n\n\n...\n...\n...\n...\n\n\n15560\n109000\n109000\n0\n\n\n15561\n105000\n105000\n0\n\n\n15562\n130000\n181000\n0\n\n\n15563\n13000\n180000\n0\n\n\n15564\n68000\n68000\n0\n\n\n\n\n15454 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nhomes[['DWNPAY', 'INTW', 'MATBUY']]\n\n\n    \n\n\n\n\n\n\nDWNPAY\nINTW\nMATBUY\n\n\n\n\n0\nother\n9\n1\n\n\n1\nother\n5\n1\n\n\n2\nother\n6\n0\n\n\n3\nprev home\n7\n1\n\n\n4\nother\n4\n1\n\n\n...\n...\n...\n...\n\n\n15560\nother\n8\n1\n\n\n15561\nother\n5\n1\n\n\n15562\nother\n7\n0\n\n\n15563\nprev home\n7\n1\n\n\n15564\nother\n6\n1\n\n\n\n\n15454 rows × 3 columns"
  },
  {
    "objectID": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#pyspark-dataframe",
    "href": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#pyspark-dataframe",
    "title": "Homework 3",
    "section": "PySpark DataFrame",
    "text": "PySpark DataFrame\n\ndf = spark.createDataFrame(homes)"
  },
  {
    "objectID": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#train-test-split",
    "href": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#train-test-split",
    "title": "Homework 3",
    "section": "Train-Test Split",
    "text": "Train-Test Split\n\ndtrain, dtest = df.randomSplit([0.7, 0.3], seed = 1234)"
  },
  {
    "objectID": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#adding-dummies",
    "href": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#adding-dummies",
    "title": "Homework 3",
    "section": "Adding Dummies",
    "text": "Adding Dummies\n\nhomes.info()  # df.printSchema()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 15454 entries, 0 to 15564\nData columns (total 31 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   LPRICE     15454 non-null  int64  \n 1   VALUE      15454 non-null  int64  \n 2   log_value  15454 non-null  float64\n 3   STATE      15454 non-null  object \n 4   METRO      15454 non-null  object \n 5   log_zinc2  15454 non-null  float64\n 6   HHGRAD     15454 non-null  object \n 7   BATHS      15454 non-null  int64  \n 8   BEDRMS     15454 non-null  int64  \n 9   PER        15454 non-null  int64  \n 10  ZADULT     15454 non-null  int64  \n 11  NUNITS     15454 non-null  int64  \n 12  EAPTBL     15454 non-null  int64  \n 13  ECOM1      15454 non-null  int64  \n 14  ECOM2      15454 non-null  int64  \n 15  EGREEN     15454 non-null  int64  \n 16  EJUNK      15454 non-null  int64  \n 17  ELOW1      15454 non-null  int64  \n 18  ESFD       15454 non-null  int64  \n 19  ETRANS     15454 non-null  int64  \n 20  EABAN      15454 non-null  int64  \n 21  HOWH       15454 non-null  object \n 22  HOWN       15454 non-null  object \n 23  ODORA      15454 non-null  int64  \n 24  STRNA      15454 non-null  int64  \n 25  AMMORT     15454 non-null  int64  \n 26  INTW       15454 non-null  int64  \n 27  MATBUY     15454 non-null  int64  \n 28  DWNPAY     15454 non-null  object \n 29  FRSTHO     15454 non-null  int64  \n 30  GT20DWN    15454 non-null  int64  \ndtypes: float64(2), int64(23), object(6)\nmemory usage: 3.8+ MB\n\n\n\nhomes['STATE'].unique()\n\narray(['GA', 'OH', 'CO', 'CT', 'IN', 'LA', 'OK', 'PA', 'TX', 'WA', 'IL',\n       'MO', 'CA'], dtype=object)\n\n\n\nhomes['METRO'].unique()\n\narray(['rural', 'urban'], dtype=object)\n\n\n\nhomes['HHGRAD'].unique()\n\narray(['No HS', 'HS Grad', 'Bach', 'Assoc', 'Grad'], dtype=object)\n\n\n\nhomes['HOWH'].unique()\n\narray(['good', 'bad'], dtype=object)\n\n\n\nhomes['HOWN'].unique()\n\narray(['good', 'bad'], dtype=object)\n\n\n\nhomes['DWNPAY'].unique()\n\narray(['other', 'prev home'], dtype=object)\n\n\n\ndummy_cols_state, ref_category_state = add_dummy_variables('STATE', 0)\ndummy_cols_metro, ref_category_metro = add_dummy_variables('METRO', 0)\ndummy_cols_hhgrad, ref_category_hhgrad = add_dummy_variables('HHGRAD', 0)\ndummy_cols_howh, ref_category_howh = add_dummy_variables('HOWH', 0)\ndummy_cols_hown, ref_category_hown = add_dummy_variables('HOWN', 0)\ndummy_cols_dwnpay, ref_category_dwnpay = add_dummy_variables('DWNPAY', 0)\n\nReference category (dummy omitted): CA\nReference category (dummy omitted): rural\nReference category (dummy omitted): Assoc\nReference category (dummy omitted): bad\nReference category (dummy omitted): bad\nReference category (dummy omitted): other"
  },
  {
    "objectID": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#adding-interactions",
    "href": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#adding-interactions",
    "title": "Homework 3",
    "section": "Adding Interactions",
    "text": "Adding Interactions\n\ninteraction_cols_FRSTHO_BEDRMS = add_interaction_terms(['FRSTHO'], ['BEDRMS'])\n\n\ninteraction_cols_FRSTHO_BEDRMS\n\n['FRSTHO_*_BEDRMS']\n\n\n\ndtrain.printSchema()\n\nroot\n |-- LPRICE: long (nullable = true)\n |-- VALUE: long (nullable = true)\n |-- log_value: double (nullable = true)\n |-- STATE: string (nullable = true)\n |-- METRO: string (nullable = true)\n |-- log_zinc2: double (nullable = true)\n |-- HHGRAD: string (nullable = true)\n |-- BATHS: long (nullable = true)\n |-- BEDRMS: long (nullable = true)\n |-- PER: long (nullable = true)\n |-- ZADULT: long (nullable = true)\n |-- NUNITS: long (nullable = true)\n |-- EAPTBL: long (nullable = true)\n |-- ECOM1: long (nullable = true)\n |-- ECOM2: long (nullable = true)\n |-- EGREEN: long (nullable = true)\n |-- EJUNK: long (nullable = true)\n |-- ELOW1: long (nullable = true)\n |-- ESFD: long (nullable = true)\n |-- ETRANS: long (nullable = true)\n |-- EABAN: long (nullable = true)\n |-- HOWH: string (nullable = true)\n |-- HOWN: string (nullable = true)\n |-- ODORA: long (nullable = true)\n |-- STRNA: long (nullable = true)\n |-- AMMORT: long (nullable = true)\n |-- INTW: long (nullable = true)\n |-- MATBUY: long (nullable = true)\n |-- DWNPAY: string (nullable = true)\n |-- FRSTHO: long (nullable = true)\n |-- GT20DWN: long (nullable = true)\n |-- STATE_CA: integer (nullable = false)\n |-- STATE_CO: integer (nullable = false)\n |-- STATE_CT: integer (nullable = false)\n |-- STATE_GA: integer (nullable = false)\n |-- STATE_IL: integer (nullable = false)\n |-- STATE_IN: integer (nullable = false)\n |-- STATE_LA: integer (nullable = false)\n |-- STATE_MO: integer (nullable = false)\n |-- STATE_OH: integer (nullable = false)\n |-- STATE_OK: integer (nullable = false)\n |-- STATE_PA: integer (nullable = false)\n |-- STATE_TX: integer (nullable = false)\n |-- STATE_WA: integer (nullable = false)\n |-- METRO_rural: integer (nullable = false)\n |-- METRO_urban: integer (nullable = false)\n |-- HHGRAD_Assoc: integer (nullable = false)\n |-- HHGRAD_Bach: integer (nullable = false)\n |-- HHGRAD_Grad: integer (nullable = false)\n |-- HHGRAD_HS_Grad: integer (nullable = false)\n |-- HHGRAD_No_HS: integer (nullable = false)\n |-- HOWH_bad: integer (nullable = false)\n |-- HOWH_good: integer (nullable = false)\n |-- HOWN_bad: integer (nullable = false)\n |-- HOWN_good: integer (nullable = false)\n |-- DWNPAY_other: integer (nullable = false)\n |-- DWNPAY_prev_home: integer (nullable = false)\n |-- FRSTHO_*_BEDRMS: double (nullable = true)"
  },
  {
    "objectID": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#correlation-heatmap",
    "href": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#correlation-heatmap",
    "title": "Homework 3",
    "section": "Correlation Heatmap",
    "text": "Correlation Heatmap\n\ndf_corr = homes.drop(['LPRICE', 'VALUE'], axis=1)\n\n# Identify categorical columns (object or category dtype)\ncategorical_cols = df_corr.select_dtypes(include=['object', 'category']).columns.tolist()\n\n# Convert categorical variables to dummies\ndf_corr = pd.get_dummies(df_corr, columns=categorical_cols, drop_first=True)\n\n# Compute correlation matrix\ncorr_matrix = df_corr.corr()\n\n# 3. Correlation heatmap using matplotlib\nfig, ax = plt.subplots(figsize=(12, 10))\ncax = ax.imshow(corr_matrix.values, aspect='auto')\nfig.colorbar(cax, ax=ax)\nax.set_xticks(range(len(corr_matrix.columns)))\nax.set_yticks(range(len(corr_matrix.columns)))\nax.set_xticklabels(corr_matrix.columns, rotation=90, fontsize=6)\nax.set_yticklabels(corr_matrix.columns, fontsize=6)\nplt.title('How Are Variables Correlated?')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe dummy DWNPAY_prev_home (whether a household tapped equity from a prior residence to fund its down payment) is strongly negatively correlated with FRSTHO (the dummy for first-time homebuyers).\n\nBecause first-time buyers have no previous equity, they may fund down payments almost entirely from savings or gifts—the strong negative correlation between DWNPAY_prev_home and FRSTHO.\nOnce buyers move into a second home, they can use rolled-over equity from their prior property, lowering their upfront cash requirement.\n\nThe variable AMMORT (the size of the initial mortgage loan taken out when a home was acquired) is strongly positively correlated with BATHS (the count of full bathrooms in the unit)\n\nHome buyers borrow more to secure properties with extra baths, whether for growing families or simply higher‐end homes."
  },
  {
    "objectID": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#marginal-effects",
    "href": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#marginal-effects",
    "title": "Homework 3",
    "section": "Marginal Effects",
    "text": "Marginal Effects\n\n# Compute means\nmeans_df = dtrain_3.select([mean(col).alias(col) for col in assembler_predictors])\n\n# Collect the results as a list\nmeans = means_df.collect()[0]\nmeans_list = [means[col] for col in assembler_predictors]\n\ntable_output, df_ME = marginal_effects(model_3, means_list) # Instead of mean values, some other representative values can also be chosen.\nprint(table_output)\n\n+------------------+-----------------+--------------+------------+--------------+--------------+\n| Variable         | Marginal Effect | Significance | Std. Error | 95% CI Lower | 95% CI Upper |\n+------------------+-----------------+--------------+------------+--------------+--------------+\n| log_zinc2        |         -0.0125 | **           |     0.0051 |      -0.0225 |      -0.0025 |\n| BATHS            |          0.0742 | ***          |     0.0076 |       0.0592 |       0.0892 |\n| BEDRMS           |          0.0060 |              |     0.0067 |      -0.0071 |       0.0191 |\n| PER              |         -0.0211 | ***          |     0.0043 |      -0.0295 |      -0.0128 |\n| ZADULT           |          0.0012 |              |     0.0074 |      -0.0133 |       0.0158 |\n| NUNITS           |          0.0005 |              |     0.0003 |      -0.0001 |       0.0012 |\n| EAPTBL           |         -0.0086 |              |     0.0166 |      -0.0412 |       0.0240 |\n| ECOM1            |         -0.0358 | ***          |     0.0137 |      -0.0626 |      -0.0090 |\n| ECOM2            |         -0.0323 |              |     0.0369 |      -0.1046 |       0.0400 |\n| EGREEN           |         -0.0014 |              |     0.0093 |      -0.0195 |       0.0167 |\n| EJUNK            |          0.0155 |              |     0.0375 |      -0.0580 |       0.0889 |\n| ELOW1            |          0.0030 |              |     0.0154 |      -0.0273 |       0.0332 |\n| ESFD             |         -0.0424 | **           |     0.0193 |      -0.0802 |      -0.0047 |\n| ETRANS           |         -0.0200 |              |     0.0179 |      -0.0551 |       0.0151 |\n| EABAN            |         -0.0122 |              |     0.0272 |      -0.0655 |       0.0411 |\n| ODORA            |          0.0264 |              |     0.0227 |      -0.0181 |       0.0710 |\n| STRNA            |         -0.0323 | ***          |     0.0111 |      -0.0541 |      -0.0105 |\n| INTW             |         -0.0177 | ***          |     0.0032 |      -0.0240 |      -0.0114 |\n| MATBUY           |          0.0535 | ***          |     0.0091 |       0.0356 |       0.0713 |\n| FRSTHO           |         -0.0914 | ***          |     0.0120 |      -0.1150 |      -0.0679 |\n| STATE_CO         |         -0.0443 | **           |     0.0195 |      -0.0826 |      -0.0061 |\n| STATE_CT         |          0.1229 | ***          |     0.0203 |       0.0831 |       0.1627 |\n| STATE_GA         |         -0.1065 | ***          |     0.0212 |      -0.1482 |      -0.0649 |\n| STATE_IL         |          0.0567 |              |     0.0388 |      -0.0194 |       0.1328 |\n| STATE_IN         |         -0.0009 |              |     0.0207 |      -0.0415 |       0.0396 |\n| STATE_LA         |          0.0653 | ***          |     0.0241 |       0.0180 |       0.1125 |\n| STATE_MO         |          0.0443 | **           |     0.0221 |       0.0010 |       0.0875 |\n| STATE_OH         |          0.0909 | ***          |     0.0213 |       0.0492 |       0.1326 |\n| STATE_OK         |         -0.0474 | **           |     0.0226 |      -0.0918 |      -0.0031 |\n| STATE_PA         |          0.0606 | ***          |     0.0225 |       0.0165 |       0.1048 |\n| STATE_TX         |         -0.0137 |              |     0.0235 |      -0.0597 |       0.0324 |\n| STATE_WA         |          0.0275 |              |     0.0202 |      -0.0121 |       0.0671 |\n| METRO_urban      |         -0.0131 |              |     0.0126 |      -0.0378 |       0.0116 |\n| HHGRAD_Bach      |          0.0393 | **           |     0.0153 |       0.0093 |       0.0694 |\n| HHGRAD_Grad      |          0.0753 | ***          |     0.0169 |       0.0422 |       0.1084 |\n| HHGRAD_HS_Grad   |         -0.0112 |              |     0.0149 |      -0.0403 |       0.0179 |\n| HHGRAD_No_HS     |         -0.0182 |              |     0.0228 |      -0.0629 |       0.0265 |\n| HOWH_good        |         -0.0277 |              |     0.0186 |      -0.0641 |       0.0086 |\n| HOWN_good        |          0.0403 | **           |     0.0158 |       0.0094 |       0.0713 |\n| DWNPAY_prev_home |          0.1449 | ***          |     0.0112 |       0.1230 |       0.1668 |\n+------------------+-----------------+--------------+------------+--------------+--------------+"
  },
  {
    "objectID": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#marginal-effects-1",
    "href": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#marginal-effects-1",
    "title": "Homework 3",
    "section": "Marginal Effects",
    "text": "Marginal Effects\n\n# Compute means\nmeans_df = dtrain_4.select([mean(col).alias(col) for col in assembler_predictors])\n\n# Collect the results as a list\nmeans = means_df.collect()[0]\nmeans_list = [means[col] for col in assembler_predictors]\n\ntable_output, df_ME = marginal_effects(model_4, means_list) # Instead of mean values, some other representative values can also be chosen.\nprint(table_output)\n\n+------------------+-----------------+--------------+------------+--------------+--------------+\n| Variable         | Marginal Effect | Significance | Std. Error | 95% CI Lower | 95% CI Upper |\n+------------------+-----------------+--------------+------------+--------------+--------------+\n| log_zinc2        |         -0.0126 | **           |     0.0051 |      -0.0226 |      -0.0026 |\n| BATHS            |          0.0738 | ***          |     0.0077 |       0.0588 |       0.0888 |\n| BEDRMS           |          0.0087 |              |     0.0075 |      -0.0061 |       0.0235 |\n| PER              |         -0.0212 | ***          |     0.0043 |      -0.0296 |      -0.0128 |\n| ZADULT           |          0.0014 |              |     0.0074 |      -0.0132 |       0.0159 |\n| NUNITS           |          0.0005 |              |     0.0003 |      -0.0001 |       0.0012 |\n| EAPTBL           |         -0.0089 |              |     0.0166 |      -0.0414 |       0.0237 |\n| ECOM1            |         -0.0359 | ***          |     0.0137 |      -0.0627 |      -0.0092 |\n| ECOM2            |         -0.0324 |              |     0.0369 |      -0.1046 |       0.0399 |\n| EGREEN           |         -0.0015 |              |     0.0093 |      -0.0196 |       0.0167 |\n| EJUNK            |          0.0154 |              |     0.0375 |      -0.0580 |       0.0889 |\n| ELOW1            |          0.0029 |              |     0.0154 |      -0.0273 |       0.0331 |\n| ESFD             |         -0.0425 | **           |     0.0193 |      -0.0802 |      -0.0048 |\n| ETRANS           |         -0.0200 |              |     0.0179 |      -0.0551 |       0.0151 |\n| EABAN            |         -0.0123 |              |     0.0272 |      -0.0655 |       0.0410 |\n| ODORA            |          0.0261 |              |     0.0227 |      -0.0185 |       0.0706 |\n| STRNA            |         -0.0322 | ***          |     0.0111 |      -0.0540 |      -0.0104 |\n| INTW             |         -0.0177 | ***          |     0.0032 |      -0.0240 |      -0.0114 |\n| MATBUY           |          0.0534 | ***          |     0.0091 |       0.0355 |       0.0713 |\n| FRSTHO           |         -0.0624 |              |     0.0389 |      -0.1385 |       0.0138 |\n| STATE_CO         |         -0.0448 | **           |     0.0195 |      -0.0831 |      -0.0065 |\n| STATE_CT         |          0.1226 | ***          |     0.0203 |       0.0828 |       0.1624 |\n| STATE_GA         |         -0.1068 | ***          |     0.0213 |      -0.1485 |      -0.0652 |\n| STATE_IL         |          0.0563 |              |     0.0388 |      -0.0198 |       0.1324 |\n| STATE_IN         |         -0.0011 |              |     0.0207 |      -0.0417 |       0.0394 |\n| STATE_LA         |          0.0651 | ***          |     0.0241 |       0.0178 |       0.1124 |\n| STATE_MO         |          0.0439 | **           |     0.0221 |       0.0006 |       0.0871 |\n| STATE_OH         |          0.0907 | ***          |     0.0213 |       0.0490 |       0.1324 |\n| STATE_OK         |         -0.0475 | **           |     0.0226 |      -0.0919 |      -0.0032 |\n| STATE_PA         |          0.0603 | ***          |     0.0225 |       0.0162 |       0.1044 |\n| STATE_TX         |         -0.0136 |              |     0.0235 |      -0.0597 |       0.0325 |\n| STATE_WA         |          0.0273 |              |     0.0202 |      -0.0123 |       0.0669 |\n| METRO_urban      |         -0.0133 |              |     0.0126 |      -0.0381 |       0.0114 |\n| HHGRAD_Bach      |          0.0391 | **           |     0.0153 |       0.0090 |       0.0691 |\n| HHGRAD_Grad      |          0.0751 | ***          |     0.0169 |       0.0420 |       0.1082 |\n| HHGRAD_HS_Grad   |         -0.0112 |              |     0.0149 |      -0.0403 |       0.0179 |\n| HHGRAD_No_HS     |         -0.0182 |              |     0.0228 |      -0.0629 |       0.0266 |\n| HOWH_good        |         -0.0274 |              |     0.0186 |      -0.0638 |       0.0089 |\n| HOWN_good        |          0.0402 | **           |     0.0158 |       0.0093 |       0.0712 |\n| DWNPAY_prev_home |          0.1445 | ***          |     0.0112 |       0.1225 |       0.1665 |\n| FRSTHO_*_BEDRMS  |         -0.0093 |              |     0.0119 |      -0.0326 |       0.0140 |\n+------------------+-----------------+--------------+------------+--------------+--------------+"
  },
  {
    "objectID": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#over-175k-value",
    "href": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#over-175k-value",
    "title": "Homework 3",
    "section": "Over 175k Value",
    "text": "Over 175k Value\n\n# df = spark.createDataFrame(homes)\ndf = df.filter(col(\"VALUE\") &gt;= 175000)\n\ndtrain, dtest = df.randomSplit([0.7, 0.3], seed = 1234)\n\ndummy_cols_state, ref_category_state = add_dummy_variables('STATE', 0)\ndummy_cols_metro, ref_category_metro = add_dummy_variables('METRO', 0)\ndummy_cols_hhgrad, ref_category_hhgrad = add_dummy_variables('HHGRAD', 0)\ndummy_cols_howh, ref_category_howh = add_dummy_variables('HOWH', 0)\ndummy_cols_hown, ref_category_hown = add_dummy_variables('HOWN', 0)\ndummy_cols_dwnpay, ref_category_dwnpay = add_dummy_variables('DWNPAY', 0)\n\nReference category (dummy omitted): CA\nReference category (dummy omitted): rural\nReference category (dummy omitted): Assoc\nReference category (dummy omitted): bad\nReference category (dummy omitted): bad\nReference category (dummy omitted): other\n\n\n\n# assembling predictors\nconti_cols = [\n 'log_zinc2',\n 'BATHS',\n 'BEDRMS',\n 'PER',\n 'ZADULT',\n 'NUNITS',\n 'EAPTBL',\n 'ECOM1',\n 'ECOM2',\n 'EGREEN',\n 'EJUNK',\n 'ELOW1',\n 'ESFD',\n 'ETRANS',\n 'EABAN',\n 'ODORA',\n 'STRNA',\n 'INTW',\n 'MATBUY',\n 'FRSTHO']\n\nassembler_predictors = (\n    conti_cols +\n    dummy_cols_state +\n    dummy_cols_metro +\n    dummy_cols_hhgrad +\n    dummy_cols_howh +\n    dummy_cols_hown +\n    dummy_cols_dwnpay\n)\n\nassembler_5 = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\n\ndtrain_5 = assembler_5.transform(dtrain)\ndtest_5  = assembler_5.transform(dtest)\n\n# training the model\nmodel_5 = (\n    GeneralizedLinearRegression(featuresCol=\"predictors\",\n                                labelCol=\"GT20DWN\",\n                                family=\"binomial\",\n                                link=\"logit\")\n    .fit(dtrain_5)\n)\n# making prediction\ndtrain_5 = model_5.transform(dtrain_5)\ndtest_5 = model_5.transform(dtest_5)\n\n# makting regression table\nmodel_5.summary\n\nCoefficients:\n         Feature Estimate Std Error T Value P Value\n     (Intercept)  -0.9444    0.4785 -1.9736  0.0484\n       log_zinc2  -0.0414    0.0341 -1.2147  0.2245\n           BATHS   0.3740    0.0488  7.6637  0.0000\n          BEDRMS  -0.0106    0.0448 -0.2366  0.8130\n             PER  -0.0848    0.0285 -2.9738  0.0029\n          ZADULT  -0.0055    0.0504 -0.1097  0.9126\n          NUNITS   0.0003    0.0053  0.0629  0.9498\n          EAPTBL   0.1634    0.1285  1.2720  0.2034\n           ECOM1   0.0229    0.1008  0.2270  0.8204\n           ECOM2  -0.3561    0.3508 -1.0151  0.3100\n          EGREEN   0.0111    0.0625  0.1771  0.8594\n           EJUNK  -0.3499    0.3340 -1.0478  0.2947\n           ELOW1  -0.0640    0.1113 -0.5753  0.5651\n            ESFD  -0.2295    0.1661 -1.3821  0.1670\n          ETRANS  -0.1628    0.1377 -1.1824  0.2371\n           EABAN  -0.4304    0.2659 -1.6187  0.1055\n           ODORA   0.3021    0.1704  1.7735  0.0761\n           STRNA  -0.2275    0.0815 -2.7911  0.0053\n            INTW  -0.0823    0.0259 -3.1832  0.0015\n          MATBUY   0.5120    0.0619  8.2768  0.0000\n          FRSTHO  -0.4224    0.0910 -4.6425  0.0000\n        STATE_CO  -0.1388    0.1055 -1.3151  0.1885\n        STATE_CT   0.6874    0.1167  5.8879  0.0000\n        STATE_GA  -0.3892    0.1298 -2.9974  0.0027\n        STATE_IL   0.1701    0.3273  0.5199  0.6031\n        STATE_IN   0.1883    0.1450  1.2984  0.1941\n        STATE_LA   0.7247    0.1839  3.9407  0.0001\n        STATE_MO   0.5225    0.1440  3.6286  0.0003\n        STATE_OH   0.7060    0.1502  4.7010  0.0000\n        STATE_OK   0.2419    0.1920  1.2598  0.2077\n        STATE_PA   0.6531    0.1790  3.6481  0.0003\n        STATE_TX   0.2097    0.2249  0.9323  0.3512\n        STATE_WA   0.0200    0.1117  0.1792  0.8578\n     METRO_urban  -0.0247    0.0936 -0.2639  0.7918\n     HHGRAD_Bach   0.2805    0.1079  2.5987  0.0094\n     HHGRAD_Grad   0.3803    0.1151  3.3050  0.0009\n  HHGRAD_HS_Grad   0.0370    0.1096  0.3371  0.7360\n    HHGRAD_No_HS  -0.1526    0.1968 -0.7754  0.4381\n       HOWH_good  -0.1439    0.1509 -0.9532  0.3405\n       HOWN_good   0.3543    0.1234  2.8711  0.0041\nDWNPAY_prev_home   0.7196    0.0751  9.5884  0.0000\n\n(Dispersion parameter for binomial family taken to be 1.0000)\n    Null deviance: 7217.9381 on 5475 degrees of freedom\nResidual deviance: 6487.1260 on 5475 degrees of freedom\nAIC: 6569.1260\n\n\n\nMarginal Effects\n\n# Compute means\nmeans_df = dtrain_5.select([mean(col).alias(col) for col in assembler_predictors])\n\n# Collect the results as a list\nmeans = means_df.collect()[0]\nmeans_list = [means[col] for col in assembler_predictors]\n\ntable_output, df_ME = marginal_effects(model_5, means_list) # Instead of mean values, some other representative values can also be chosen.\nprint(table_output)\n\n+------------------+-----------------+--------------+------------+--------------+--------------+\n| Variable         | Marginal Effect | Significance | Std. Error | 95% CI Lower | 95% CI Upper |\n+------------------+-----------------+--------------+------------+--------------+--------------+\n| log_zinc2        |         -0.0093 |              |     0.0077 |      -0.0243 |       0.0057 |\n| BATHS            |          0.0840 | ***          |     0.0110 |       0.0625 |       0.1054 |\n| BEDRMS           |         -0.0024 |              |     0.0101 |      -0.0221 |       0.0173 |\n| PER              |         -0.0190 | ***          |     0.0064 |      -0.0316 |      -0.0065 |\n| ZADULT           |         -0.0012 |              |     0.0113 |      -0.0234 |       0.0210 |\n| NUNITS           |          0.0001 |              |     0.0012 |      -0.0023 |       0.0024 |\n| EAPTBL           |          0.0367 |              |     0.0288 |      -0.0198 |       0.0932 |\n| ECOM1            |          0.0051 |              |     0.0226 |      -0.0392 |       0.0495 |\n| ECOM2            |         -0.0799 |              |     0.0788 |      -0.2343 |       0.0744 |\n| EGREEN           |          0.0025 |              |     0.0140 |      -0.0250 |       0.0300 |\n| EJUNK            |         -0.0786 |              |     0.0750 |      -0.2255 |       0.0684 |\n| ELOW1            |         -0.0144 |              |     0.0250 |      -0.0634 |       0.0346 |\n| ESFD             |         -0.0515 |              |     0.0373 |      -0.1246 |       0.0215 |\n| ETRANS           |         -0.0366 |              |     0.0309 |      -0.0972 |       0.0240 |\n| EABAN            |         -0.0966 |              |     0.0597 |      -0.2137 |       0.0204 |\n| ODORA            |          0.0678 | *            |     0.0382 |      -0.0071 |       0.1428 |\n| STRNA            |         -0.0511 | ***          |     0.0183 |      -0.0869 |      -0.0152 |\n| INTW             |         -0.0185 | ***          |     0.0058 |      -0.0299 |      -0.0071 |\n| MATBUY           |          0.1150 | ***          |     0.0139 |       0.0877 |       0.1422 |\n| FRSTHO           |         -0.0948 | ***          |     0.0204 |      -0.1349 |      -0.0548 |\n| STATE_CO         |         -0.0312 |              |     0.0237 |      -0.0776 |       0.0153 |\n| STATE_CT         |          0.1543 | ***          |     0.0262 |       0.1030 |       0.2057 |\n| STATE_GA         |         -0.0874 | ***          |     0.0292 |      -0.1445 |      -0.0302 |\n| STATE_IL         |          0.0382 |              |     0.0735 |      -0.1058 |       0.1822 |\n| STATE_IN         |          0.0423 |              |     0.0326 |      -0.0215 |       0.1061 |\n| STATE_LA         |          0.1627 | ***          |     0.0413 |       0.0818 |       0.2436 |\n| STATE_MO         |          0.1173 | ***          |     0.0323 |       0.0539 |       0.1807 |\n| STATE_OH         |          0.1585 | ***          |     0.0337 |       0.0924 |       0.2246 |\n| STATE_OK         |          0.0543 |              |     0.0431 |      -0.0302 |       0.1388 |\n| STATE_PA         |          0.1466 | ***          |     0.0402 |       0.0678 |       0.2254 |\n| STATE_TX         |          0.0471 |              |     0.0505 |      -0.0519 |       0.1460 |\n| STATE_WA         |          0.0045 |              |     0.0251 |      -0.0447 |       0.0536 |\n| METRO_urban      |         -0.0055 |              |     0.0210 |      -0.0468 |       0.0357 |\n| HHGRAD_Bach      |          0.0630 | ***          |     0.0242 |       0.0155 |       0.1105 |\n| HHGRAD_Grad      |          0.0854 | ***          |     0.0258 |       0.0347 |       0.1360 |\n| HHGRAD_HS_Grad   |          0.0083 |              |     0.0246 |      -0.0400 |       0.0565 |\n| HHGRAD_No_HS     |         -0.0343 |              |     0.0442 |      -0.1209 |       0.0523 |\n| HOWH_good        |         -0.0323 |              |     0.0339 |      -0.0987 |       0.0341 |\n| HOWN_good        |          0.0796 | ***          |     0.0277 |       0.0252 |       0.1339 |\n| DWNPAY_prev_home |          0.1616 | ***          |     0.0169 |       0.1285 |       0.1946 |\n+------------------+-----------------+--------------+------------+--------------+--------------+\n\n\n\n\nClassification\n\n# Filter training data for atRisk == 1 and atRisk == 0\npdf = dtrain_5.select(\"prediction\", \"GT20DWN\").toPandas()\n\ntrain_true = pdf[pdf[\"GT20DWN\"] == 1]\ntrain_false = pdf[pdf[\"GT20DWN\"] == 0]\n\n# Create the first density plot\nplt.figure(figsize=(8, 6))\nsns.kdeplot(train_true[\"prediction\"], label=\"TRUE\", color=\"red\", fill=True)\nsns.kdeplot(train_false[\"prediction\"], label=\"FALSE\", color=\"blue\", fill=True)\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Density\")\nplt.title(\"Density Plot of Predictions\")\nplt.legend(title=\"GT20DWN\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Define threshold for vertical line\nthreshold = 0.3767  # Replace with actual value\n\n# Create the second density plot with vertical line\nplt.figure(figsize=(8, 6))\nsns.kdeplot(train_true[\"prediction\"], label=\"TRUE\", color=\"red\", fill=True)\nsns.kdeplot(train_false[\"prediction\"], label=\"FALSE\", color=\"blue\", fill=True)\nplt.axvline(x=threshold, color=\"blue\", linestyle=\"dashed\", label=f\"Threshold = {threshold}\")\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Density\")\nplt.title(\"Density Plot of Predictions with Threshold\")\nplt.legend(title=\"GT20DWN\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Compute confusion matrix\ndtest_5 = dtest_5.withColumn(\"predicted_class\", when(col(\"prediction\") &gt; .3767, 1).otherwise(0))\nconf_matrix = dtest_5.groupBy(\"GT20DWN\", \"predicted_class\").count().orderBy(\"GT20DWN\", \"predicted_class\")\n\nTP = dtest_5.filter((col(\"GT20DWN\") == 1) & (col(\"predicted_class\") == 1)).count()\nFP = dtest_5.filter((col(\"GT20DWN\") == 0) & (col(\"predicted_class\") == 1)).count()\nFN = dtest_5.filter((col(\"GT20DWN\") == 1) & (col(\"predicted_class\") == 0)).count()\nTN = dtest_5.filter((col(\"GT20DWN\") == 0) & (col(\"predicted_class\") == 0)).count()\n\naccuracy = (TP + TN) / (TP + FP + FN + TN)\nprecision = TP / (TP + FP)\nrecall = TP / (TP + FN)\nspecificity = TN / (TN + FP)\naverage_rate = (TP + FN) / (TP + TN + FP + FN)  # Proportion of actual at-risk babies\nenrichment = precision / average_rate\n\n\n# Print formatted confusion matrix with labels\nprint(\"\\n Confusion Matrix:\\n\")\nprint(\"                     Predicted\")\nprint(\"            |  Negative  |  Positive  \")\nprint(\"------------+------------+------------\")\nprint(f\"Actual Neg. |    {TN:5}   |    {FP:5}  |\")\nprint(\"------------+------------+------------\")\nprint(f\"Actual Pos. |    {FN:5}   |    {TP:5}  |\")\nprint(\"------------+------------+------------\")\n\n\nprint(f\"Accuracy:  {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall (Sensitivity): {recall:.4f}\")\nprint(f\"Specificity:  {specificity:.4f}\")\nprint(f\"Average Rate: {average_rate:.4f}\")\nprint(f\"Enrichment:   {enrichment:.4f} (Relative Precision)\")\n\n\n Confusion Matrix:\n\n                     Predicted\n            |  Negative  |  Positive  \n------------+------------+------------\nActual Neg. |      919   |      460  |\n------------+------------+------------\nActual Pos. |      333   |      544  |\n------------+------------+------------\nAccuracy:  0.6485\nPrecision: 0.5418\nRecall (Sensitivity): 0.6203\nSpecificity:  0.6664\nAverage Rate: 0.3887\nEnrichment:   1.3938 (Relative Precision)\n\n\n\n# Use probability of the positive class (y=1)\nevaluator = BinaryClassificationEvaluator(labelCol=\"GT20DWN\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n\n# Evaluate AUC\nauc = evaluator.evaluate(dtest_5)\n\nprint(f\"AUC: {auc:.4f}\")  # Higher is better (closer to 1)\n\n# Convert to Pandas\npdf = dtest_5.select(\"prediction\", \"GT20DWN\").toPandas()\n\n# Compute ROC curve\nfpr, tpr, _ = roc_curve(pdf[\"GT20DWN\"], pdf[\"prediction\"])\n\n# Plot ROC curve\nplt.figure(figsize=(8,6))\nplt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc:.4f})\")\nplt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\nplt.show()\n\nAUC: 0.7084\n\n\n\n\n\n\n\n\n\n\npdf = dtest_5.select(\"prediction\", \"GT20DWN\").toPandas()\n\n# Extract predictions and true labels\ny_true = pdf[\"GT20DWN\"]  # True labels\ny_scores = pdf[\"prediction\"]  # Predicted probabilities\n\n# Compute precision, recall, and thresholds\nprecision_plot, recall_plot, thresholds = precision_recall_curve(y_true, y_scores)\n\n# Compute enrichment: precision divided by average at-risk rate\naverage_rate = np.mean(y_true)\nenrichment_plot = precision_plot / average_rate\n\n# Define optimal threshold (example: threshold where recall ≈ enrichment balance)\nthreshold = 0.3767  # Adjust based on the plot\n\n# Plot Enrichment vs. Recall vs. Threshold\nplt.figure(figsize=(8, 6))\nplt.plot(thresholds, enrichment_plot[:-1], label=\"Enrichment\", color=\"blue\", linestyle=\"--\")\nplt.plot(thresholds, recall_plot[:-1], label=\"Recall\", color=\"red\", linestyle=\"-\")\n\n# Add vertical line for chosen threshold\nplt.axvline(x=threshold, color=\"black\", linestyle=\"dashed\", label=f\"Threshold = {threshold}\")\n\n# Labels and legend\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Score\")\nplt.title(\"Enrichment vs. Recall\")\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#under-175k-value",
    "href": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#under-175k-value",
    "title": "Homework 3",
    "section": "Under 175k Value",
    "text": "Under 175k Value\n\ndf = spark.createDataFrame(homes)\ndf = df.filter(col(\"VALUE\") &lt; 175000)\n\ndtrain, dtest = df.randomSplit([0.7, 0.3], seed = 1234)\n\ndummy_cols_state, ref_category_state = add_dummy_variables('STATE', 0)\ndummy_cols_metro, ref_category_metro = add_dummy_variables('METRO', 0)\ndummy_cols_hhgrad, ref_category_hhgrad = add_dummy_variables('HHGRAD', 0)\ndummy_cols_howh, ref_category_howh = add_dummy_variables('HOWH', 0)\ndummy_cols_hown, ref_category_hown = add_dummy_variables('HOWN', 0)\ndummy_cols_dwnpay, ref_category_dwnpay = add_dummy_variables('DWNPAY', 0)\n\nReference category (dummy omitted): CA\nReference category (dummy omitted): rural\nReference category (dummy omitted): Assoc\nReference category (dummy omitted): bad\nReference category (dummy omitted): bad\nReference category (dummy omitted): other\n\n\n\n# assembling predictors\nconti_cols = [\n 'log_zinc2',\n 'BATHS',\n 'BEDRMS',\n 'PER',\n 'ZADULT',\n 'NUNITS',\n 'EAPTBL',\n 'ECOM1',\n 'ECOM2',\n 'EGREEN',\n 'EJUNK',\n 'ELOW1',\n 'ESFD',\n 'ETRANS',\n 'EABAN',\n 'ODORA',\n 'STRNA',\n 'INTW',\n 'MATBUY',\n 'FRSTHO']\n\nassembler_predictors = (\n    conti_cols +\n    dummy_cols_state +\n    dummy_cols_metro +\n    dummy_cols_hhgrad +\n    dummy_cols_howh +\n    dummy_cols_hown +\n    dummy_cols_dwnpay\n)\n\nassembler_6 = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\n\ndtrain_6 = assembler_6.transform(dtrain)\ndtest_6  = assembler_6.transform(dtest)\n\n# training the model\nmodel_6 = (\n    GeneralizedLinearRegression(featuresCol=\"predictors\",\n                                labelCol=\"GT20DWN\",\n                                family=\"binomial\",\n                                link=\"logit\")\n    .fit(dtrain_6)\n)\n# making prediction\ndtrain_6 = model_6.transform(dtrain_6)\ndtest_6 = model_6.transform(dtest_6)\n\n# makting regression table\nmodel_6.summary\n\nCoefficients:\n         Feature Estimate Std Error T Value P Value\n     (Intercept)   1.5459    0.5823  2.6546  0.0079\n       log_zinc2  -0.1247    0.0417 -2.9944  0.0028\n           BATHS   0.1486    0.0696  2.1349  0.0328\n          BEDRMS   0.0110    0.0560  0.1961  0.8445\n             PER  -0.1603    0.0357 -4.4872  0.0000\n          ZADULT   0.0811    0.0591  1.3720  0.1701\n          NUNITS   0.0022    0.0016  1.3485  0.1775\n          EAPTBL   0.0289    0.1099  0.2633  0.7923\n           ECOM1  -0.3379    0.0977 -3.4595  0.0005\n           ECOM2  -0.1490    0.2234 -0.6667  0.5049\n          EGREEN  -0.0439    0.0745 -0.5890  0.5559\n           EJUNK  -0.1720    0.2479 -0.6937  0.4879\n           ELOW1   0.0458    0.1164  0.3931  0.6942\n            ESFD  -0.2187    0.1301 -1.6815  0.0927\n          ETRANS   0.0740    0.1189  0.6222  0.5338\n           EABAN  -0.0901    0.1612 -0.5589  0.5762\n           ODORA   0.0859    0.1574  0.5456  0.5853\n           STRNA  -0.0301    0.0806 -0.3735  0.7088\n            INTW  -0.0624    0.0210 -2.9746  0.0029\n          MATBUY  -0.1199    0.0715 -1.6767  0.0936\n          FRSTHO  -0.3768    0.0855 -4.4051  0.0000\n        STATE_CO  -1.2002    0.3436 -3.4926  0.0005\n        STATE_CT  -0.1410    0.3118 -0.4521  0.6512\n        STATE_GA  -1.1278    0.3107 -3.6294  0.0003\n        STATE_IL  -0.3090    0.3513 -0.8796  0.3791\n        STATE_IN  -0.8464    0.3008 -2.8141  0.0049\n        STATE_LA  -0.4525    0.3103 -1.4584  0.1447\n        STATE_MO  -0.7814    0.3110 -2.5126  0.0120\n        STATE_OH  -0.2886    0.3031 -0.9522  0.3410\n        STATE_OK  -0.9326    0.3027 -3.0812  0.0021\n        STATE_PA  -0.5283    0.3044 -1.7353  0.0827\n        STATE_TX  -0.7327    0.3051 -2.4015  0.0163\n        STATE_WA  -0.5304    0.3546 -1.4956  0.1348\n     METRO_urban  -0.1059    0.0910 -1.1632  0.2448\n     HHGRAD_Bach   0.1352    0.1175  1.1498  0.2502\n     HHGRAD_Grad   0.2345    0.1399  1.6755  0.0938\n  HHGRAD_HS_Grad  -0.0571    0.1057 -0.5399  0.5892\n    HHGRAD_No_HS  -0.2540    0.1541 -1.6478  0.0994\n       HOWH_good  -0.1156    0.1226 -0.9430  0.3457\n       HOWN_good   0.1000    0.1057  0.9456  0.3444\nDWNPAY_prev_home   0.6405    0.0939  6.8220  0.0000\n\n(Dispersion parameter for binomial family taken to be 1.0000)\n    Null deviance: 5724.5375 on 5407 degrees of freedom\nResidual deviance: 5372.3945 on 5407 degrees of freedom\nAIC: 5454.3945\n\n\n\nMarginal Effects\n\n# Compute means\nmeans_df = dtrain_6.select([mean(col).alias(col) for col in assembler_predictors])\n\n# Collect the results as a list\nmeans = means_df.collect()[0]\nmeans_list = [means[col] for col in assembler_predictors]\n\ntable_output, df_ME = marginal_effects(model_6, means_list) # Instead of mean values, some other representative values can also be chosen.\nprint(table_output)\n\n+------------------+-----------------+--------------+------------+--------------+--------------+\n| Variable         | Marginal Effect | Significance | Std. Error | 95% CI Lower | 95% CI Upper |\n+------------------+-----------------+--------------+------------+--------------+--------------+\n| log_zinc2        |         -0.0201 | ***          |     0.0067 |      -0.0332 |      -0.0069 |\n| BATHS            |          0.0239 | **           |     0.0112 |       0.0020 |       0.0458 |\n| BEDRMS           |          0.0018 |              |     0.0090 |      -0.0159 |       0.0194 |\n| PER              |         -0.0258 | ***          |     0.0057 |      -0.0370 |      -0.0145 |\n| ZADULT           |          0.0130 |              |     0.0095 |      -0.0056 |       0.0317 |\n| NUNITS           |          0.0004 |              |     0.0003 |      -0.0002 |       0.0009 |\n| EAPTBL           |          0.0047 |              |     0.0177 |      -0.0300 |       0.0393 |\n| ECOM1            |         -0.0543 | ***          |     0.0157 |      -0.0851 |      -0.0235 |\n| ECOM2            |         -0.0239 |              |     0.0359 |      -0.0944 |       0.0465 |\n| EGREEN           |         -0.0071 |              |     0.0120 |      -0.0305 |       0.0164 |\n| EJUNK            |         -0.0276 |              |     0.0399 |      -0.1058 |       0.0505 |\n| ELOW1            |          0.0074 |              |     0.0187 |      -0.0293 |       0.0440 |\n| ESFD             |         -0.0352 | *            |     0.0209 |      -0.0762 |       0.0058 |\n| ETRANS           |          0.0119 |              |     0.0191 |      -0.0256 |       0.0494 |\n| EABAN            |         -0.0145 |              |     0.0259 |      -0.0653 |       0.0363 |\n| ODORA            |          0.0138 |              |     0.0253 |      -0.0358 |       0.0634 |\n| STRNA            |         -0.0048 |              |     0.0130 |      -0.0302 |       0.0206 |\n| INTW             |         -0.0100 | ***          |     0.0034 |      -0.0167 |      -0.0034 |\n| MATBUY           |         -0.0193 | *            |     0.0115 |      -0.0418 |       0.0033 |\n| FRSTHO           |         -0.0606 | ***          |     0.0138 |      -0.0875 |      -0.0336 |\n| STATE_CO         |         -0.1930 | ***          |     0.0552 |      -0.3012 |      -0.0847 |\n| STATE_CT         |         -0.0227 |              |     0.0501 |      -0.1209 |       0.0756 |\n| STATE_GA         |         -0.1813 | ***          |     0.0500 |      -0.2792 |      -0.0834 |\n| STATE_IL         |         -0.0497 |              |     0.0565 |      -0.1604 |       0.0610 |\n| STATE_IN         |         -0.1361 | ***          |     0.0484 |      -0.2309 |      -0.0413 |\n| STATE_LA         |         -0.0728 |              |     0.0499 |      -0.1705 |       0.0250 |\n| STATE_MO         |         -0.1256 | **           |     0.0500 |      -0.2236 |      -0.0276 |\n| STATE_OH         |         -0.0464 |              |     0.0487 |      -0.1419 |       0.0491 |\n| STATE_OK         |         -0.1499 | ***          |     0.0487 |      -0.2453 |      -0.0546 |\n| STATE_PA         |         -0.0849 | *            |     0.0489 |      -0.1809 |       0.0110 |\n| STATE_TX         |         -0.1178 | **           |     0.0491 |      -0.2139 |      -0.0217 |\n| STATE_WA         |         -0.0853 |              |     0.0570 |      -0.1970 |       0.0265 |\n| METRO_urban      |         -0.0170 |              |     0.0146 |      -0.0457 |       0.0117 |\n| HHGRAD_Bach      |          0.0217 |              |     0.0189 |      -0.0153 |       0.0588 |\n| HHGRAD_Grad      |          0.0377 | *            |     0.0225 |      -0.0064 |       0.0818 |\n| HHGRAD_HS_Grad   |         -0.0092 |              |     0.0170 |      -0.0425 |       0.0241 |\n| HHGRAD_No_HS     |         -0.0408 | *            |     0.0248 |      -0.0894 |       0.0077 |\n| HOWH_good        |         -0.0186 |              |     0.0197 |      -0.0572 |       0.0200 |\n| HOWN_good        |          0.0161 |              |     0.0170 |      -0.0172 |       0.0494 |\n| DWNPAY_prev_home |          0.1030 | ***          |     0.0151 |       0.0734 |       0.1326 |\n+------------------+-----------------+--------------+------------+--------------+--------------+\n\n\n\n\nClassification\n\n# Filter training data for atRisk == 1 and atRisk == 0\npdf = dtrain_6.select(\"prediction\", \"GT20DWN\").toPandas()\n\ntrain_true = pdf[pdf[\"GT20DWN\"] == 1]\ntrain_false = pdf[pdf[\"GT20DWN\"] == 0]\n\n# Create the first density plot\nplt.figure(figsize=(8, 6))\nsns.kdeplot(train_true[\"prediction\"], label=\"TRUE\", color=\"red\", fill=True)\nsns.kdeplot(train_false[\"prediction\"], label=\"FALSE\", color=\"blue\", fill=True)\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Density\")\nplt.title(\"Density Plot of Predictions\")\nplt.legend(title=\"GT20DWN\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Define threshold for vertical line\nthreshold = 0.24  # Replace with actual value\n\n# Create the second density plot with vertical line\nplt.figure(figsize=(8, 6))\nsns.kdeplot(train_true[\"prediction\"], label=\"TRUE\", color=\"red\", fill=True)\nsns.kdeplot(train_false[\"prediction\"], label=\"FALSE\", color=\"blue\", fill=True)\nplt.axvline(x=threshold, color=\"blue\", linestyle=\"dashed\", label=f\"Threshold = {threshold}\")\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Density\")\nplt.title(\"Density Plot of Predictions with Threshold\")\nplt.legend(title=\"GT20DWN\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Compute confusion matrix\ndtest_6 = dtest_6.withColumn(\"predicted_class\", when(col(\"prediction\") &gt; .24, 1).otherwise(0))\nconf_matrix = dtest_6.groupBy(\"GT20DWN\", \"predicted_class\").count().orderBy(\"GT20DWN\", \"predicted_class\")\n\nTP = dtest_6.filter((col(\"GT20DWN\") == 1) & (col(\"predicted_class\") == 1)).count()\nFP = dtest_6.filter((col(\"GT20DWN\") == 0) & (col(\"predicted_class\") == 1)).count()\nFN = dtest_6.filter((col(\"GT20DWN\") == 1) & (col(\"predicted_class\") == 0)).count()\nTN = dtest_6.filter((col(\"GT20DWN\") == 0) & (col(\"predicted_class\") == 0)).count()\n\naccuracy = (TP + TN) / (TP + FP + FN + TN)\nprecision = TP / (TP + FP)\nrecall = TP / (TP + FN)\nspecificity = TN / (TN + FP)\naverage_rate = (TP + FN) / (TP + TN + FP + FN)  # Proportion of actual at-risk babies\nenrichment = precision / average_rate\n\n\n# Print formatted confusion matrix with labels\nprint(\"\\n Confusion Matrix:\\n\")\nprint(\"                     Predicted\")\nprint(\"            |  Negative  |  Positive  \")\nprint(\"------------+------------+------------\")\nprint(f\"Actual Neg. |    {TN:5}   |    {FP:5}  |\")\nprint(\"------------+------------+------------\")\nprint(f\"Actual Pos. |    {FN:5}   |    {TP:5}  |\")\nprint(\"------------+------------+------------\")\n\n\nprint(f\"Accuracy:  {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall (Sensitivity): {recall:.4f}\")\nprint(f\"Specificity:  {specificity:.4f}\")\nprint(f\"Average Rate: {average_rate:.4f}\")\nprint(f\"Enrichment:   {enrichment:.4f} (Relative Precision)\")\n\n\n Confusion Matrix:\n\n                     Predicted\n            |  Negative  |  Positive  \n------------+------------+------------\nActual Neg. |     1248   |      503  |\n------------+------------+------------\nActual Pos. |      218   |      265  |\n------------+------------+------------\nAccuracy:  0.6773\nPrecision: 0.3451\nRecall (Sensitivity): 0.5487\nSpecificity:  0.7127\nAverage Rate: 0.2162\nEnrichment:   1.5960 (Relative Precision)\n\n\n\n# Use probability of the positive class (y=1)\nevaluator = BinaryClassificationEvaluator(labelCol=\"GT20DWN\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n\n# Evaluate AUC\nauc = evaluator.evaluate(dtest_6)\n\nprint(f\"AUC: {auc:.4f}\")  # Higher is better (closer to 1)\n\n# Convert to Pandas\npdf = dtest_6.select(\"prediction\", \"GT20DWN\").toPandas()\n\n# Compute ROC curve\nfpr, tpr, _ = roc_curve(pdf[\"GT20DWN\"], pdf[\"prediction\"])\n\n# Plot ROC curve\nplt.figure(figsize=(8,6))\nplt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc:.4f})\")\nplt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\nplt.show()\n\nAUC: 0.6708\n\n\n\n\n\n\n\n\n\n\npdf = dtest_6.select(\"prediction\", \"GT20DWN\").toPandas()\n\n# Extract predictions and true labels\ny_true = pdf[\"GT20DWN\"]  # True labels\ny_scores = pdf[\"prediction\"]  # Predicted probabilities\n\n# Compute precision, recall, and thresholds\nprecision_plot, recall_plot, thresholds = precision_recall_curve(y_true, y_scores)\n\n# Compute enrichment: precision divided by average at-risk rate\naverage_rate = np.mean(y_true)\nenrichment_plot = precision_plot / average_rate\n\n# Define optimal threshold (example: threshold where recall ≈ enrichment balance)\nthreshold = 0.24  # Adjust based on the plot\n\n# Plot Enrichment vs. Recall vs. Threshold\nplt.figure(figsize=(8, 6))\nplt.plot(thresholds, enrichment_plot[:-1], label=\"Enrichment\", color=\"blue\", linestyle=\"--\")\nplt.plot(thresholds, recall_plot[:-1], label=\"Recall\", color=\"red\", linestyle=\"-\")\n\n# Add vertical line for chosen threshold\nplt.axvline(x=threshold, color=\"black\", linestyle=\"dashed\", label=f\"Optimal Threshold = {threshold}\")\n\n# Labels and legend\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Score\")\nplt.title(\"Enrichment vs. Recall\")\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#comparison",
    "href": "danl-ml/danl_320_hw3_linear_regression_logistic_regression_housing_markets.html#comparison",
    "title": "Homework 3",
    "section": "Comparison",
    "text": "Comparison\n\nResidual Deviance\n\nOver 175k Value\n\nResidual deviance: 6487.1260 on 5475 degrees of freedom\n\nUnder 175k Value\n\nResidual deviance: 5372.3945 on 5407 degrees of freedom\n\n\n\n\nMarginal Effects\n\n\n\n\n\n\n\n\n\n\nVariable\nMarginal Effect (Over 175k)\nSignificance (Over 175k)\nMarginal Effect (Under 175k)\nSignificance (Under 175k)\n\n\n\n\nlog_zinc2\n-0.0093\n\n-0.0201\n***\n\n\nBATHS\n0.0840\n***\n0.0239\n**\n\n\nBEDRMS\n-0.0024\n\n0.0018\n\n\n\nPER\n-0.0190\n***\n-0.0258\n***\n\n\nZADULT\n-0.0012\n\n0.0130\n\n\n\nNUNITS\n0.0001\n\n0.0004\n\n\n\nEAPTBL\n0.0367\n\n0.0047\n\n\n\nECOM1\n0.0051\n\n-0.0543\n***\n\n\nECOM2\n-0.0799\n\n-0.0239\n\n\n\nEGREEN\n0.0025\n\n-0.0071\n\n\n\nEJUNK\n-0.0786\n\n-0.0276\n\n\n\nELOW1\n-0.0144\n\n0.0074\n\n\n\nESFD\n-0.0515\n\n-0.0352\n*\n\n\nETRANS\n-0.0366\n\n0.0119\n\n\n\nEABAN\n-0.0966\n\n-0.0145\n\n\n\nODORA\n0.0678\n*\n0.0138\n\n\n\nSTRNA\n-0.0511\n***\n-0.0048\n\n\n\nINTW\n-0.0185\n***\n-0.0100\n***\n\n\nMATBUY\n0.1150\n***\n-0.0193\n*\n\n\nFRSTHO\n-0.0948\n***\n-0.0606\n***\n\n\nSTATE_CO\n-0.0312\n\n-0.1930\n***\n\n\nSTATE_CT\n0.1543\n***\n-0.0227\n\n\n\nSTATE_GA\n-0.0874\n***\n-0.1813\n***\n\n\nSTATE_IL\n0.0382\n\n-0.0497\n\n\n\nSTATE_IN\n0.0423\n\n-0.1361\n***\n\n\nSTATE_LA\n0.1627\n***\n-0.0728\n\n\n\nSTATE_MO\n0.1173\n***\n-0.1256\n**\n\n\nSTATE_OH\n0.1585\n***\n-0.0464\n\n\n\nSTATE_OK\n0.0543\n\n-0.1499\n***\n\n\nSTATE_PA\n0.1466\n***\n-0.0849\n*\n\n\nSTATE_TX\n0.0471\n\n-0.1178\n**\n\n\nSTATE_WA\n0.0045\n\n-0.0853\n\n\n\nMETRO_urban\n-0.0055\n\n-0.0170\n\n\n\nHHGRAD_Bach\n0.0630\n***\n0.0217\n\n\n\nHHGRAD_Grad\n0.0854\n***\n0.0377\n*\n\n\nHHGRAD_HS_Grad\n0.0083\n\n-0.0092\n\n\n\nHHGRAD_No_HS\n-0.0343\n\n-0.0408\n*\n\n\nHOWH_good\n-0.0323\n\n-0.0186\n\n\n\nHOWN_good\n0.0796\n***\n0.0161\n\n\n\nDWNPAY_prev_home\n0.1616\n***\n0.1030\n***\n\n\n\n\n\nClassification\nThresholds Used\n- Over $175k: 0.3767\n- Under $175k: 0.24\n\n\n\nMetric\nOver 175k\nUnder 175k\n\n\n\n\nAccuracy\n0.6485\n0.6773\n\n\nPrecision\n0.5418\n0.3451\n\n\nRecall (Sensitivity)\n0.6203\n0.5487\n\n\nSpecificity\n0.6664\n0.7127\n\n\nAverage Rate\n0.3887\n0.2162\n\n\nEnrichment(Rel. Precision)\n1.3938\n1.5960\n\n\nAUC\n0.7084\n0.6708\n\n\n\n\nChoosing the Right Threshold\nThe thresholds used in our two samples were initially chosen to give clean separation in density plots of the model’s predicted probabilities. However, this may not be optimal. The best classifier is the one that best balances your budget, risk tolerance, and operational capacity. Always tie your classifier threshold back to business costs and benefits, and re-assess as those factors change.\n\nPut More Emphasis on Precision (Enrichment)\nUse Case:\n\nBudget‑constrained marketing: You only have $X amount of budget to spend on outreach, so you want to target the very top scorers.\n\nHigh cost of a false positive: e.g., sending a promotional offer or pre‑approval letter that you’ll have to honor, even if the buyer can’t actually make a 20% down payment.\n\nWhat happens as you raise the cutoff?\n\nPrecision ↑ (fewer false positives)\n\nRecall ↓ (you’ll miss more true positives)\n\nFewer people reached, but each one is more likely to convert\n\nExample: raising the cutoff from 0.38 to 0.50 might boost precision from 0.54 → 0.65, but drop recall from 0.62 → 0.40.\nPut More Emphasis on Recall\nUse Case:\n\nRisk management or compliance: You must flag every buyer who could afford a 20% down payment, even if that means extra follow‑up.\n\nCost of missing a true positive is high: e.g., an underwriter must check every possible strong applicant to meet regulatory or fairness mandates.\n\nWhat happens as you lower the cutoff?\n\nRecall ↑ (fewer false negatives)\n\nPrecision ↓ (you’ll incur more false positives)\n\nMore people flagged, increasing verification workload\n\nExample: lowering the cutoff from 0.24 to 0.10 might push recall from 0.55 → 0.80, but precision falls from 0.35 → 0.20."
  },
  {
    "objectID": "danl-ml/danl_320_lasso_logistic_regression_nhl_player_evaluation.html",
    "href": "danl-ml/danl_320_lasso_logistic_regression_nhl_player_evaluation.html",
    "title": "Lasso Logistic Regression",
    "section": "",
    "text": "from google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n\n\n# 2002-2003 season\nnhl = pd.read_csv('https://bcdanl.github.io/data/NHL_data_2002_2003.csv')\nnhl\n\nWarning: Total number of columns (964) exceeds max_columns (20). Falling back to pandas display.\n\n\n\n    \n\n\n\n\n\n\nhomegoal\nperiod\ndifferential\nplayoffs\nS6v5\nS6v4\nS6v3\nS5v4\nS5v3\nS4v3\n...\nCHRIS_PRONGER\nKURTIS_FOSTER\nMILAN_BARTOVIC\nJOE_DIPENTA\nKAMIL_PIROS\nKENT_MCDONELL\nBILL_MUCKALT\nMATT_STAJAN\nTOMI_PETTINEN\nPETER_SEJNA\n\n\n\n\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n1\n-1\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n2\n-2\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n2\n-1\n0\n0\n0\n0\n-1\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n1\n3\n-2\n0\n0\n0\n0\n0\n1\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5300\n1\n1\n2\n1\n0\n0\n0\n1\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n5301\n0\n2\n3\n1\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n5302\n1\n2\n2\n1\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n5303\n1\n3\n3\n1\n0\n0\n0\n1\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n5304\n0\n3\n4\n1\n0\n0\n0\n-1\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n5305 rows × 964 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nnhl.columns\n\nIndex(['homegoal', 'period', 'differential', 'playoffs', 'S6v5', 'S6v4',\n       'S6v3', 'S5v4', 'S5v3', 'S4v3',\n       ...\n       'CHRIS_PRONGER', 'KURTIS_FOSTER', 'MILAN_BARTOVIC', 'JOE_DIPENTA',\n       'KAMIL_PIROS', 'KENT_MCDONELL', 'BILL_MUCKALT', 'MATT_STAJAN',\n       'TOMI_PETTINEN', 'PETER_SEJNA'],\n      dtype='object', length=964)\n\n\n\n# Split into train and test (70% train, 30% test)\n\n# Using a fixed random state for reproducibility (seed = 24351)\nnhl_train, nhl_test = train_test_split(nhl, test_size=0.3, random_state=24351)\n\n# Define predictors: all columns except \"rating\" and \"fail\"\npredictors = [col for col in nhl.columns if col not in ['homegoal']]\n\nX_train = nhl_train[predictors]\nX_test = nhl_train[predictors]\n\n# Outcome variable\ny_train = nhl_train['homegoal']\ny_test = nhl_test['homegoal']\n\n\n# Revised LogisticRegressionCV with fewer candidate Cs, fewer folds, and looser tolerance:\nlasso_cv = LogisticRegressionCV(\n    Cs=10,         # Fewer candidate values\n    cv=3,          # Fewer CV folds\n    penalty='l1',\n    solver='saga',\n    max_iter=1000,\n    tol=1e-3,      # Looser tolerance for faster convergence\n    scoring='neg_log_loss'\n)\nlasso_cv.fit(X_train.values, y_train.values)\n\nprint(\"Best alpha:\", 1 / lasso_cv.C_[0])\n\nintercept = float(lasso_cv.intercept_[0])\ncoef_lasso = pd.DataFrame({\n    'predictor': list(X_train.columns),\n    'coefficient': list(lasso_cv.coef_[0]),\n    'exp_coefficient': np.exp( list(lasso_cv.coef_[0]) ),\n})\n\nBest alpha: 2.782559402207126\n\n\n\nnp.exp(lasso_cv.intercept_[0])\n\nnp.float64(1.052809822818754)\n\n\n\ncoef_lasso = coef_lasso.query('coefficient != 0')\n\n\nHometeam Effect\n\nintercept\n\n0.05146261173037637\n\n\n\nnp.exp(intercept)\n\nnp.float64(1.052809822818754)\n\n\n\n\nPlayer Evaluation\n\n\nfree_cols = ['period', 'differential', 'playoffs',\n             'S6v5','S6v4','S5v4','S5v3','S4v3','SNG',\n             'ATL.20022003',\n             'ANA.20022003','BOS.20022003','BUF.20022003','CAR.20022003',\n             'CBJ.20022003','CGY.20022003','CHI.20022003','COL.20022003',\n             'DAL.20022003','DET.20022003','EDM.20022003','FLA.20022003',\n             'LAK.20022003','MIN.20022003','MTL.20022003','NJD.20022003',\n             'NSH.20022003','NYI.20022003','NYR.20022003','OTT.20022003',\n             'PHI.20022003','PHX.20022003','PIT.20022003','SJS.20022003',\n             'STL.20022003','TBL.20022003','TOR.20022003','VAN.20022003',\n             'WPG.20022003','WSH.20022003']\n\ncoef_lasso = coef_lasso[ ~coef_lasso['predictor'].isin(free_cols) ]\n\n\ncoef_lasso.sort_values('coefficient', ascending = False)\n\n\n    \n\n\n\n\n\n\npredictor\ncoefficient\nexp_coefficient\n\n\n\n\n612\nPETER_FORSBERG\n0.968515\n2.634030\n\n\n449\nBRAD_RICHARDS\n0.937330\n2.553155\n\n\n729\nCHRIS_NEIL\n0.703128\n2.020062\n\n\n653\nNICKLAS_LIDSTROM\n0.556389\n1.744362\n\n\n149\nPETER_SCHAEFER\n0.544168\n1.723174\n\n\n...\n...\n...\n...\n\n\n774\nMARTIN_LAPOINTE\n-0.603984\n0.546630\n\n\n599\nJAMIE_MCLENNAN\n-0.625651\n0.534913\n\n\n421\nBEN_CLYMER\n-0.747552\n0.473524\n\n\n484\nKARLIS_SKRASTINS\n-0.754297\n0.470341\n\n\n668\nPETER_WORRELL\n-0.815675\n0.442341\n\n\n\n\n224 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\nPM vs. Lasso-based Expected PM\n\nimport numpy as np\nimport pandas as pd\n\n# Instead of deriving player_cols from nhl, we use the predictors from coef_lasso.\n# These should be the players that were penalized (i.e. not in free_cols).\nplayer_cols = list(coef_lasso['predictor'])\n\n# Subset the nhl DataFrame to include only these player columns.\nnhl_players = nhl[player_cols]\n\n# -------------------------------------\n# 1. Traditional Plus-Minus (PM)\n# -------------------------------------\n# Each row in nhl_players should contain:\n#   +1 if the goal was for the player's team,\n#   -1 if it was against,\n#    0 if the player was not on the ice.\n# The traditional PM for each player is simply the column sum.\ntraditional_pm = nhl_players.sum()\n\n# -------------------------------------\n# 2. Total Number of Goals (ng)\n# -------------------------------------\n# For each player, the total goals they're on the ice for is the sum of the absolute values.\nng = nhl_players.abs().sum()\n\n# -------------------------------------\n# 3. Expected Plus-Minus (ppm)\n# -------------------------------------\n# Create beta as a Series from coef_lasso:\nbeta = pd.Series(coef_lasso['coefficient'].values, index=coef_lasso['predictor'])\n\n# Convert beta to a probability using the logistic function:\n#   p = exp(beta) / (1 + exp(beta))\np = np.exp(beta) / (1 + np.exp(beta))\n\n# Compute expected plus-minus:\n#   ppm = ng * (p) - ng * (1-p) = ng * (2p - 1)\nexpected_pm = ng * (p) - ng * (1-p)\n\n# -------------------------------------\n# 4. Combine and Display the Results\n# -------------------------------------\neffect_df = pd.DataFrame({\n    'pm': traditional_pm,\n    'ng': ng,\n    'beta': beta,\n    'exp_beta': np.exp(beta),\n    'p': p,\n    'ppm': expected_pm\n})\n\n# Display the top 10 players sorted by expected plus-minus (descending).\neffect_df\n\n\n    \n\n\n\n\n\n\npm\nng\nbeta\nexp_beta\np\nppm\n\n\n\n\nMIKE_COMRIE\n-19\n121\n-0.108854\n0.896862\n0.472813\n-6.579147\n\n\nDERIAN_HATCHER\n8\n178\n0.207467\n1.230557\n0.551681\n18.398612\n\n\nMANNY_MALHOTRA\n1\n31\n-0.277163\n0.757931\n0.431149\n-4.268742\n\n\nTODD_MARCHANT\n0\n134\n0.220145\n1.246258\n0.554815\n14.690460\n\n\nRICHARD_MATVICHUK\n7\n91\n-0.185825\n0.830419\n0.453677\n-8.430777\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\nSAMUEL_PAHLSSON\n21\n61\n0.031758\n1.032268\n0.507939\n0.968551\n\n\nKYLE_MCLAREN\n-26\n26\n-0.011054\n0.989007\n0.497237\n-0.143694\n\n\nBRIAN_POTHIER\n1\n15\n0.474937\n1.607914\n0.616552\n3.496551\n\n\nRYAN_BAYDA\n-5\n39\n0.283464\n1.327721\n0.570395\n5.490826\n\n\nBURKE_HENRY\n4\n30\n-0.261737\n0.769713\n0.434937\n-3.903798\n\n\n\n\n224 rows × 6 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\ntop10_by_pm = effect_df.nlargest(10, 'pm', keep = 'all')\ntop10_by_pm\n\n\n    \n\n\n\n\n\n\npm\nng\nbeta\nexp_beta\np\nppm\n\n\n\n\nCHRIS_OSGOOD\n94\n226\n0.138557\n1.148616\n0.534584\n15.631994\n\n\nSERGEI_GONCHAR\n78\n196\n0.181795\n1.199368\n0.545324\n17.766998\n\n\nMARTY_TURCO\n74\n252\n0.435821\n1.546232\n0.607263\n54.060453\n\n\nJOSE_THEODORE\n58\n268\n-0.012309\n0.987766\n0.496923\n-1.649451\n\n\nPATRICK_ROY\n54\n318\n0.399092\n1.490471\n0.598470\n62.626629\n\n\nROBERT_LANG\n49\n125\n0.406744\n1.501919\n0.600307\n25.076706\n\n\nJEAN-SEBASTIEN_GIGUERE\n47\n305\n0.341136\n1.406544\n0.584466\n51.524524\n\n\nSEAN_BURKE\n40\n74\n0.161521\n1.175298\n0.540293\n5.963336\n\n\nDAN_CLOUTIER\n39\n337\n0.054591\n1.056109\n0.513644\n9.196343\n\n\nED_BELFOUR\n38\n316\n0.054281\n1.055782\n0.513567\n8.574336\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\ntop10_by_beta = effect_df.nlargest(10, 'beta', keep = 'all')\ntop10_by_beta\n\n\n    \n\n\n\n\n\n\npm\nng\nbeta\nexp_beta\np\nppm\n\n\n\n\nPETER_FORSBERG\n16\n170\n0.968515\n2.634030\n0.724823\n76.439952\n\n\nBRAD_RICHARDS\n-64\n66\n0.937330\n2.553155\n0.718560\n28.849915\n\n\nCHRIS_NEIL\n-2\n32\n0.703128\n2.020062\n0.668881\n10.808380\n\n\nNICKLAS_LIDSTROM\n12\n222\n0.556389\n1.744362\n0.635617\n60.213773\n\n\nPETER_SCHAEFER\n5\n83\n0.544168\n1.723174\n0.632781\n22.041725\n\n\nLADISLAV_NAGY\n25\n103\n0.527763\n1.695136\n0.628961\n26.566011\n\n\nMARTIN_HAVLAT\n22\n116\n0.481076\n1.617814\n0.618002\n27.376455\n\n\nBRIAN_POTHIER\n1\n15\n0.474937\n1.607914\n0.616552\n3.496551\n\n\nDAVID_VYBORNY\n-8\n98\n0.441356\n1.554814\n0.608582\n21.282089\n\n\nMARTY_TURCO\n74\n252\n0.435821\n1.546232\n0.607263\n54.060453\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\ntop10_by_ppm = effect_df.nlargest(10, 'ppm', keep = 'all')\ntop10_by_ppm\n\n\n    \n\n\n\n\n\n\npm\nng\nbeta\nexp_beta\np\nppm\n\n\n\n\nPETER_FORSBERG\n16\n170\n0.968515\n2.634030\n0.724823\n76.439952\n\n\nPATRICK_ROY\n54\n318\n0.399092\n1.490471\n0.598470\n62.626629\n\n\nNICKLAS_LIDSTROM\n12\n222\n0.556389\n1.744362\n0.635617\n60.213773\n\n\nMARTY_TURCO\n74\n252\n0.435821\n1.546232\n0.607263\n54.060453\n\n\nJEAN-SEBASTIEN_GIGUERE\n47\n305\n0.341136\n1.406544\n0.584466\n51.524524\n\n\nROMAN_CECHMANEK\n22\n244\n0.355871\n1.427423\n0.588040\n42.963741\n\n\nMIKE_DUNHAM\n14\n212\n0.371373\n1.449723\n0.591791\n38.919209\n\n\nSANDIS_OZOLINSH\n20\n184\n0.407174\n1.502566\n0.600410\n36.950905\n\n\nROMAN_TUREK\n20\n242\n0.288680\n1.334665\n0.571673\n34.689751\n\n\nED_JOVANOVSKI\n-2\n172\n0.369647\n1.447223\n0.591374\n31.432520\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "danl-ml/danl_320_hw4_part2_tree_models.html",
    "href": "danl-ml/danl_320_hw4_part2_tree_models.html",
    "title": "Homework 4 - Part 2: Tree-based Models",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\nfrom sklearn.ensemble import RandomForestRegressor\nmlb_battings_2024 = pd.read_csv(\"https://bcdanl.github.io/data/mlb_battings_2024.csv\")\nmlb_battings_2024.shape\n\n(230, 19)\nmlb_battings_2024\n\n\n    \n\n\n\n\n\n\ng\npa\nhr\nr\nrbi\nsb\nbb_percent\nk_percent\niso\nbabip\navg\nobp\nslg\nw_oba\nw_rc\nbs_r\noff\ndef\nwar\n\n\n\n\n0\n421\n1858\n157\n334\n350\n29\n17.9\n25.6\n0.370\n0.341\n0.304\n0.433\n0.674\n0.455\n202\n-1.8\n216.3\n-16.4\n27.0\n\n\n1\n473\n2082\n90\n313\n296\n76\n8.7\n19.1\n0.207\n0.291\n0.266\n0.340\n0.473\n0.350\n128\n8.9\n77.5\n43.1\n19.6\n\n\n2\n451\n1996\n132\n326\n320\n90\n12.2\n23.3\n0.309\n0.332\n0.296\n0.385\n0.605\n0.411\n168\n10.6\n169.9\n-48.1\n19.2\n\n\n3\n467\n2076\n72\n329\n291\n45\n11.3\n15.6\n0.206\n0.347\n0.314\n0.399\n0.520\n0.391\n153\n4.7\n136.8\n-25.2\n18.6\n\n\n4\n469\n2035\n82\n304\n285\n110\n6.2\n17.8\n0.217\n0.316\n0.288\n0.336\n0.505\n0.356\n128\n19.3\n84.4\n28.9\n18.5\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n225\n412\n1587\n51\n166\n162\n12\n10.3\n26.1\n0.177\n0.272\n0.221\n0.303\n0.397\n0.305\n91\n-2.2\n-18.0\n-41.9\n-0.8\n\n\n226\n417\n1603\n53\n159\n189\n13\n5.5\n25.1\n0.158\n0.300\n0.247\n0.289\n0.405\n0.300\n87\n-5.6\n-30.8\n-33.2\n-1.0\n\n\n227\n288\n1017\n25\n119\n85\n15\n8.3\n20.7\n0.120\n0.263\n0.223\n0.293\n0.343\n0.282\n73\n-1.3\n-33.5\n-18.2\n-1.8\n\n\n228\n313\n1105\n36\n124\n133\n15\n6.1\n24.2\n0.160\n0.261\n0.221\n0.268\n0.381\n0.280\n75\n-0.2\n-32.8\n-24.0\n-2.1\n\n\n229\n381\n1255\n35\n93\n141\n3\n7.8\n20.6\n0.140\n0.262\n0.227\n0.291\n0.368\n0.288\n84\n-5.1\n-28.4\n-34.7\n-2.2\n\n\n\n\n230 rows × 19 columns"
  },
  {
    "objectID": "danl-ml/danl_320_hw4_part2_tree_models.html#train-vs.-test-mse-for-treebased-models",
    "href": "danl-ml/danl_320_hw4_part2_tree_models.html#train-vs.-test-mse-for-treebased-models",
    "title": "Homework 4 - Part 2: Tree-based Models",
    "section": "Train vs. Test MSE for Tree‑Based Models",
    "text": "Train vs. Test MSE for Tree‑Based Models\n\n\n\nModel\nTrain MSE\nTest MSE\n\n\n\n\nRegression tree (no depth limit)\n0.092\n6.154\n\n\nRegression tree (max_depth = 3)\n3.616\n7.208\n\n\nCV‑pruned tree\n0.210\n5.972\n\n\nRandom forest\n0.329\n3.332\n\n\nGradient boosted tree\n0.120\n0.706\n\n\n\n\n\nPredictive Performance Comparison\n\nUnrestricted Regression Tree\n\nTrain MSE: 0.092 (very low)\n\nTest MSE: 6.154 (high)\n\nInterpretation: Severe overfitting—model fits noise in training data but generalizes poorly.\n\nShallow Tree (max_depth = 3)\n\nTrain MSE: 3.616\n\nTest MSE: 7.208 (highest)\n\nInterpretation: Underfitting—model is too simple, yielding both high training and test errors.\n\nCV‑Pruned Tree\n\nTrain MSE: 0.210\n\nTest MSE: 5.972\n\nInterpretation: Pruning reduces complexity relative to the unrestricted tree, lowering the gap between train and test error but still overfits somewhat.\n\nRandom Forest\n\nTrain MSE: 0.329\n\nTest MSE: 3.332\n\nInterpretation: Bagging of many trees greatly reduces variance and overfitting compared to single trees, cutting test error by nearly half.\n\nGradient Boosted Tree\n\nTrain MSE: 0.120\n\nTest MSE: 0.706 (lowest)\n\nInterpretation: Sequential boosting captures complex patterns with controlled regularization, achieving the best predictive performance and modest overfit.\n\n\n\n\n\nOverfitting Analysis\n\n\n\n\n\n\n\n\n\nModel\nTrain MSE\nTest MSE\nAbs. Difference (Test – Train)\n\n\n\n\nRegression tree (no depth limit)\n0.092\n6.154\n6.062\n\n\nRegression tree (max_depth = 3)\n3.616\n7.208\n3.592\n\n\nCV‑pruned tree\n0.210\n5.972\n5.762\n\n\nRandom forest\n0.329\n3.332\n3.003\n\n\nGradient boosted tree\n0.120\n0.706\n0.586\n\n\n\n\nOverfit severity can be gauged by the gap between Train and Test MSE:\n\nUnrestricted tree: gap ≈ 6.062 → very high overfit\n\nPruned tree: gap ≈ 5.762 → still substantial overfit\n\nRandom forest: gap ≈ 3.003 → moderate overfit\n\nGradient boosted tree: gap ≈ 0.586 → low overfit\n\nUnderfitting is evident in the shallow tree (max_depth = 3), which shows both high train and test error.\n\nOverfitting Summary:\n- Single trees either overfit (no limit) or underfit (too shallow).\n- Pruning helps but does not eliminate overfitting entirely.\n- Ensembles (random forest and boosting), particularly boosting, shows a minimal overfit on the test set."
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_OJ.html",
    "href": "danl-ml/danl_320_linear_regression_OJ.html",
    "title": "Linear Regression",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n\n\n\n\n\n\n\nCode\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper | Exp(95% CI Lower) | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: 95% CI Upper, 8: Exp(95% CI Lower), 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"95% CI Upper\", \"Exp(95% CI Lower)\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))\n\n\n\n\n\n\n\nCode\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\n\n\n\n\n\nCode\ndef add_interaction_terms(var_list1, var_list2, var_list3=None):\n    \"\"\"\n    Creates interaction term columns in the global DataFrames dtrain and dtest.\n\n    For two sets of variable names (which may represent categorical (dummy) or continuous variables),\n    this function creates two-way interactions by multiplying each variable in var_list1 with each\n    variable in var_list2.\n\n    Optionally, if a third list of variable names (var_list3) is provided, the function also creates\n    three-way interactions among each variable in var_list1, each variable in var_list2, and each variable\n    in var_list3.\n\n    Parameters:\n        var_list1 (list): List of column names for the first set of variables.\n        var_list2 (list): List of column names for the second set of variables.\n        var_list3 (list, optional): List of column names for the third set of variables for three-way interactions.\n\n    Returns:\n        A flat list of new interaction column names.\n    \"\"\"\n    global dtrain, dtest\n\n    interaction_cols = []\n\n    # Create two-way interactions between var_list1 and var_list2.\n    for var1 in var_list1:\n        for var2 in var_list2:\n            col_name = f\"{var1}_*_{var2}\"\n            dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list1 and var_list3.\n    if var_list3 is not None:\n      for var1 in var_list1:\n          for var3 in var_list3:\n              col_name = f\"{var1}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list2 and var_list3.\n    if var_list3 is not None:\n      for var2 in var_list2:\n          for var3 in var_list3:\n              col_name = f\"{var2}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # If a third list is provided, create three-way interactions.\n    if var_list3 is not None:\n        for var1 in var_list1:\n            for var2 in var_list2:\n                for var3 in var_list3:\n                    col_name = f\"{var1}_*_{var2}_*_{var3}\"\n                    dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    interaction_cols.append(col_name)\n\n    return interaction_cols\n\n # Example\n # interaction_cols_brand_price = add_interaction_terms(dummy_cols_brand, ['log_price'])\n # interaction_cols_brand_ad_price = add_interaction_terms(dummy_cols_brand, dummy_cols_ad, ['log_price'])\n\n\n\n\n\n\n\nCode\ndef compare_reg_models(models, assemblers, names=None):\n    \"\"\"\n    Produces a single formatted table comparing multiple regression models.\n\n    For each predictor (the union across models, ordered by first appearance), the table shows\n    the beta estimate (with significance stars) from each model (blank if not used).\n    For a predictor, if a model's outcome (model.getLabelCol()) starts with \"log\", the cell displays\n    both the beta and its exponential (separated by \" / \"), except when the predictor's name includes \"log_\".\n    (The intercept row does not display exp(.))\n\n    Additional rows for Intercept, Observations, R², and RMSE are appended.\n\n    The header's first column is labeled \"Predictor\", and subsequent columns are\n    \"y: [outcome] ([name])\" for each model.\n\n    The table is produced in grid format (with vertical lines). A dashed line (using '-' characters)\n    is inserted at the top, immediately after the header, and at the bottom.\n    Additionally, immediately after the Intercept row, the border line is replaced with one using '='\n    (to appear as, for example, \"+==============================================+==========================+...\").\n\n    Parameters:\n        models (list): List of fitted LinearRegression models.\n        assemblers (list): List of corresponding VectorAssembler objects.\n        names (list, optional): List of model names; defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing the combined regression table.\n    \"\"\"\n    # Default model names.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(models))]\n\n    # For each model, get outcome and determine if that model is log-transformed.\n    outcomes = [m.getLabelCol() for m in models]\n    is_log_flags = [out.lower().startswith(\"log\") for out in outcomes]\n\n    # Build an ordered union of predictors based on first appearance.\n    ordered_predictors = []\n    for assembler in assemblers:\n        for feat in assembler.getInputCols():\n            if feat not in ordered_predictors:\n                ordered_predictors.append(feat)\n\n    # Helper for significance stars.\n    def significance_stars(p):\n        if p is None:\n            return \"\"\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build rows for each predictor.\n    rows = []\n    for feat in ordered_predictors:\n        row = [feat]\n        for m, a, is_log in zip(models, assemblers, is_log_flags):\n            feats_model = a.getInputCols()\n            if feat in feats_model:\n                idx = feats_model.index(feat)\n                beta = m.coefficients.toArray()[idx]\n                p_val = m.summary.pValues[idx] if m.summary.pValues is not None else None\n                stars = significance_stars(p_val)\n                cell = f\"{beta:.3f}{stars}\"\n                # Only add exp(beta) if model is log and predictor name does NOT include \"log_\"\n                if is_log and (\"log_\" not in feat.lower()):\n                    cell += f\" / {np.exp(beta):,.3f}\"\n                row.append(cell)\n            else:\n                row.append(\"\")\n        rows.append(row)\n\n    # Build intercept row (do NOT compute exp(intercept)).\n    intercept_row = [\"Intercept\"]\n    for m in models:\n        std_all = np.array(m.summary.coefficientStandardErrors)\n        coeffs = m.coefficients.toArray()\n        if len(std_all) == len(coeffs) + 1:\n            intercept_p = m.summary.pValues[0] if m.summary.pValues is not None else None\n        else:\n            intercept_p = None\n        sig = significance_stars(intercept_p)\n        cell = f\"{m.intercept:.3f}{sig}\"\n        intercept_row.append(cell)\n    rows.append(intercept_row)\n\n    # Add Observations row.\n    obs_row = [\"Observations\"]\n    for m in models:\n        obs = m.summary.numInstances\n        obs_row.append(f\"{int(obs):,}\")\n    rows.append(obs_row)\n\n    # Add R² row.\n    r2_row = [\"R²\"]\n    for m in models:\n        r2_row.append(f\"{m.summary.r2:.3f}\")\n    rows.append(r2_row)\n\n    # Add RMSE row.\n    rmse_row = [\"RMSE\"]\n    for m in models:\n        rmse_row.append(f\"{m.summary.rootMeanSquaredError:.3f}\")\n    rows.append(rmse_row)\n\n    # Build header: first column \"Predictor\", then for each model: \"y: [outcome] ([name])\"\n    header = [\"Predictor\"]\n    for out, name in zip(outcomes, names):\n        header.append(f\"y: {out} ({name})\")\n\n    # Create table string using grid format.\n    table_str = tabulate(rows, headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(models))\n\n    # Split into lines.\n    lines = table_str.split(\"\\n\")\n\n    # Create a dashed line spanning the full width.\n    full_width = len(lines[0])\n    dash_line = '-' * full_width\n    # Create an equals line by replacing '-' with '='.\n    eq_line = dash_line.replace('-', '=')\n\n    # Insert a dashed line after the header row.\n    lines = table_str.split(\"\\n\")\n    # In grid format, header and separator are usually the first two lines.\n    lines.insert(2, dash_line)\n\n    # Insert an equals line after the Intercept row.\n    for i, line in enumerate(lines):\n        if line.startswith(\"|\") and \"Intercept\" in line:\n            if i+1 &lt; len(lines):\n                lines[i+1] = eq_line\n            break\n\n    # Add dashed lines at the very top and bottom.\n    final_table = dash_line + \"\\n\" + \"\\n\".join(lines) + \"\\n\" + dash_line\n\n    return final_table\n\n# Example usage:\n# print(compare_reg_models([model_1, model_2, model_3],\n#                          [assembler_1, assembler_2, assembler_3],\n#                          [\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\n\n\n\n\n\nCode\ndef compare_rmse(test_dfs, label_col, pred_col=\"prediction\", names=None):\n    \"\"\"\n    Computes and compares RMSE values for a list of test DataFrames.\n\n    For each DataFrame in test_dfs, this function calculates the RMSE between the actual outcome\n    (given by label_col) and the predicted value (given by pred_col, default \"prediction\"). It then\n    produces a formatted table where the first column header is empty and the first row's first cell is\n    \"RMSE\", with each model's RMSE in its own column.\n\n    Parameters:\n        test_dfs (list): List of test DataFrames.\n        label_col (str): The name of the outcome column.\n        pred_col (str, optional): The name of the prediction column (default \"prediction\").\n        names (list, optional): List of model names corresponding to the test DataFrames.\n                                Defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing a table that compares RMSE values for each test DataFrame,\n        with one model per column.\n    \"\"\"\n    # Set default model names if none provided.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(test_dfs))]\n\n    rmse_values = []\n    for df in test_dfs:\n        # Create a column for squared error.\n        df = df.withColumn(\"error_sq\", pow(col(label_col) - col(pred_col), 2))\n        # Calculate RMSE: square root of the mean squared error.\n        rmse = df.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n        rmse_values.append(rmse)\n\n    # Build a single row table: first cell \"RMSE\", then one cell per model with the RMSE value.\n    row = [\"RMSE\"] + [f\"{rmse:.3f}\" for rmse in rmse_values]\n\n    # Build header: first column header is empty, then model names.\n    header = [\"\"] + names\n\n    table_str = tabulate([row], headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(names))\n    return table_str\n\n# Example usage:\n# print(compare_rmse([dtest_1, dtest_2, dtest_3], \"log_sales\", names=[\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\n\n\n\n\n\nCode\ndef residual_plot(df, label_col, model_name):\n    \"\"\"\n    Generates a residual plot for a given test dataframe.\n\n    Parameters:\n        df (DataFrame): Spark DataFrame containing the test set with predictions.\n        label_col (str): The column name of the actual outcome variable.\n        title (str): The title for the residual plot.\n\n    Returns:\n        None (displays the plot)\n    \"\"\"\n    # Convert to Pandas DataFrame\n    df_pd = df.select([\"prediction\", label_col]).toPandas()\n    df_pd[\"residual\"] = df_pd[label_col] - df_pd[\"prediction\"]\n\n    # Scatter plot of residuals vs. predicted values\n    plt.scatter(df_pd[\"prediction\"], df_pd[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n    # Use LOWESS smoothing for trend line\n    smoothed = sm.nonparametric.lowess(df_pd[\"residual\"], df_pd[\"prediction\"])\n    plt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\n\n    # Add reference line at y=0\n    plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n\n    # Labels and title (model_name)\n    plt.xlabel(\"Predicted Values\")\n    plt.ylabel(\"Residuals\")\n    model_name = \"Residual Plot for \" + model_name\n    plt.title(model_name)\n\n    # Show plot\n    plt.show()\n\n# Example usage:\n# residual_plot(dtest_1, \"log_sales\", \"Model 1\")"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_OJ.html#required-libraries-and-spark-session",
    "href": "danl-ml/danl_320_linear_regression_OJ.html#required-libraries-and-spark-session",
    "title": "Linear Regression",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_OJ.html#udfs",
    "href": "danl-ml/danl_320_linear_regression_OJ.html#udfs",
    "title": "Linear Regression",
    "section": "",
    "text": "Code\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper | Exp(95% CI Lower) | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: 95% CI Upper, 8: Exp(95% CI Lower), 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"95% CI Upper\", \"Exp(95% CI Lower)\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))\n\n\n\n\n\n\n\nCode\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\n\n\n\n\n\nCode\ndef add_interaction_terms(var_list1, var_list2, var_list3=None):\n    \"\"\"\n    Creates interaction term columns in the global DataFrames dtrain and dtest.\n\n    For two sets of variable names (which may represent categorical (dummy) or continuous variables),\n    this function creates two-way interactions by multiplying each variable in var_list1 with each\n    variable in var_list2.\n\n    Optionally, if a third list of variable names (var_list3) is provided, the function also creates\n    three-way interactions among each variable in var_list1, each variable in var_list2, and each variable\n    in var_list3.\n\n    Parameters:\n        var_list1 (list): List of column names for the first set of variables.\n        var_list2 (list): List of column names for the second set of variables.\n        var_list3 (list, optional): List of column names for the third set of variables for three-way interactions.\n\n    Returns:\n        A flat list of new interaction column names.\n    \"\"\"\n    global dtrain, dtest\n\n    interaction_cols = []\n\n    # Create two-way interactions between var_list1 and var_list2.\n    for var1 in var_list1:\n        for var2 in var_list2:\n            col_name = f\"{var1}_*_{var2}\"\n            dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list1 and var_list3.\n    if var_list3 is not None:\n      for var1 in var_list1:\n          for var3 in var_list3:\n              col_name = f\"{var1}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list2 and var_list3.\n    if var_list3 is not None:\n      for var2 in var_list2:\n          for var3 in var_list3:\n              col_name = f\"{var2}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # If a third list is provided, create three-way interactions.\n    if var_list3 is not None:\n        for var1 in var_list1:\n            for var2 in var_list2:\n                for var3 in var_list3:\n                    col_name = f\"{var1}_*_{var2}_*_{var3}\"\n                    dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    interaction_cols.append(col_name)\n\n    return interaction_cols\n\n # Example\n # interaction_cols_brand_price = add_interaction_terms(dummy_cols_brand, ['log_price'])\n # interaction_cols_brand_ad_price = add_interaction_terms(dummy_cols_brand, dummy_cols_ad, ['log_price'])\n\n\n\n\n\n\n\nCode\ndef compare_reg_models(models, assemblers, names=None):\n    \"\"\"\n    Produces a single formatted table comparing multiple regression models.\n\n    For each predictor (the union across models, ordered by first appearance), the table shows\n    the beta estimate (with significance stars) from each model (blank if not used).\n    For a predictor, if a model's outcome (model.getLabelCol()) starts with \"log\", the cell displays\n    both the beta and its exponential (separated by \" / \"), except when the predictor's name includes \"log_\".\n    (The intercept row does not display exp(.))\n\n    Additional rows for Intercept, Observations, R², and RMSE are appended.\n\n    The header's first column is labeled \"Predictor\", and subsequent columns are\n    \"y: [outcome] ([name])\" for each model.\n\n    The table is produced in grid format (with vertical lines). A dashed line (using '-' characters)\n    is inserted at the top, immediately after the header, and at the bottom.\n    Additionally, immediately after the Intercept row, the border line is replaced with one using '='\n    (to appear as, for example, \"+==============================================+==========================+...\").\n\n    Parameters:\n        models (list): List of fitted LinearRegression models.\n        assemblers (list): List of corresponding VectorAssembler objects.\n        names (list, optional): List of model names; defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing the combined regression table.\n    \"\"\"\n    # Default model names.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(models))]\n\n    # For each model, get outcome and determine if that model is log-transformed.\n    outcomes = [m.getLabelCol() for m in models]\n    is_log_flags = [out.lower().startswith(\"log\") for out in outcomes]\n\n    # Build an ordered union of predictors based on first appearance.\n    ordered_predictors = []\n    for assembler in assemblers:\n        for feat in assembler.getInputCols():\n            if feat not in ordered_predictors:\n                ordered_predictors.append(feat)\n\n    # Helper for significance stars.\n    def significance_stars(p):\n        if p is None:\n            return \"\"\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build rows for each predictor.\n    rows = []\n    for feat in ordered_predictors:\n        row = [feat]\n        for m, a, is_log in zip(models, assemblers, is_log_flags):\n            feats_model = a.getInputCols()\n            if feat in feats_model:\n                idx = feats_model.index(feat)\n                beta = m.coefficients.toArray()[idx]\n                p_val = m.summary.pValues[idx] if m.summary.pValues is not None else None\n                stars = significance_stars(p_val)\n                cell = f\"{beta:.3f}{stars}\"\n                # Only add exp(beta) if model is log and predictor name does NOT include \"log_\"\n                if is_log and (\"log_\" not in feat.lower()):\n                    cell += f\" / {np.exp(beta):,.3f}\"\n                row.append(cell)\n            else:\n                row.append(\"\")\n        rows.append(row)\n\n    # Build intercept row (do NOT compute exp(intercept)).\n    intercept_row = [\"Intercept\"]\n    for m in models:\n        std_all = np.array(m.summary.coefficientStandardErrors)\n        coeffs = m.coefficients.toArray()\n        if len(std_all) == len(coeffs) + 1:\n            intercept_p = m.summary.pValues[0] if m.summary.pValues is not None else None\n        else:\n            intercept_p = None\n        sig = significance_stars(intercept_p)\n        cell = f\"{m.intercept:.3f}{sig}\"\n        intercept_row.append(cell)\n    rows.append(intercept_row)\n\n    # Add Observations row.\n    obs_row = [\"Observations\"]\n    for m in models:\n        obs = m.summary.numInstances\n        obs_row.append(f\"{int(obs):,}\")\n    rows.append(obs_row)\n\n    # Add R² row.\n    r2_row = [\"R²\"]\n    for m in models:\n        r2_row.append(f\"{m.summary.r2:.3f}\")\n    rows.append(r2_row)\n\n    # Add RMSE row.\n    rmse_row = [\"RMSE\"]\n    for m in models:\n        rmse_row.append(f\"{m.summary.rootMeanSquaredError:.3f}\")\n    rows.append(rmse_row)\n\n    # Build header: first column \"Predictor\", then for each model: \"y: [outcome] ([name])\"\n    header = [\"Predictor\"]\n    for out, name in zip(outcomes, names):\n        header.append(f\"y: {out} ({name})\")\n\n    # Create table string using grid format.\n    table_str = tabulate(rows, headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(models))\n\n    # Split into lines.\n    lines = table_str.split(\"\\n\")\n\n    # Create a dashed line spanning the full width.\n    full_width = len(lines[0])\n    dash_line = '-' * full_width\n    # Create an equals line by replacing '-' with '='.\n    eq_line = dash_line.replace('-', '=')\n\n    # Insert a dashed line after the header row.\n    lines = table_str.split(\"\\n\")\n    # In grid format, header and separator are usually the first two lines.\n    lines.insert(2, dash_line)\n\n    # Insert an equals line after the Intercept row.\n    for i, line in enumerate(lines):\n        if line.startswith(\"|\") and \"Intercept\" in line:\n            if i+1 &lt; len(lines):\n                lines[i+1] = eq_line\n            break\n\n    # Add dashed lines at the very top and bottom.\n    final_table = dash_line + \"\\n\" + \"\\n\".join(lines) + \"\\n\" + dash_line\n\n    return final_table\n\n# Example usage:\n# print(compare_reg_models([model_1, model_2, model_3],\n#                          [assembler_1, assembler_2, assembler_3],\n#                          [\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\n\n\n\n\n\nCode\ndef compare_rmse(test_dfs, label_col, pred_col=\"prediction\", names=None):\n    \"\"\"\n    Computes and compares RMSE values for a list of test DataFrames.\n\n    For each DataFrame in test_dfs, this function calculates the RMSE between the actual outcome\n    (given by label_col) and the predicted value (given by pred_col, default \"prediction\"). It then\n    produces a formatted table where the first column header is empty and the first row's first cell is\n    \"RMSE\", with each model's RMSE in its own column.\n\n    Parameters:\n        test_dfs (list): List of test DataFrames.\n        label_col (str): The name of the outcome column.\n        pred_col (str, optional): The name of the prediction column (default \"prediction\").\n        names (list, optional): List of model names corresponding to the test DataFrames.\n                                Defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing a table that compares RMSE values for each test DataFrame,\n        with one model per column.\n    \"\"\"\n    # Set default model names if none provided.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(test_dfs))]\n\n    rmse_values = []\n    for df in test_dfs:\n        # Create a column for squared error.\n        df = df.withColumn(\"error_sq\", pow(col(label_col) - col(pred_col), 2))\n        # Calculate RMSE: square root of the mean squared error.\n        rmse = df.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n        rmse_values.append(rmse)\n\n    # Build a single row table: first cell \"RMSE\", then one cell per model with the RMSE value.\n    row = [\"RMSE\"] + [f\"{rmse:.3f}\" for rmse in rmse_values]\n\n    # Build header: first column header is empty, then model names.\n    header = [\"\"] + names\n\n    table_str = tabulate([row], headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(names))\n    return table_str\n\n# Example usage:\n# print(compare_rmse([dtest_1, dtest_2, dtest_3], \"log_sales\", names=[\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\n\n\n\n\n\nCode\ndef residual_plot(df, label_col, model_name):\n    \"\"\"\n    Generates a residual plot for a given test dataframe.\n\n    Parameters:\n        df (DataFrame): Spark DataFrame containing the test set with predictions.\n        label_col (str): The column name of the actual outcome variable.\n        title (str): The title for the residual plot.\n\n    Returns:\n        None (displays the plot)\n    \"\"\"\n    # Convert to Pandas DataFrame\n    df_pd = df.select([\"prediction\", label_col]).toPandas()\n    df_pd[\"residual\"] = df_pd[label_col] - df_pd[\"prediction\"]\n\n    # Scatter plot of residuals vs. predicted values\n    plt.scatter(df_pd[\"prediction\"], df_pd[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n    # Use LOWESS smoothing for trend line\n    smoothed = sm.nonparametric.lowess(df_pd[\"residual\"], df_pd[\"prediction\"])\n    plt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\n\n    # Add reference line at y=0\n    plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n\n    # Labels and title (model_name)\n    plt.xlabel(\"Predicted Values\")\n    plt.ylabel(\"Residuals\")\n    model_name = \"Residual Plot for \" + model_name\n    plt.title(model_name)\n\n    # Show plot\n    plt.show()\n\n# Example usage:\n# residual_plot(dtest_1, \"log_sales\", \"Model 1\")"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_OJ.html#log-transformation",
    "href": "danl-ml/danl_320_linear_regression_OJ.html#log-transformation",
    "title": "Linear Regression",
    "section": "Log Transformation",
    "text": "Log Transformation\n\ndf = spark.createDataFrame(oj)\ndf = (\n    df\n    .withColumn(\"log_sales\",\n                log(df['sales']) )\n    .withColumn(\"log_price\",\n                log(df['price']) )\n)"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_OJ.html#training-test-split",
    "href": "danl-ml/danl_320_linear_regression_OJ.html#training-test-split",
    "title": "Linear Regression",
    "section": "Training-Test Split",
    "text": "Training-Test Split\n\ndtrain, dtest = df.randomSplit([0.9, 0.1], seed = 1234)"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_OJ.html#adding-dummies",
    "href": "danl-ml/danl_320_linear_regression_OJ.html#adding-dummies",
    "title": "Linear Regression",
    "section": "Adding Dummies",
    "text": "Adding Dummies\n\ndummy_cols_brand, ref_category_brand = add_dummy_variables('brand', 0)\ndummy_cols_ad, ref_category_ad = add_dummy_variables('ad_status', 1)\n\nReference category (dummy omitted): dominicks\nReference category (dummy omitted): No Ad"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_OJ.html#adding-interaction-terms",
    "href": "danl-ml/danl_320_linear_regression_OJ.html#adding-interaction-terms",
    "title": "Linear Regression",
    "section": "Adding Interaction Terms",
    "text": "Adding Interaction Terms\n\ninteraction_cols_brand_price = add_interaction_terms(dummy_cols_brand, ['log_price'])\ninteraction_cols_brand_ad_price = add_interaction_terms(dummy_cols_brand, dummy_cols_ad, ['log_price'])"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_OJ.html#betas",
    "href": "danl-ml/danl_320_linear_regression_OJ.html#betas",
    "title": "Linear Regression",
    "section": "Betas",
    "text": "Betas\n\nprint(\n    compare_reg_models(\n        [model_1, model_2, model_3],\n        [assembler_1, assembler_2, assembler_3]\n        )\n    )\n\n---------------------------------------------------------------------------------------------------------------------------------\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| Predictor                                    |   y: log_sales (Model 1) |   y: log_sales (Model 2) |   y: log_sales (Model 3) |\n---------------------------------------------------------------------------------------------------------------------------------\n+==============================================+==========================+==========================+==========================+\n| log_price                                    |                -3.152*** |                -3.410*** |                -2.797*** |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| brand_minute_maid                            |         0.876*** / 2.400 |         0.868*** / 2.382 |            0.029 / 1.029 |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| brand_tropicana                              |         1.537*** / 4.649 |         0.955*** / 2.598 |         0.681*** / 1.976 |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| brand_minute_maid_*_log_price                |                          |                    0.095 |                 0.813*** |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| brand_tropicana_*_log_price                  |                          |                 0.688*** |                 0.766*** |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| ad_status_Ad                                 |                          |                          |         1.095*** / 2.990 |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| brand_minute_maid_*_ad_status_Ad             |                          |                          |         1.183*** / 3.265 |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| brand_tropicana_*_ad_status_Ad               |                          |                          |         0.814*** / 2.256 |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| ad_status_Ad_*_log_price                     |                          |                          |                -0.499*** |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| brand_minute_maid_*_ad_status_Ad_*_log_price |                          |                          |                -1.096*** |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| brand_tropicana_*_ad_status_Ad_*_log_price   |                          |                          |                -0.975*** |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| Intercept                                    |                10.836*** |                10.972*** |                10.423*** |\n=================================================================================================================================\n| Observations                                 |                   26,040 |                   26,040 |                   26,040 |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| R²                                           |                    0.394 |                    0.398 |                    0.535 |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n| RMSE                                         |                    0.794 |                    0.792 |                    0.696 |\n+----------------------------------------------+--------------------------+--------------------------+--------------------------+\n---------------------------------------------------------------------------------------------------------------------------------"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_OJ.html#rmses",
    "href": "danl-ml/danl_320_linear_regression_OJ.html#rmses",
    "title": "Linear Regression",
    "section": "RMSEs",
    "text": "RMSEs\n\n# Create a new column for squared error\ndtest_1 = dtest_1.withColumn(\"error_sq\", pow(col(\"log_sales\") - col(\"prediction\"), 2))\n\n# Calculate RMSE as the square root of the mean squared error\nrmse_val_1 = dtest_1.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n\nprint(f'RMSE_1: {rmse_val_1:.3f}')\n\nRMSE_1: 0.785\n\n\n\nprint(compare_rmse([dtest_1, dtest_2, dtest_3], \"log_sales\"))\n\n+------+-----------+-----------+-----------+\n|      |   Model 1 |   Model 2 |   Model 3 |\n+======+===========+===========+===========+\n| RMSE |     0.785 |     0.783 |     0.682 |\n+------+-----------+-----------+-----------+"
  },
  {
    "objectID": "danl-ml/danl_320_linear_regression_OJ.html#residual-plots",
    "href": "danl-ml/danl_320_linear_regression_OJ.html#residual-plots",
    "title": "Linear Regression",
    "section": "Residual Plots",
    "text": "Residual Plots\n\nresidual_plot(dtest_1, \"log_sales\", \"Model 1\")\n\n\n\n\n\n\n\n\n\nresidual_plot(dtest_2, \"log_sales\", \"Model 2\")\n\n\n\n\n\n\n\n\n\nresidual_plot(dtest_3, \"log_sales\", \"Model 3\")"
  },
  {
    "objectID": "danl-ml/danl_320_hw4_part_1_lasso_beer_model_3.html",
    "href": "danl-ml/danl_320_hw4_part_1_lasso_beer_model_3.html",
    "title": "Homework 4 - Part 1: Lasso Linear Regerssion - Model 3 with Discussions",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import scale # zero mean & one s.d.\nfrom sklearn.linear_model import LassoCV, lasso_path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n#beer = pd.read_csv(\"https://bcdanl.github.io/data/beer_markets_xbeer_xdemog.zip\")\n#beer = pd.read_csv(\"https://bcdanl.github.io/data/beer_markets_xbeer_brand_xdemog.zip\")\nbeer = pd.read_csv(\"https://bcdanl.github.io/data/beer_markets_xbeer_brand_promo_xdemog.zip\")\nX = beer.drop('ylogprice', axis = 1)\ny = beer['ylogprice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ny_train = y_train.values\ny_test = y_test.values\n# LassoCV with a range of alpha values\nlasso_cv = LassoCV(n_alphas = 100,\n                   alphas = None, # alphas=None automatically generate 100 candidate alpha values\n                   cv = 5,\n                   random_state=42,\n                   max_iter=100000)\nlasso_cv.fit(X_train.values, y_train)\n#lasso_cv.fit(X_train.values, y_train.ravel())\nprint(\"LassoCV - Best alpha:\", lasso_cv.alpha_)\n\nLassoCV - Best alpha: 0.00022539867869301466\n# Create a DataFrame including the intercept and the coefficients:\ncoef_lasso_beer = pd.DataFrame({\n    'predictor': list(X_train.columns),\n    'coefficient':  list(lasso_cv.coef_),\n    'exp_coefficient': np.exp(  list(lasso_cv.coef_) )\n})\n\n\n# Evaluate\ny_pred_lasso = lasso_cv.predict(X_test)\nmse_lasso = mean_squared_error(y_test, y_pred_lasso)\nprint(\"LassoCV - MSE:\", mse_lasso)\n\nLassoCV - MSE: 0.02863537005154527\ncoef_lasso_beer_n0 = coef_lasso_beer[coef_lasso_beer['coefficient'] != 0]\nX_train.shape[1]\n\n2655\ncoef_lasso_beer_n0.shape[0]\n\n163\ncoef_lasso_beer_n0\n\n\n    \n\n\n\n\n\n\npredictor\ncoefficient\nexp_coefficient\n\n\n\n\n0\nlogquantity\n-0.136277\n0.872601\n\n\n1\ncontainer_CAN\n-0.056243\n0.945309\n\n\n2\nbrandBUSCH_LIGHT\n-0.072086\n0.930451\n\n\n4\nbrandMILLER_LITE\n0.001853\n1.001854\n\n\n5\nbrandNATURAL_LIGHT\n-0.457505\n0.632861\n\n\n...\n...\n...\n...\n\n\n2372\nmarketRURAL_WEST_VIRGINIA:npeople2\n-0.032313\n0.968203\n\n\n2385\nmarketTAMPA:npeople2\n0.018947\n1.019127\n\n\n2386\nmarketURBAN_NY:npeople2\n0.001501\n1.001502\n\n\n2426\nmarketRALEIGH-DURHAM:npeople3\n-0.005387\n0.994627\n\n\n2582\nmarketEXURBAN_NJ:npeople5plus\n0.080731\n1.084079\n\n\n\n\n163 rows × 3 columns\n# Compute the mean and standard deviation of the CV errors for each alpha.\nmean_cv_errors = np.mean(lasso_cv.mse_path_, axis=1)\nstd_cv_errors = np.std(lasso_cv.mse_path_, axis=1)\n\nplt.figure(figsize=(8, 6))\nplt.errorbar(lasso_cv.alphas_, mean_cv_errors, yerr=std_cv_errors, marker='o', linestyle='-', capsize=5)\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Mean CV Error (MSE)')\nplt.title('Cross-Validation Error vs. Alpha')\n#plt.gca().invert_xaxis()  # Optionally invert the x-axis so lower alphas (less regularization) appear to the right.\nplt.axvline(x=lasso_cv.alpha_, color='red', linestyle='--', label='Best alpha')\nplt.legend()\nplt.show()\n# Compute the coefficient path over the alpha grid that LassoCV used\nalphas, coefs, _ = lasso_path(X_train, y_train,\n                              alphas=lasso_cv.alphas_,\n                              max_iter=100000)\n\n# Count nonzero coefficients for each alpha (coefs shape: (n_features, n_alphas))\nnonzero_counts = np.sum(coefs != 0, axis=0)\n\n# Plot the number of nonzero coefficients versus alpha\nplt.figure(figsize=(8,6))\nplt.plot(alphas, nonzero_counts, marker='o', linestyle='-')\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Number of nonzero coefficients')\nplt.title('Nonzero Coefficients vs. Alpha')\n#plt.gca().invert_xaxis()  # Lower alphas (less regularization) on the right\nplt.axvline(x=lasso_cv.alpha_, color='red', linestyle='--', label='Best alpha')\nplt.legend()\nplt.show()\n# Compute the lasso path. Note: we use np.log(y_train) because that's what you used in LassoCV.\nalphas, coefs, _ = lasso_path(X_train, y_train,\n                              alphas=lasso_cv.alphas_,\n                              max_iter=100000)\nplt.figure(figsize=(8, 6))\n# Iterate over each predictor and plot its coefficient path.\nfor i, col in enumerate(X_train.columns):\n    plt.plot(alphas, coefs[i, :], label=col)\n\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Coefficient value')\nplt.title('Lasso Coefficient Paths')\n#plt.gca().invert_xaxis()  # Lower alphas (weaker regularization) to the right.\n#plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.axvline(x=lasso_cv.alpha_, color='red', linestyle='--', label='Best alpha')\n#plt.legend()\nplt.show()"
  },
  {
    "objectID": "danl-ml/danl_320_hw4_part_1_lasso_beer_model_3.html#model-1-common-elasticity",
    "href": "danl-ml/danl_320_hw4_part_1_lasso_beer_model_3.html#model-1-common-elasticity",
    "title": "Homework 4 - Part 1: Lasso Linear Regerssion - Model 3 with Discussions",
    "section": "Model 1 (Common Elasticity)",
    "text": "Model 1 (Common Elasticity)\n\n\n\nSpecification\nElasticity\n\n\n\n\nWithout demographics\n–0.1420\n\n\nWith demographics\n–0.1429\n\n\nΔ\n–0.0009\n\n\nEffect\nSlightly more elastic (negligible change)"
  },
  {
    "objectID": "danl-ml/danl_320_hw4_part_1_lasso_beer_model_3.html#model-2-brandspecific-elasticities",
    "href": "danl-ml/danl_320_hw4_part_1_lasso_beer_model_3.html#model-2-brandspecific-elasticities",
    "title": "Homework 4 - Part 1: Lasso Linear Regerssion - Model 3 with Discussions",
    "section": "Model 2 (Brand‑Specific Elasticities)",
    "text": "Model 2 (Brand‑Specific Elasticities)\n\n\n\n\n\n\n\n\n\n\nBrand\nWithout Demog\nWith Demog\nΔ\nEffect\n\n\n\n\nBud\n–0.1460\n–0.1408\n+0.0052\nLess elastic\n\n\nBusch\n–0.1590\n–0.1719\n–0.0129\nMore elastic\n\n\nCoors\n–0.1460\n–0.1414\n+0.0046\nLess elastic\n\n\nMiller\n–0.1630\n–0.1443\n+0.0187\nMuch less elastic\n\n\nNatural\n–0.0940\n–0.1113\n–0.0173\nMore elastic\n\n\n\n\nInterpretation: Demographics slightly dampen sensitivity for Bud, Coors, and Miller but increase it for Busch and Natural."
  },
  {
    "objectID": "danl-ml/danl_320_hw4_part_1_lasso_beer_model_3.html#model-3-fullprice-no-promo",
    "href": "danl-ml/danl_320_hw4_part_1_lasso_beer_model_3.html#model-3-fullprice-no-promo",
    "title": "Homework 4 - Part 1: Lasso Linear Regerssion - Model 3 with Discussions",
    "section": "Model 3 Full‑Price (No Promo)",
    "text": "Model 3 Full‑Price (No Promo)\n\n\n\n\n\n\n\n\n\n\nBrand\nWithout Demog\nWith Demog\nΔ\nEffect\n\n\n\n\nBud\n–0.1400\n–0.1363\n+0.0037\nLess elastic\n\n\nBusch\n–0.1610\n–0.1708\n–0.0098\nMore elastic\n\n\nCoors\n–0.1480\n–0.1365\n+0.0115\nLess elastic\n\n\nMiller\n–0.1630\n–0.1401\n+0.0229\nMuch less elastic\n\n\nNatural\n–0.1030\n–0.1105\n–0.0075\nMore elastic\n\n\n\n\nInterpretation: Similar pattern to Model 2; demographic controls modestly increase or decrease brand‑specific slopes."
  },
  {
    "objectID": "danl-ml/danl_320_hw4_part_1_lasso_beer_model_3.html#model-3-promotionalprice",
    "href": "danl-ml/danl_320_hw4_part_1_lasso_beer_model_3.html#model-3-promotionalprice",
    "title": "Homework 4 - Part 1: Lasso Linear Regerssion - Model 3 with Discussions",
    "section": "Model 3 Promotional‑Price",
    "text": "Model 3 Promotional‑Price\n\n\n\n\n\n\n\n\n\n\nBrand\nWithout Demog\nWith Demog\nΔ\nEffect\n\n\n\n\nBud\n–0.1480\n–0.1403\n+0.0077\nLess elastic\n\n\nBusch\n–0.1220\n–0.1418\n–0.0198\nMore elastic\n\n\nCoors\n–0.1190\n–0.1355\n–0.0165\nMore elastic\n\n\nMiller\n–0.0960\n–0.1353\n–0.0393\nMuch more elastic\n\n\nNatural\n–0.0770\n–0.1058\n–0.0288\nMore elastic\n\n\n\n\nInterpretation: Demographic adjustments have the largest impact here, especially increasing sensitivity for Miller and others."
  },
  {
    "objectID": "danl-ml/danl_320_hw4_part_1_lasso_beer_model_3.html#summary",
    "href": "danl-ml/danl_320_hw4_part_1_lasso_beer_model_3.html#summary",
    "title": "Homework 4 - Part 1: Lasso Linear Regerssion - Model 3 with Discussions",
    "section": "Summary",
    "text": "Summary\n\nModel 1: Virtually no change when demographics are added.\nModels 2 & 3 without Promo: Elasticities shift modestly by brand—some become slightly more price‑sensitive, others slightly less.\nModel 3 Promo: Most brands become notably more elastic once demographics are included. This implies that consumer characteristics play a key role in how promotions affect beer pricing."
  },
  {
    "objectID": "danl-ml/danl_320_cw12_california_housing_values.html",
    "href": "danl-ml/danl_320_cw12_california_housing_values.html",
    "title": "From Linear Regression to Tree-based Models",
    "section": "",
    "text": "# Below is for an interactive display of Pandas DataFrame in Colab\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\nfrom tabulate import tabulate  # for table summary\n\n# For basic libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport statsmodels.api as sm  # for lowess smoothing\n\n# `scikit-learn`\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import (confusion_matrix, accuracy_score, precision_score, recall_score, roc_curve, roc_auc_score)\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.inspection import PartialDependenceDisplay\n\nfrom sklearn.preprocessing import scale # zero mean & one s.d.\nfrom sklearn.linear_model import LassoCV, lasso_path\nfrom sklearn.linear_model import RidgeCV, Ridge\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor, plot_importance\n\n# PySpark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
  },
  {
    "objectID": "danl-ml/danl_320_cw12_california_housing_values.html#add_dummy_variables",
    "href": "danl-ml/danl_320_cw12_california_housing_values.html#add_dummy_variables",
    "title": "From Linear Regression to Tree-based Models",
    "section": "add_dummy_variables()",
    "text": "add_dummy_variables()\n\n\nCode\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)"
  },
  {
    "objectID": "danl-ml/danl_320_cw12_california_housing_values.html#regression_table",
    "href": "danl-ml/danl_320_cw12_california_housing_values.html#regression_table",
    "title": "From Linear Regression to Tree-based Models",
    "section": "regression_table()",
    "text": "regression_table()\n\n\nCode\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                ci_upper,\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: Exp(95% CI Lower), 8: 95% CI Upper, 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"Exp(95% CI Lower)\", \"95% CI Upper\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))"
  },
  {
    "objectID": "danl-ml/danl_320_cw12_california_housing_values.html#add_interaction_terms",
    "href": "danl-ml/danl_320_cw12_california_housing_values.html#add_interaction_terms",
    "title": "From Linear Regression to Tree-based Models",
    "section": "add_interaction_terms()",
    "text": "add_interaction_terms()\n\n\nCode\ndef add_interaction_terms(var_list1, var_list2, var_list3=None):\n    \"\"\"\n    Creates interaction term columns in the global DataFrames dtrain and dtest.\n\n    For two sets of variable names (which may represent categorical (dummy) or continuous variables),\n    this function creates two-way interactions by multiplying each variable in var_list1 with each\n    variable in var_list2.\n\n    Optionally, if a third list of variable names (var_list3) is provided, the function also creates\n    three-way interactions among each variable in var_list1, each variable in var_list2, and each variable\n    in var_list3.\n\n    Parameters:\n        var_list1 (list): List of column names for the first set of variables.\n        var_list2 (list): List of column names for the second set of variables.\n        var_list3 (list, optional): List of column names for the third set of variables for three-way interactions.\n\n    Returns:\n        A flat list of new interaction column names.\n    \"\"\"\n    global dtrain, dtest\n\n    interaction_cols = []\n\n    # Create two-way interactions between var_list1 and var_list2.\n    for var1 in var_list1:\n        for var2 in var_list2:\n            col_name = f\"{var1}_*_{var2}\"\n            dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list1 and var_list3.\n    if var_list3 is not None:\n      for var1 in var_list1:\n          for var3 in var_list3:\n              col_name = f\"{var1}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list2 and var_list3.\n    if var_list3 is not None:\n      for var2 in var_list2:\n          for var3 in var_list3:\n              col_name = f\"{var2}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # If a third list is provided, create three-way interactions.\n    if var_list3 is not None:\n        for var1 in var_list1:\n            for var2 in var_list2:\n                for var3 in var_list3:\n                    col_name = f\"{var1}_*_{var2}_*_{var3}\"\n                    dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    interaction_cols.append(col_name)\n\n    return interaction_cols\n\n # Example\n # interaction_cols_brand_price = add_interaction_terms(dummy_cols_brand, ['log_price'])\n # interaction_cols_brand_ad_price = add_interaction_terms(dummy_cols_brand, dummy_cols_ad, ['log_price'])"
  },
  {
    "objectID": "danl-ml/danl_320_cw12_california_housing_values.html#adding-the-outcome-variable",
    "href": "danl-ml/danl_320_cw12_california_housing_values.html#adding-the-outcome-variable",
    "title": "From Linear Regression to Tree-based Models",
    "section": "Adding the Outcome Variable",
    "text": "Adding the Outcome Variable\n\ndfpd['log_medianHouseValue'] = np.log(dfpd['medianHouseValue'])"
  },
  {
    "objectID": "danl-ml/danl_320_cw12_california_housing_values.html#adding-the-interactions",
    "href": "danl-ml/danl_320_cw12_california_housing_values.html#adding-the-interactions",
    "title": "From Linear Regression to Tree-based Models",
    "section": "Adding the Interactions",
    "text": "Adding the Interactions\n\ndfpd['housingMedianAge_*_longitude'] = dfpd['housingMedianAge'] * dfpd['longitude']\ndfpd['medianIncome_*_longitude'] = dfpd['medianIncome'] * dfpd['longitude']\ndfpd['housingMedianAge_*_longitude'] = dfpd['housingMedianAge'] * dfpd['longitude']\ndfpd['AveBedrms_*_longitude'] = dfpd['AveBedrms'] * dfpd['longitude']\ndfpd['AveRooms_*_longitude'] = dfpd['AveRooms'] * dfpd['longitude']\ndfpd['AveOccupancy_*_longitude'] = dfpd['AveOccupancy'] * dfpd['longitude']\n\ndfpd['housingMedianAge_*_latitude'] = dfpd['housingMedianAge'] * dfpd['latitude']\ndfpd['medianIncome_*_latitude'] = dfpd['medianIncome'] * dfpd['latitude']\ndfpd['housingMedianAge_*_latitude'] = dfpd['housingMedianAge'] * dfpd['latitude']\ndfpd['AveBedrms_*_latitude'] = dfpd['AveBedrms'] * dfpd['latitude']\ndfpd['AveRooms_*_latitude'] = dfpd['AveRooms'] * dfpd['latitude']\ndfpd['AveOccupancy_*_latitude'] = dfpd['AveOccupancy'] * dfpd['latitude']"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html",
    "href": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "# Below is for an interactive display of Pandas DataFrame in Colab\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\nimport pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_curve\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n\n\n\n\n\n\nCode\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\n\n\n\n\n\nCode\ndef marginal_effects(model, means):\n    \"\"\"\n    Compute marginal effects for all predictors in a PySpark GeneralizedLinearRegression model (logit)\n    and return a formatted table with statistical significance and standard errors.\n\n    Parameters:\n        model: Fitted GeneralizedLinearRegression model (with binomial family and logit link).\n        means: List of mean values for the predictor variables.\n\n    Returns:\n        - A formatted string containing the marginal effects table.\n        - A Pandas DataFrame with marginal effects, standard errors, confidence intervals, and significance stars.\n    \"\"\"\n    global assembler_predictors  # Use the global assembler_predictors list\n\n    # Extract model coefficients, standard errors, and intercept\n    coeffs = np.array(model.coefficients)\n    std_errors = np.array(model.summary.coefficientStandardErrors)\n    intercept = model.intercept\n\n    # Compute linear combination of means and coefficients (XB)\n    XB = np.dot(means, coeffs) + intercept\n\n    # Compute derivative of logistic function (G'(XB))\n    G_prime_XB = np.exp(XB) / ((1 + np.exp(XB)) ** 2)\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Create lists to store results\n    results = []\n    df_results = []  # For Pandas DataFrame\n\n    for i, predictor in enumerate(assembler_predictors):\n        # Compute marginal effect\n        marginal_effect = G_prime_XB * coeffs[i]\n\n        # Compute standard error of the marginal effect\n        std_error = G_prime_XB * std_errors[i]\n\n        # Compute z-score and p-value\n        z_score = marginal_effect / std_error if std_error != 0 else np.nan\n        p_value = 2 * (1 - norm.cdf(abs(z_score))) if not np.isnan(z_score) else np.nan\n\n        # Compute confidence interval (95%)\n        ci_lower = marginal_effect - 1.96 * std_error\n        ci_upper = marginal_effect + 1.96 * std_error\n\n        # Append results for table formatting\n        results.append([\n            predictor,\n            f\"{marginal_effect: .6f}\",\n            significance_stars(p_value),\n            f\"{std_error: .6f}\",\n            f\"{ci_lower: .6f}\",\n            f\"{ci_upper: .6f}\"\n        ])\n\n        # Append results for Pandas DataFrame\n        df_results.append({\n            \"Variable\": predictor,\n            \"Marginal Effect\": marginal_effect,\n            \"Significance\": significance_stars(p_value),\n            \"Std. Error\": std_error,\n            \"95% CI Lower\": ci_lower,\n            \"95% CI Upper\": ci_upper\n        })\n\n    # Convert results to formatted table\n    table_str = tabulate(results, headers=[\"Variable\", \"Marginal Effect\", \"Significance\", \"Std. Error\", \"95% CI Lower\", \"95% CI Upper\"],\n                         tablefmt=\"pretty\", colalign=(\"left\", \"decimal\", \"left\", \"decimal\", \"decimal\", \"decimal\"))\n\n    # Convert results to Pandas DataFrame\n    df_results = pd.DataFrame(df_results)\n\n    return table_str, df_results\n\n# Example usage:\n# means = [0.5, 30]  # Mean values for x1 and x2\n# assembler_predictors = ['x1', 'x2']  # Define globally before calling the function\n# table_output, df_output = marginal_effects(fitted_model, means)\n# print(table_output)\n# display(df_output)"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html#udfs",
    "href": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html#udfs",
    "title": "Logistic Regression",
    "section": "",
    "text": "Code\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\n\n\n\n\n\nCode\ndef marginal_effects(model, means):\n    \"\"\"\n    Compute marginal effects for all predictors in a PySpark GeneralizedLinearRegression model (logit)\n    and return a formatted table with statistical significance and standard errors.\n\n    Parameters:\n        model: Fitted GeneralizedLinearRegression model (with binomial family and logit link).\n        means: List of mean values for the predictor variables.\n\n    Returns:\n        - A formatted string containing the marginal effects table.\n        - A Pandas DataFrame with marginal effects, standard errors, confidence intervals, and significance stars.\n    \"\"\"\n    global assembler_predictors  # Use the global assembler_predictors list\n\n    # Extract model coefficients, standard errors, and intercept\n    coeffs = np.array(model.coefficients)\n    std_errors = np.array(model.summary.coefficientStandardErrors)\n    intercept = model.intercept\n\n    # Compute linear combination of means and coefficients (XB)\n    XB = np.dot(means, coeffs) + intercept\n\n    # Compute derivative of logistic function (G'(XB))\n    G_prime_XB = np.exp(XB) / ((1 + np.exp(XB)) ** 2)\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Create lists to store results\n    results = []\n    df_results = []  # For Pandas DataFrame\n\n    for i, predictor in enumerate(assembler_predictors):\n        # Compute marginal effect\n        marginal_effect = G_prime_XB * coeffs[i]\n\n        # Compute standard error of the marginal effect\n        std_error = G_prime_XB * std_errors[i]\n\n        # Compute z-score and p-value\n        z_score = marginal_effect / std_error if std_error != 0 else np.nan\n        p_value = 2 * (1 - norm.cdf(abs(z_score))) if not np.isnan(z_score) else np.nan\n\n        # Compute confidence interval (95%)\n        ci_lower = marginal_effect - 1.96 * std_error\n        ci_upper = marginal_effect + 1.96 * std_error\n\n        # Append results for table formatting\n        results.append([\n            predictor,\n            f\"{marginal_effect: .6f}\",\n            significance_stars(p_value),\n            f\"{std_error: .6f}\",\n            f\"{ci_lower: .6f}\",\n            f\"{ci_upper: .6f}\"\n        ])\n\n        # Append results for Pandas DataFrame\n        df_results.append({\n            \"Variable\": predictor,\n            \"Marginal Effect\": marginal_effect,\n            \"Significance\": significance_stars(p_value),\n            \"Std. Error\": std_error,\n            \"95% CI Lower\": ci_lower,\n            \"95% CI Upper\": ci_upper\n        })\n\n    # Convert results to formatted table\n    table_str = tabulate(results, headers=[\"Variable\", \"Marginal Effect\", \"Significance\", \"Std. Error\", \"95% CI Lower\", \"95% CI Upper\"],\n                         tablefmt=\"pretty\", colalign=(\"left\", \"decimal\", \"left\", \"decimal\", \"decimal\", \"decimal\"))\n\n    # Convert results to Pandas DataFrame\n    df_results = pd.DataFrame(df_results)\n\n    return table_str, df_results\n\n# Example usage:\n# means = [0.5, 30]  # Mean values for x1 and x2\n# assembler_predictors = ['x1', 'x2']  # Define globally before calling the function\n# table_output, df_output = marginal_effects(fitted_model, means)\n# print(table_output)\n# display(df_output)"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html#loading-data",
    "href": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html#loading-data",
    "title": "Logistic Regression",
    "section": "Loading Data",
    "text": "Loading Data\n\ndfpd = pd.read_csv('https://bcdanl.github.io/data/NatalRiskData.csv')\ndf = spark.createDataFrame(dfpd)\ndfpd\n\nWarning: total number of rows (26313) exceeds max_rows (20000). Falling back to pandas display.\n\n\n\n    \n\n\n\n\n\n\nPWGT\nUPREVIS\nCIG_REC\nGESTREC3\nDPLURAL\nULD_MECO\nULD_PRECIP\nULD_BREECH\nURF_DIAB\nURF_CHYPER\nURF_PHYPER\nURF_ECLAM\natRisk\nDBWT\nORIGRANDGROUP\n\n\n\n\n0\n155\n14\n0\n&gt;= 37 weeks\nsingle\n1\n0\n0\n0\n0\n0\n0\n0\n3714\n2\n\n\n1\n140\n13\n0\n&gt;= 37 weeks\nsingle\n0\n0\n0\n0\n0\n0\n0\n0\n3715\n4\n\n\n2\n151\n15\n0\n&gt;= 37 weeks\nsingle\n0\n0\n0\n0\n0\n0\n0\n0\n3447\n2\n\n\n3\n118\n4\n0\n&gt;= 37 weeks\nsingle\n0\n0\n0\n0\n0\n0\n0\n0\n3175\n6\n\n\n4\n134\n11\n0\n&gt;= 37 weeks\nsingle\n0\n0\n0\n0\n0\n0\n0\n0\n4038\n10\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26308\n135\n11\n0\n&gt;= 37 weeks\nsingle\n0\n0\n0\n0\n0\n0\n0\n0\n3771\n9\n\n\n26309\n135\n12\n0\n&gt;= 37 weeks\nsingle\n1\n0\n0\n0\n0\n0\n0\n0\n3210\n8\n\n\n26310\n153\n11\n0\n&gt;= 37 weeks\nsingle\n0\n0\n0\n0\n0\n0\n0\n0\n3515\n5\n\n\n26311\n132\n10\n0\n&gt;= 37 weeks\nsingle\n0\n0\n0\n0\n0\n0\n0\n0\n3147\n9\n\n\n26312\n170\n8\n0\n&gt;= 37 weeks\nsingle\n1\n0\n0\n0\n0\n0\n0\n0\n3325\n9\n\n\n\n\n26313 rows × 15 columns"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html#training-test-data-split",
    "href": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html#training-test-data-split",
    "title": "Logistic Regression",
    "section": "Training-Test Data Split",
    "text": "Training-Test Data Split\n\ndtrain, dtest = df.randomSplit([0.5, 0.5], seed = 1234)"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html#adding-dummies",
    "href": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html#adding-dummies",
    "title": "Logistic Regression",
    "section": "Adding Dummies",
    "text": "Adding Dummies\n\ndfpd['GESTREC3'].unique() # to see categories in GESTREC3 using the pandas' unique() method\n\narray(['&gt;= 37 weeks', '&lt; 37 weeks'], dtype=object)\n\n\n\ndfpd['DPLURAL'].unique() # to see categories in DPLURAL\n\narray(['single', 'twin', 'triplet or higher'], dtype=object)\n\n\n\ndummy_cols_GESTREC3, ref_category_GESTREC3 = add_dummy_variables('GESTREC3', 1)\ndummy_cols_DPLURAL, ref_category_DPLURAL = add_dummy_variables('DPLURAL', 0)\n\nReference category (dummy omitted): &gt;= 37 weeks\nReference category (dummy omitted): single"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html#assembling-predictors",
    "href": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html#assembling-predictors",
    "title": "Logistic Regression",
    "section": "Assembling Predictors",
    "text": "Assembling Predictors\n\n# assembling predictors\nx_cols = ['PWGT', 'UPREVIS', 'CIG_REC',\n          'ULD_MECO', 'ULD_PRECIP', 'ULD_BREECH', 'URF_DIAB',\n          'URF_CHYPER', 'URF_PHYPER', 'URF_ECLAM']\n\n# Keep the name assembler_predictors unchanged,\n#   as it will be used as a global variable in the marginal_effects UDF.\nassembler_predictors = (\n    x_cols +\n    dummy_cols_GESTREC3 + dummy_cols_DPLURAL\n)\n\nassembler_1 = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\n\ndtrain_1 = assembler_1.transform(dtrain)\ndtest_1  = assembler_1.transform(dtest)"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html#model-fitting",
    "href": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html#model-fitting",
    "title": "Logistic Regression",
    "section": "Model Fitting",
    "text": "Model Fitting\n\n# training the model\nmodel_1 = (\n    GeneralizedLinearRegression(featuresCol=\"predictors\",\n                                labelCol=\"atRisk\",\n                                family=\"binomial\",\n                                link=\"logit\")\n    .fit(dtrain_1)\n)"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html#making-predictions",
    "href": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html#making-predictions",
    "title": "Logistic Regression",
    "section": "Making Predictions",
    "text": "Making Predictions\n\n# making prediction on both training and test\ndtrain_1 = model_1.transform(dtrain_1)\ndtest_1 = model_1.transform(dtest_1)"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html#model-summary",
    "href": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html#model-summary",
    "title": "Logistic Regression",
    "section": "Model Summary",
    "text": "Model Summary\n\nmodel_1.summary\n\nCoefficients:\n             Feature Estimate Std Error  T Value P Value\n         (Intercept)  -4.7385    0.3154 -15.0256  0.0000\n                PWGT   0.0029    0.0016   1.8572  0.0633\n             UPREVIS  -0.0285    0.0164  -1.7352  0.0827\n             CIG_REC   0.3991    0.1965   2.0306  0.0423\n            ULD_MECO   1.0701    0.2330   4.5921  0.0000\n          ULD_PRECIP   0.4391    0.3564   1.2318  0.2180\n          ULD_BREECH   0.3139    0.2193   1.4312  0.1524\n            URF_DIAB  -0.0954    0.2874  -0.3320  0.7399\n          URF_CHYPER   0.2442    0.4772   0.5117  0.6089\n          URF_PHYPER   0.1417    0.2728   0.5196  0.6033\n           URF_ECLAM   0.7946    0.7677   1.0350  0.3006\n GESTREC3_&lt;_37_weeks   1.5390    0.1524  10.0977  0.0000\nDPLURAL_triplet_o...   1.5956    0.5906   2.7016  0.0069\n        DPLURAL_twin   0.5490    0.2400   2.2877  0.0222\n\n(Dispersion parameter for binomial family taken to be 1.0000)\n    Null deviance: 2332.2568 on 13108 degrees of freedom\nResidual deviance: 2158.8170 on 13108 degrees of freedom\nAIC: 2186.8170"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html#marginal-effects",
    "href": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html#marginal-effects",
    "title": "Logistic Regression",
    "section": "Marginal Effects",
    "text": "Marginal Effects\n\nCalculating the Mean Value of Each Predictor\n\n# Compute means\nmeans_df = dtrain_1.select([mean(col).alias(col) for col in assembler_predictors])\n\n# Collect the results as a list\nmeans = means_df.collect()[0]\nmeans_list = [means[col] for col in assembler_predictors]\n\n\ntable_output, df_ME = marginal_effects(model_1, means_list) # Instead of mean values, some other representative values can also be chosen.\nprint(table_output)\n\n+---------------------------+-----------------+--------------+------------+--------------+--------------+\n| Variable                  | Marginal Effect | Significance | Std. Error | 95% CI Lower | 95% CI Upper |\n+---------------------------+-----------------+--------------+------------+--------------+--------------+\n| PWGT                      |          0.0000 | *            |     0.0000 |      -0.0000 |       0.0001 |\n| UPREVIS                   |         -0.0004 | *            |     0.0002 |      -0.0008 |       0.0000 |\n| CIG_REC                   |          0.0053 | **           |     0.0026 |       0.0002 |       0.0104 |\n| ULD_MECO                  |          0.0142 | ***          |     0.0031 |       0.0082 |       0.0203 |\n| ULD_PRECIP                |          0.0058 |              |     0.0047 |      -0.0035 |       0.0151 |\n| ULD_BREECH                |          0.0042 |              |     0.0029 |      -0.0015 |       0.0099 |\n| URF_DIAB                  |         -0.0013 |              |     0.0038 |      -0.0088 |       0.0062 |\n| URF_CHYPER                |          0.0033 |              |     0.0064 |      -0.0092 |       0.0157 |\n| URF_PHYPER                |          0.0019 |              |     0.0036 |      -0.0052 |       0.0090 |\n| URF_ECLAM                 |          0.0106 |              |     0.0102 |      -0.0095 |       0.0306 |\n| GESTREC3_&lt;_37_weeks       |          0.0205 | ***          |     0.0020 |       0.0165 |       0.0245 |\n| DPLURAL_triplet_or_higher |          0.0212 | ***          |     0.0079 |       0.0058 |       0.0367 |\n| DPLURAL_twin              |          0.0073 | **           |     0.0032 |       0.0010 |       0.0136 |\n+---------------------------+-----------------+--------------+------------+--------------+--------------+\n\n\n\n\nMarginal Effect Plot\n\n# Increase figure size to prevent overlapping\nplt.figure(figsize=(10, 6))\n\n# Plot using the DataFrame columns\nplt.errorbar(df_ME[\"Variable\"], df_ME[\"Marginal Effect\"],\n             yerr=1.96 * df_ME[\"Std. Error\"], fmt='o', capsize=5)\n\n# Labels and title\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Marginal Effect\")\nplt.title(\"Marginal Effect at the Mean\")\n\n# Add horizontal line at 0 for reference\nplt.axhline(0, color=\"red\", linestyle=\"--\")\n\n# Adjust x-axis labels to avoid overlap\nplt.xticks(rotation=45, ha=\"right\")  # Rotate and align labels to the right\nplt.tight_layout()  # Adjust layout to prevent overlap\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n# Compute means for smokers\nmeans_df_smoker = (\n    dtrain_1\n    .filter(\n        ( col(\"CIG_REC\") == 1 )\n    )\n    .select([mean(col).alias(col) for col in assembler_predictors])\n)\n\n# Collect the results as a list\nmeans_smoker = means_df_smoker.collect()[0]\nmeans_list_smoker = [means_smoker[col] for col in assembler_predictors]\n\n\ntable_output_s, df_ME_s = marginal_effects(model_1, means_list_smoker) # Instead of mean values, some other representative values can also be chosen.\nprint(table_output_s)\n\n+---------------------------+-----------------+--------------+------------+--------------+--------------+\n| Variable                  | Marginal Effect | Significance | Std. Error | 95% CI Lower | 95% CI Upper |\n+---------------------------+-----------------+--------------+------------+--------------+--------------+\n| PWGT                      |          0.0001 | *            |     0.0000 |      -0.0000 |       0.0001 |\n| UPREVIS                   |         -0.0006 | *            |     0.0003 |      -0.0012 |       0.0001 |\n| CIG_REC                   |          0.0081 | **           |     0.0040 |       0.0003 |       0.0160 |\n| ULD_MECO                  |          0.0218 | ***          |     0.0047 |       0.0125 |       0.0311 |\n| ULD_PRECIP                |          0.0089 |              |     0.0073 |      -0.0053 |       0.0232 |\n| ULD_BREECH                |          0.0064 |              |     0.0045 |      -0.0024 |       0.0152 |\n| URF_DIAB                  |         -0.0019 |              |     0.0059 |      -0.0134 |       0.0095 |\n| URF_CHYPER                |          0.0050 |              |     0.0097 |      -0.0141 |       0.0240 |\n| URF_PHYPER                |          0.0029 |              |     0.0056 |      -0.0080 |       0.0138 |\n| URF_ECLAM                 |          0.0162 |              |     0.0156 |      -0.0145 |       0.0469 |\n| GESTREC3_&lt;_37_weeks       |          0.0314 | ***          |     0.0031 |       0.0253 |       0.0375 |\n| DPLURAL_triplet_or_higher |          0.0325 | ***          |     0.0120 |       0.0089 |       0.0561 |\n| DPLURAL_twin              |          0.0112 | **           |     0.0049 |       0.0016 |       0.0208 |\n+---------------------------+-----------------+--------------+------------+--------------+--------------+"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html#classifier-threshold---double-density-plot",
    "href": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html#classifier-threshold---double-density-plot",
    "title": "Logistic Regression",
    "section": "Classifier Threshold - Double Density Plot",
    "text": "Classifier Threshold - Double Density Plot\n\n# Filter training data for atRisk == 1 and atRisk == 0\npdf = dtrain_1.select(\"prediction\", \"atRisk\").toPandas()\n\ntrain_true = pdf[pdf[\"atRisk\"] == 1]\ntrain_false = pdf[pdf[\"atRisk\"] == 0]\n\n# Create the first density plot\nplt.figure(figsize=(8, 6))\nsns.kdeplot(train_true[\"prediction\"], label=\"TRUE\", color=\"red\", fill=True)\nsns.kdeplot(train_false[\"prediction\"], label=\"FALSE\", color=\"blue\", fill=True)\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Density\")\nplt.title(\"Density Plot of Predictions\")\nplt.legend(title=\"atRisk\")\nplt.show()\n\n# Define threshold for vertical line\nthreshold = 0.02  # Replace with actual value\n\n# Create the second density plot with vertical line\nplt.figure(figsize=(8, 6))\nsns.kdeplot(train_true[\"prediction\"], label=\"TRUE\", color=\"red\", fill=True)\nsns.kdeplot(train_false[\"prediction\"], label=\"FALSE\", color=\"blue\", fill=True)\nplt.axvline(x=threshold, color=\"blue\", linestyle=\"dashed\", label=f\"Threshold = {threshold}\")\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Density\")\nplt.title(\"Density Plot of Predictions with Threshold\")\nplt.legend(title=\"atRisk\")\nplt.show()"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html#performance-of-classifier",
    "href": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html#performance-of-classifier",
    "title": "Logistic Regression",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\n\nConfusion Matrix & Performance Metrics\n\n# Compute confusion matrix\ndtest_1 = dtest_1.withColumn(\"predicted_class\", when(col(\"prediction\") &gt; .02, 1).otherwise(0))\nconf_matrix = dtest_1.groupBy(\"atRisk\", \"predicted_class\").count().orderBy(\"atRisk\", \"predicted_class\")\n\nTP = dtest_1.filter((col(\"atRisk\") == 1) & (col(\"predicted_class\") == 1)).count()\nFP = dtest_1.filter((col(\"atRisk\") == 0) & (col(\"predicted_class\") == 1)).count()\nFN = dtest_1.filter((col(\"atRisk\") == 1) & (col(\"predicted_class\") == 0)).count()\nTN = dtest_1.filter((col(\"atRisk\") == 0) & (col(\"predicted_class\") == 0)).count()\n\naccuracy = (TP + TN) / (TP + FP + FN + TN)\nprecision = TP / (TP + FP)\nrecall = TP / (TP + FN)\nspecificity = TN / (TN + FP)\naverage_rate = (TP + FN) / (TP + TN + FP + FN)  # Proportion of actual at-risk babies\nenrichment = precision / average_rate\n\n\n# Print formatted confusion matrix with labels\nprint(\"\\n Confusion Matrix:\\n\")\nprint(\"                     Predicted\")\nprint(\"            |  Negative  |  Positive  \")\nprint(\"------------+------------+------------\")\nprint(f\"Actual Neg. |    {TN:5}   |    {FP:5}  |\")\nprint(\"------------+------------+------------\")\nprint(f\"Actual Pos. |    {FN:5}   |    {TP:5}  |\")\nprint(\"------------+------------+------------\")\n\n\nprint(f\"Accuracy:  {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall (Sensitivity): {recall:.4f}\")\nprint(f\"Specificity:  {specificity:.4f}\")\nprint(f\"Average Rate: {average_rate:.4f}\")\nprint(f\"Enrichment:   {enrichment:.4f} (Relative Precision)\")\n\n\n Confusion Matrix:\n\n                     Predicted\n            |  Negative  |  Positive  \n------------+------------+------------\nActual Neg. |    10774   |     2167  |\n------------+------------+------------\nActual Pos. |      120   |      130  |\n------------+------------+------------\nAccuracy:  0.8266\nPrecision: 0.0566\nRecall (Sensitivity): 0.5200\nSpecificity:  0.8325\nAverage Rate: 0.0190\nEnrichment:   2.9862 (Relative Precision)\n\n\n\n\nTrade-off Between Recall and Precision/Enrichment\n\npdf = dtest_1.select(\"prediction\", \"atRisk\").toPandas()\n\n# Extract predictions and true labels\ny_true = pdf[\"atRisk\"]  # True labels\ny_scores = pdf[\"prediction\"]  # Predicted probabilities\n\n# Compute precision, recall, and thresholds\nprecision_plot, recall_plot, thresholds = precision_recall_curve(y_true, y_scores)\n\n# Compute enrichment: precision divided by average at-risk rate\naverage_rate = np.mean(y_true)\nenrichment_plot = precision_plot / average_rate\n\n# Define optimal threshold (example: threshold where recall ≈ enrichment balance)\nthreshold = 0.02  # Adjust based on the plot\n\n# Plot Enrichment vs. Recall vs. Threshold\nplt.figure(figsize=(8, 6))\nplt.plot(thresholds, enrichment_plot[:-1], label=\"Enrichment\", color=\"blue\", linestyle=\"--\")\nplt.plot(thresholds, recall_plot[:-1], label=\"Recall\", color=\"red\", linestyle=\"-\")\n\n# Add vertical line for chosen threshold\nplt.axvline(x=threshold, color=\"black\", linestyle=\"dashed\", label=f\"Threshold = {threshold}\")\n\n# Labels and legend\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Score\")\nplt.title(\"Enrichment vs. Recall\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Filter the threshold range (0 to 0.15)\nmask = (thresholds &gt;= 0) & (thresholds &lt;= 0.15)\nthresholds_filtered = thresholds[mask]\nenrichment_filtered = enrichment[:-1][mask]\nrecall_filtered = recall[:-1][mask]\n\n# Define optimal threshold (example: threshold where recall ≈ enrichment balance)\nthreshold = 0.02  # Adjust based on the plot\n\n# Plot Enrichment vs. Recall vs. Threshold (Limited to 0 - 0.15)\nplt.figure(figsize=(8, 6))\nplt.plot(thresholds_filtered, enrichment_filtered, label=\"Enrichment\", color=\"blue\", linestyle=\"--\")\nplt.plot(thresholds_filtered, recall_filtered, label=\"Recall\", color=\"red\", linestyle=\"-\")\n\n# Add vertical line for chosen threshold\nplt.axvline(x=threshold, color=\"black\", linestyle=\"dashed\", label=f\"Threshold = {threshold}\")\n\n# Labels and legend\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Score\")\nplt.title(\"Enrichment vs. Recall (Threshold: 0 - 0.15)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAUC and ROC\n\n# Use probability of the positive class (y=1)\nevaluator = BinaryClassificationEvaluator(labelCol=\"atRisk\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n\n# Evaluate AUC\nauc = evaluator.evaluate(dtest_1)\n\nprint(f\"AUC: {auc:.4f}\")  # Higher is better (closer to 1)\n\n# Convert to Pandas\npdf = dtest_1.select(\"prediction\", \"atRisk\").toPandas()\n\n# Compute ROC curve\nfpr, tpr, _ = roc_curve(pdf[\"atRisk\"], pdf[\"prediction\"])\n\n# Plot ROC curve\nplt.figure(figsize=(8,6))\nplt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc:.4f})\")\nplt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\nplt.show()\n\nAUC: 0.7310"
  },
  {
    "objectID": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html#deploying-the-classifier-to-the-real-world",
    "href": "danl-ml/danl_320_logistic_regression_newborn_babies_with_deployment.html#deploying-the-classifier-to-the-real-world",
    "title": "Logistic Regression",
    "section": "Deploying the Classifier to the Real-World",
    "text": "Deploying the Classifier to the Real-World\n\nReal-world Data for NY and MA Hospitals\n\npd_dtest = dtest.toPandas()\n\n# Set seed for reproducibility\nnp.random.seed(23464)\n\n# Sample 1000 random indices from the test dataset without replacement\nsample_indices = np.random.choice(pd_dtest.index, size=1000, replace=False)\n\n# Separate the selected observations from testing data\nseparated = pd_dtest.loc[sample_indices]\n\n# Remove the selected observations from the testing data\n# Consider this as data from NY hospitals\npd_dtest_NY = pd_dtest.drop(sample_indices)\n\n# Split the separated sample into at-risk and not-at-risk groups\nat_risk_sample = separated[separated[\"atRisk\"] == 1]  # Only at-risk cases\nnot_at_risk_sample = separated[separated[\"atRisk\"] == 0]  # Only not-at-risk cases\n\n# Create test sets for MA hospitals with different at-risk average rates compared to NY\npd_dtest_MA_moreRisk = pd.concat([pd_dtest_NY, at_risk_sample])  # Adds back only at-risk cases\npd_dtest_MA_lessRisk = pd.concat([pd_dtest_NY, not_at_risk_sample])  # Adds back only not-at-risk cases\n\n# Show counts to verify results\nprint(\"Original Test Set Size:\", pd_dtest.shape[0])\nprint(\"Sampled Separated Size:\", separated.shape[0])\nprint(\"NY Hospitals Data Size:\", pd_dtest_NY.shape[0])\nprint(\"MA More Risk Data Size:\", pd_dtest_MA_moreRisk.shape[0])\nprint(\"MA Less Risk Data Size:\", pd_dtest_MA_lessRisk.shape[0])\n\ndtest_MA_moreRisk = spark.createDataFrame(pd_dtest_MA_moreRisk)\ndtest_MA_lessRisk = spark.createDataFrame(pd_dtest_MA_lessRisk)\n\nOriginal Test Set Size: 13191\nSampled Separated Size: 1000\nNY Hospitals Data Size: 12191\nMA More Risk Data Size: 12217\nMA Less Risk Data Size: 13165\n\n\n\n\nMA with more at-risk babies\n\ndtest_MA_moreRisk = assembler_1.transform(dtest_MA_moreRisk)\ndtest_MA_moreRisk = model_1.transform(dtest_MA_moreRisk)\n\n# Compute confusion matrix\ndtest_MA_moreRisk = dtest_MA_moreRisk.withColumn(\"predicted_class\", when(col(\"prediction\") &gt; .02, 1).otherwise(0))\nconf_matrix_MA_moreRisk = dtest_MA_moreRisk.groupBy(\"atRisk\", \"predicted_class\").count().orderBy(\"atRisk\", \"predicted_class\")\n\n# Collect as a dictionary for easy lookup\nconf_dict_MA_moreRisk = {(row[\"atRisk\"], row[\"predicted_class\"]): row[\"count\"] for row in conf_matrix_MA_moreRisk.collect()}\n\n# Extract values safely (handle missing values if any)\nTN = conf_dict_MA_moreRisk.get((0, 0), 0)  # True Negative\nFP = conf_dict_MA_moreRisk.get((0, 1), 0)  # False Positive\nFN = conf_dict_MA_moreRisk.get((1, 0), 0)  # False Negative\nTP = conf_dict_MA_moreRisk.get((1, 1), 0)  # True Positive\n\naccuracy_moreRisk = (TP + TN) / (TP + FP + FN + TN)\nprecision_moreRisk = TP / (TP + FP)\nrecall_moreRisk = TP / (TP + FN) # (Recall) + (False Negative Rate) = 1\nspecificity_moreRisk = TN / (TN + FP) # (Specificity) + (False Positive Rate) = 1\naverage_rate_moreRisk = (TP + FN) / (TP + TN + FP + FN)  # Proportion of actual at-risk babies\nenrichment_moreRisk = precision_moreRisk / average_rate_moreRisk\n\n# Print formatted confusion matrix with labels\nprint(\"\\n Confusion Matrix with More At-Risk Babies in MA:\\n\")\nprint(\"                     Predicted\")\nprint(\"            |  Negative  |  Positive  \")\nprint(\"------------+------------+------------\")\nprint(f\"Actual Neg. |    {TN:5}   |    {FP:5}  |\")\nprint(\"------------+------------+------------\")\nprint(f\"Actual Pos. |    {FN:5}   |    {TP:5}  |\")\nprint(\"------------+------------+------------\")\n\nprint(f\"Accuracy:             {accuracy_moreRisk:.4f}\")\nprint(f\"Precision:            {precision_moreRisk:.4f}\")\nprint(f\"Recall (Sensitivity): {recall_moreRisk:.4f}\")\nprint(f\"Specificity:          {specificity_moreRisk:.4f}\")\nprint(f\"Average Rate:         {average_rate_moreRisk:.4f}\")  # Proportion of actual at-risk babies\nprint(f\"Enrichment:           {enrichment_moreRisk:.4f} (Relative Precision)\")\n\n# Evaluate AUC\nauc_moreRisk = evaluator.evaluate(dtest_MA_moreRisk)\n\nprint(f\"AUC: {auc_moreRisk:.4f}\")  # Higher is better (closer to 1)\n\n\n Confusion Matrix with More At-Risk Babies in MA:\n\n                     Predicted\n            |  Negative  |  Positive  \n------------+------------+------------\nActual Neg. |     9965   |     2002  |\n------------+------------+------------\nActual Pos. |      120   |      130  |\n------------+------------+------------\nAccuracy:             0.8263\nPrecision:            0.0610\nRecall (Sensitivity): 0.5200\nSpecificity:          0.8327\nAverage Rate:         0.0205\nEnrichment:           2.9798 (Relative Precision)\nAUC: 0.7307\n\n\n\n\nMA with less at-risk babies\n\ndtest_MA_lessRisk = assembler_1.transform(dtest_MA_lessRisk)\ndtest_MA_lessRisk = model_1.transform(dtest_MA_lessRisk)\n\n# Compute confusion matrix\ndtest_MA_lessRisk = dtest_MA_lessRisk.withColumn(\"predicted_class\", when(col(\"prediction\") &gt; .02, 1).otherwise(0))\nconf_matrix_MA_lessRisk = dtest_MA_lessRisk.groupBy(\"atRisk\", \"predicted_class\").count().orderBy(\"atRisk\", \"predicted_class\")\n\n# Collect as a dictionary for easy lookup\nconf_dict_MA_lessRisk = {(row[\"atRisk\"], row[\"predicted_class\"]): row[\"count\"] for row in conf_matrix_MA_lessRisk.collect()}\n\n# Extract values safely (handle missing values if any)\nTN = conf_dict_MA_lessRisk.get((0, 0), 0)  # True Negative\nFP = conf_dict_MA_lessRisk.get((0, 1), 0)  # False Positive\nFN = conf_dict_MA_lessRisk.get((1, 0), 0)  # False Negative\nTP = conf_dict_MA_lessRisk.get((1, 1), 0)  # True Positive\n\naccuracy_lessRisk = (TP + TN) / (TP + FP + FN + TN)\nprecision_lessRisk = TP / (TP + FP)\nrecall_lessRisk = TP / (TP + FN) # (Recall) + (False Negative Rate) = 1\nspecificity_lessRisk = TN / (TN + FP) # (Specificity) + (False Positive Rate) = 1\naverage_rate_lessRisk = (TP + FN) / (TP + TN + FP + FN)  # Proportion of actual at-risk babies\nenrichment_lessRisk = precision_lessRisk / average_rate_lessRisk\n\n# Print formatted confusion matrix with labels\nprint(\"\\n Confusion Matrix with Less At-Risk Babies in MA:\\n\")\nprint(\"                     Predicted\")\nprint(\"            |  Negative  |  Positive  \")\nprint(\"------------+------------+------------\")\nprint(f\"Actual Neg. |    {TN:5}   |    {FP:5}  |\")\nprint(\"------------+------------+------------\")\nprint(f\"Actual Pos. |    {FN:5}   |    {TP:5}  |\")\nprint(\"------------+------------+------------\")\n\nprint(f\"Accuracy:             {accuracy_lessRisk:.4f}\")\nprint(f\"Precision:            {precision_lessRisk:.4f}\")\nprint(f\"Recall (Sensitivity): {recall_lessRisk:.4f}\")\nprint(f\"Specificity:          {specificity_lessRisk:.4f}\")\nprint(f\"Average Rate:         {average_rate_lessRisk:.4f}\")  # Proportion of actual at-risk babies\nprint(f\"Enrichment:           {enrichment_lessRisk:.4f} (Relative Precision)\")\n\n# Evaluate AUC\nauc_lessRisk = evaluator.evaluate(dtest_MA_lessRisk)\n\nprint(f\"AUC: {auc_lessRisk:.4f}\")  # Higher is better (closer to 1)\n\n\n Confusion Matrix with Less At-Risk Babies in MA:\n\n                     Predicted\n            |  Negative  |  Positive  \n------------+------------+------------\nActual Neg. |    10774   |     2167  |\n------------+------------+------------\nActual Pos. |      106   |      118  |\n------------+------------+------------\nAccuracy:             0.8273\nPrecision:            0.0516\nRecall (Sensitivity): 0.5268\nSpecificity:          0.8325\nAverage Rate:         0.0170\nEnrichment:           3.0351 (Relative Precision)\nAUC: 0.7316\n\n\n\n\nPerformance Comparison\n\n# Define baseline, more risk, and less risk metrics\nbaseline_metrics = {\n    \"Average Rate\": average_rate,\n    \"Accuracy\": accuracy,\n    \"Precision\": precision,\n    \"Recall (Sensitivity)\": recall,\n    \"False Negative Rate\": 1 - recall,\n    \"Specificity\": specificity,\n    \"False Positive Rate\": 1 - specificity,\n    \"Enrichment\": enrichment,\n    \"AUC\": auc\n}\n\nmore_risk_metrics = {\n    \"Average Rate\": average_rate_moreRisk,\n    \"Accuracy\": accuracy_moreRisk,\n    \"Precision\": precision_moreRisk,\n    \"Recall (Sensitivity)\": recall_moreRisk,\n    \"False Negative Rate\": 1 - recall_moreRisk,\n    \"Specificity\": specificity_moreRisk,\n    \"False Positive Rate\": 1 - specificity_moreRisk,\n    \"Enrichment\": enrichment_moreRisk,\n    \"AUC\": auc_moreRisk\n}\n\nless_risk_metrics = {\n    \"Average Rate\": average_rate_lessRisk,\n    \"Accuracy\": accuracy_lessRisk,\n    \"Precision\": precision_lessRisk,\n    \"Recall (Sensitivity)\": recall_lessRisk,\n    \"False Negative Rate\": 1 - recall_lessRisk,\n    \"Specificity\": specificity_lessRisk,\n    \"False Positive Rate\": 1 - specificity_lessRisk,\n    \"Enrichment\": enrichment_lessRisk,\n    \"AUC\": auc_lessRisk\n}\n\n# Compute percentage change for each scenario\npercentage_change = {\n    \"More At-Risk\": [],\n    \"Less At-Risk\": []\n}\n\nfor metric in baseline_metrics.keys():\n    percentage_change[\"More At-Risk\"].append(more_risk_metrics[metric])\n    percentage_change[\"Less At-Risk\"].append(less_risk_metrics[metric])\n\n# Create a DataFrame including baseline metrics with rounded NY values\ndf_metrics = pd.DataFrame({\n    \"Metric\": list(baseline_metrics.keys()),\n    \"NY\": [round(value, 4) for value in baseline_metrics.values()],  # Round US values\n    \"MA with More At-Risk\": [\n        round(more_risk_metrics[metric], 4)\n        for metric in baseline_metrics.keys()\n    ],\n    \"MA with Less At-Risk\": [\n        round(less_risk_metrics[metric], 4)\n        for metric in baseline_metrics.keys()\n    ]\n})\n\n# Print formatted table\nprint(tabulate(df_metrics, headers=\"keys\", tablefmt=\"pretty\", showindex=False, colalign=(\"left\", \"decimal\", \"decimal\")))\n\n+----------------------+--------+----------------------+----------------------+\n| Metric               |     NY | MA with More At-Risk | MA with Less At-Risk |\n+----------------------+--------+----------------------+----------------------+\n| Average Rate         | 0.019  |               0.0205 |        0.017         |\n| Accuracy             | 0.8266 |               0.8263 |        0.8273        |\n| Precision            | 0.0566 |               0.061  |        0.0516        |\n| Recall (Sensitivity) | 0.52   |               0.52   |        0.5268        |\n| False Negative Rate  | 0.48   |               0.48   |        0.4732        |\n| Specificity          | 0.8325 |               0.8327 |        0.8325        |\n| False Positive Rate  | 0.1675 |               0.1673 |        0.1675        |\n| Enrichment           | 2.9862 |               2.9798 |        3.0351        |\n| AUC                  | 0.731  |               0.7307 |        0.7316        |\n+----------------------+--------+----------------------+----------------------+\n\n\n\n# Define baseline, more risk, and less risk metrics\nbaseline_metrics = {\n    \"Average Rate\": average_rate,\n    \"Accuracy\": accuracy,\n    \"Precision\": precision,\n    \"Recall (Sensitivity)\": recall,\n    \"False Negative Rate\": 1 - recall,\n    \"Specificity\": specificity,\n    \"False Positive Rate\": 1 - specificity,\n    \"Enrichment\": enrichment,\n    \"AUC\": auc\n}\n\nmore_risk_metrics = {\n    \"Average Rate\": average_rate_moreRisk,\n    \"Accuracy\": accuracy_moreRisk,\n    \"Precision\": precision_moreRisk,\n    \"Recall (Sensitivity)\": recall_moreRisk,\n    \"False Negative Rate\": 1 - recall_moreRisk,\n    \"Specificity\": specificity_moreRisk,\n    \"False Positive Rate\": 1 - specificity_moreRisk,\n    \"Enrichment\": enrichment_moreRisk,\n    \"AUC\": auc_moreRisk\n}\n\nless_risk_metrics = {\n    \"Average Rate\": average_rate_lessRisk,\n    \"Accuracy\": accuracy_lessRisk,\n    \"Precision\": precision_lessRisk,\n    \"Recall (Sensitivity)\": recall_lessRisk,\n    \"False Negative Rate\": 1 - recall_lessRisk,\n    \"Specificity\": specificity_lessRisk,\n    \"False Positive Rate\": 1 - specificity_lessRisk,\n    \"Enrichment\": enrichment_lessRisk,\n    \"AUC\": auc_lessRisk\n}\n\n# Compute percentage change for each scenario\npercentage_change = {\n    \"More At-Risk (%)\": [],\n    \"Less At-Risk (%)\": []\n}\n\nfor metric in baseline_metrics.keys():\n    percentage_change[\"More At-Risk (%)\"].append(round(((more_risk_metrics[metric] - baseline_metrics[metric]) / baseline_metrics[metric]) * 100, 2))\n    percentage_change[\"Less At-Risk (%)\"].append(round(((less_risk_metrics[metric] - baseline_metrics[metric]) / baseline_metrics[metric]) * 100, 2))\n\n# Create a DataFrame including baseline metrics with rounded NY values\ndf_metrics = pd.DataFrame({\n    \"Metric\": list(baseline_metrics.keys()),\n    \"NY\": [round(value, 2) for value in baseline_metrics.values()],  # Round US values\n    \"MA with More At-Risk (%)\": [\n        round(((more_risk_metrics[metric] - baseline_metrics[metric]) / baseline_metrics[metric]) * 100, 2)\n        for metric in baseline_metrics.keys()\n    ],\n    \"MA with Less At-Risk (%)\": [\n        round(((less_risk_metrics[metric] - baseline_metrics[metric]) / baseline_metrics[metric]) * 100, 2)\n        for metric in baseline_metrics.keys()\n    ]\n})\n\n# Print formatted table\nprint(tabulate(df_metrics, headers=\"keys\", tablefmt=\"pretty\", showindex=False, colalign=(\"left\", \"decimal\", \"decimal\")))\n\n+----------------------+------+--------------------------+--------------------------+\n| Metric               |   NY | MA with More At-Risk (%) | MA with Less At-Risk (%) |\n+----------------------+------+--------------------------+--------------------------+\n| Average Rate         | 0.02 |                     7.97 |          -10.22          |\n| Accuracy             | 0.83 |                    -0.04 |           0.09           |\n| Precision            | 0.06 |                     7.74 |          -8.75           |\n| Recall (Sensitivity) | 0.52 |                     0.0  |           1.3            |\n| False Negative Rate  | 0.48 |                     0.0  |          -1.41           |\n| Specificity          | 0.83 |                     0.02 |           0.0            |\n| False Positive Rate  | 0.17 |                    -0.09 |           0.0            |\n| Enrichment           | 2.99 |                    -0.22 |           1.64           |\n| AUC                  | 0.73 |                    -0.05 |           0.07           |\n+----------------------+------+--------------------------+--------------------------+"
  },
  {
    "objectID": "listing-danl-320-rw.html",
    "href": "listing-danl-320-rw.html",
    "title": "DANL 320 - Project",
    "section": "",
    "text": "Title\n\n\n\nDate\n\n\n\n\n\n\n\n\nTeam Project - Guideline\n\n\nMay 2, 2025\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "danl-qa/danl-320-qa.html",
    "href": "danl-qa/danl-320-qa.html",
    "title": "DANL 320 - Discussion and Q & A Board",
    "section": "",
    "text": "Welcome to our Discussion and Q & A Board! 👋 \nThis space is designed for you to engage with your classmates about the course materials.\nWhether you are looking to delve deeper into the slides, share insights, or have questions about the content, this is the perfect place for you.\nIf you have any specific questions to Byeong-Hak (@bcdanl) or your classmates or need clarification on any points, don’t hesitate to ask here.\nLet’s collaborate and learn from each other!\nPlease note that all our comments are recorded in here, regardless of whether comments are displayed in this page or not.\n\n\n\n Back to top"
  },
  {
    "objectID": "listing-danl-320-qa.html",
    "href": "listing-danl-320-qa.html",
    "title": "DANL 320 - Q & A",
    "section": "",
    "text": "Title\n\n\n\nSubtitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nDANL 320 - Discussion and Q & A Board\n\n\n \n\n\nJanuary 21, 2025\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "blog-listing.html",
    "href": "blog-listing.html",
    "title": "Big & tiny insights through data",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nNBA\n\n3 min\n\n\nByeong-Hak Choe\n\n\nFebruary 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython Basics\n\n1 min\n\n\nByeong-Hak Choe\n\n\nFebruary 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeer Markets\n\n5 min\n\n\nByeong-Hak Choe\n\n\nNovember 2, 2023\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n1 min\n\n\nByeong-Hak Choe\n\n\nOctober 27, 2023\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "danl-hw/danl-320-hw-03.html",
    "href": "danl-hw/danl-320-hw-03.html",
    "title": "Homework 3",
    "section": "",
    "text": "Please submit your Jupyter Notebook for Part 1 in Homework 3 to Brightspace with the name below:\n\ndanl-320-hw3-LASTNAME-FIRSTNAME.ipynb\n( e.g., danl-320-hw3-choe-byeonghak.ipynb )\n\nThe due is April 4, 2025, 11:59 P.M.\nPlease send Byeong-Hak an email (bchoe@geneseo.edu) if you have any questions."
  },
  {
    "objectID": "danl-hw/danl-320-hw-03.html#variable-description",
    "href": "danl-hw/danl-320-hw-03.html#variable-description",
    "title": "Homework 3",
    "section": "Variable Description",
    "text": "Variable Description\n\n\n\nVariable\nDescription\n\n\n\n\nLPRICE\nPurchase price of unit and land\n\n\nVALUE\nCurrent market value of unit\n\n\nSTATE\nState code\n\n\nMETRO\nCentral city/suburban status\n\n\nZINC2\nHousehold income\n\n\nHHGRAD\nEducational level of householder\n\n\nBATHS\nNumber of full bathrooms in unit\n\n\nBEDRMS\nNumber of bedrooms in unit\n\n\nPER\nNumber of persons in household\n\n\nZADULT\nNumber of adults (18+) in household\n\n\nNUNITS\nNumber of units in building\n\n\nEAPTBL\nApartment buildings within 1/2 block of unit\n\n\nECOM1\nBusiness/institutions within 1/2 block\n\n\nECOM2\nFactories/other industry within 1/2 block\n\n\nEGREEN\nOpen spaces within 1/2 block of unit\n\n\nEJUNK\nTrash/junk in streets/properties in 1/2 block\n\n\nELOW1\nSingle-family town/rowhouses in 1/2 block\n\n\nESFD\nSingle-family homes within 1/2 block\n\n\nETRANS\nRR/airport/4-lane highway within 1/2 block\n\n\nEABAN\nAbandoned/vandalized buildings within 1/2 block\n\n\nHOWH\nRating of unit as a place to live\n\n\nHOWN\nRating of neighborhood as a place to live\n\n\nODORA\nNeighborhood has bad smells\n\n\nSTRNA\nNeighborhood has heavy street noise/traffic\n\n\nFRSTHO\nFirst home\n\n\nAMMORT\nAmount of 1st mortgage when acquired\n\n\nINTW\nInterest rate of 1st mortgage (whole number %)\n\n\nMATBUY\nGot 1st mortgage in the same year bought unit\n\n\nDWNPAY\nMain source of down payment on unit"
  },
  {
    "objectID": "danl-hw/danl-320-hw-03.html#question-1",
    "href": "danl-hw/danl-320-hw-03.html#question-1",
    "title": "Homework 3",
    "section": "Question 1",
    "text": "Question 1\nPlot some relationships and tell a story."
  },
  {
    "objectID": "danl-hw/danl-320-hw-03.html#question-2",
    "href": "danl-hw/danl-320-hw-03.html#question-2",
    "title": "Homework 3",
    "section": "Question 2",
    "text": "Question 2\n\nFit a linear regression model with the following specifications:\n\nOutcome variable: \\(\\log(VALUE)\\)\nPredictors: all but AMORT and LPRICE"
  },
  {
    "objectID": "danl-hw/danl-320-hw-03.html#question-3",
    "href": "danl-hw/danl-320-hw-03.html#question-3",
    "title": "Homework 3",
    "section": "Question 3",
    "text": "Question 3\n\nRefit the linear regression model, retaining only statistically significant predictors from Question 1.\nCompare the revised model to the initial model from Question 2 using:\n\n\\(\\beta\\) estimates\n\\(R^2\\)\nRMSE\nResidual plots"
  },
  {
    "objectID": "danl-hw/danl-320-hw-03.html#question-4",
    "href": "danl-hw/danl-320-hw-03.html#question-4",
    "title": "Homework 3",
    "section": "Question 4",
    "text": "Question 4\n\nFit a logistic regression model with the following specifications:\n\nOutcome variable: \\(\\text{GT20DWN}\\) (indicating whether the buyer made a down payment of 20% or more)\nPredictors: All available variables except AMORT and LPRICE\n\nThe outcome variable is defined as: \\[\n\\begin{align}\n\\text{GT20DWN} \\,=\\,\\begin{cases}\n1 & \\text{if}\\; \\frac{\\text{LPRICE} - \\text{AMMORT}}{\\text{LPRICE}} &gt; 0.2 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\end{align}\n\\]\nAnalyze and interpret the following relationships:\nThe association between first-time homeownership (\\(\\text{FRSTHO}\\)) and the probability of making a 20%+ down payment.\nThe association between number of bedrooms (\\(\\text{BEDRMS}\\)) and the probability of making a 20%+ down payment."
  },
  {
    "objectID": "danl-hw/danl-320-hw-03.html#question-5",
    "href": "danl-hw/danl-320-hw-03.html#question-5",
    "title": "Homework 3",
    "section": "Question 5",
    "text": "Question 5\n\nRefit the logistic regression model, adding interaction terms:\n\nPredictors: all previously included predictors in Question 4 plus the interaction between \\(\\text{FRSTHO}\\) and \\(\\text{BEDRMS}\\)\n\nInterpret how the relationship between \\(\\text{BEDRMS}\\) and the probability of a 20%+ down payment varies depending on whether the buyer is a first-time homeowner (\\(\\text{FRSTHO}\\))."
  },
  {
    "objectID": "danl-hw/danl-320-hw-03.html#question-6",
    "href": "danl-hw/danl-320-hw-03.html#question-6",
    "title": "Homework 3",
    "section": "Question 6",
    "text": "Question 6\n\nFit separate logistic regression models (with the same model specification as in Question 4) for two subsets of home data:\n\n\nHomes worth \\(\\text{VALUE} \\geq 175k\\).\nHomes worth \\(\\text{VALUE} &lt; 175k\\).\n\n\nCompare residual deviance, \\(RMSE\\), and classification performance between the two models."
  },
  {
    "objectID": "danl-hw/danl-320-hw-01.html",
    "href": "danl-hw/danl-320-hw-01.html",
    "title": "Homework 1",
    "section": "",
    "text": "Please submit your Jupyter Notebook for Part 3 in Homework 1 to Brightspace with the name below:\n\ndanl-320-hw1-LASTNAME-FIRSTNAME.ipynb\n( e.g., danl-320-hw1-choe-byeonghak.ipynb )\n\nThe due is February 17, 2024, 3:30 P.M.\nPlease send Byeong-Hak an email (bchoe@geneseo.edu) if you have any questions."
  },
  {
    "objectID": "danl-hw/danl-320-hw-01.html#question-0",
    "href": "danl-hw/danl-320-hw-01.html#question-0",
    "title": "Homework 1",
    "section": "Question 0",
    "text": "Question 0\nProvide your GitHub username."
  },
  {
    "objectID": "danl-hw/danl-320-hw-01.html#question-1",
    "href": "danl-hw/danl-320-hw-01.html#question-1",
    "title": "Homework 1",
    "section": "Question 1",
    "text": "Question 1\n\nQ1a\n\nCreate a list of integers from 1 to 10.\nAppend the number 11 to the list and remove the number 5.\n\n\nlist_numbers = list(range(1, 11))\nlist_numbers.append(11)\nlist_numbers.remove(5)\n\n\n\n\nQ1b\n\nConsider the following dictionary of three employees and their salaries:\n\ndict_salaries = {'Alice': 50000, 'Bob': 60000, 'Charlie': 70000}\n\nAdd a new employee 'Dana' with a salary of 65000.\nUpdate 'Alice'’s salary to 55000.\nPrint all employee names and their salaries.\n\n\n\ndict_salaries = {'Alice': 50000, 'Bob': 60000, 'Charlie': 70000}\ndict_salaries['Dana'] = 65000\ndict_salaries['Alice'] = 55000\n\nfor name, salary in dict_salaries.items():\n    print(name, \":\", salary)\n    \n# An f-string (formatted string literal) is a way to embed expressions \n  # inside string literals using curly braces `{}`.\nfor name, salary in dict_salaries.items():\n    print(f'{name}: {salary}')"
  },
  {
    "objectID": "danl-hw/danl-320-hw-01.html#question-2",
    "href": "danl-hw/danl-320-hw-01.html#question-2",
    "title": "Homework 1",
    "section": "Question 2",
    "text": "Question 2\n\nQ2a\n\nAssign a variable salary to 75000.\nUse an if-elif-else statement to print:\n\n'Low' if salary is less than 50,000\n'Medium' if salary is between 50,000 and 99,999\n'High' if salary is 100,000 or more\n\n\n\n\nsalary = 75000\nif salary &lt; 50000:\n    print('Low')\nelif 50000 &lt;= salary &lt; 100000:\n    print('Medium')\nelse:\n    print('High')\n\n\n\n\nQ2b\n\nAssign two variables, role and salary, to 'Manager' and 85000, respectively.\nUse nested if-else statements to print:\n\n'Eligible for bonus' if the role is 'Manager' and the salary is greater than 80,000.\n'Eligible for raise' if the role is 'Analyst' and the salary is less than 60,000.\n'No action needed' for all other cases.\n\n\n\n\nrole = 'Manager'\nsalary = 85000\n\nif role == 'Manager':\n    if salary &gt; 80000:\n        print('Eligible for bonus')\n    else:\n        print('No action needed')\nelif role == 'Analyst':\n    if salary &lt; 60000:\n        print('Eligible for raise')\n    else:\n        print('No action needed')\nelse:\n    print('No action needed')"
  },
  {
    "objectID": "danl-hw/danl-320-hw-01.html#question-3",
    "href": "danl-hw/danl-320-hw-01.html#question-3",
    "title": "Homework 1",
    "section": "Question 3",
    "text": "Question 3\n\nQ3a\n\nConsider the following list of salaries:\n\nlist_salaries = [45000, 60000, 75000, 120000, 30000]\n\nCalculate the average salary.\nUse a for loop to print whether each salary is 'Above Average' or 'Below Average'.\n\n\n\nlist_salaries = [45000, 60000, 75000, 120000, 30000]\naverage_salary = sum(list_salaries) / len(list_salaries)\n\nfor salary in list_salaries:\n    if salary &gt; average_salary:\n        print(f'{salary} is Above Average')\n    else:\n        print(f'{salary} is Below Average')\n\n\n\n\nQ3b\n\nStart with a salary of 50000.\nUse a while loop to increase the salary by 5000 each year until it exceeds 80000.\nPrint the salary after each increment.\n\n\n\nsalary = 50000\nwhile salary &lt;= 80000:\n    print(f'Salary: {salary}')\n    salary += 5000\n\n\n\n\nQ3c\n\nConsider the following dictionary of employee salaries:\n\nsalaries = {'Alice': 50000, 'Bob': 60000, 'Charlie': 70000, 'Dana': 45000}\n\nUse a for loop to print the names of employees who earn more than 55000.\n\n\n\nsalaries = {'Alice': 50000, 'Bob': 60000, 'Charlie': 70000, 'Dana': 45000}\n\nfor name, salary in salaries.items():\n    if salary &gt; 55000:\n        print(f'{name} earns more than 55000')\n\n\n\n\nQ3d\ndata_list = [42, 3.14, 'Data Analytics', True, None, [1, 2, 3], {'key': 'value'}, (4, 5)]\n\nGiven the list above, print the data type of each element using the type() function in a for loop. In the loop:\n\nConvert the integer 42 to a string.\nConvert the float 3.14 to a string, then back to a float.\nConvert the boolean True to an integer.\n\n\n\n\ndata_list = [42, 3.14, 'Data Analytics', True, None, [1, 2, 3], {'key': 'value'}, (4, 5)]\n\nfor item in data_list:\n    print(f'Original: {item}, Type: {type(item)}')\n    \n    if item == 42:\n        item = str(item)\n        print(f'Converted 42 to string: {item}, Type: {type(item)}')\n    \n    if item == 3.14:\n        item = float(str(item))\n        print(f'Converted \"3.14\" to float: {item}, Type: {type(item)}')\n\n    if item is True:\n        item = int(item)\n        print(f'Converted True to integer: {item}, Type: {type(item)}')"
  },
  {
    "objectID": "danl-hw/danl-320-hw-01.html#question-4",
    "href": "danl-hw/danl-320-hw-01.html#question-4",
    "title": "Homework 1",
    "section": "Question 4",
    "text": "Question 4\n\nQ4a\nConsider the variables a and b:\na = 10\nb = 0\n\nUse a try-except block to print the result of a / b.\n\nIf there is an error, print 'Cannot divide by zero!'.\n\n\n\n\na = 10\nb = 0\n\ntry:\n    result = a / b\n    print(result)\nexcept ZeroDivisionError:\n    print('Cannot divide by zero!')\n\n\n\n\nQ4b\n\nConsider the following dictionary of salaries with some missing (None) values:\n\nsalaries = {'Alice': 50000, 'Bob': None, 'Charlie': 70000, 'Dana': None, 'Eve': 80000}\n\nUse a for loop with a try-except block to calculate the total of non-missing salaries.\n\n\n\nsalaries = {'Alice': 50000, 'Bob': None, 'Charlie': 70000, 'Dana': None, 'Eve': 80000}\ntotal = 0\n\nfor name, salary in salaries.items():\n    try:\n        total += salary\n    except TypeError:\n        continue"
  },
  {
    "objectID": "danl-hw/danl-320-hw-01.html#question-5",
    "href": "danl-hw/danl-320-hw-01.html#question-5",
    "title": "Homework 1",
    "section": "Question 5",
    "text": "Question 5\n\nImport the math library and calculate the square root of 81 using the sqrt() function provided by the math library.\n\n\n\nimport math\nmath.sqrt(81)"
  },
  {
    "objectID": "listing-danl-320-ml.html",
    "href": "listing-danl-320-ml.html",
    "title": "DANL 320 - Machine Learning Notebooks",
    "section": "",
    "text": "Title\n\n\n\nSubtitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nLinear Regression\n\n\nBikeshare in DC\n\n\nFebruary 19, 2025\n\n\n\n\n\n\nLinear Regression\n\n\nOrange Juice\n\n\nFebruary 26, 2025\n\n\n\n\n\n\nHomework 2\n\n\nBeer Markets\n\n\nMarch 5, 2025\n\n\n\n\n\n\nLogistic Regression\n\n\nNew Born Baby at Risk\n\n\nMarch 24, 2025\n\n\n\n\n\n\nHomework 3\n\n\nAmerican Housing Survey 2004\n\n\nMarch 26, 2025\n\n\n\n\n\n\nQuasi-Separation and Regularized Logistic Regression\n\n\nCar Safety Rating\n\n\nMarch 31, 2025\n\n\n\n\n\n\nLasso Linear Regression\n\n\nOnline Shopping\n\n\nApril 2, 2025\n\n\n\n\n\n\nLasso Logistic Regression\n\n\nNHL Player Evaluation\n\n\nApril 7, 2025\n\n\n\n\n\n\nOmitted Variable Bias\n\n\nOrange Juice\n\n\nApril 9, 2025\n\n\n\n\n\n\nTree-based Models\n\n\nNBC Shows; Boston Housing Markets\n\n\nApril 14, 2025\n\n\n\n\n\n\nFrom Linear Regression to Tree-based Models\n\n\nClaifornia Housing Markets\n\n\nApril 16, 2025\n\n\n\n\n\n\nHomework 4 - Part 1: Lasso Linear Regerssion - Model 1\n\n\nBeer Markets with Big Demographic Design\n\n\nApril 19, 2025\n\n\n\n\n\n\nHomework 4 - Part 1: Lasso Linear Regerssion - Model 2\n\n\nBeer Markets with Big Demographic Design\n\n\nApril 19, 2025\n\n\n\n\n\n\nHomework 4 - Part 1: Lasso Linear Regerssion - Model 3 with Discussions\n\n\nBeer Markets with Big Demographic Design\n\n\nApril 19, 2025\n\n\n\n\n\n\nHomework 4 - Part 2: Tree-based Models\n\n\nMLB Batting\n\n\nApril 19, 2025\n\n\n\n\n\n\nNo matching items\n Back to top"
  }
]