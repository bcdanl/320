[
  {
    "objectID": "listing-danl-320-cw.html",
    "href": "listing-danl-320-cw.html",
    "title": "DANL 320 - Classwork",
    "section": "",
    "text": "Title\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nClasswork 1\n\n\nBuilding a Personal Website using Git, GitHub, and RStudio with Quarto\n\n\nJanuary 22, 2025\n\n\n\n\nClasswork 2\n\n\nMarkdown Basics\n\n\nJanuary 27, 2025\n\n\n\n\nClasswork 3\n\n\nQuarto Website Basics\n\n\nJanuary 27, 2025\n\n\n\n\nClasswork 4\n\n\nPython Basics\n\n\nJanuary 29, 2025\n\n\n\n\nClasswork 5\n\n\nPySpark Basics - Loading, Summarizing, Selecting, Counting, and Sorting Data\n\n\nFebruary 10, 2025\n\n\n\n\nClasswork 6\n\n\nPySpark Basics - Convering Data Types; Filtering Data; Dealing with Missing Values/Duplicates\n\n\nFebruary 12, 2025\n\n\n\n\nClasswork 7\n\n\nPySpark Basics - Group Operations\n\n\nFebruary 17, 2025\n\n\n\n\nClasswork 8\n\n\nLinear Regression I\n\n\nMarch 5, 2025\n\n\n\n\nClasswork 9\n\n\nLinear Regression II\n\n\nMarch 5, 2025\n\n\n\n\nClasswork 10\n\n\nLogistic Regression\n\n\nMarch 12, 2025\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "blog-listing.html",
    "href": "blog-listing.html",
    "title": "Big & tiny insights through data",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nNBA\n\n\n3 min\n\n\n\nByeong-Hak Choe\n\n\nFebruary 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython Basics\n\n\n1 min\n\n\n\nByeong-Hak Choe\n\n\nFebruary 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeer Markets\n\n\n5 min\n\n\n\nByeong-Hak Choe\n\n\nNovember 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n1 min\n\n\n\nByeong-Hak Choe\n\n\nOctober 27, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "listing-danl-320-qa.html",
    "href": "listing-danl-320-qa.html",
    "title": "DANL 320 - Q & A",
    "section": "",
    "text": "Title\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nDANL 320 - Discussion and Q & A Board\n\n\nundefined\n\n\nJanuary 21, 2025\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "danl-qa/danl-320-qa.html",
    "href": "danl-qa/danl-320-qa.html",
    "title": "DANL 320 - Discussion and Q & A Board",
    "section": "",
    "text": "Welcome to our Discussion and Q & A Board! üëã \nThis space is designed for you to engage with your classmates about the course materials.\nWhether you are looking to delve deeper into the slides, share insights, or have questions about the content, this is the perfect place for you.\nIf you have any specific questions to Byeong-Hak (@bcdanl) or your classmates or need clarification on any points, don‚Äôt hesitate to ask here.\nLet‚Äôs collaborate and learn from each other!\nPlease note that all our comments are recorded in here, regardless of whether comments are displayed in this page or not.\n\n\n\n Back to top"
  },
  {
    "objectID": "listing-danl-320-rw.html",
    "href": "listing-danl-320-rw.html",
    "title": "DANL 320 - Project",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "danl-cw/danl-320-cw-02.html",
    "href": "danl-cw/danl-320-cw-02.html",
    "title": "Classwork 2",
    "section": "",
    "text": "Markdown is a lightweight markup language with plain-text formatting syntax. Its main goal is to be readable and easy to write, even when viewed as plain text. Markdown is widely used for creating formatted text on the web and in various applications such as Quarto.\n\n\n\n\nHeadings are created by adding one or more # symbols before your heading text. The number of # symbols indicates the level of the heading.\n# Heading 1\n## Heading 2\n### Heading 3\n\n\n\nYou can make text bold by wrapping it with two asterisks **, and italic by using one asterisk *.\n*italic* or _italic_\n**bold** or __bold__\n\n\n\nUnordered lists are created using *, -, or +, while ordered lists are numbered.\n- Item 1\n- Item 2\n  - Subitem 2.1\n  - Subitem 2.2\n1. First item\n2. Second item\n\n\n\nLinks are created using [Link Text](URL)\n[DANL 320](https://bcdanl.github.io/320)\n\n\n\nImages are created using ![Alt Text](Image URL).\n![Geneseo Logo](https://bcdanl.github.io/img/geneseo-logo.gif)\n\n\n\n\n\n\n&gt; Be yourself. Everyone else is already taken. - Oscar Wilde.\n\n\n\n\nA ton of markdown emojis are available here üòÑ (:smile:)\n\nhttps://github.com/ikatyang/emoji-cheat-sheet\n\n\n\n\n\nCode blocks are created by using triple backticks (```). Optionally, you can specify the language for syntax highlighting.\n```\n\"string\"\n```\n```python\n# Python code block\nimport numpy as np\n```\n\n\n\n\n\nDo the following tasks on this Classwork 2 Discussion Board:\n\nBasic Syntax: Write a comment with a heading, an unordered list, an ordered list, a link, and an image.\nAdvanced Syntax: Write a comment that includes a Python code block, a blockquote, and an emoji.\n\n\n\n\n\n\nQuarto Markdown Basics\nStart writing on GitHub"
  },
  {
    "objectID": "danl-cw/danl-320-cw-02.html#basic-syntax",
    "href": "danl-cw/danl-320-cw-02.html#basic-syntax",
    "title": "Classwork 2",
    "section": "",
    "text": "Headings are created by adding one or more # symbols before your heading text. The number of # symbols indicates the level of the heading.\n# Heading 1\n## Heading 2\n### Heading 3\n\n\n\nYou can make text bold by wrapping it with two asterisks **, and italic by using one asterisk *.\n*italic* or _italic_\n**bold** or __bold__\n\n\n\nUnordered lists are created using *, -, or +, while ordered lists are numbered.\n- Item 1\n- Item 2\n  - Subitem 2.1\n  - Subitem 2.2\n1. First item\n2. Second item\n\n\n\nLinks are created using [Link Text](URL)\n[DANL 320](https://bcdanl.github.io/320)\n\n\n\nImages are created using ![Alt Text](Image URL).\n![Geneseo Logo](https://bcdanl.github.io/img/geneseo-logo.gif)"
  },
  {
    "objectID": "danl-cw/danl-320-cw-02.html#advanced-syntax",
    "href": "danl-cw/danl-320-cw-02.html#advanced-syntax",
    "title": "Classwork 2",
    "section": "",
    "text": "&gt; Be yourself. Everyone else is already taken. - Oscar Wilde.\n\n\n\n\nA ton of markdown emojis are available here üòÑ (:smile:)\n\nhttps://github.com/ikatyang/emoji-cheat-sheet\n\n\n\n\n\nCode blocks are created by using triple backticks (```). Optionally, you can specify the language for syntax highlighting.\n```\n\"string\"\n```\n```python\n# Python code block\nimport numpy as np\n```"
  },
  {
    "objectID": "danl-cw/danl-320-cw-02.html#practice-problems",
    "href": "danl-cw/danl-320-cw-02.html#practice-problems",
    "title": "Classwork 2",
    "section": "",
    "text": "Do the following tasks on this Classwork 2 Discussion Board:\n\nBasic Syntax: Write a comment with a heading, an unordered list, an ordered list, a link, and an image.\nAdvanced Syntax: Write a comment that includes a Python code block, a blockquote, and an emoji."
  },
  {
    "objectID": "danl-cw/danl-320-cw-02.html#references",
    "href": "danl-cw/danl-320-cw-02.html#references",
    "title": "Classwork 2",
    "section": "",
    "text": "Quarto Markdown Basics\nStart writing on GitHub"
  },
  {
    "objectID": "danl-cw/danl-320-cw-01.html",
    "href": "danl-cw/danl-320-cw-01.html",
    "title": "Classwork 1",
    "section": "",
    "text": "Getting a GitHub account\nStep 1. Create the GitHub account with your Geneseo email.\n\nGo to GitHub.\nClick ‚ÄúSign up for GitHub‚Äù.\n\n\nChoose your GitHub username carefully:\n\nhttps://USERNAME.github.io will be the address for your website.\nByeong-Hak‚Äôs GitHub username is bcdanl, so that Byeong-Hak owns the web address https://bcdanl.github.io.\n\nIt is recommended to have a username with all lower cases.\n\n\n\n\n\nInstalling git if you do not have one.\nStep 2.\n\nCheck whether git is installed in your laptop.\n\n\nFrom the Console Pane in RStudio, click Terminal tab.\n\n\n\n\n\nFrom the Terminal, run the following command to check if your laptop has git installed.\n\ngit --version\n\nIf your computer has git installed, you will see the message below and you do not need to install git:\n\ngit version 2.xx\n\nIf your computer does not have git installed, you will see the message below and you need to install git:\n\n'git' is not recognized as an internal or external command\n\n\nInstall git if you do not have one. Move to the next step if you have git installed in your laptop.\n\n\n\n\nMac\n\nGo to http://git-scm.com/downloads, and download the file.\nClick ‚ÄúmacOS‚Äù, scroll down the webpage, and then click ‚Äúinstaller‚Äù from the Binary installer section.\nRun the downloaded file.\n\n\n\n\nWindows\n\nGo to https://gitforwindows.org, and download the file.\nRun the downloaded file.\n\n\n\n\n\nKeep clicking ‚ÄúNext‚Äù to complete the installation of git.\nAfter the git installation is done, close RStudio and re-open it.\n\n\nHow to open git installation file on Mac?\n\nRun the downloaded file.\nClick Okay\nGo to ‚ÄúSetting‚Äù &gt; ‚ÄúPrivacy and Security‚Äù\nGo to ‚ÄúGeneral‚Äù or scroll down\nClick ‚ÄúOpen Anyway‚Äù\n\n\n\n\n\n\n\n\nSetting up GitHub Credential on your local Git.\nStep 3. In Terminal, run the following commands one by one:\ngit config --global user.email \"YOUR_EMAIL_ADDRESS\"\ngit config --global user.name \"YOUR_USERNAME\"\nFor example, the email address for my GitHub account is bchoe@geneseo.edu, and my GitHub username is bcdanl, so that I ran below:\ngit config --global user.email \"bchoe@geneseo.edu\"\ngit config --global user.name \"bcdanl\"\n\nStep 4. Obtain a personal access token (PAT) from GitHub.\n\nIn RStudio Console, run the followings line by line:\n\ninstall.packages(\"usethis\")\nusethis::create_github_token()\n\nThen, click ‚ÄúGenerate token‚Äù in the pop-upped web browser.\nWe can think of GitHub‚Äôs personal access token as a password that expires. You can decide how long it remains valid. My recommendation is to set its expiration for May 31, 2025, or later.\n\n\n\n\n\nThen, copy the generated PAT, and paste it to your clipboard or R script.\n\n\nStep 5. Set the GitHub credential using the PAT.\n\nIn RStudio Console, run the followings line by line:\n\ninstall.packages(\"gitcreds\")\ngitcreds::gitcreds_set()\n\nYou will be asked to provide your PAT.\nPaste your PAT to the RStudio Console, and then hit Enter.\n\n\n\n\n\n\n\nNote\n\n\n\n\nIt does not harm to create multiple PAT for one GitHub account.\nAfter the PAT expires, you should repeat the following if you want to update your GitHub website:\n\n\nCreate a new PAT:\n\nusethis::create_github_token()\n\nReplace the current PAT with the new PAT:\n\ngitcreds::gitcreds_set()\n\nSelect the option 2: Replace these credentials by typing 2 and hitting Enter on R Console.\n\n\n\n\n\n\nEstablishing the Connection between GitHub repo and your local Git\nStep 6. Login to your GitHib and make the repository.\n\nFrom https://github.com, click the plus [+] icon in the upper right corner and select ‚ÄúNew repository‚Äù.\nName this repo USERNAME.github.io, which will be the domain for your website.\n\n\ne.g., If your GitHub username is abc9, the name of your repo should be abc9.github.io, not abc_9.github.io.\n\n\nThen, copy the web address of your GitHub repo, https://github.com/USERNAME/USERNAME.github.io\n\n\nFor example, the web address for Byeong-Hak‚Äôs GitHub repo is https://github.com/bcdanl/bcdanl.github.io.\n\n\nStep 7. Create a RStudio project with Version Control\n\n\n\n\nClick ‚ÄúProject (None)‚Äù at the top-right corner in RStudio.\nClick ‚ÄúNew Project‚Äù &gt; ‚ÄúVersion Control‚Äù &gt; ‚ÄúGit‚Äù\nPaste the web address of your GitHub repo to the Repository URL menu.\nClick ‚ÄúBrowse‚Äù to select the parent directory for your local project directory (I recommend ‚ÄúDocuments‚Äù folder.)\nClick ‚ÄúCreate‚Äù\n\n\n\n\n\n\n\nNote\n\n\n\nIf Step 7 does not work on your laptop, try below Steps 7-1 and 7-2 instead. If Step 7 DOES work well, skip Steps 7-1 and 7-2.\n\n\nStep 7-1. Use git clone to establish the connection between GitHub repo and your local laptop:\n\nChange directory to ‚ÄúDocuments‚Äù in Terminal using cd command.\n\ncd &lt;pathname of \"Documents\" directory&gt;\n\nHere, you need to know the pathname of ‚ÄúDocuments‚Äù directory.\nFor example, LAPTOP_USERNAME below is not your GitHub username but one for your local laptop.\n\nMac\ncd /Users/LAPTOP_USERNAME/Documents\nWindows\ncd C:/Users/LAPTOP_USERNAME/Documents\n\nUse git clone to creates a local copy of the GitHub Repository.\n\ngit clone &lt;repository-url&gt;\n\nFor example,\n\ngit clone https://github.com/USERNAME/USERNAME.github.io\n\nStep 7-2. Create a RStudio project from Existing Directory\n\nClick ‚ÄúProject (None)‚Äù at the top-right corner in RStudio.\nClick ‚ÄúNew Project‚Äù &gt; ‚ÄúExisting Directory‚Äù\nClick ‚ÄúBrowse‚Äù to select the local copy of the GitHub Repository\nClick ‚ÄúCreate Project‚Äù\n\n\n\n\nDownloading Website Template Files\nStep 8. Download the files of website template:\n\nGo to the following webpage: https://github.com/bcdanl/danl-website-template\nFrom the webpage above, click the green icon &lt; &gt; Code, and then click ‚ÄúDownload Zip‚Äù\nExtract the Zip file you have downloaded\nIf there are the files, .gitignore, .DS_Store, or *.Rproj, in the folder, delete all of them.\nMove all the files that were compressed in the Zip file to your local project directory, USERNAME.github.io.\n\n\nSelect all the files in the danl-website-template folder (Tip: Ctrl + A (Windows) / command + A (Mac) selects all files in a directory).\nThen, Ctrl + C (Windows) / command + C (Mac) to copy them.\nThen, go to your local project directory USERNAME.github.io.\nThen, Ctrl + V (Windows) / command + V (Mac) to paste them to your local project directory USERNAME.github.io.\n\n\nRemove the danl-website-template directory from your local project directory, if you have one.\n\n\nAll the website files should be located at the same level with the R Project file (USERNAME.github.io.Rproj), shown below.\n\n\n\n\n\n\n\nPushing the Website Files to the GitHub repository\n\n\n\nStep 8. Push the files to your GitHub repository\n\nOn Terminal within RStudio, execute the following 3-step git commands, which will stage, commit, and push all the files in the local working directory to your GitHub repository:\n\n\ngit add . adds changes in your local working directory (e.g., edited files, new files, deleted files) to the staging area, which is a temporary area where you can prepare your next commit\n\ngit add .\n\ngit commit -m \"...\" records the changes in the staging area as a new snapshot in the local working directory, along with a message describing the changes.\n\ngit commit -m \"any message to describe the changes\"\n\ngit push uploads the local changes to the online repository in GitHub.\n\ngit push\n\nStep 9. Check whether the files are well uploaded.\n\nGo to the webpages of your GitHub repository and your website:\n\nhttps://github.com/USERNAME/USERNAME.github.io.git\nhttps://USERNAME.github.io\nRefresh the webpages (Ctrl + R for Windows users; cmd + R for Mac users)\n\nAdd a URL for your website (https://YOUR_GITHUB_USERNAME.github.io/) in About section in your GihtHub repository webpage by clicking the setting. Below describes how to do it:\n\n\n\n\nDiscussion\nWelcome to our Classwork 1 Discussion Board! üëã \nThis space is designed for you to engage with your classmates about the material covered in Classwork 1.\nWhether you are looking to delve deeper into the content, share insights, or have questions about the content, this is the perfect place for you.\nIf you have any specific questions for Byeong-Hak (@bcdanl) regarding the Classwork 1 materials or need clarification on any points, don‚Äôt hesitate to ask here.\nAll comments will be stored here.\nLet‚Äôs collaborate and learn from each other!\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html",
    "href": "posts/beer-markets/beer-markets.html",
    "title": "Beer Markets",
    "section": "",
    "text": "Diving into the complex world of what people like in their beer, the beer_markets.csv dataset comes across as a goldmine of data, showing us the detailed interactions between buyers and their favorite beers. This dataset covers everything from how much and at what price people are buying beer to how deals and brand loyalty influence their decisions, across different types of people and places. As we start digging into this dataset, we aim to uncover the patterns that show what really influences the modern beer drinker‚Äôs choices, offering up valuable insights for marketers, industry watchers, and beer lovers. By breaking down the data, our exploration will shine a light on the factors that drive consumer behavior in the beer market, giving us a full picture of the trends that shape this lively industry.\nCode\n# Creating an interactive table\n!pip install itables\nfrom itables import init_notebook_mode\nfrom itables import show\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Reading the CSV file\nbeer_data = pd.read_csv(\"https://bcdanl.github.io/data/beer_markets.csv\")\nshow(beer_data)\n\n\n\n\n\n\n\n    \n      \n      hh\n      _purchase_desc\n      quantity\n      brand\n      dollar_spent\n      beer_floz\n      price_per_floz\n      container\n      promo\n      market\n      buyertype\n      income\n      childrenUnder6\n      children6to17\n      age\n      employment\n      degree\n      cow\n      race\n      microwave\n      dishwasher\n      tvcable\n      singlefamilyhome\n      npeople\n    \n  Loading... (need help?)\nCode\n# Setting up the visualization settings\nsns.set(style=\"whitegrid\")\n\n# Calculate total quantity and spending for each brand\nbrand_summary = beer_data.groupby('brand').agg({'quantity':'sum', 'dollar_spent':'sum'}).reset_index()\n\n# Sort by total quantity and spending\nbrand_summary_sorted_quantity = brand_summary.sort_values('quantity', ascending=False)\nbrand_summary_sorted_spent = brand_summary.sort_values('dollar_spent', ascending=False)\nCode\n# Plotting total quantity for each brand\nplt.figure(figsize=(10, 8))\nsns.barplot(x='quantity', y='brand', data=brand_summary_sorted_quantity, palette='viridis')\nplt.title('Total Quantity of Beer Purchased by Brand')\nplt.xlabel('Total Quantity')\nplt.ylabel('Brand')\nplt.show()\nThe bar charts above display the total quantity of beer purchased and the total spending by brand. From the looks of it, certain brands dominate in terms of quantity sold and total spending, indicating their popularity.\nNow, let‚Äôs calculate the average quantity purchased and average spending per purchase. For this, we‚Äôll consider each row in the dataset as a separate purchase and compute the averages accordingly.\nCode\n# Calculate average quantity purchased and average spending per purchase\naverage_purchase = beer_data.groupby('brand').agg({\n    'quantity': 'mean',\n    'dollar_spent': 'mean'\n}).reset_index()\n\n# Sort by average quantity and average spending\naverage_purchase_sorted_quantity = average_purchase.sort_values('quantity', ascending=False)\naverage_purchase_sorted_spent = average_purchase.sort_values('dollar_spent', ascending=False)\n\n# Plotting average quantity for each brand\nplt.figure(figsize=(10, 8))\nsns.barplot(x='quantity', y='brand', data=average_purchase_sorted_quantity, palette='viridis')\nplt.title('Average Quantity of Beer Purchased by Brand')\nplt.xlabel('Average Quantity')\nplt.ylabel('Brand')\nplt.show()\nThe visualizations above depict the average quantity of beer purchased per brand and the average spending per brand. This shows which brands tend to be bought in larger quantities on average and which brands tend to have higher spending per purchase, which could be indicative of their price point or the purchase of premium products.\nNext, we‚Äôll look at the total spending across different markets to see if there are any notable differences in spending habits geographically. To do this, we‚Äôll sum up the spending in each market and visualize it.\nCode\n# Calculate total spending in each market\nmarket_spending_summary = beer_data.groupby('market').agg({'dollar_spent':'sum'}).reset_index()\n\n# Sort by total spending\nmarket_spending_summary_sorted = market_spending_summary.sort_values('dollar_spent', ascending=False)\n\n# Plotting total spending in each market\nplt.figure(figsize=(12, 18))\nsns.barplot(x='dollar_spent', y='market', data=market_spending_summary_sorted, palette='viridis')\nplt.title('Total Spending on Beer by Market')\nplt.xlabel('Total Spending')\nplt.ylabel('Market')\nplt.show()\nThe bar chart illustrates the total spending on beer by market, showcasing the differences in spending habits across various regions. Some markets have significantly higher spending, which could be due to a variety of factors including market size, consumer preferences, or economic factors.\nNow, let‚Äôs move on to the second analysis:"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html#demographic-analysis",
    "href": "posts/beer-markets/beer-markets.html#demographic-analysis",
    "title": "Beer Markets",
    "section": "Demographic Analysis",
    "text": "Demographic Analysis\nWe will examine which demographics are buying what kind of beer and whether spending habits vary by demographics such as age, employment, and race. For this, we could look at:\n\nSpending by age group\nSpending by employment status\nSpending by race\n\nI‚Äôll start by analyzing spending by age group.\n\n\nCode\n# Calculate total spending by age group\nage_group_spending = beer_data.groupby('age').agg({'dollar_spent':'sum'}).reset_index()\n\n# Sort by total spending\nage_group_spending_sorted = age_group_spending.sort_values('dollar_spent', ascending=False)\n\n# Plotting total spending by age group\nplt.figure(figsize=(10, 6))\nsns.barplot(x='dollar_spent', y='age', data=age_group_spending_sorted, palette='viridis')\nplt.title('Total Spending on Beer by Age Group')\nplt.xlabel('Total Spending')\nplt.ylabel('Age Group')\nplt.show()\n\n\n\n\n\nThe bar chart demonstrates the total spending on beer segmented by age group, highlighting which age groups spend the most on beer. It appears that certain age groups are more dominant in beer spending, which may align with the purchasing power or preferences of those groups.\nNext, we will examine spending by employment status.\n\n\nCode\n# Calculate total spending by employment status\nemployment_spending = beer_data.groupby('employment').agg({'dollar_spent':'sum'}).reset_index()\n\n# Sort by total spending\nemployment_spending_sorted = employment_spending.sort_values('dollar_spent', ascending=False)\n\n# Plotting total spending by employment status\nplt.figure(figsize=(10, 6))\nsns.barplot(x='dollar_spent', y='employment', data=employment_spending_sorted, palette='viridis')\nplt.title('Total Spending on Beer by Employment Status')\nplt.xlabel('Total Spending')\nplt.ylabel('Employment Status')\nplt.show()\n\n\n\n\n\nThe visualization shows the total spending on beer by employment status. We can see that certain employment groups, such as full-time workers, are spending more on beer, which might be related to their disposable income.\nFinally, let‚Äôs look at spending by race to complete the demographic analysis.\n\n\nCode\n# Calculate total spending by race\nrace_spending = beer_data.groupby('race').agg({'dollar_spent':'sum'}).reset_index()\n\n# Sort by total spending\nrace_spending_sorted = race_spending.sort_values('dollar_spent', ascending=False)\n\n# Plotting total spending by race\nplt.figure(figsize=(10, 6))\nsns.barplot(x='dollar_spent', y='race', data=race_spending_sorted, palette='viridis')\nplt.title('Total Spending on Beer by Race')\nplt.xlabel('Total Spending')\nplt.ylabel('Race')\nplt.show()\n\n\n\n\n\nThe bar chart above indicates the total spending on beer broken down by race, highlighting which racial groups account for the most beer spending within the dataset. This could reflect both the demographics of the regions where the data was collected and cultural preferences regarding beer.\nNow, let‚Äôs proceed to the third analysis:"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html#price-sensitivity",
    "href": "posts/beer-markets/beer-markets.html#price-sensitivity",
    "title": "Beer Markets",
    "section": "Price Sensitivity",
    "text": "Price Sensitivity\nWe‚Äôll look at the price per fluid ounce and see if there are any trends or correlations with the quantity purchased or the brand popularity. To do this, we‚Äôll visualize how the price is sensitive to the quantity purchased by brand.\n\n\nCode\n# Ensure there's no entries with 0 for 'price_per_floz' or 'quantity' to avoid log(0) issues\nfiltered_data = beer_data[(beer_data['price_per_floz'] &gt; 0) & (beer_data['quantity'] &gt; 0)]\n\n# Calculate log values for both 'price_per_floz' and 'quantity'\nfiltered_data['log_price_per_floz'] = np.log(filtered_data['price_per_floz'])\nfiltered_data['log_quantity'] = np.log(filtered_data['quantity'])\n\n# Use seaborn to create a scatterplot with fitted lines, facetted by 'brand'\ng = sns.lmplot(data=filtered_data, x='log_quantity', y='log_price_per_floz', col='brand', col_wrap=4, height=3, line_kws={'color': 'red'}, scatter_kws={'alpha':0.5}, aspect = .75)\n\n# Adjusting plot aesthetics\ng.set_titles(\"{col_name}\")\ng.set_axis_labels(\"Log of Quantity\", \"Log of Price per Floz\")\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle('Log of Price per Floz vs. Log of Quantity')\n\nplt.show()\n\n\n\n\n\nHere‚Äôs the scatterplot with fitted straight lines for the log of price_per_floz versus the log of quantity, facetted by brands. Each subplot represents a different brand, showing the relationship between these two logarithmic variables along with a fitted line to illustrate the trend within each brand‚Äôs data.\n\n\nCode\n# Adjust the facetting to split rows by 'brand' and columns by 'promo' for a more detailed comparative analysis\ng = sns.lmplot(data=filtered_data, x='log_quantity', y='log_price_per_floz', row='brand', col='promo', height=3, aspect=.75, line_kws={'color': 'red'}, scatter_kws={'alpha':0.5})\n\n# Adjusting plot aesthetics\ng.set_titles(\"Brand: {row_name}\\n Promo: {col_name}\")\ng.set_axis_labels(\"Log of Quantity\", \"Log of Price per Floz\")\nplt.subplots_adjust(top=0.9, wspace = .4, hspace = .4)\ng.fig.suptitle('Log of Price per Floz vs. Log of Quantity')\n\nplt.show()\n\n\n\n\n\nThe scatterplot has been reorganized to split rows by brand and columns by promo status, offering a comprehensive view across different brands and their promotional status. Each subplot now provides a clear comparison of the log of price_per_floz versus the log of quantity for purchases made on promotion versus those that were not, across various beer brands.\nThis layout facilitates an easier comparison across brands and how promotion impacts the relationship between quantity and price per fluid ounce within each brand.\nLastly, let‚Äôs move to the fourth analysis:"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html#promotional-impact-on-quantity-purchased",
    "href": "posts/beer-markets/beer-markets.html#promotional-impact-on-quantity-purchased",
    "title": "Beer Markets",
    "section": "Promotional Impact on Quantity Purchased",
    "text": "Promotional Impact on Quantity Purchased\nWe‚Äôll assess the impact of promotions on the quantity of beer purchased. For this analysis, we can calculate the average quantity purchased with and without promotions and visualize the difference. We‚Äôll do this for each brand to see which brands are most affected by promotions.\nLet‚Äôs begin this analysis by looking at the average quantity purchased with and without promotions for each brand.\n\n\nCode\n# Calculate average quantity purchased with and without promotions for each brand\npromo_impact = beer_data.groupby(['brand', 'promo']).agg({'quantity':'mean'}).reset_index()\n\n# Pivot the data to have promo and non-promo side by side for each brand\npromo_impact_pivot = promo_impact.pivot(index='brand', columns='promo', values='quantity').reset_index()\npromo_impact_pivot.columns = ['brand', 'non_promo', 'promo']\n\n# Calculate the difference in average quantity purchased between promo and non-promo\npromo_impact_pivot['promo_impact'] = promo_impact_pivot['promo'] - promo_impact_pivot['non_promo']\n\n# Sort by the impact of promo\npromo_impact_pivot_sorted = promo_impact_pivot.sort_values('promo_impact', ascending=False)\n\n# Plotting the difference in average quantity purchased between promo and non-promo for each brand\nplt.figure(figsize=(12, 10))\nsns.barplot(x='promo_impact', y='brand', data=promo_impact_pivot_sorted, palette='viridis')\nplt.title('Impact of Promotions on Average Quantity Purchased by Brand')\nplt.xlabel('Difference in Average Quantity Purchased (Promo - Non-Promo)')\nplt.ylabel('Brand')\nplt.show()\n\n\n\n\n\nThe bar chart illustrates the impact of promotions on the average quantity of beer purchased by brand. A positive value indicates that, on average, more beer is purchased when there is a promotion compared to when there isn‚Äôt. Some brands appear to be significantly more influenced by promotions, with customers buying more when the products are on sale or promotion.\nThis comprehensive analysis has provided insights into purchase patterns, demographic preferences, price sensitivity, and the impact of promotions on beer purchases."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\n\n\n\n Back to top"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-1",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-1",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nVariables Are Names, Not Places\n\n\n\n\nA value is datum (literal) such as a number or text.\nThere are different types of values:\n\n352.3 is known as a float or double;\n22 is an integer;\n‚ÄúHello World!‚Äù is a string."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-2",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-2",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nValues, Variables, and Types\na = 10\nprint(a)\n\n\n\n\n\n\n\nA variable is a name that refers to a value.\n\nWe can think of a variable as a box that has a value, or multiple values, packed inside it.\n\nA variable is just a name!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-3",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-3",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nValues, Variables, and Types\n\n\nSometimes you will hear variables referred to as objects.\nEverything that is not a literal value, such as 10, is an object."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-4",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-4",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-5",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-5",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nAssignment ( = )\n# Here we assign the integer value 5 to the variable x.\nx = 5   \n\n# Now we can use the variable x in the next line.\ny = x + 12  \ny\n\nIn Python, we use = to assign a value to a variable.\nIn math, = means equality of both sides.\nIn programs, = means assignment: assign the value on the right side to the variable on the left side."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-6",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-6",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nCode and comment style\n\nThe two main principles for coding and managing data are:\n\nMake things easier for your future self.\nDon‚Äôt trust your future self.\n\nThe # mark is Google Colab‚Äôs comment character.\n\nThe # character has many names: hash, sharp, pound, or octothorpe.\n# indicates that the rest of the line is to be ignored.\nWrite comments before the line that you want the comment to apply to.\n\nConsider adding more comments on code cells and their results using text cells."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-7",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-7",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nAssignment\n\nIn programming code, everything on the right side needs to have a value.\n\nThe right side can be a literal value, or a variable that has already been assigned a value, or a combination.\n\nWhen Python reads y = x + 12, it does the following:\n\nSees the = in the middle.\nKnows that this is an assignment.\nCalculates the right side (gets the value of the object referred to by x and adds it to 12).\nAssigns the result to the left-side variable, y."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-8",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-8",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nVariables Are Names, Not Places\nlist_example = [10, 1.23, \"like this\", True, None]\nprint(list_example)\ntype(list_example)\n\nThe most basic built-in data types that we‚Äôll need to know about are:\n\nintegers 10\nfloats 1.23\nstrings \"like this\"\nbooleans True\nnothing None\n\nPython also has a built-in type of data container called a list (e.g., [10, 15, 20]) that can contain anything, even different types"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-9",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-9",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nTypes\n\n\n\n\nThe second column (Type) contains the Python name of that type.\nThe third column (Mutable?) indicates whether the value can be changed after creation."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-10",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-10",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nBrackets\n\n\nThere are several kinds of brackets in Python, including [], {}, and ().\n\n\n[]{}()\n\n\nvector = ['a', 'b']\nvector[0]\n\n[] is used to denote a list or to signify accessing a position using an index.\n\n\n\n{'a', 'b'}  # set\n{'first_letter': 'a', 'second_letter': 'b'}  # dictionary\n\n{} is used to denote a set or a dictionary (with key-value pairs).\n\n\n\nnum_tup = (1, 2, 3)\nsum(num_tup)\n\n() is used to denote\n\na tuple, or\nthe arguments to a function, e.g., function(x) where x is the input passed to the function."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-11",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-11",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nOperators\nstring_one = \"This is an example \"\nstring_two = \"of string concatenation\"\nstring_full = string_one + string_two\nprint(string_full)\n\nAll of the basic operators we see in mathematics are available to use:\n\n\n\n\n+ for addition\n- for subtraction\n\n\n\n* for multiplication\n** for powers\n\n\n\n/ for division\n// for integer division\n\n\n\n\nThese work as you‚Äôd expect on numbers.\nThese operators are sometimes defined for other built-in data types too.\n\nWe can ‚Äòsum‚Äô strings (which really concatenates them)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-12",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-12",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nOperators\n\n\nlist_one = [\"apples\", \"oranges\"]\nlist_two = [\"pears\", \"satsumas\"]\nlist_full = list_one + list_two\nprint(list_full)\n\nIt works for lists too:\n\n\nstring = \"apples, \"\nprint(string * 3)\n\nWe can multiply strings!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-13",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-13",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nOperators\nQ. Classwork 4.1\nUsing Python operations only, calculate below: \\[\\frac{2^5}{7 \\cdot (4 - 2^3)}\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-14",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-14",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nCasting Variables\n\n\norig_number = 4.39898498\ntype(orig_number)\n\nmod_number = int(orig_number)\nmod_number\ntype(mod_number)\n\n\n\nSometimes we need to explicitly cast a value from one type to another.\n\nWe can do this using built-in functions like str(), int(), and float().\nIf we try these, Python will do its best to interpret the input and convert it to the output type we‚Äôd like and, if they can‚Äôt, the code will throw a great big error."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-15",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-15",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nTuples and (im)mutability\n\n\nA tuple is an object that is defined by parentheses and entries that are separated by commas, for example (15, 20, 32). (They are of type tuple.)\nTuples are immutable, while lists are mutable.\nImmutable objects, such as tuples and strings, can‚Äôt have their elements changed, appended, extended, or removed.\n\nMutable objects, such as lists, can do all of these things.\n\nIn everyday programming, we use lists and dictionaries more than tuples."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-16",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-16",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nDictionaries\ncities_to_temps = {\"Paris\": 28, \"London\": 22, \"New York\": 36, \"Seoul\": 29}\n\ncities_to_temps.keys()\ncities_to_temps.values()\ncities_to_temps.items()\n\nAnother built-in Python type that is enormously useful is the dictionary.\n\nThis provides a mapping one set of variables to another (either one-to-one or many-to-one).\nIf you need to create associations between objects, use a dictionary.\n\nWe can obtain keys, values, or key-value paris from dictionaries."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-17",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-17",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nRunning on Empty\n\nBeing able to create empty containers is sometimes useful, especially when using loops.\nThe commands to create empty lists, tuples, dictionaries, and sets are lst = [], tup=(), dic={}, and st = set() respectively.\nQ. What is the type of an empty list?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-18",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-18",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\nname_list = [\"Lovelace\", \"Smith\", \"Hopper\", \"Babbage\"]\n\nprint(\"Lovelace\" in name_list)\n\nprint(\"Bob\" in name_list)\n\nOne of the most useful conditional keywords is in.\n\nThis one must pop up ten times a day in most coders‚Äô lives because it can pick out a variable or make sure something is where it‚Äôs supposed to be.\n\nQ. Check if ‚Äúa‚Äù is in the string ‚ÄúSun Devil Arena‚Äù using in. Is ‚Äúa‚Äù in ‚ÄúAnyone‚Äù?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-19",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-19",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nBooleans and Conditions\nscore = 98\n\nif score == 100:\n    print(\"Top marks!\")\nelif score &gt; 90 and score &lt; 100:\n    print(\"High score!\")\nelif score &gt; 10 and score &lt;= 90:\n    pass\nelse:\n    print(\"Better luck next time.\")\n\nOne conditional construct we‚Äôre bound to use at some point, is the if-else chain:"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-20",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-20",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nCasting Variables\n\n\norig_number = 4.39898498\ntype(orig_number)\n\nmod_number = int(orig_number)\nmod_number\ntype(mod_number)\n\n\n\nSometimes we need to explicitly cast a value from one type to another.\n\nWe can do this using built-in functions like str(), int(), and float().\nIf we try these, Python will do its best to interpret the input and convert it to the output type we‚Äôd like and, if they can‚Äôt, the code will throw a great big error."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-21",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-21",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nTuples and (im)mutability\n\n\nA tuple is an object that is defined by parentheses and entries that are separated by commas, for example (15, 20, 32). (They are of type tuple.)\nTuples are immutable, while lists are mutable.\nImmutable objects, such as tuples and strings, can‚Äôt have their elements changed, appended, extended, or removed.\n\nMutable objects, such as lists, can do all of these things.\n\nIn everyday programming, we use lists and dictionaries more than tuples."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-22",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-22",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nIndentation\n\nWe have seen that certain parts of the code examples are indented.\nCode that is part of a function, a conditional clause, or loop is indented.\nIndention is actually what tells the Python interpreter that some code is to be executed as part of, say, a loop and not to executed after the loop is finished."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-23",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-23",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nIndentation\nx = 10\n\nif x &gt; 2:\n    print(\"x is greater than 2\")\n\nHere‚Äôs a basic example of indentation as part of an if statement.\nThe standard practice for indentation is that each sub-statement should be indented by 4 spaces."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-24",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-24",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nDictionaries\ncities_to_temps = {\"Paris\": 28, \"London\": 22, \"New York\": 36, \"Seoul\": 29}\n\ncities_to_temps.keys()\ncities_to_temps.values()\ncities_to_temps.items()\n\nAnother built-in Python type that is enormously useful is the dictionary.\n\nThis provides a mapping one set of variables to another (either one-to-one or many-to-one).\nIf you need to create associations between objects, use a dictionary.\n\nWe can obtain keys, values, or key-value paris from dictionaries."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-25",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-25",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nRunning on Empty\n\nBeing able to create empty containers is sometimes useful, especially when using loops.\nThe commands to create empty lists, tuples, dictionaries, and sets are lst = [], tup=(), dic={}, and st = set() respectively.\nQ. What is the type of an empty list?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-26",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-26",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nSlicing Methods\n\n\n\n\nWith slicing methods, we can get subset of the data object.\nSlicing methods can apply for strings, lists, arrays, and DataFrames.\nThe above example describes indexing in Python"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-27",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-27",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nStrings\nstring = \"cheesecake\"\nprint( string[-4:] )\n\nFrom strings, we can access the individual characters via slicing and indexing.\n\n\n\nstring = \"cheesecake\"\nprint(\"String has length:\")\nprint( len(string) )\n\nlist_of_numbers = range(1, 20)\nprint(\"List of numbers has length:\")\nprint( len(list_of_numbers) )\n\n\n\nBoth lists and strings will allow us to use the len() command to get their length:"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-28",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-28",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nString-related Functions\nDot operation\n\nIn Python, we can access attributes by using a dot notation (.).\nUnlike len(), some functions use a dot to access to strings.\nTo use those string functions, type (1) the name of the string, (2) a dot, (3) the name of the function, and (4) any arguments that the function needs:\n\nstring_name.some_function(arguments)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-29",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-29",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nString-related Functions\nSplit with split()\n\nWe can use the built-in string split() function to break a string into a list of smaller strings based on some separator.\n\nIf we don‚Äôt specify a separator, split() uses any sequence of white space characters‚Äînewlines, spaces, and tabs:\n\ntasks = 'get gloves,get mask,give cat vitamins,call ambulance'\ntasks.split(',')\ntasks.split()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-30",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-30",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nString-related Functions\nCombine by Using join()\n\njoin() collapses a list of strings into a single string.\n\ncrypto_list = ['Yeti', 'Bigfoot', 'Loch Ness Monster']\ncrypto_string = ', '.join(crypto_list)\nprint('Found and signing book deals:', crypto_string)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-31",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-31",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nStrings and Slicing\n\nWe can extract a substring (a part of a string) from a string by using a slice.\nWe define a slice by using square brackets ([]), a start index, an end index, and an optional step count between them.\n\nWe can omit some of these.\n\nThe slice will include characters from index start to one before end:"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-32",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-32",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nGet a Substring with a Slice\n\n[:][ start :][: end ][ start : end ][ start : end : step ]\n\n\nletters = 'abcdefghij'\nletters[:]\n\n[:] extracts the entire sequence from start to end.\n\n\n\nletters = 'abcdefghij'\nletters[4:]\nletters[2:]\nletters[-3:]\nletters[-50:]\n\n[ start :] specifies from the start index to the end.\n\n\n\nletters = 'abcdefghij'\nletters[:3]\nletters[:-3]\nletters[:70]\n\n[: end ] specifies from the beginning to the end index minus 1.\n\n\n\nletters = 'abcdefghij'\nletters[2:5]\nletters[-26:-24]\nletters[35:37]\n\n[ start : end ] indicates from the start index to the end index minus 1.\n\n\n\nletters = 'abcdefghij'\nletters[2 : 6 : 2]   # From index 2 to 5, by steps of 2 characters\nletters[ : : 3]     # From the start to the end, in steps of 3 characters\nletters[ 6 : : 4 ]    # From index 19 to the end, by 4\nletters[ : 7 : 5 ]    # From the start to index 6 by 5:\nletters[-1 : : -1 ]   # Starts at the end and ends at the start\nletters[: : -1 ]\n\n[ start : end : step ] extracts from the start index to the end index minus 1, skipping characters by step."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-33",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-33",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nLists and Slicing\n\nPython is\n\na zero-indexed language (things start counting from zero);\nleft inclusive;\nright exclusive when we are specifying a range of values."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-34",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-34",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nLists and Slicing\nlist_example = ['one', 'two', 'three']\nlist_example[ 0 : 1 ]\nlist_example[ 1 : 3 ]\n\n\n\n\nWe can think of items in a list-like object as being fenced in.\n\nThe index represents the fence post."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-35",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-35",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nLists and Slicing\n\n[index]slice\n\n\nGet an Item by [index]\nsuny = ['Geneseo', 'Brockport', 'Oswego', 'Binghamton', \n        'Stony Brook', 'New Paltz'] \n\nWe can extract a single value from a list by specifying its index:\n\n\n\nsuny[0]\nsuny[1]\nsuny[2]\nsuny[7]\n\nsuny[-1]\nsuny[-2]\nsuny[-3]\nsuny[-7]\n\n\n\n\nGet an Item with a Slice\n\nWe can extract a subsequence of a list by using a slice:\n\nsuny = ['Geneseo', 'Brockport', 'Oswego', 'Binghamton', \n        'Stony Brook', 'New Paltz'] \nsuny[0:2]    # A slice of a list is also a list.\n\n\nsuny[ : : 2]\nsuny[ : : -2]\nsuny[ : : -1]\n\nsuny[4 : ]\nsuny[-6 : ]\nsuny[-6 : -2]\nsuny[-6 : -4]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-36",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-36",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nLists and Slicing\n\nQ. Classwork 4.3"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-37",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-37",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nFunctions\nint(\"20\") \nfloat(\"14.3\")\nstr(5)\nint(\"xyz\")\n\nA function can take any number and type of input parameters and return any number and type of output results.\nPython ships with more than 65 built-in functions.\nPython also allows a user to define a new function.\nWe will mostly use built-in functions."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-38",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-38",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nFunctions, Arguments, and Parameters\nprint(\"Cherry\", \"Strawberry\", \"Key Lime\")\nprint(\"Cherry\", \"Strawberry\", \"Key Lime\", sep = \"!\")\nprint(\"Cherry\", \"Strawberry\", \"Key Lime\", sep=\" \")\n\nWe invoke a function by entering its name and a pair of opening and closing parentheses.\nMuch as a cooking recipe can accept ingredients, a function invocation can accept inputs called arguments.\nWe pass arguments sequentially inside the parentheses (, separated by commas).\nA parameter is a name given to an expected function argument.\nA default argument is a fallback value that Python passes to a parameter if the function invocation does not explicitly provide one."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-39",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#python-basics-39",
    "title": "Lecture 3",
    "section": "Python Basics",
    "text": "Python Basics\nFunctions, Arguments, and Parameters\n\nQ. Classwork 4.4"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#repeat-with-while",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#repeat-with-while",
    "title": "Lecture 3",
    "section": "Repeat with while",
    "text": "Repeat with while\n\n\ncount = 1        \nwhile count &lt;= 5:\n    print(count)\n    count += 1\n\n\nWe first assigned the value 1 to count.\nThe while loop compared the value of count to 5 and continued if count was less than or equal to 5.\nInside the loop, we printed the value of count and then incremented its value by one with the statement count += 1.\nPython goes back to the top of the loop, and again compares count with 5.\nThe value of count is now 2, so the contents of the while loop are again executed, and count is incremented to 3.\nThis continues until count is incremented from 5 to 6 at the bottom of the loop.\nOn the next trip to the top, count &lt;= 5 is now False, and the while loop ends."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#repeat-with-while-1",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#repeat-with-while-1",
    "title": "Lecture 3",
    "section": "Repeat with while",
    "text": "Repeat with while\nAsking the user for input\n\n\nstuff = input()\n# Type something and press Return/Enter on Console \n# before running print(stuff)\nprint(stuff)\n\n\nSometimes we would like to take the value for a variable from the user via their keyboard.\n\nThe input() function gets input from the keyboard.\nWhen the input() is called, the program stops and waits for the user to type something on Console (interactive Python interpreter).\nWhen the user presses Return or Enter on Console, the program resumes and input returns what the user typed as a string."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#repeat-with-while-2",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#repeat-with-while-2",
    "title": "Lecture 3",
    "section": "Repeat with while",
    "text": "Repeat with while\nCancel with break\n\n\nwhile True:\n    user_input = input(\"Enter 'yes' to continue or 'no' to stop: \")\n    if user_input.lower() == 'no':\n        print(\"Exiting the loop. Goodbye!\")\n        break\n    elif user_input.lower() == 'yes':\n        print(\"You chose to continue.\")\n    else:\n        print(\"Invalid input, please enter 'yes' or 'no'.\")\n\n\nWhile loop is used to execute a block of code repeatedly until given boolean condition evaluated to False.\n\nwhile True loop will run forever unless we write it with a break statement.\n\nIf we want to loop until something occurs, but we‚Äôre not sure when that might happen, we can use an infinite loop with a break statement."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#repeat-with-while-3",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#repeat-with-while-3",
    "title": "Lecture 3",
    "section": "Repeat with while",
    "text": "Repeat with while\nSkip Ahead with continue\n\n\nwhile True:\n    value = input(\"Integer, please [q to quit]: \")\n    if value == 'q': # quit\n        break\n    number = int(value)\n    if number % 2 == 0: # an even number\n        continue\n    print(number, \"squared is\", number*number)\n\n\nSometimes, we don‚Äôt want to break out of a loop but just want to skip ahead to the next iteration for some reason.\nThe continue statement is used to skip the rest of the code inside a loop for the current iteration only."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in",
    "title": "Lecture 3",
    "section": "Iterate with for and in",
    "text": "Iterate with for and in\n\nSometimes we want to loop through a set of things such as a string of text, a list of words or a list of numbers.\n\nWhen we have a list of things to loop through, we can construct a for loop.\nA for loop makes it possible for you to traverse data structures without knowing how large they are or how they are implemented."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in-1",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in-1",
    "title": "Lecture 3",
    "section": "Iterate with for and in",
    "text": "Iterate with for and in\n\nLet‚Äôs see two ways to walk through a string here:\n\n\n\nword = 'thud'\noffset = 0\nwhile offset &lt; len(word):\n    print(word[offset])\n    offset += 1\n\nword = 'thud'\nfor letter in word:\n    print(letter)\n\n\n\nWhich one do you prefer?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in-2",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in-2",
    "title": "Lecture 3",
    "section": "Iterate with for and in",
    "text": "Iterate with for and in\nCancel with break\nword = 'thud'\nfor letter in word:\n    if letter == 'u':\n        break\n    print(letter)\n\nA break in a for loop breaks out of the loop, as it does for a while loop:"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in-3",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in-3",
    "title": "Lecture 3",
    "section": "Iterate with for and in",
    "text": "Iterate with for and in\nSkip with continue\nword = 'thud'\nfor letter in word:\n    if letter == 'u':\n        continue\n    print(letter)\n\nInserting a continue in a for loop jumps to the next iteration of the loop, as it does for a while loop."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in-4",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in-4",
    "title": "Lecture 3",
    "section": "Iterate with for and in",
    "text": "Iterate with for and in\nGenerate Number Sequences with range()\n\nThe range() function returns a stream of numbers within a specified range, without first having to create and store a large data structure such as a list or tuple.\n\nThis lets us create huge ranges without using all the memory in our computers and crashing our program.\nrange() returns an iterable object, so we need to step through the values with for ‚Ä¶ in, or convert the object to a sequence like a list."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in-5",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in-5",
    "title": "Lecture 3",
    "section": "Iterate with for and in",
    "text": "Iterate with for and in\nfor ‚Ä¶ in range()\n\n\nfor x in range(0, 3):\n    print(x)\nlist( range(0, 3) )\n\n\nWe use range() similar to how we use slices: range( start, stop, step ).\n\nIf we omit start, the range begins at 0.\nThe only required value is stop; as with slices, the last value created will be just before stop.\nThe default value of step is 1, but we can change it."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#loop-with-while-and-for-1",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#loop-with-while-and-for-1",
    "title": "Lecture 3",
    "section": "Loop with while and for",
    "text": "Loop with while and for\nClass Exercises\n\nQ. Classwork 4.5\nQ. Classwork 4.6"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-1",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-1",
    "title": "Lecture 3",
    "section": "Handle Errors with try and except",
    "text": "Handle Errors with try and except\nException handlers"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-2",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-2",
    "title": "Lecture 3",
    "section": "Handle Errors with try and except",
    "text": "Handle Errors with try and except\nException handlers\n\nIn some languages, errors are indicated by special function return values.\n\nPython uses exceptions: code that is executed when an associated error occurs.\n\nWhen we run code that might fail under some circumstances, we also need appropriate exception handlers to intercept any potential errors.\n\nAccessing a list or tuple with an out-of-range position, or a dictionary with a nonexistent key."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-3",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-3",
    "title": "Lecture 3",
    "section": "Handle Errors with try and except",
    "text": "Handle Errors with try and except\nErrors\nshort_list = [1, 2, 3]\nposition = 5\nshort_list[position]\n\nIf we don‚Äôt provide your own exception handler, Python prints an error message and some information about where the error occurred and then terminates the program:"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-4",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-4",
    "title": "Lecture 3",
    "section": "Handle Errors with try and except",
    "text": "Handle Errors with try and except\nshort_list = [1, 2, 3]\nposition = 5\n\ntry:\n    short_list[position]\nexcept:\n    print('Need a position between 0 and', len(short_list)-1, ' but got',\n    position)\n\nRather than leaving things to chance, use try to wrap your code, and except to provide the error handling:"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-5",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-5",
    "title": "Lecture 3",
    "section": "Handle Errors with try and except",
    "text": "Handle Errors with try and except\nshort_list = [1, 2, 3]\nposition = 5\ntry:\n    short_list[position]\nexcept:\n    print('Need a position between 0 and', len(short_list)-1, ' but got',\n    position)\n\nThe code inside the try block is run.\n\nIf there is an error, an exception is raised and the code inside the except block runs.\n\nIf there are no errors, the except block is skipped."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-6",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-6",
    "title": "Lecture 3",
    "section": "Handle Errors with try and except",
    "text": "Handle Errors with try and except\nexcept type\n\nSpecifying a plain except with no arguments, as we did here, is a catchall for any exception type.\nIf more than one type of exception could occur, it‚Äôs best to provide a separate exception handler for each.\nWe get the full exception object in the variable name if we use the form:"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-7",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-7",
    "title": "Lecture 3",
    "section": "Handle Errors with try and except",
    "text": "Handle Errors with try and except\nexcept type\nshort_list = [1, 2, 3]\nwhile True:\n    value = input('Position [q to quit]? ')\n    if value == 'q':\n        break\n    try:\n        position = int(value)\n        print(short_list[position])\n    except IndexError as err:\n        print('Bad index:', position, '-', err)\n    except Exception as other:\n        print('Something else broke:', other)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-8",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-8",
    "title": "Lecture 3",
    "section": "Handle Errors with try and except",
    "text": "Handle Errors with try and except\nexcept type\n\nThe example looks for an IndexError first, because that‚Äôs the exception type raised when we provide an illegal position to a sequence.\nIt saves an IndexError exception in the variable err, and any other exception in the variable other.\nThe example prints everything stored in other to show what you get in that object.\n\nInputting position 3 raised an IndexError as expected.\nEntering two annoyed the int() function, which we handled in our second, catchall except code."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-9",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#handle-errors-with-try-and-except-9",
    "title": "Lecture 3",
    "section": "Handle Errors with try and except",
    "text": "Handle Errors with try and except\nClass Exercises\n\nQ. Classwork 4.7"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#importing-modules-packages-and-libraries-1",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#importing-modules-packages-and-libraries-1",
    "title": "Lecture 3",
    "section": "Importing Modules, Packages, and Libraries",
    "text": "Importing Modules, Packages, and Libraries\npandas\n\n\n\n\npandas provides Series and DataFrames which are used to store data in an easy-to-use format."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#instructor-1",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#instructor-1",
    "title": "Lecture 1",
    "section": "Instructor",
    "text": "Instructor\nCurrent Appointment & Education\n\nName: Byeong-Hak Choe.\nAssistant Professor of Data Analytics and Economics, School of Business at SUNY Geneseo.\nPh.D.¬†in Economics from University of Wyoming.\nM.S. in Economics from Arizona State University.\nM.A.¬†in Economics from SUNY Stony Brook.\nB.A. in Economics & B.S. in Applied Mathematics from Hanyang University at Ansan, South Korea.\n\nMinor in Business Administration.\nConcentration in Finance."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#instructor-2",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#instructor-2",
    "title": "Lecture 1",
    "section": "Instructor",
    "text": "Instructor\nEconomics and Data Science\n\nChoe, B.H., Newbold, S. and James, A., ‚ÄúEstimating the Value of Statistical Life through Big Data‚Äù\n\nQuestion: How much is the society willing to pay to reduce the likelihood of fatality?\n\nChoe, B.H., ‚ÄúSocial Media Campaigns, Lobbying and Legislation: Evidence from #climatechange and Energy Lobbies.‚Äù\n\nQuestion: To what extent do social media campaigns compete with fossil fuel lobbying on climate change legislation?\n\nChoe, B.H. and Ore-Monago, T., 2024. ‚ÄúGovernance and Climate Finance in the Developing World‚Äù\n\nQuestion: In what ways and through what forms does poor governance act as a significant barrier to reducing greenhouse gas emissions in developing countries?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-1",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-1",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nEmail, Class & Office Hours\n\nEmail: bchoe@geneseo.edu\nClass Homepage:\n\nhttps://brightspace.geneseo.edu/\nhttp://bcdanl.github.io/320/\n\nOffice: South Hall 227B\nOffice Hours:\n\nMondays 5:00 P.M. ‚Äì 6:30 P.M.\n\nWednesdays 5:00 P.M. ‚Äì 6:30 P.M."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-2",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-2",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nCourse Description\n\nThis course teaches you how to analyze big data sets, and this course is specifically designed to bring you up to speed on one of the best technologies for this task, Apache Spark!\nThe top technology companies like Google, Facebook, Netflix, Airbnb, 3 Amazon, and many more are all using Spark to solve their big data problems!\nWith the Spark 3.0 DataFrame framework, it can perform up to 100x faster than Hadoop MapReduce."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-3",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-3",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nCourse Description\n\nThis course will review the basics in Python, continuing on to learning how to use Spark DataFrame API with the latest Spark 3.0 syntax!\nIn addition, you will learn how to perform supervised an unsupervised machine learning on massive datasets using the Machine Learning Library (MLlib).\nYou will also gain hands-on experience using PySpark within the Jupyter Notebook environment. This course also covers the latest Spark technologies, like Spark SQL, Spark Streaming, and advanced data analytics modeling methodologies."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-4",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-4",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nRequired Materials\n\nIntroduction to pyspark by Pedro Duarte Faria\n\nA free online version of this book is available."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-5",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-5",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nReference Materials - PySpark\n\nSpark by Examples\nApache PySpark - PySpark Documentation"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-6",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-6",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nReference Materials - Sports Data\n\nIntroduction to Sports Analytics Using R by Ryan Elmore and Andrew Urbaczewski\n\nAn eBook version of this book is available at:\n\nVitalSource\nRedShelf"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-7",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-7",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nReference Materials - Python\n\nPython for Data Analysis (3rd Edition) by Wes McKinney\nIPython Interactive Computing and Visualization Cookbook\nPython Programming for Data Science by Tomas Beuzen\nCoding for Economists by Arthur Turrell\nPython for Econometrics in Economics by Fabian H. C. Raters\nQuantEcon DataScience - Python Fundamentals by Chase Coleman, Spencer Lyon, and Jesse Perla\nQuantEcon DataScience - pandas by Chase Coleman, Spencer Lyon, and Jesse Perla"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-8",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-8",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nReference Materials - Website\n\nGuide for Quarto"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-9",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-9",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nCourse Requirements\n\nLaptop: You should bring your own laptop (Mac or Windows) to the classroom.\n\nThe minimum specification for your laptop in this course is 2+ core CPU, 4+ GB RAM, and 500+ GB disk storage.\n\nHomework: There will be six homework assignments.\nProject: There will be one project on a personal website.\nExams: There will be one Midterm Exam.\nParticipation: You are encouraged to participate in GitHub-based online discussions and class discussion, and office hours.\n\nCheckout the netiquette policy in the syllabus."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-10",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-10",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nPersonal Website\n\nYou will create your own website using Quarto, R Studio, and Git.\nYou will publish your homework assignments and team project on your website.\nYour website will be hosted in GitHub.\nThe basics in Markdown will be discussed.\nReferences:\n\nQuarto Guide"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-11",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-11",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nTeam Project\n\nTeam formation is scheduled for early April.\n\nEach team must have one to two students.\n\nFor the team project, a team must choose data related to business or socioeconomic issues.\nThe project report should include both (1) exploratory data analysis using summary statistics, visual representations, and data wrangling, and (2) machine learning analysis.\nThe document for the team project must be published in each member‚Äôs website.\nAny changes to team composition require approval from Byeong-Hak Choe."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-12",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-12",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nClass Schedule and Exams\n\nThere will be tentatively 28 class sessions.\nThe Midterm Exam is scheduled on March 31, 2025, Wednesday, during the class time.\nThe Project Presentation is scheduled on May 9, 2025, Friday, 3:30 P.M.-5:30 P.M.\nThe due for the Project write-up is May 16, 2024, Friday."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-13",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-13",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nCourse Contents"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-14",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-14",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nCourse Contents"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-15",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-15",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nGrading\n\\[\n\\begin{align}\n(\\text{Total Percentage Grade}) =&\\quad\\;\\, 0.05\\times(\\text{Total Attendance Score})\\notag\\\\\n&\\,+\\, 0.05\\times(\\text{Total Participation Score})\\notag\\\\\n&\\,+\\, 0.10\\times(\\text{Website Score})\\notag\\\\\n&\\,+\\, 0.30\\times(\\text{Total Homework Score})\\notag\\\\\n&\\,+\\, 0.50\\times(\\text{Total Exam and Project Score}).\\notag\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-16",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-16",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nGrading\n\nYou are allowed up to 2 absences without penalty.\n\nSend me an email if you have standard excused reasons (illness, family emergency, transportation problems, etc.).\n\nFor each absence beyond the initial two, there will be a deduction of 1% from the Total Percentage Grade.\nParticipation will be evaluated by quantity and quality of GitHub-based online discussions and in-person discussion.\nThe single lowest homework score will be dropped when calculating the total homework score."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-17",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#syllabus-17",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nMake-up Policy\n\nMake-up exams will not be given unless you have either a medically verified excuse or an absence excused by the University.\nIf you cannot take exams because of religious obligations, notify me by email at least two weeks in advance so that an alternative exam time may be set.\nA missed exam without an excused absence earns a grade of zero.\nLate submissions for homework assignment will be accepted with a penalty.\nA zero will be recorded for a missed assignment.\n\n:::"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-1",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-1",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nIntroduction: Share Your Background and Experience\nMarcie Hogan, Class of 2023\nJaehyung Lee (Andy), Class of 2022\nJason Rappazzo, Class of 2023 - Part 1\nJason Rappazzo, Class of 2023 - Part 2"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-2",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-2",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nQ. What challenges have you encountered in your role at the workplace?\nMarcie Hogan, Class of 2023\nJason Rappazzo, Class of 2023\nJaehyung Lee (Andy), Class of 2022"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-3",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-3",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nQ. How do you envision the future of AI?\nJaehyung Lee (Andy), Class of 2022\nMarcie Hogan, Class of 2023"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-4",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-4",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nQ. How does working at a large company like Meta compare to your previous experiences?\nMarcie Hogan, Class of 2023"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-5",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-5",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nQ. What has been the most fulfilling aspect of your work?\nJason Rappazzo, Class of 2023\nJaehyung Lee (Andy), Class of 2022\nMarcie Hogan, Class of 2023"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-6",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-6",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nQ. Can you share advice for students who are preparing for internships or job applications?\nMarcie Hogan, Class of 2023\nJaehyung Lee (Andy), Class of 2022"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---1.-marcie-hogan",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---1.-marcie-hogan",
    "title": "Lecture 1",
    "section": "DANL Career Session - 1. Marcie Hogan",
    "text": "DANL Career Session - 1. Marcie Hogan\n\nBackground\n\nGraduated from SUNY Geneseo in 2023 with a degree in Geography and minors in Mathematics and Physics.\nTook several data analytics courses and was a tutor in the Data Analytics Lab.\nInitially worked at Crown Castle as a Geospatial Data Analyst, including an internship, totaling about two years.\n\nCareer Path\n\nAt Crown Castle, Marcie managed database operations for spatial data, frequently working with Python and spatial packages.\nIn July 2024, she began a new role at Meta as a Community Project Manager within the Maps Organization, working on a contract basis."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---1.-marcie-hogan-1",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---1.-marcie-hogan-1",
    "title": "Lecture 1",
    "section": "DANL Career Session - 1. Marcie Hogan",
    "text": "DANL Career Session - 1. Marcie Hogan\n\nRole at Meta\n\nWorks extensively on Mapillary, an open-source street-level imagery hosting platform acquired by Meta.\nConducts analytics to understand the user community and guide product development.\nWhile her job title doesn‚Äôt explicitly mention data, data analytics is heavily integrated into her role."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---2.-jaehyung-lee-andy",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---2.-jaehyung-lee-andy",
    "title": "Lecture 1",
    "section": "DANL Career Session - 2. Jaehyung Lee (Andy)",
    "text": "DANL Career Session - 2. Jaehyung Lee (Andy)\n\nBackground\n\nGraduated from SUNY Geneseo in 2022 with a major in Mathematics and a concentration in Data Analytics.\nServed in the South Korean military for two years before resuming his academic and professional pursuits.\nTook several data analytics courses and was a tutor in the Data Analytics Lab."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---2.-jaehyung-lee-andy-1",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---2.-jaehyung-lee-andy-1",
    "title": "Lecture 1",
    "section": "DANL Career Session - 2. Jaehyung Lee (Andy)",
    "text": "DANL Career Session - 2. Jaehyung Lee (Andy)\n\nInvisible Technologies\n\nWorks as Analytics Associate at Invisible Technologies, a fast-growing startup that partners with leading AI platforms like OpenAI, Cohere, Google, and Character.AI.\nThe company specializes in refining AI models through techniques like Reinforcement Learning from Human Feedback (RLHF) and Supervised Fine-Tuning (SFT)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---2.-jaehyung-lee-andy-2",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---2.-jaehyung-lee-andy-2",
    "title": "Lecture 1",
    "section": "DANL Career Session - 2. Jaehyung Lee (Andy)",
    "text": "DANL Career Session - 2. Jaehyung Lee (Andy)\n\nRole at Invisible Technologies\n\nHolds monthly business reviews for executives.\nProvides visibility on departmental targets for hiring, people operations, and client services.\nAnalyzes client data to ensure targets are met, particularly for clients like OpenAI.\nCreates numerous data pipelines using Python and develops data models with SQL."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---2.-jaehyung-lee-andy-3",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---2.-jaehyung-lee-andy-3",
    "title": "Lecture 1",
    "section": "DANL Career Session - 2. Jaehyung Lee (Andy)",
    "text": "DANL Career Session - 2. Jaehyung Lee (Andy)\n\nProjects\n\nSelf-Evaluation Project: Tracked over 20 personal metrics daily for two years to improve his lifestyle and productivity, such as wake-up times, tasks completed, reading, and meditation.\nUsed this project to enhance his coding skills and had valuable discussions during job interviews.\nEmphasized the importance of personal projects in learning and showcasing skills."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---2.-jaehyung-lee-andy-4",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---2.-jaehyung-lee-andy-4",
    "title": "Lecture 1",
    "section": "DANL Career Session - 2. Jaehyung Lee (Andy)",
    "text": "DANL Career Session - 2. Jaehyung Lee (Andy)\n\nImpact of Academic Courses\n\nTook courses like Data Analytics 100, 200, 210, and 310, which helped him build a meaningful relationship with data.\nLearned programming languages like R and Python, and developed skills in data visualization and presentation.\nCreated interactive dashboards using Shiny, which impressed interviewers and aided in job acquisition.\nStressed the applicability of classroom knowledge to real-world scenarios."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---3.-jason-rappazzo",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---3.-jason-rappazzo",
    "title": "Lecture 1",
    "section": "DANL Career Session - 3. Jason Rappazzo",
    "text": "DANL Career Session - 3. Jason Rappazzo\n\nBackground\n\nGraduated from SUNY Geneseo in 2023 with a major in Economics and a minor in Data Analytics.\nServed as a tutor in the Data Analytics Lab and was part of the track team alongside Marcie.\n\nRole at Momentive\n\nWorks as a data analyst in the Global IT Automation and Reporting at Momentive, a chemical manufacturing company.\nThe company produces a wide array of products found in everyday items, from tires to mattresses and aerospace components.\nResponsible for automating manual business processes and turning complex data into actionable insights."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---3.-jason-rappazzo-1",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---3.-jason-rappazzo-1",
    "title": "Lecture 1",
    "section": "DANL Career Session - 3. Jason Rappazzo",
    "text": "DANL Career Session - 3. Jason Rappazzo\n\nTechnologies Used\n\nAlteryx: For data extraction and consolidating data from various company sources.\nSnowflake: Serves as the data warehouse where data is transformed and prepared.\nTableau: Used to create dashboards and visualizations for different departments."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---3.-jason-rappazzo-2",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---3.-jason-rappazzo-2",
    "title": "Lecture 1",
    "section": "DANL Career Session - 3. Jason Rappazzo",
    "text": "DANL Career Session - 3. Jason Rappazzo\n\nInsights on Learning\n\nEmphasized the importance of adaptability and the ability to learn new technologies in a rapidly changing tech landscape.\nEncouraged persistence in coding, highlighting that overcoming errors and frustrations is part of the learning process.\nBelieves that proficiency in one programming language can facilitate learning others."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---3.-jason-rappazzo-3",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session---3.-jason-rappazzo-3",
    "title": "Lecture 1",
    "section": "DANL Career Session - 3. Jason Rappazzo",
    "text": "DANL Career Session - 3. Jason Rappazzo\n\nProjects\n\nSG&A Dashboard: Acted as the project manager for a dashboard visualizing the company‚Äôs Selling, General, and Administrative expenses.\nDigital Growth Dashboard: Visualizes online sales data and tracks the company‚Äôs shift toward automated online product sales.\nRegularly combines data from various ERP-related tables into coherent views for analysis and presentation."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-8",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-8",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nChallenges Faced\n\nMarcie\n\nExperienced corporate instability at Crown Castle, witnessing four rounds of layoffs, a CEO change, and office closures.\nHad to make an earlier-than-expected career move due to the company‚Äôs uncertain future.\nAdjusted to Meta‚Äôs faster-paced environment and the need to learn new internal tools and processes.\nNavigated strict data privacy and legal considerations at Meta, a shift from her previous role."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-9",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-9",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nChallenges Faced\n\nAndy\n\nInitially believed data analytics was primarily technical but realized the importance of interpersonal skills.\nLearned to communicate effectively with stakeholders to meet their needs and deliver valuable insights.\nContinues to work on improving communication and presentation skills."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-10",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-10",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nChallenges Faced\n\nJason\n\nFaced a learning curve due to a limited background in finance, necessitating on-the-job learning of financial data analysis.\nRecognized the importance of understanding data context to create meaningful visualizations for stakeholders.\nLearned that real-world data is often messy and inconsistent, requiring significant data preparation and cleaning."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-11",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-11",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nInsights on AI\n\nMarcie\n\nObserved Meta‚Äôs significant investment in AI technologies, such as open-source models like Llama 3.\nBelieves AI applications will expand across various sectors, including geospatial data and map-making.\nEmphasized the importance of having industry-specific knowledge to optimize AI models effectively."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-12",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-12",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nInsights on AI\n\nAndy\n\nAcknowledged the hype surrounding AI and the possibility of an AI bubble.\nMentioned that his company is preparing by diversifying into other industries beyond AI.\nBelieves AI will become more prominent across all businesses, requiring adaptability and broader skill sets."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-13",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-13",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nMost Fulfilling Aspects\n\nMarcie\n\nFinds fulfillment when her work aligns with her passions, particularly in geospatial data and map-making.\nEnjoys seeing the direct impact of her insights on public-facing products and contributing to an interconnected world.\nPrefers roles where she can witness the end results and user impact of her work."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-14",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-14",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nMost Fulfilling Aspects\n\nAndy\n\nDerives satisfaction from building solutions from scratch and seeing them positively impact stakeholders.\nValues moments when his work translates into tangible benefits for the company and receives appreciation."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-15",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-15",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nMost Fulfilling Aspects\n\nJason\n\nFinds it rewarding when code runs flawlessly and data is accurately prepared.\nEnjoys impressing stakeholders with the final product, especially when it simplifies their work.\nValues making a tangible impact on colleagues‚Äô efficiency and effectiveness."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-16",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-16",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nAdvice to Students Who Are Looking for a Job\n\nMarcie\n\nEncouraged students to leverage opportunities at Geneseo to build their resumes, such as tutoring, research, and extracurricular activities.\nHighlighted the competitive job market and the necessity of relevant experience to secure internships and jobs.\nAdvised students to combine their data analytics skills with industries they are passionate about for a more fulfilling career."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-17",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-17",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nAdvice to Students Who Are Looking for a Job\n\nAndy\n\nEmphasized the significance of working on personal projects to deepen coding and data science expertise.\nRecommended mastering Python, R, and SQL as essential tools for data-related jobs.\nAdvised students to have one or two significant projects to discuss during interviews.\nEncouraged networking with professionals in the field to gain insights and collaboration opportunities.\nHighlighted that exploring various projects builds confidence and aids in transitioning from academia to industry."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-18",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-18",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nAdvice to Students Who Are Looking for a Job\n\nJason\n\nRecommended learning basic finance and accounting concepts to better understand and meet business needs.\nAdvised developing strong interpersonal and communication skills to effectively collaborate with non-technical stakeholders.\nEncouraged being open to learning new tools and technologies to stay relevant in the field."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-19",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-session-19",
    "title": "Lecture 1",
    "section": "DANL Career Session",
    "text": "DANL Career Session\nSummary\n\nThe speakers highlighted the importance of:\n\nTechnical Skills: Proficiency in programming languages like Python, R, and SQL is crucial.\nContinuous Learning: Staying adaptable and willing to learn new technologies and methodologies.\nInterpersonal Skills: Effective communication with stakeholders and team members is essential.\nPassion Alignment: Combining data analytics skills with personal interests leads to a more fulfilling career.\nPractical Experience: Engaging in projects, internships, and leveraging academic opportunities enhances employability."
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career",
    "title": "Lecture 1",
    "section": "DANL Career",
    "text": "DANL Career\nUseful Skills for DANL Students\n\nData Analysis with Python and R\nData Science with Python and R\nProgramming Languages for databases such as SQL\nBusiness Intelligence Software such as Power BI and/or Tableau\nVersion Control with Git and GitHub\nData Analytics/Science Portfolio with a Capstone Project on GitHub\nPrompt Engineering in Generative AI"
  },
  {
    "objectID": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-1",
    "href": "danl-lec/danl-320-lec-01-2025-0122.html#danl-career-1",
    "title": "Lecture 1",
    "section": "DANL Career",
    "text": "DANL Career\nData Analytics-related Certificates on Coursera\n\nGeneral Data Analytics, SQL, and Business Intelligence\n\nGoogle Data Analytics Professional Certificate\nIBM Databases and SQL for Data Science with Python\nTableau Business Intelligence Analyst Professional Certificate\nMicrosoft Power BI Data Analyst Professional Certificate\n\nData Science\n\nIBM Data Science Professional Certificate\n\nGenerative AI\n\nGoogle AI Essentials\nIBM Generative AI: Prompt Engineering Basics"
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-1",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-1",
    "title": "Lecture 4",
    "section": "Big Data",
    "text": "Big Data\n\nBig data and analytics are likely to be significant components of future careers across various fields.\nBig data refers to enormous and complex data collections that traditional data management tools can‚Äôt handle effectively.\nFive key characteristics of big data (5 V‚Äôs):\n\nVolume\nVelocity\nValue\nVeracity\nVariety"
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-2",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-2",
    "title": "Lecture 4",
    "section": "Big Data",
    "text": "Big Data\n1. Volume\n\nIn 2017, the digital universe contained an estimated 16.1 zettabytes of data.\nExpected to grow to 163 zettabytes by 2025.\nMuch new data will come from embedded systems in smart devices."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-3",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-3",
    "title": "Lecture 4",
    "section": "Big Data",
    "text": "Big Data\n1. Volume\n\n\n\n\n\nName\nSymbol\nValue\n\n\n\n\nKilobyte\nkB\n10¬≥\n\n\nMegabyte\nMB\n10‚Å∂\n\n\nGigabyte\nGB\n10‚Åπ\n\n\nTerabyte\nTB\n10¬π¬≤\n\n\nPetabyte\nPB\n10¬π‚Åµ\n\n\nExabyte\nEB\n10¬π‚Å∏\n\n\nZettabyte\nZB\n10¬≤¬π\n\n\nYottabyte\nYB\n10¬≤‚Å¥\n\n\nBrontobyte*\nBB\n10¬≤‚Å∑\n\n\nGegobyte*\nGeB\n10¬≥‚Å∞\n\n\n\nNote: The asterisks (*) next to Brontobyte and Gegobyte in the original image have been preserved in this table. These likely indicate that these units are less commonly used or are proposed extensions to the standard system of byte units.\n\n\n\n\n\n\nIncrease in size of the global datasphere"
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-4",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-4",
    "title": "Lecture 4",
    "section": "Big Data",
    "text": "Big Data\n2. Velocity\n\nRefers to the rate at which new data is generated.\nEstimated at 0.33 zetabytes each day (120 zetabytes annually).\n90% of the world‚Äôs data was generated in just the past two years."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-5",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-5",
    "title": "Lecture 4",
    "section": "Big Data",
    "text": "Big Data\n3. Value\n\nRefers to the worth of data in decision-making.\nEmphasizes the need to quickly identify and process relevant data.\nUsers may be able to find more patterns and interesting anomalies from ‚Äúbig‚Äù data than from ‚Äúsmall‚Äù data."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-6",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-6",
    "title": "Lecture 4",
    "section": "Big Data",
    "text": "Big Data\n4. Veracity\n\nMeasures the quality of the data.\nConsiders accuracy, completeness, and currency of data.\nDetermines if the data can be trusted for good decision-making."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-7",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#big-data-7",
    "title": "Lecture 4",
    "section": "Big Data",
    "text": "Big Data\n5. Variety\n\n\n\n\n\n\nData comes in various formats.\nStructured data: Has a predefined format, fits into traditional databases.\nUnstructured data: Not organized in a predefined manner, comes from sources like documents, social media, emails, photos, videos, etc."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#sources-of-an-organizations-data",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#sources-of-an-organizations-data",
    "title": "Lecture 4",
    "section": "Sources of an Organization‚Äôs Data",
    "text": "Sources of an Organization‚Äôs Data"
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#free-sources-of-useful-big-data",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#free-sources-of-useful-big-data",
    "title": "Lecture 4",
    "section": "Free Sources of Useful (Big) Data",
    "text": "Free Sources of Useful (Big) Data\nEconomics/Finance\n\n\n\n\nData Source\nDescription\nURL\n\n\n\n\nBureau of Labor Statistics (BLS)\nProvides access to data on inflation and prices, wages and benefits, employment, spending and time use, productivity, and workplace injuries\nBLS\n\n\nFRED (Federal Reserve Economic Data)\nProvides access to a vast collection of U.S. economic data, including interest rates, GDP, inflation, employment, and more\nFRED\n\n\nYahoo Finance\nProvides comprehensive financial news, data, and analysis, including stock quotes, market data, and financial reports\nYahoo Finance\n\n\nIMF (International Monetary Fund)\nProvides access to a range of economic data and reports on countries‚Äô economies\nIMF Data\n\n\nWorld Bank Open Data\nFree and open access to global development data, including world development indicators\nWorld Bank Open Data\n\n\nOECD Data\nProvides access to economic, environmental, and social data and indicators from OECD member countries\nOECD Data"
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#free-sources-of-useful-big-data-1",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#free-sources-of-useful-big-data-1",
    "title": "Lecture 4",
    "section": "Free Sources of Useful (Big) Data",
    "text": "Free Sources of Useful (Big) Data\nGovernment/Public Data\n\n\n\n\nData Source\nDescription\nURL\n\n\n\n\nData.gov\nPortal providing access to over 186,000 government data sets, related to topics such as agriculture, education, health, and public safety\nData.gov\n\n\nCIA World Factbook\nPortal to information on the economy, government, history, infrastructure, military, and population of 267 countries\nCIA World Factbook\n\n\nU.S. Census Bureau\nPortal to a huge variety of government statistics and data relating to the U.S. economy and its population\nU.S. Census Bureau\n\n\nEuropean Union Open Data Portal\nProvides access to public data from EU institutions\nEU Open Data Portal\n\n\nNew York City Open Data\nProvides access to datasets from New York City, covering a wide range of topics such as public safety, transportation, and health\nNYC Open Data\n\n\nLos Angeles Open Data\nPortal for accessing public data from the City of Los Angeles, including transportation, public safety, and city services\nLA Open Data\n\n\nChicago Data Portal\nOffers access to datasets from the City of Chicago, including crime data, transportation, and health statistics\nChicago Data Portal"
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#free-sources-of-useful-big-data-2",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#free-sources-of-useful-big-data-2",
    "title": "Lecture 4",
    "section": "Free Sources of Useful (Big) Data",
    "text": "Free Sources of Useful (Big) Data\nHealth, Climate/Environment, and Social Data\n\n\n\n\nData Source\nDescription\nURL\n\n\n\n\nHealthdata.gov\nPortal to 125 years of U.S. health care data, including national health care expenditures, claim-level Medicare data, and other topics\nHealthdata.gov\n\n\nCMS Open Payments\nCenters for Medicare & Medicaid Services (CMS) Open Payments houses a publicly accessible database of payments that reporting entities, including drug and medical device companies, make to covered recipients like physicians.\nCMS\n\n\nWorld Health Organization (WHO)\nPortal to data and statistics on global health issues\nWHO Data\n\n\nNational Centers for Environmental Information (NOAA)\nPortal for accessing a variety of climate and weather data sets\nNCEI\n\n\nNOAA National Weather Service\nProvides weather, water, and climate data, forecasts and warnings\nNOAA NWS\n\n\nFAO (Food and Agriculture Organization)\nProvides access to data on food and agriculture, including data on production, trade, food security, and sustainability\nFAOSTAT\n\n\nPew Research Center Internet & Technology\nPortal to research on U.S. politics, media and news, social trends, religion, Internet and technology, science, Hispanic, and global topics\nPew Research\n\n\nData for Good from Facebook\nProvides access to anonymized data from Facebook to help non-profits and research communities with insights on crises, health, and well-being\nFacebook Data for Good\n\n\nData for Good from Canada\nProvides open access to datasets that address pressing social challenges across Canada\nData for Good Canada"
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#free-sources-of-useful-big-data-3",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#free-sources-of-useful-big-data-3",
    "title": "Lecture 4",
    "section": "Free Sources of Useful (Big) Data",
    "text": "Free Sources of Useful (Big) Data\nGeneral Data Repositories\n\n\n\n\nData Source\nDescription\nURL\n\n\n\n\nAmazon Web Services (AWS) public data sets\nPortal to a huge repository of public data, including climate data, the million song dataset, and data from the 1000 Genomes project\nAWS Datasets\n\n\nOpportunity Insights\nHarvard-based team offers big data to solve economic and social problems\nOpportunity Insights\n\n\nGapminder\nPortal to data from the World Health Organization and World Bank on economic, medical, and social issues\nGapminder\n\n\nGoogle Dataset Search\nHelps find datasets stored across the web\nGoogle Dataset Search\n\n\nKaggle Datasets\nA community-driven platform with datasets from various fields, useful for machine learning and data science projects\nKaggle Datasets\n\n\nUCI Machine Learning Repository\nA collection of databases, domain theories, and datasets used for machine learning research\nUCI ML Repository\n\n\nUnited Nations Data\nProvides access to global statistical data compiled by the United Nations\nUN Data\n\n\nHumanitarian Data Exchange (HDX)\nProvides humanitarian data from the United Nations, NGOs, and other organizations\nHDX\n\n\nDemocratizing Data from data.org\nA platform providing access to high-impact datasets, tools, and resources aimed at solving critical global challenges\nDemocratizing Data\n\n\nJustia Federal District Court Opinions and Orders database\nA free searchable database of full-text opinions and orders from civil cases heard in U.S. Federal District Courts\nJustia"
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#hadoop-1",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#hadoop-1",
    "title": "Lecture 4",
    "section": "Hadoop",
    "text": "Hadoop\nIntroduction to Hadoop\n\n\n\n\n\n\nPurpose\n\nEnables distributed processing of large data sets across clusters of computers."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#hadoop-2",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#hadoop-2",
    "title": "Lecture 4",
    "section": "Hadoop",
    "text": "Hadoop\nHadoop Architecture - HDFS\n\n\n\n\n\n\n\n\n\nHDFS\n\nDivides data into blocks and distributes them across different servers for processing.\nProvides a highly redundant computing environment\n\nAllows the application to keep running even if individual servers fail."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#hadoop-3",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#hadoop-3",
    "title": "Lecture 4",
    "section": "Hadoop",
    "text": "Hadoop\nHadoop Architecture - MapReduce\n\nMapReduce: Distributes the processing of big data files across a large cluster of machines.\n\nHigh performance is achieved by breaking the processing into small units of work that can be run in parallel across nodes in the cluster.\n\nMap Phase: Filters and sorts data.\n\ne.g., Sorting customer orders based on their product IDs, with each group corresponding to a specific product ID.\n\nReduce Phase: Summarizes and aggregates results.\n\ne.g., Counting the number of orders within each group, thereby determining the frequency of each product ID."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#hadoop-4",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#hadoop-4",
    "title": "Lecture 4",
    "section": "Hadoop",
    "text": "Hadoop\nHadoop Architecture - MapReduce"
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#hadoop-5",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#hadoop-5",
    "title": "Lecture 4",
    "section": "Hadoop",
    "text": "Hadoop\nHow Hadoop Works\n\nData Distribution\n\nLarge data sets are split into smaller blocks.\n\nData Storage\n\nBlocks are stored across multiple servers in the cluster.\n\nProcessing with MapReduce\n\nMap Tasks: Executed on servers where data resides, minimizing data movement.\nReduce Tasks: Combine results from map tasks to produce final output.\n\nFault Tolerance\n\nData replication ensures processing continues even if servers fail."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#hadoop-6",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#hadoop-6",
    "title": "Lecture 4",
    "section": "Hadoop",
    "text": "Hadoop\nExtending Hadoop for Real-Time Processing\n\nLimitation of Hadoop\n\nHadoop is originally designed for batch processing.\n\nBatch Processing: Data or tasks are collected over a period of time and then processed all at once, typically at scheduled times or during periods of low activity.\nResults come after the entire dataset is analyzed.\n\n\nReal-Time Processing Limitation:\n\nHadoop cannot natively process real-time streaming data (e.g., stock prices flowing into stock exchanges, live sensor data)\n\nExtending Hadoop‚Äôs Capabilities\n\nBoth Apache Storm and Apache Spark can run on top of Hadoop clusters, utilizing HDFS for storage."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#hadoop-7",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#hadoop-7",
    "title": "Lecture 4",
    "section": "Hadoop",
    "text": "Hadoop\nApache Storm and Apache Spark\n\n\nApache Storm\n\nFunctionality:\n\nProcesses real-time data streams.\nHandles unbounded streams of data reliably and efficiently.\n\nUse Cases:\n\nReal-time analytics\nOnline machine learning\nContinuous computation\nReal-time data integration\n\n\n\nApache Spark\n\nFunctionality:\n\nProvides in-memory computations for increased speed.\nSupports both batch and streaming data processing through Spark Streaming.\n\nUse Cases:\n\nInteractive queries for quick, on-the-fly data analysis\nMachine learning"
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#hadoop-8",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#hadoop-8",
    "title": "Lecture 4",
    "section": "Hadoop",
    "text": "Hadoop\nApache Storm and Apache Spark\n\n\nApache Storm\n\nFunctionality:\n\nProcesses real-time data streams.\nHandles unbounded streams of data reliably and efficiently.\n\nUse Cases:\n\nReal-time analytics\nOnline machine learning\nContinuous computation\nReal-time data integration\n\n\n\nApache Spark\n\nFunctionality:\n\nProvides in-memory computations for increased speed.\nSupports both batch and streaming data processing through Spark Streaming.\n\nUse Cases:\n\nInteractive queries for quick, on-the-fly data analysis\nMachine learning"
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#apache-storm-and-apache-spark-1",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#apache-storm-and-apache-spark-1",
    "title": "Lecture 4",
    "section": "Apache Storm and Apache Spark",
    "text": "Apache Storm and Apache Spark\nMedscape: Real-Time Medical News for Healthcare Professionals\n\n\nA medical news app for smartphones and tablets designed to keep healthcare professionals informed.\n\nProvides up-to-date medical news and expert perspectives.\n\n\n\n\n\n\n\n\n\n\n\n\nReal-Time Updates:\n\nUses Apache Storm/Spark to process about 500 million tweets per day.\nAutomatic Twitter feed integration helps users track important medical trends shared by physicians and medical commentators."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#building-a-personal-website-on-github",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#building-a-personal-website-on-github",
    "title": "Lecture 2",
    "section": "Building a Personal Website on GitHub",
    "text": "Building a Personal Website on GitHub\n\nFollow steps described in Classwork 1."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#lets-practice-markdown",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#lets-practice-markdown",
    "title": "Lecture 2",
    "section": "Let‚Äôs Practice Markdown!",
    "text": "Let‚Äôs Practice Markdown!\n\nJupyter Notebook, Quarto, and GitHub-based Discussion Boards use markdown as its underlying document syntax.\nLet‚Äôs do Classwork 2."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#getting-started-with-jupyter-notebook-and-html",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#getting-started-with-jupyter-notebook-and-html",
    "title": "Lecture 2",
    "section": "Getting Started with Jupyter Notebook and HTML",
    "text": "Getting Started with Jupyter Notebook and HTML\nYAML\n\n\n\n\n\n\nAn YAML (yet another markup language) header surrounded by ---.\n\nIt is commonly used for document configuration (e.g., title, author, date, style, ‚Ä¶).\n\nIn YAML, indentation really matters!\n\ntab (or four spaces) defines a level in YAML."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#getting-started-with-jupyter-notebook-and-html-1",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#getting-started-with-jupyter-notebook-and-html-1",
    "title": "Lecture 2",
    "section": "Getting Started with Jupyter Notebook and HTML",
    "text": "Getting Started with Jupyter Notebook and HTML\nKnitting / Rendering\n\n\nWhen we knit the document, Quarto sends the .qmd file to jupyter/knitr, which executes all of the code chunks and creates a new markdown (.md) document which includes the code and its output.\nThe markdown file (*.md) generated by jupyter/knitr is then processed by pandoc, which is responsible for creating the output file."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#getting-started-with-jupyter-notebook-and-html-2",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#getting-started-with-jupyter-notebook-and-html-2",
    "title": "Lecture 2",
    "section": "Getting Started with Jupyter Notebook and HTML",
    "text": "Getting Started with Jupyter Notebook and HTML\nMarkdown, Jupyter Notebook, and HTML\n\nThe very original version of Markdown was invented mainly to write HTML content more easily.\n\nFor example, - SOME_TEXT in ‚Äú.md‚Äù is equivalent to &lt;ul&gt;&lt;li&gt; SOME_TEXT &lt;/li&gt; in ‚Äù.html‚Äù\n\nPandoc makes it possible to convert a Markdown document to a large variety of output formats, such as HTML."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#getting-started-with-jupyter-notebook-and-html-3",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#getting-started-with-jupyter-notebook-and-html-3",
    "title": "Lecture 2",
    "section": "Getting Started with Jupyter Notebook and HTML",
    "text": "Getting Started with Jupyter Notebook and HTML\nMarkdown, Jupyter Notebook, and HTML\n---\ntitle: \"Habits\"\nauthor: YOUR_NAME\ndate: January 27, 2025\nformat: \n  html\n---\n\nTo create an HTML document from Jupyter Notebook, we specify the html output format in the YAML metadata of our document.\n\nBy default, format: html is set.\n\nOpen an empty Jupyter Notebook file from Google Colab (or VSCode).\n\nCreate the first cell that is Text.\nType the above YAML metadata to the first Text cell."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#getting-started-with-jupyter-notebook-and-html-4",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#getting-started-with-jupyter-notebook-and-html-4",
    "title": "Lecture 2",
    "section": "Getting Started with Jupyter Notebook and HTML",
    "text": "Getting Started with Jupyter Notebook and HTML\nMarkdown, Jupyter Notebook, and HTML\n---\ntitle: \"Python Basics\"\nauthor: YOUR_NAME\ndate: \"2025-01-27\"\n---\n\nDownload the Jupyter Notebook file, danl-320-python-basic.ipynb from Brightspace, and open it from Google Colab (or VSCode if you prefer).\nThe above syntax is part of YAML metadata in danl-320-python-basic.ipynb.\n\nYAML should be always in the first cell, and the first cell should be text, not code."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-_quarto.yml",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-_quarto.yml",
    "title": "Lecture 2",
    "section": "Quarto Website: _quarto.yml",
    "text": "Quarto Website: _quarto.yml\n\n\n---\nproject:\n  type: website\n\nwebsite:\n  title: \"YOUR NAME\"\n  navbar:\n    left:\n      - text: Project\n        href: danl_proj_nba.ipynb\n      - text: Blog\n        href: blog-listing.qmd\n\nformat:\n  html:\n    theme: cosmo\n    css: styles.css\n    toc: false\n---\n\nThe _quarto.yml file configures the website settings.\nIndentation matters!\n\n\n\n\nIn RStudio, open the project USERNAME.github.io.Rporj.\n\nClick Project: (None) at the top-right corner.\nClick USERNAME.github.io.Rproj.\n\n_quarto.yml configures a website, and provides various options for HTML documents within the website."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website",
    "title": "Lecture 2",
    "section": "Quarto Website",
    "text": "Quarto Website\nCustom CSS\n\nCascading Style Sheets (CSS) is used to format the layout of a webpage (color, font, text size, background, display, etc.).\n\nHTML will format the architecture of the house.\nCSS will be the carpet and walls to decorate the house.\nJavaScript adds interactive elements in the house, such as opening doors and lighting.\n\nWe are not front-end web developers.\n\nWe will not cover the use of CSS and JavaScript."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-1",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-1",
    "title": "Lecture 2",
    "section": "Quarto Website",
    "text": "Quarto Website\nRendering\n\n\nThe Render button (command/Ctrl + shift + K) renders a single Quarto document file (e.g., index.qmd) to create an output document.\nquarto render from Terminal renders ALL Quarto documents and Jupyter Notebook files in your local working directory:\n\nquarto render\n\nquarto render should be used if there is any change in _quarto.yml.\n\n\n\n\n\nTip\n\n\n\nEdit _quarto.yml, *.qmd, or *.ipynb files ONLY from your local laptop or Google Colab.\n\nDo not edit them from your GitHub repo for the website."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-2",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-2",
    "title": "Lecture 2",
    "section": "Quarto Website",
    "text": "Quarto Website\nAdding *.ipynb to a Quarto website\n\nBy default, quarto render doesn‚Äôt execute any code in .ipynb notebooks.\nquarto render renders .ipynb notebooks, so that corresponding html files are rendered.\n\nIf you need to update cell outputs in *.ipynb, run that *.ipynb on Google Colab, save the notebook, and download it to your local working directory."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-3",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-3",
    "title": "Lecture 2",
    "section": "Quarto Website",
    "text": "Quarto Website\nAppearance and Style\n\ntheme specifies the Bootstrap theme to use for the page (themes are drawn from the Bootswatch theme library).\n\nValid themes include default, bootstrap, cerulean, cosmo, darkly, flatly, journal, lumen, paper, readable, sandstone, simplex, spacelab, united, and yeti.\n\nhighlight-style specifies the code highlighting style.\n\nSupported styles include default, tango, pygments, kate, monochrome, espresso, zenburn, haddock, breezedark, and textmate."
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-4",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-4",
    "title": "Lecture 2",
    "section": "Quarto Website",
    "text": "Quarto Website\nAbout\n\nYour index.qmd sets a front page about you.\n\nDetails in about pages are available here:\nhttps://quarto.org/docs/websites/website-about.html.\n\nQuarto includes 5 built-in templates:\n\njolla\ntrestles\nsolana\nmarquee\nbroadside"
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-5",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-5",
    "title": "Lecture 2",
    "section": "Quarto Website",
    "text": "Quarto Website\nIcons and Emojis\n\nA ton of Bootstrap icons are available here:\n\nhttps://icons.getbootstrap.com.\n\nA ton of markdown emojis are available here üòÑ:\n\nhttps://github.com/ikatyang/emoji-cheat-sheet\nhttps://gist.github.com/rxaviers/7360908"
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-6",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-6",
    "title": "Lecture 2",
    "section": "Quarto Website",
    "text": "Quarto Website\nNaviation and Adding Pages\nleft:\n- text: Project\n  href: project.ipynb\n- text: Blog\n  href: blog-listing.qmd\n- text: Homeowrk\n  href: hw.ipynb\n\nWe can add a new page to the website through navbar in _quarto.yml"
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-7",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-7",
    "title": "Lecture 2",
    "section": "Quarto Website",
    "text": "Quarto Website\nNaviation and Adding Pages\nleft:\n  - text: \"Python Data Analysis\"\n    menu:\n      - pandas_basic.ipynb\n      - seaborn_basic.ipynb\n\nWe can also create a drop-down menu by including a menu\nMore details about navbar are available here:\n\nhttps://quarto.org/docs/websites/website-navigation.html"
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-8",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-8",
    "title": "Lecture 2",
    "section": "Quarto Website",
    "text": "Quarto Website\nColors\n\nA ton of hex codes for colors are available here:\n\nhttps://www.color-hex.com"
  },
  {
    "objectID": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-basics",
    "href": "danl-lec/danl-320-lec-02-2025-0127.html#quarto-website-basics",
    "title": "Lecture 2",
    "section": "Quarto Website Basics",
    "text": "Quarto Website Basics\n\nLet‚Äôs do Classwork 3."
  },
  {
    "objectID": "posts/nba/nba.html#salary-distribution-among-teams",
    "href": "posts/nba/nba.html#salary-distribution-among-teams",
    "title": "NBA",
    "section": "Salary Distribution Among Teams",
    "text": "Salary Distribution Among Teams\nLet‚Äôs start with the salary distribution among teams using seaborn for visualization. ‚Äã‚Äã\n\n\nCode\n\n# Handle missing values in 'Salary' by replacing them with the median salary\nmedian_salary = nba['Salary'].median()\nnba['Salary'].fillna(median_salary, inplace=True)\n\n\n\n\nCode\n# Set the aesthetic style of the plots\nsns.set_style(\"whitegrid\")\n\n# Calculate total salary by team\nteam_salary = (\n    nba\n    .groupby('Team')['Salary']\n    .sum()\n    .reset_index()\n    .sort_values(by='Salary', ascending=False)\n)\n\n# Plot total salary by team\nplt.figure(figsize=(10, 8))\nsns.barplot(data = team_salary,\n            x = 'Salary', y = 'Team',\n            palette = 'coolwarm')\nplt.title('Total Salary Distribution Among NBA Teams')\nplt.xlabel('Total Salary')\nplt.ylabel('Team')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\nThe visualization above displays the total salary distribution among NBA teams, with teams sorted by their total salary expenditure. This bar plot reveals which teams are the biggest spenders on player salaries and which are more conservative. The color gradient provides a visual cue to easily distinguish between the higher and lower spending teams. Portland Trail Blazers spent most in their players‚Äô salary, followed by Golden State Warriors and Philadelphia 76ers."
  },
  {
    "objectID": "posts/nba/nba.html#player-age-distribution",
    "href": "posts/nba/nba.html#player-age-distribution",
    "title": "NBA",
    "section": "Player Age Distribution",
    "text": "Player Age Distribution\nNext, let‚Äôs explore the Player Age Distribution across the NBA. We‚Äôll create a histogram to visualize how player ages are distributed, which will help us understand if the league trends younger, older, or has a balanced age mix. ‚Äã‚Äã\n\n\nCode\n# Convert 'Birthday' column to datetime format\nfrom dateutil import parser\nnba['Birthday'] = nba['Birthday'].apply(lambda x: parser.parse(x))\n\n# Now, let's calculate the age of each player\nnba['Age'] = (datetime.now() - nba['Birthday']).dt.days // 365\n\n# Plot the age distribution of NBA players\nplt.figure(figsize=(10, 6))\nsns.histplot(nba['Age'],\n             bins = 15,\n             kde = True,\n             color = 'skyblue')\nplt.title('Age Distribution of NBA Players')\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\nThe histogram above shows the age distribution of NBA players, with a kernel density estimate (KDE) overlay to indicate the distribution shape. The majority of players fall within a certain age range from 25 to 35, illustrating the league‚Äôs age dynamics. The plot helps identify the common ages for NBA players and whether there are significant numbers of very young or older players."
  },
  {
    "objectID": "posts/nba/nba.html#position-wise-salary-insights",
    "href": "posts/nba/nba.html#position-wise-salary-insights",
    "title": "NBA",
    "section": "Position-wise Salary Insights",
    "text": "Position-wise Salary Insights\nMoving on to Position-wise Salary Insights, we‚Äôll examine how average salaries differ across player positions. This analysis could reveal which positions are typically higher-paid, potentially reflecting their value on the basketball court. Let‚Äôs create a box plot to visualize the salary distribution for each position. ‚Äã‚Äã\n\n\nCode\n# Plot salary distribution by player position\nplt.figure(figsize=(10, 6))\nsns.boxplot(data = nba,\n            x = 'Position', y = 'Salary',\n            palette = 'Set2')\nplt.title('Salary Distribution by Position')\nplt.xlabel('Position')\nplt.ylabel('Salary')\nplt.show()\n\n\n\n\n\nThe box plot above illustrates the salary distribution by player position, showcasing the variation in salaries among different positions within the NBA. This visualization helps us understand which positions tend to have higher median salaries and the spread of salaries within each position, including outliers that represent exceptionally high or low salaries. While the positions of C and PG have the widest interquantiles of salaries, the positions of FC, F, G, and GF have the narrowest interquantiles of them."
  },
  {
    "objectID": "posts/nba/nba.html#top-10-highest-paid-players",
    "href": "posts/nba/nba.html#top-10-highest-paid-players",
    "title": "NBA",
    "section": "Top 10 Highest Paid Players",
    "text": "Top 10 Highest Paid Players\nLastly, we‚Äôll identify the Top 10 Highest Paid Players in the NBA. This analysis highlights the star earners of the league, providing insights into which players command the highest salaries and potentially why. Let‚Äôs extract and visualize this information. ‚Äã‚Äã\n\n\nCode\n# Identify the top 10 highest paid players\ntop_10_salaries = nba.sort_values(by='Salary', ascending=False).head(10)\n\n# Plot the top 10 highest paid players\nplt.figure(figsize=(12, 8))\nsns.barplot(data = top_10_salaries,\n            x = 'Salary', y = 'Name',\n            palette = 'viridis')\nplt.title('Top 10 Highest Paid NBA Players')\nplt.xlabel('Salary')\nplt.ylabel('Player')\nplt.show()\n\n\n\n\n\nThe bar chart above reveals the top 10 highest-paid NBA players, showcasing those who stand at the pinnacle of the league in terms of salary. Stephen Curry is the highest-paid NBA player, followed by Russel Westbrook and Chris Paul. This visualization not only highlights the star players who command the highest salaries but also may reflect their marketability, performance, and contribution to their respective teams."
  },
  {
    "objectID": "posts/py-basic/blog-python-basics.html",
    "href": "posts/py-basic/blog-python-basics.html",
    "title": "Python Basics",
    "section": "",
    "text": "Python is a high-level, interpreted programming language. This is a simple Python code:\n\n\nCode\nprint('Hello, World!')\n\n\n\n\n\nIn Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\n\nCode\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n\n10.5\n\n\n\n\n\nPython supports the usual logical conditions from mathematics:\n\n\nCode\n# Equals: a == b\n# Not Equals: a != b\n# Less than: a &lt; b\n# Less than or equal to: a &lt;= b\n# Greater than: a &gt; b\n# Greater than or equal to: a &gt;= b\n\n\nThese conditions can be used in several ways, most commonly in ‚Äòif statements‚Äô and loops.\n\n\nCode\n# if statement:\nif 5 &gt; 2:\n    print('Five is greater than two!')\n\n\n\n\n\nA function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\n\nCode\n# Defining a function:\ndef my_function():\n    print('Hello from a function')\n\n# Calling a function:\nmy_function()\n\n\n\n\n\nA list is a collection which is ordered and changeable.\nA dictionary is a collection which is unordered, changeable and indexed.\n\n\nCode\n# List example:\nmy_list = ['apple', 'banana', 'cherry']\n\n# Dictionary example:\nmy_dict = {'name': 'John', 'age': 36}"
  },
  {
    "objectID": "posts/py-basic/blog-python-basics.html#what-is-python",
    "href": "posts/py-basic/blog-python-basics.html#what-is-python",
    "title": "Python Basics",
    "section": "",
    "text": "Python is a high-level, interpreted programming language. This is a simple Python code:\n\n\nCode\nprint('Hello, World!')"
  },
  {
    "objectID": "posts/py-basic/blog-python-basics.html#variables-and-data-types",
    "href": "posts/py-basic/blog-python-basics.html#variables-and-data-types",
    "title": "Python Basics",
    "section": "",
    "text": "In Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\n\nCode\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n\n10.5"
  },
  {
    "objectID": "posts/py-basic/blog-python-basics.html#control-structures",
    "href": "posts/py-basic/blog-python-basics.html#control-structures",
    "title": "Python Basics",
    "section": "",
    "text": "Python supports the usual logical conditions from mathematics:\n\n\nCode\n# Equals: a == b\n# Not Equals: a != b\n# Less than: a &lt; b\n# Less than or equal to: a &lt;= b\n# Greater than: a &gt; b\n# Greater than or equal to: a &gt;= b\n\n\nThese conditions can be used in several ways, most commonly in ‚Äòif statements‚Äô and loops.\n\n\nCode\n# if statement:\nif 5 &gt; 2:\n    print('Five is greater than two!')"
  },
  {
    "objectID": "posts/py-basic/blog-python-basics.html#functions",
    "href": "posts/py-basic/blog-python-basics.html#functions",
    "title": "Python Basics",
    "section": "",
    "text": "A function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\n\nCode\n# Defining a function:\ndef my_function():\n    print('Hello from a function')\n\n# Calling a function:\nmy_function()"
  },
  {
    "objectID": "posts/py-basic/blog-python-basics.html#lists-and-dictionaries",
    "href": "posts/py-basic/blog-python-basics.html#lists-and-dictionaries",
    "title": "Python Basics",
    "section": "",
    "text": "A list is a collection which is ordered and changeable.\nA dictionary is a collection which is unordered, changeable and indexed.\n\n\nCode\n# List example:\nmy_list = ['apple', 'banana', 'cherry']\n\n# Dictionary example:\nmy_dict = {'name': 'John', 'age': 36}"
  },
  {
    "objectID": "danl-cw/danl-320-cw-04.html",
    "href": "danl-cw/danl-320-cw-04.html",
    "title": "Classwork 4",
    "section": "",
    "text": "Using Python operations only, calculate below: \\[\\frac{2^5}{7 \\cdot (4 - 2^3)}\\]\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-320-cw-04.html#question-1",
    "href": "danl-cw/danl-320-cw-04.html#question-1",
    "title": "Classwork 4",
    "section": "",
    "text": "Using Python operations only, calculate below: \\[\\frac{2^5}{7 \\cdot (4 - 2^3)}\\]\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-320-cw-04.html#question-2",
    "href": "danl-cw/danl-320-cw-04.html#question-2",
    "title": "Classwork 4",
    "section": "Question 2",
    "text": "Question 2\nFor each expression below, what is the value of the expression? Explain thoroughly.\n\n20 == '20'\n\n\nx = 4.0\ny = .5\n\nx &lt; y or 3*y &lt; x\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-320-cw-04.html#question-3",
    "href": "danl-cw/danl-320-cw-04.html#question-3",
    "title": "Classwork 4",
    "section": "Question 3",
    "text": "Question 3\n\nfare = \"$10.00\"\ntip = \"2.00$\"\ntax = \"$ 0.80\"\n\nWrite a Python code that uses slicing and the print() function to print out the following message:\n\nThe total trip cost is: $12.80\n\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-320-cw-04.html#question-4",
    "href": "danl-cw/danl-320-cw-04.html#question-4",
    "title": "Classwork 4",
    "section": "Question 4",
    "text": "Question 4\n\nlist_variable = [100, 144, 169, 1000, 8]\n\nWrite a Python code that uses print() and max() functions to print out the largest value in the list, list_variable, as follows:\n\nThe largest value in the list is: 1000\n\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-320-cw-04.html#question-5",
    "href": "danl-cw/danl-320-cw-04.html#question-5",
    "title": "Classwork 4",
    "section": "Question 5",
    "text": "Question 5\n\nvals = [3, 2, 1, 0]\n\n\nUse a while loop to print each value of the list [3, 2, 1, 0], one at a time.\nUse a for loop to print each value of the list [3, 2, 1, 0], one at a time.\n\n\nAnswer"
  },
  {
    "objectID": "danl-cw/danl-320-cw-03.html",
    "href": "danl-cw/danl-320-cw-03.html",
    "title": "Classwork 3",
    "section": "",
    "text": "_quarto.yml configures the website:\n\nIt determines the structure of the website.\n\ne.g., Navigation bar, themes, HTML options, etc.\n\nIf _quarto.yml is edited, use quarto render to render all qmd and ipynb files.\n\nindex.qmd renders index.html, the front page of the website.\n\nDo not create Quarto files something like index2.qmd within the working directory.\n\nblog-listing.qmd configures the blog listing page.\nposts directory includes sub-directories of blog posts.\nimg directory can be used to store picture files.\n\n\n\n\nA file in the working directory can have its own web address.\n\nFor example, if you have resume-example.pdf in your working directory, it has the web address, https://USERNAME.github.io/resume-example.pdf.\n\nWhen naming a file in the website, do not have any space in a file name!\nBe systematic when naming a series of files in the website.\n\nE.g., danl-320-cw-01.ipynb, danl-320-cw-02.ipynb, danl-320-cw-03.ipynb.\n\n\n\n\n\n\n\nRules\n\nOne blog post corresponds to:\n\n\nOne sub-directory in the posts directory.\nOne *.ipynb (or *.qmd) file.\n\n\nPut all files for one blog post (e.g., *.ipynb (or *.qmd), *.png) in one corresponding sub-directory in the posts directory.\nWhen inserting an image file to a blog post, use a relative path, i.e., a file name of the image file.\n\n\n\n\n\n\n\nDecorate your website:\n\n\nReplace YOUR NAME with your name in _quarto.yml and index.qmd.\nDescribe yourself in index.qmd.\nAdd the picture (png) file of your profile photo to img directory. Then correct img/profile.png in index.qmd accordingly.\nCorrect links for your resum√©, linkedin, email, and social media.\n\n\nAdd a menu of ‚ÄúProject‚Äù to the navigation bar using danl_proj_nba.ipynb.\nAdd a drop-down menu of ‚ÄúPython Data Analysis‚Äù to the navigation bar.\n\n\nUnder the menu of ‚ÄúPython Data Analysis‚Äù, add links for the following webpage:\n\nPandas Basics using pandas_basic.ipynb\nSeaborn Basics using seaborn_basic.ipynb\n\n\n\nUse the 3-step git commands (git add, git commit, and git push) to update your website.\n\n\n\n\n\n\nQuarto - Creating a Website\nQuarto - HTML Basics\nQuarto - HTML Code Blocks\nQuarto - HTML Theming\nQuarto - Creating a Blog"
  },
  {
    "objectID": "danl-cw/danl-320-cw-03.html#website-files",
    "href": "danl-cw/danl-320-cw-03.html#website-files",
    "title": "Classwork 3",
    "section": "",
    "text": "_quarto.yml configures the website:\n\nIt determines the structure of the website.\n\ne.g., Navigation bar, themes, HTML options, etc.\n\nIf _quarto.yml is edited, use quarto render to render all qmd and ipynb files.\n\nindex.qmd renders index.html, the front page of the website.\n\nDo not create Quarto files something like index2.qmd within the working directory.\n\nblog-listing.qmd configures the blog listing page.\nposts directory includes sub-directories of blog posts.\nimg directory can be used to store picture files.\n\n\n\n\nA file in the working directory can have its own web address.\n\nFor example, if you have resume-example.pdf in your working directory, it has the web address, https://USERNAME.github.io/resume-example.pdf.\n\nWhen naming a file in the website, do not have any space in a file name!\nBe systematic when naming a series of files in the website.\n\nE.g., danl-320-cw-01.ipynb, danl-320-cw-02.ipynb, danl-320-cw-03.ipynb."
  },
  {
    "objectID": "danl-cw/danl-320-cw-03.html#blogging",
    "href": "danl-cw/danl-320-cw-03.html#blogging",
    "title": "Classwork 3",
    "section": "",
    "text": "Rules\n\nOne blog post corresponds to:\n\n\nOne sub-directory in the posts directory.\nOne *.ipynb (or *.qmd) file.\n\n\nPut all files for one blog post (e.g., *.ipynb (or *.qmd), *.png) in one corresponding sub-directory in the posts directory.\nWhen inserting an image file to a blog post, use a relative path, i.e., a file name of the image file."
  },
  {
    "objectID": "danl-cw/danl-320-cw-03.html#practice-problems",
    "href": "danl-cw/danl-320-cw-03.html#practice-problems",
    "title": "Classwork 3",
    "section": "",
    "text": "Decorate your website:\n\n\nReplace YOUR NAME with your name in _quarto.yml and index.qmd.\nDescribe yourself in index.qmd.\nAdd the picture (png) file of your profile photo to img directory. Then correct img/profile.png in index.qmd accordingly.\nCorrect links for your resum√©, linkedin, email, and social media.\n\n\nAdd a menu of ‚ÄúProject‚Äù to the navigation bar using danl_proj_nba.ipynb.\nAdd a drop-down menu of ‚ÄúPython Data Analysis‚Äù to the navigation bar.\n\n\nUnder the menu of ‚ÄúPython Data Analysis‚Äù, add links for the following webpage:\n\nPandas Basics using pandas_basic.ipynb\nSeaborn Basics using seaborn_basic.ipynb\n\n\n\nUse the 3-step git commands (git add, git commit, and git push) to update your website."
  },
  {
    "objectID": "danl-cw/danl-320-cw-03.html#references",
    "href": "danl-cw/danl-320-cw-03.html#references",
    "title": "Classwork 3",
    "section": "",
    "text": "Quarto - Creating a Website\nQuarto - HTML Basics\nQuarto - HTML Code Blocks\nQuarto - HTML Theming\nQuarto - Creating a Blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DANL 320-01: Big Data Analytics, Spring 2025",
    "section": "",
    "text": "Welcome! üëã\n\\(-\\) Explore, Learn, and Grow with Data Analytics! üåü"
  },
  {
    "objectID": "index.html#bullet-lecture-slides",
    "href": "index.html#bullet-lecture-slides",
    "title": "DANL 320-01: Big Data Analytics, Spring 2025",
    "section": "\\(\\bullet\\,\\) Lecture Slides üöÄ",
    "text": "\\(\\bullet\\,\\) Lecture Slides üöÄ\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nLecture 8\n\n\nMarch 10, 2025\n\n\n\n\nLecture 7\n\n\nFebruary 17, 2025\n\n\n\n\nLecture 6\n\n\nFebruary 10, 2025\n\n\n\n\nLecture 5\n\n\nFebruary 5, 2025\n\n\n\n\nLecture 4\n\n\nFebruary 3, 2025\n\n\n\n\nLecture 3\n\n\nJanuary 29, 2025\n\n\n\n\nLecture 2\n\n\nJanuary 27, 2025\n\n\n\n\nLecture 1\n\n\nJanuary 22, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#bullet-classwork",
    "href": "index.html#bullet-classwork",
    "title": "DANL 320-01: Big Data Analytics, Spring 2025",
    "section": "\\(\\bullet\\,\\) Classwork ‚å®Ô∏è",
    "text": "\\(\\bullet\\,\\) Classwork ‚å®Ô∏è\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nClasswork 10\n\n\nMarch 12, 2025\n\n\n\n\nClasswork 9\n\n\nMarch 5, 2025\n\n\n\n\nClasswork 8\n\n\nMarch 5, 2025\n\n\n\n\nClasswork 7\n\n\nFebruary 17, 2025\n\n\n\n\nClasswork 6\n\n\nFebruary 12, 2025\n\n\n\n\nClasswork 5\n\n\nFebruary 10, 2025\n\n\n\n\nClasswork 4\n\n\nJanuary 29, 2025\n\n\n\n\nClasswork 3\n\n\nJanuary 27, 2025\n\n\n\n\nClasswork 2\n\n\nJanuary 27, 2025\n\n\n\n\nClasswork 1\n\n\nJanuary 22, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#bullet-homework",
    "href": "index.html#bullet-homework",
    "title": "DANL 320-01: Big Data Analytics, Spring 2025",
    "section": "\\(\\bullet\\,\\) Homework üíª",
    "text": "\\(\\bullet\\,\\) Homework üíª\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nHomework 3\n\n\nMarch 18, 2025\n\n\n\n\nHomework 2\n\n\nMarch 13, 2025\n\n\n\n\nHomework 1\n\n\nMarch 3, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "listing-danl-320-hw.html",
    "href": "listing-danl-320-hw.html",
    "title": "DANL 320 - Homework",
    "section": "",
    "text": "Title\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nHomework 1\n\n\nSurvey, Personal Website, and Python Basics\n\n\nMarch 3, 2025\n\n\n\n\nHomework 2\n\n\nLinear Regression; Jupyter Notebook Blogging\n\n\nMarch 13, 2025\n\n\n\n\nHomework 3\n\n\nRegression; Jupyter Notebook Blogging\n\n\nMarch 18, 2025\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "listing-danl-320-lec.html",
    "href": "listing-danl-320-lec.html",
    "title": "DANL 320 - Lecture",
    "section": "",
    "text": "Title\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nLecture 1\n\n\nSyllabus, Course Outline, and DANL Career\n\n\nJanuary 22, 2025\n\n\n\n\nLecture 2\n\n\nGetting Started with Jupyter Notebook and Quarto\n\n\nJanuary 27, 2025\n\n\n\n\nLecture 3\n\n\nPython Basics\n\n\nJanuary 29, 2025\n\n\n\n\nLecture 4\n\n\nBig Data\n\n\nFebruary 3, 2025\n\n\n\n\nLecture 5\n\n\nDistributed Computing Framework; Apache Hadoop and Spark; PySpark\n\n\nFebruary 5, 2025\n\n\n\n\nLecture 6\n\n\nPySpark Basics\n\n\nFebruary 10, 2025\n\n\n\n\nLecture 7\n\n\nLinear Regression\n\n\nFebruary 17, 2025\n\n\n\n\nLecture 8\n\n\nLogistic Regression\n\n\nMarch 10, 2025\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "econml.html",
    "href": "econml.html",
    "title": "Causal Machine Learning Bookmarks",
    "section": "",
    "text": "Dive into Causal Machine Learning, The World Bank and Pontificia Universidad Cat√≥lica del Per√∫, Alexander Quispe et. al. \nMIT 14.388: Inference on Causal and Structural Parameters Using ML and AI, Department of Economics, MIT, Victor Chernozukhov\n\nPython Website\nJulia Website \n\nMGTECON 634: ML-based Causal Inference, Stanford, Susan Athey \nMachine Learning & Causal Inference: A Short Course, Stanford, Susan Athey, Jan Spiess, and Stefan Wager\n\nTutorial\nYouTube \n\n2018 American Economic Association Continuing Education: Machine Learning and Econometrics, Susan Athey and Guido Imbens \nCausal Inference and Machine Learning in Practice with EconML and CausalML: Industrial Use Cases at Microsoft, TripAdvisor, Uber \nDoubleML: Python and R Packages for the Double/Debiased Machine Learning Framework, P. Bach, V. Chernozhukov, M. S. Kurz, and M. Spindler \nEconML: A Python Package for ML-based Heterogeneous Treatment Effects Estimation, Microsoft \nCausalML: A Python Package for ML-based Causal Inference, Uber"
  },
  {
    "objectID": "econml.html#causal-machine-learning",
    "href": "econml.html#causal-machine-learning",
    "title": "Causal Machine Learning Bookmarks",
    "section": "",
    "text": "Dive into Causal Machine Learning, The World Bank and Pontificia Universidad Cat√≥lica del Per√∫, Alexander Quispe et. al. \nMIT 14.388: Inference on Causal and Structural Parameters Using ML and AI, Department of Economics, MIT, Victor Chernozukhov\n\nPython Website\nJulia Website \n\nMGTECON 634: ML-based Causal Inference, Stanford, Susan Athey \nMachine Learning & Causal Inference: A Short Course, Stanford, Susan Athey, Jan Spiess, and Stefan Wager\n\nTutorial\nYouTube \n\n2018 American Economic Association Continuing Education: Machine Learning and Econometrics, Susan Athey and Guido Imbens \nCausal Inference and Machine Learning in Practice with EconML and CausalML: Industrial Use Cases at Microsoft, TripAdvisor, Uber \nDoubleML: Python and R Packages for the Double/Debiased Machine Learning Framework, P. Bach, V. Chernozhukov, M. S. Kurz, and M. Spindler \nEconML: A Python Package for ML-based Heterogeneous Treatment Effects Estimation, Microsoft \nCausalML: A Python Package for ML-based Causal Inference, Uber"
  },
  {
    "objectID": "econml.html#machine-learning-and-big-data",
    "href": "econml.html#machine-learning-and-big-data",
    "title": "Causal Machine Learning Bookmarks",
    "section": "Machine Learning and Big Data",
    "text": "Machine Learning and Big Data\n\n2023 American Economic Association Continuing Education: Machine Learning and Big Data, Melissa Dell and Matthew Harding \nMachine Learning for Economists (ml4econ), Bank of Israel, Itamar Caspi and Ariel Mansura"
  },
  {
    "objectID": "econml.html#causal-inference",
    "href": "econml.html#causal-inference",
    "title": "Causal Machine Learning Bookmarks",
    "section": "Causal Inference",
    "text": "Causal Inference\n\nCausal Inference: The Mixtape, Scott Cunningham \nCausal Inference for the Brave and True, Matheus Facure \nCausal Inference and Its Applications in Online Industry, Alex Deng \nApplied Empirical Methods, Yale SOM, Paul Goldsmith-Pinkham\n\nYouTube \n\nCausal Inference with Panel Data, Department of Political Science, Stanford, Yiqing Xu\n\nYouTube \n\nCausal Inference: What If, Miguel A. Hern√°n and James M. Robins \nRecent Developments in Difference-in-Differences, Vienna University of Economics and Business, Asjad Naqvi \nDifference-in-Differences Blog \nGov 2003: Causal Inference, Department of Government, Harvard, Matthew Blackwell"
  },
  {
    "objectID": "econml.html#researchers-in-causal-machine-learning",
    "href": "econml.html#researchers-in-causal-machine-learning",
    "title": "Causal Machine Learning Bookmarks",
    "section": "Researchers in Causal Machine Learning",
    "text": "Researchers in Causal Machine Learning\n\nSusan Athey \nAlexandre Belloni \nVictor Chernozhukov \nCarlos Cinelli \nChristian Hansen \nGuido Imbens\nJann Spiess \nStefan Wager"
  },
  {
    "objectID": "danl-cw/danl-320-cw-04.html#question-6",
    "href": "danl-cw/danl-320-cw-04.html#question-6",
    "title": "Classwork 4",
    "section": "Question 6",
    "text": "Question 6\n\nAssign the value 7 to the variable guess_me, and the value 1 to the variable number.\nWrite a while loop that compares number with guess_me.\n\nPrint ‚Äòtoo low‚Äô if number is less than guess me.\nIf number equals guess_me, print ‚Äòfound it!‚Äô and then exit the loop.\nIf number is greater than guess_me, print ‚Äòoops‚Äô and then exit the loop.\nIncrement number at the end of the loop.\n\nWrite a for loop that compares number with guess_me.\n\nPrint ‚Äòtoo low‚Äô if number is less than guess me.\nIf number equals guess_me, print ‚Äòfound it!‚Äô and then exit the loop.\nIf number is greater than guess_me, print ‚Äòoops‚Äô and then exit the loop.\nIncrement number at the end of the loop.\n\n\n\nAnswer"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#repeat-with-while-4",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#repeat-with-while-4",
    "title": "Lecture 3",
    "section": "Repeat with while",
    "text": "Repeat with while\nCheck break Use with else\n\nWe can consider using while with else when we‚Äôve coded a while loop to check for something, and breaking as soon as it‚Äôs found. \n\nnumbers = [1, 3, 5]\nposition = 0\n\nwhile position &lt; len(numbers):\n    number = numbers[position]\n    if number &gt; 4:  # Condition changed to checking if the number is greater than 4\n        print('Found a number greater than 4:', number)\n        break\n    position += 1\nelse:  # break not called\n    print('No number greater than 4 found')\n\nConsider it a break checker."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in-6",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#iterate-with-for-and-in-6",
    "title": "Lecture 3",
    "section": "Iterate with for and in",
    "text": "Iterate with for and in\nCheck break Use with else\n\nSimilar to while, for has an optional else that checks whether the for completed normally.\n\nIf break was not called, the else statement is run.\n\n\nword = 'thud'\nfor letter in word:\n    if letter == 'x':\n        print(\"Eek! An 'x'!\")\n        break\n    print(letter)\nelse:\n    print(\"No 'x' in there.\")"
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#distributed-computing-framework-dcf",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#distributed-computing-framework-dcf",
    "title": "Lecture 4",
    "section": "Distributed Computing Framework (DCF)",
    "text": "Distributed Computing Framework (DCF)\n\n\n\n\n\n\n\n\nMassive Jigsaw Puzzle:\n\nSolving alone takes forever.\nInvite friends to work on different sections simultaneously.\n\n\n\n\nDCF Role:\n\nActs like a team manager.\nSplits large problems into manageable tasks.\nCoordinates parallel work across multiple computers (nodes)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#how-dcf-works-the-process",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#how-dcf-works-the-process",
    "title": "Lecture 4",
    "section": "How DCF Works: The Process",
    "text": "How DCF Works: The Process\n\nTask Distribution:\n\nBreaks problems into smaller tasks.\nAssigns tasks to different nodes in the network.\n\nParallel Processing:\n\nMultiple nodes work at the same time.\nLike an assembly line where different parts are built simultaneously.\n\nCommunication & Aggregation:\n\nEnsures nodes communicate effectively.\nGathers individual results into a final solution."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#distributed-computing-framework-dcf-1",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#distributed-computing-framework-dcf-1",
    "title": "Lecture 4",
    "section": "Distributed Computing Framework (DCF)",
    "text": "Distributed Computing Framework (DCF)\nReal-World Analogies\n\nFactory Manager:\n\nEach worker builds a part of a toy (arms, legs, wheels).\nThe manager (DCF) ensures all parts come together to form a complete toy.\n\nRace Organizer:\n\nDifferent computers have varying speeds and capabilities.\nTougher tasks are assigned to faster or more capable nodes."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#distributed-computing-framework-dcf-2",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#distributed-computing-framework-dcf-2",
    "title": "Lecture 4",
    "section": "Distributed Computing Framework (DCF)",
    "text": "Distributed Computing Framework (DCF)\nRobustness and Scalability\n\nFault Tolerance:\n\nHandles failures gracefully.\nIf a node fails, tasks are reassigned or retried (like a substitute worker in a factory).\n\nResource Allocation:\n\nDistributes tasks based on node capability.\nOptimizes efficiency across the network.\n\nScalability:\n\nEasily adds more computers to the network.\nMore helpers = faster puzzle solving."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#distributed-computing-framework-dcf-3",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#distributed-computing-framework-dcf-3",
    "title": "Lecture 4",
    "section": "Distributed Computing Framework (DCF)",
    "text": "Distributed Computing Framework (DCF)\n\nDCF is the Conductor of the Orchestra:\n\nEvery musician (node) plays their part.\nThe conductor (DCF) synchronizes the performance to create a harmonious final result.\n\nKey Benefits:\n\nFaster problem-solving through parallelism.\nEfficient management of tasks and resources.\nResilient to failures and scalable for growing problems."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#distributed-computing-framework-dcf-4",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#distributed-computing-framework-dcf-4",
    "title": "Lecture 4",
    "section": "Distributed Computing Framework (DCF)",
    "text": "Distributed Computing Framework (DCF)\nReal-World Examples\n\nHadoop\nApache Spark"
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#hadoop",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#hadoop",
    "title": "Lecture 4",
    "section": "Hadoop",
    "text": "Hadoop\nIntroduction to Hadoop\n\n\n\n\n\n\nDefinition\n\nAn open-source software framework for storing and processing large data sets.\n\nComponents\n\nHadoop Distributed File System (HDFS): Distributed data storage.\nMapReduce: Data processing model."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#spark",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#spark",
    "title": "Lecture 4",
    "section": "Spark",
    "text": "Spark\n\n\n\n\n\n\nApache Spark: distributed processing system used for big data workloads. a unified computing engine and computer clusters\n\nIt contains a set of libraries for parallel processing for data analysis, machine learning, graph analysis, and streaming live data."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#what-is-the-history-of-apache-spark",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#what-is-the-history-of-apache-spark",
    "title": "Lecture 4",
    "section": "What is the history of Apache Spark?",
    "text": "What is the history of Apache Spark?\n\nApache Spark started in 2009 as a research project at UC Berkley‚Äôs AMPLab, a collaboration involving students, researchers, and faculty, focused on data-intensive application domains.\nThe goal of Spark was to create a new framework, optimized for fast iterative processing like machine learning, and interactive data analysis, while retaining the scalability, and fault tolerance of Hadoop MapReduce.\nThe first paper entitled, ‚ÄúSpark: Cluster Computing with Working Sets‚Äù was published in June 2010, and Spark was open sourced under a BSD license.\nIn June, 2013, Spark entered incubation status at the Apache Software Foundation (ASF), and established as an Apache Top-Level Project in February, 2014.\nSpark can run standalone, on Apache Mesos, or most frequently on Apache Hadoop."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#spark-vs.-hadoop",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#spark-vs.-hadoop",
    "title": "Lecture 4",
    "section": "Spark vs.¬†Hadoop",
    "text": "Spark vs.¬†Hadoop\nHadoop MapReduce: The Challenge\n\nSequential Multi-Step Process:\n\nReads data from the cluster.\nProcesses data.\nWrites results back to HDFS.\n\nDisk Input/Output Latency:\n\nEach step requires disk read/write.\nResults in slower performance due to latency."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#spark-vs.-hadoop-1",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#spark-vs.-hadoop-1",
    "title": "Lecture 4",
    "section": "Spark vs.¬†Hadoop",
    "text": "Spark vs.¬†Hadoop\nApache Spark: The Solution\n\nIn-Memory Processing:\n\nLoads data into memory once.\nPerforms all operations in-memory.\n\nData Reuse:\n\nCaches data for reuse in multiple operations (ideal for iterative tasks like machine learning).\n\nFaster Execution:\n\nEliminates multiple disk I/O steps.\nDramatically reduces latency for interactive analytics and real-time processing."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#spark-vs.-hadoop-2",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#spark-vs.-hadoop-2",
    "title": "Lecture 4",
    "section": "Spark vs.¬†Hadoop",
    "text": "Spark vs.¬†Hadoop\n\n\nApache Hadoop\n\nFramework Components:\n\nHDFS: Distributed storage system.\nMapReduce: Programming model for parallel processing.\n\nEcosystem:\n\nTypically integrates multiple execution engines (e.g., Spark) within a single deployment.\n\n\n\nSpark\n\nFocus Areas:\n\nInteractive queries, machine learning, and real-time analytics.\n\nStorage Agnostic:\n\nDoes not have its own storage system.\nOperates on data stored in systems like HDFS, etc.\n\nIntegration:\n\nCan run alongside Hadoop"
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#spark-vs.-hadoop-3",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#spark-vs.-hadoop-3",
    "title": "Lecture 4",
    "section": "Spark vs.¬†Hadoop",
    "text": "Spark vs.¬†Hadoop\nComplementary Use\n\nMany organizations store massive datasets in HDFS and utilize Spark for fast, interactive data processing.\n\nSpark can read data directly from HDFS, enabling seamless integration between storage and computation.\n\nHadoop provides robust storage and processing capabilities.\nSpark brings speed and versatility to data analytics, making them a powerful combination for solving complex business challenges.\nUse Case Example: An e-commerce company might store historical sales data in HDFS while using Spark to analyze customer behavior in real time to recommend products or detect fraudulent transactions."
  },
  {
    "objectID": "danl-lec/danl-320-lec-04-2025-0203.html#apache-spark-1",
    "href": "danl-lec/danl-320-lec-04-2025-0203.html#apache-spark-1",
    "title": "Lecture 4",
    "section": "Apache Spark",
    "text": "Apache Spark\nMedscape: Real-Time Medical News for Healthcare Professionals\n\n\nA medical news app for smartphones and tablets designed to keep healthcare professionals informed.\n\nProvides up-to-date medical news and expert perspectives.\n\n\n\n\n\n\n\n\n\n\n\n\nReal-Time Updates:\n\nUses Apache Storm/Spark to process about 500 million tweets per day.\nAutomatic Twitter feed integration helps users track important medical trends shared by physicians and medical commentators."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html",
    "title": "Lecture 3",
    "section": "",
    "text": "A value is datum (literal) such as a number or text.\nThere are different types of values:\n\n352.3 is known as a float or double;\n22 is an integer;\n‚ÄúHello World!‚Äù is a string.\n\n\n\n\n\n\n\n\na = 10\nprint(a)\n\n\n\n\n\n\n\nA variable is a name that refers to a value.\n\nWe can think of a variable as a box that has a value, or multiple values, packed inside it.\n\nA variable is just a name!\n\n\n\n\n\n\n\n\n\n\n\nSometimes you will hear variables referred to as objects.\nEverything that is not a literal value, such as 10, is an object.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition: A data.frame is a table-like data structure used for storing data in a tabular format with rows and columns.\nStructure: Consists of:\n\nVariables (Columns)\nObservations (Rows)\nValues (Cells): Individual data points within each cell of the data.frame.\n\n\n\n\n\n\n\n# Here we assign the integer value 5 to the variable x.\nx = 5   \n\n# Now we can use the variable x in the next line.\ny = x + 12  \ny\n\nIn Python, we use = to assign a value to a variable.\nIn math, = means equality of both sides.\nIn programs, = means assignment: assign the value on the right side to the variable on the left side.\n\n\n\n\n\n\n\n\nThe two main principles for coding and managing data are:\n\nMake things easier for your future self.\nDon‚Äôt trust your future self.\n\nThe # mark is Google Colab‚Äôs comment character.\n\nThe # character has many names: hash, sharp, pound, or octothorpe.\n# indicates that the rest of the line is to be ignored.\nWrite comments before the line that you want the comment to apply to.\n\nConsider adding more comments on code cells and their results using text cells.\n\n\n\n\n\n\n\n\nIn programming code, everything on the right side needs to have a value.\n\nThe right side can be a literal value, or a variable that has already been assigned a value, or a combination.\n\nWhen Python reads y = x + 12, it does the following:\n\nSees the = in the middle.\nKnows that this is an assignment.\nCalculates the right side (gets the value of the object referred to by x and adds it to 12).\nAssigns the result to the left-side variable, y.\n\n\n\n\n\n\n\n\nlist_example = [10, 1.23, \"like this\", True, None]\nprint(list_example)\ntype(list_example)\n\nThe most basic built-in data types that we‚Äôll need to know about are:\n\nintegers 10\nfloats 1.23\nstrings \"like this\"\nbooleans True\nnothing None\n\nPython also has a built-in type of data container called a list (e.g., [10, 15, 20]) that can contain anything, even different types\n\n\n\n\n\n\n\n\n\n\n\nThe second column (Type) contains the Python name of that type.\nThe third column (Mutable?) indicates whether the value can be changed after creation.\n\n\n\n\n\n\n\n\n\nThere are several kinds of brackets in Python, including [], {}, and ().\n\n\n[]{}()\n\n\nvector = ['a', 'b']\nvector[0]\n\n[] is used to denote a list or to signify accessing a position using an index.\n\n\n\n{'a', 'b'}  # set\n{'first_letter': 'a', 'second_letter': 'b'}  # dictionary\n\n{} is used to denote a set or a dictionary (with key-value pairs).\n\n\n\nnum_tup = (1, 2, 3)\nsum(num_tup)\n\n() is used to denote\n\na tuple, or\nthe arguments to a function, e.g., function(x) where x is the input passed to the function.\n\n\n\n\n\n\n\n\n\n\n\n\nstring_one = \"This is an example \"\nstring_two = \"of string concatenation\"\nstring_full = string_one + string_two\nprint(string_full)\n\nAll of the basic operators we see in mathematics are available to use:\n\n\n\n\n+ for addition\n- for subtraction\n\n\n\n* for multiplication\n** for powers\n\n\n\n/ for division\n// for integer division\n\n\n\n\nThese work as you‚Äôd expect on numbers.\nThese operators are sometimes defined for other built-in data types too.\n\nWe can ‚Äòsum‚Äô strings (which really concatenates them).\n\n\n\n\n\n\n\n\n\n\nlist_one = [\"apples\", \"oranges\"]\nlist_two = [\"pears\", \"satsumas\"]\nlist_full = list_one + list_two\nprint(list_full)\n\nIt works for lists too:\n\n\nstring = \"apples, \"\nprint(string * 3)\n\nWe can multiply strings!\n\n\n\n\n\n\n\n\n\nQ. Classwork 4.1\nUsing Python operations only, calculate below: \\[\\frac{2^5}{7 \\cdot (4 - 2^3)}\\]\n\n\n\n\n\n\n\n\norig_number = 4.39898498\ntype(orig_number)\n\nmod_number = int(orig_number)\nmod_number\ntype(mod_number)\n\n\n\nSometimes we need to explicitly cast a value from one type to another.\n\nWe can do this using built-in functions like str(), int(), and float().\nIf we try these, Python will do its best to interpret the input and convert it to the output type we‚Äôd like and, if they can‚Äôt, the code will throw a great big error.\n\n\n\n\n\n\n\n\n\n\nA tuple is an object that is defined by parentheses and entries that are separated by commas, for example (15, 20, 32). (They are of type tuple.)\nTuples are immutable, while lists are mutable.\nImmutable objects, such as tuples and strings, can‚Äôt have their elements changed, appended, extended, or removed.\n\nMutable objects, such as lists, can do all of these things.\n\nIn everyday programming, we use lists and dictionaries more than tuples.\n\n\n\n\n\n\n\n\ncities_to_temps = {\"Paris\": 28, \"London\": 22, \"New York\": 36, \"Seoul\": 29}\n\ncities_to_temps.keys()\ncities_to_temps.values()\ncities_to_temps.items()\n\nAnother built-in Python type that is enormously useful is the dictionary.\n\nThis provides a mapping one set of variables to another (either one-to-one or many-to-one).\nIf you need to create associations between objects, use a dictionary.\n\nWe can obtain keys, values, or key-value paris from dictionaries.\n\n\n\n\n\n\n\n\nBeing able to create empty containers is sometimes useful, especially when using loops.\nThe commands to create empty lists, tuples, dictionaries, and sets are lst = [], tup=(), dic={}, and st = set() respectively.\nQ. What is the type of an empty list?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#variable-in-data.frame",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#variable-in-data.frame",
    "title": "Lecture 3",
    "section": "Variable in data.frame",
    "text": "Variable in data.frame\n\n\n\n\n\n\nDefinition: A data.frame is a table-like data structure used for storing data in a tabular format with rows and columns.\nStructure: Consists of:\n\nVariables (Columns)\nObservations (Rows)\nValues (Cells): Individual data points within each cell of the data.frame."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-1",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-1",
    "title": "Lecture 3",
    "section": "Booleans, Conditions, and if Statements",
    "text": "Booleans, Conditions, and if Statements\n10 == 20\n10 == '10'\n\nBoolean data have either True or False value."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-2",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-2",
    "title": "Lecture 3",
    "section": "Booleans, Conditions, and if Statements",
    "text": "Booleans, Conditions, and if Statements\n\n\n\n\n\n\n\n\nExisting booleans can be combined, which create a boolean when executed."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-3",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-3",
    "title": "Lecture 3",
    "section": "Booleans, Conditions, and if Statements",
    "text": "Booleans, Conditions, and if Statements\nConditions are expressions that evaluate as booleans."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-4",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-4",
    "title": "Lecture 3",
    "section": "Booleans, Conditions, and if Statements",
    "text": "Booleans, Conditions, and if Statements\nboolean_condition1 = 10 == 20\nprint(boolean_condition1)\n\nboolean_condition2 = 10 == '10'\nprint(boolean_condition2)\n\nThe == is an operator that compares the objects on either side and returns True if they have the same values\nQ. What does not (not True) evaluate to?\nQ. Classwork 4.2"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-5",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-5",
    "title": "Lecture 3",
    "section": "Booleans, Conditions, and if Statements",
    "text": "Booleans, Conditions, and if Statements\nname = \"Geneseo\"\nscore = 99\n\nif name == \"Geneseo\" and score &gt; 90:\n    print(\"Geneseo, you achieved a high score.\")\n\nif name == \"Geneseo\" or score &gt; 90:\n    print(\"You could be called Geneseo or have a high score\")\n\nif name != \"Geneseo\" and score &gt; 90:\n    print(\"You are not called Geneseo and you have a high score\")\n\nThe real power of conditions comes when we start to use them in more complex examples, such as if statements."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-6",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-6",
    "title": "Lecture 3",
    "section": "Booleans, Conditions, and if Statements",
    "text": "Booleans, Conditions, and if Statements\nname_list = [\"Lovelace\", \"Smith\", \"Hopper\", \"Babbage\"]\n\nprint(\"Lovelace\" in name_list)\n\nprint(\"Bob\" in name_list)\n\nOne of the most useful conditional keywords is in.\n\nThis one must pop up ten times a day in most coders‚Äô lives because it can pick out a variable or make sure something is where it‚Äôs supposed to be.\n\nQ. Check if ‚Äúa‚Äù is in the string ‚ÄúSun Devil Arena‚Äù using in. Is ‚Äúa‚Äù in ‚ÄúAnyone‚Äù?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-7",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-7",
    "title": "Lecture 3",
    "section": "Booleans, Conditions, and if Statements",
    "text": "Booleans, Conditions, and if Statements\nscore = 98\n\nif score == 100:\n    print(\"Top marks!\")\nelif score &gt; 90 and score &lt; 100:\n    print(\"High score!\")\nelif score &gt; 10 and score &lt;= 90:\n    pass\nelse:\n    print(\"Better luck next time.\")\n\nOne conditional construct we‚Äôre bound to use at some point, is the if-else chain:"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-8",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-8",
    "title": "Lecture 3",
    "section": "Booleans, Conditions, and if Statements",
    "text": "Booleans, Conditions, and if Statements\nIndentation\n\nWe have seen that certain parts of the code examples are indented.\nCode that is part of a function, a conditional clause, or loop is indented.\nIndention is actually what tells the Python interpreter that some code is to be executed as part of, say, a loop and not to executed after the loop is finished."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-9",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#booleans-conditions-and-if-statements-9",
    "title": "Lecture 3",
    "section": "Booleans, Conditions, and if Statements",
    "text": "Booleans, Conditions, and if Statements\nIndentation\nx = 10\n\nif x &gt; 2:\n    print(\"x is greater than 2\")\n\nHere‚Äôs a basic example of indentation as part of an if statement.\nThe standard practice for indentation is that each sub-statement should be indented by 4 spaces."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods",
    "title": "Lecture 3",
    "section": "Slicing Methods",
    "text": "Slicing Methods\n\n\n\n\nWith slicing methods, we can get subset of the data object.\nSlicing methods can apply for strings, lists, arrays, and DataFrames.\nThe above example describes indexing in Python"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-1",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-1",
    "title": "Lecture 3",
    "section": "Slicing Methods",
    "text": "Slicing Methods\nStrings\nstring = \"cheesecake\"\nprint( string[-4:] )\n\nFrom strings, we can access the individual characters via slicing and indexing.\n\n\n\nstring = \"cheesecake\"\nprint(\"String has length:\")\nprint( len(string) )\n\nlist_of_numbers = range(1, 20)\nprint(\"List of numbers has length:\")\nprint( len(list_of_numbers) )\n\n\n\nBoth lists and strings will allow us to use the len() command to get their length:"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-2",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-2",
    "title": "Lecture 3",
    "section": "Slicing Methods",
    "text": "Slicing Methods\nString-related Functions\nDot operation\n\nIn Python, we can access attributes by using a dot notation (.).\nUnlike len(), some functions use a dot to access to strings.\nTo use those string functions, type (1) the name of the string, (2) a dot, (3) the name of the function, and (4) any arguments that the function needs:\n\nstring_name.some_function(arguments)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-3",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-3",
    "title": "Lecture 3",
    "section": "Slicing Methods",
    "text": "Slicing Methods\nString-related Functions\nSplit with split()\n\nWe can use the built-in string split() function to break a string into a list of smaller strings based on some separator.\n\nIf we don‚Äôt specify a separator, split() uses any sequence of white space characters‚Äînewlines, spaces, and tabs:\n\ntasks = 'get gloves,get mask,give cat vitamins,call ambulance'\ntasks.split(',')\ntasks.split()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-4",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-4",
    "title": "Lecture 3",
    "section": "Slicing Methods",
    "text": "Slicing Methods\nString-related Functions\nCombine by Using join()\n\njoin() collapses a list of strings into a single string.\n\ncrypto_list = ['Yeti', 'Bigfoot', 'Loch Ness Monster']\ncrypto_string = ', '.join(crypto_list)\nprint('Found and signing book deals:', crypto_string)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-5",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-5",
    "title": "Lecture 3",
    "section": "Slicing Methods",
    "text": "Slicing Methods\nStrings and Slicing\n\nWe can extract a substring (a part of a string) from a string by using a slice.\nWe define a slice by using square brackets ([]), a start index, an end index, and an optional step count between them.\n\nWe can omit some of these.\n\nThe slice will include characters from index start to one before end:"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-6",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-6",
    "title": "Lecture 3",
    "section": "Slicing Methods",
    "text": "Slicing Methods\nGet a Substring with a Slice\n\n[:][ start :][: end ][ start : end ][ start : end : step ]\n\n\nletters = 'abcdefghij'\nletters[:]\n\n[:] extracts the entire sequence from start to end.\n\n\n\nletters = 'abcdefghij'\nletters[4:]\nletters[2:]\nletters[-3:]\nletters[-50:]\n\n[ start :] specifies from the start index to the end.\n\n\n\nletters = 'abcdefghij'\nletters[:3]\nletters[:-3]\nletters[:70]\n\n[: end ] specifies from the beginning to the end index minus 1.\n\n\n\nletters = 'abcdefghij'\nletters[2:5]\nletters[-26:-24]\nletters[35:37]\n\n[ start : end ] indicates from the start index to the end index minus 1.\n\n\n\nletters = 'abcdefghij'\nletters[2 : 6 : 2]   # From index 2 to 5, by steps of 2 characters\nletters[ : : 3]     # From the start to the end, in steps of 3 characters\nletters[ 6 : : 4 ]    # From index 19 to the end, by 4\nletters[ : 7 : 5 ]    # From the start to index 6 by 5:\nletters[-1 : : -1 ]   # Starts at the end and ends at the start\nletters[: : -1 ]\n\n[ start : end : step ] extracts from the start index to the end index minus 1, skipping characters by step."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-7",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-7",
    "title": "Lecture 3",
    "section": "Slicing Methods",
    "text": "Slicing Methods\nLists\n\nPython is\n\na zero-indexed language (things start counting from zero);\nleft inclusive;\nright exclusive when we are specifying a range of values."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-8",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-8",
    "title": "Lecture 3",
    "section": "Slicing Methods",
    "text": "Slicing Methods\nLists\nlist_example = ['one', 'two', 'three']\nlist_example[ 0 : 1 ]\nlist_example[ 1 : 3 ]\n\n\n\n\nWe can think of items in a list-like object as being fenced in.\n\nThe index represents the fence post."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-9",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-9",
    "title": "Lecture 3",
    "section": "Slicing Methods",
    "text": "Slicing Methods\nLists\n\n[index]Slicing Methods\n\n\nGet an Item by [index]\nsuny = ['Geneseo', 'Brockport', 'Oswego', 'Binghamton', \n        'Stony Brook', 'New Paltz'] \n\nWe can extract a single value from a list by specifying its index:\n\n\n\nsuny[0]\nsuny[1]\nsuny[2]\nsuny[7]\n\nsuny[-1]\nsuny[-2]\nsuny[-3]\nsuny[-7]\n\n\n\n\nGet an Item with a Slice\n\nWe can extract a subsequence of a list by using a slice:\n\nsuny = ['Geneseo', 'Brockport', 'Oswego', 'Binghamton', \n        'Stony Brook', 'New Paltz'] \nsuny[0:2]    # A slice of a list is also a list.\n\n\nsuny[ : : 2]\nsuny[ : : -2]\nsuny[ : : -1]\n\nsuny[4 : ]\nsuny[-6 : ]\nsuny[-6 : -2]\nsuny[-6 : -4]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-11",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#slicing-methods-11",
    "title": "Lecture 3",
    "section": "Slicing Methods",
    "text": "Slicing Methods\n\nQ. Classwork 4.3"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#functions-arguments-and-parameters-1",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#functions-arguments-and-parameters-1",
    "title": "Lecture 3",
    "section": "Functions, Arguments, and Parameters",
    "text": "Functions, Arguments, and Parameters\nFunctions\nint(\"20\") \nfloat(\"14.3\")\nstr(5)\nint(\"xyz\")\n\nA function can take any number and type of input parameters and return any number and type of output results.\nPython ships with more than 65 built-in functions.\nPython also allows a user to define a new function.\nWe will mostly use built-in functions."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#functions-arguments-and-parameters-2",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#functions-arguments-and-parameters-2",
    "title": "Lecture 3",
    "section": "Functions, Arguments, and Parameters",
    "text": "Functions, Arguments, and Parameters\nprint(\"Cherry\", \"Strawberry\", \"Key Lime\")\nprint(\"Cherry\", \"Strawberry\", \"Key Lime\", sep = \"!\")\nprint(\"Cherry\", \"Strawberry\", \"Key Lime\", sep=\" \")\n\nWe invoke a function by entering its name and a pair of opening and closing parentheses.\nMuch as a cooking recipe can accept ingredients, a function invocation can accept inputs called arguments.\nWe pass arguments sequentially inside the parentheses (, separated by commas).\nA parameter is a name given to an expected function argument.\nA default argument is a fallback value that Python passes to a parameter if the function invocation does not explicitly provide one."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#functions-arguments-and-parameters-3",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#functions-arguments-and-parameters-3",
    "title": "Lecture 3",
    "section": "Functions, Arguments, and Parameters",
    "text": "Functions, Arguments, and Parameters\n\nQ. Classwork 4.4"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#google-colab-settings",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#google-colab-settings",
    "title": "Lecture 3",
    "section": "Google Colab Settings",
    "text": "Google Colab Settings\nTurn off AI Assistance\n\nOn Google Colab\n\nFrom the top-right corner, click ‚öôÔ∏è\nClick ‚ÄúAI Assistance‚Äù from the side menu.\nDisable all options."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf",
    "title": "Lecture 5",
    "section": "Distributed Computing Framework (DCF)",
    "text": "Distributed Computing Framework (DCF)\n\n\n\n\n\n\n\n\nMassive Jigsaw Puzzle:\n\nSolving alone takes forever.\nInvite friends to work on different sections simultaneously.\n\n\n\n\nDCF Role:\n\nActs like a team manager.\nSplits large problems into manageable tasks.\nCoordinates parallel work across multiple computers (nodes)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#how-dcf-works-the-process",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#how-dcf-works-the-process",
    "title": "Lecture 5",
    "section": "How DCF Works: The Process",
    "text": "How DCF Works: The Process\n\nTask Distribution:\n\nBreaks problems into smaller tasks.\nAssigns tasks to different nodes in the network.\n\nParallel Processing:\n\nMultiple nodes work at the same time.\nLike an assembly line where different parts are built simultaneously.\n\nCommunication & Aggregation:\n\nEnsures nodes communicate effectively.\nGathers individual results into a final solution."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf-1",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf-1",
    "title": "Lecture 5",
    "section": "Distributed Computing Framework (DCF)",
    "text": "Distributed Computing Framework (DCF)\nReal-World Analogies\n\nFactory Manager:\n\nEach worker builds a part of a toy (arms, legs, wheels).\nThe manager (DCF) ensures all parts come together to form a complete toy.\n\nRace Organizer:\n\nDifferent computers have varying speeds and capabilities.\nTougher tasks are assigned to faster or more capable nodes."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf-2",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf-2",
    "title": "Lecture 5",
    "section": "Distributed Computing Framework (DCF)",
    "text": "Distributed Computing Framework (DCF)\nRobustness and Scalability\n\nFault Tolerance:\n\nHandles failures gracefully.\nIf a node fails, tasks are reassigned or retried (like a substitute worker in a factory).\n\nResource Allocation:\n\nDistributes tasks based on node capability.\nOptimizes efficiency across the network.\n\nScalability:\n\nEasily adds more computers to the network.\nMore helpers = faster puzzle solving."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf-3",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf-3",
    "title": "Lecture 5",
    "section": "Distributed Computing Framework (DCF)",
    "text": "Distributed Computing Framework (DCF)\n\nDCF is the Conductor of the Orchestra:\n\nEvery musician (node) plays their part.\nThe conductor (DCF) synchronizes the performance to create a harmonious final result.\n\nKey Benefits:\n\nFaster problem-solving through parallelism.\nEfficient management of tasks and resources.\nResilient to failures and scalable for growing problems."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf-4",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf-4",
    "title": "Lecture 5",
    "section": "Distributed Computing Framework (DCF)",
    "text": "Distributed Computing Framework (DCF)\nReal-World Examples\n\nHadoop\nApache Spark"
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop",
    "title": "Lecture 5",
    "section": "Hadoop",
    "text": "Hadoop\nIntroduction to Hadoop\n\n\n\n\n\n\nDefinition\n\nAn open-source software framework for storing and processing large data sets.\n\nComponents\n\nHadoop Distributed File System (HDFS): Distributed data storage.\nMapReduce: Data processing model."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop-1",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop-1",
    "title": "Lecture 5",
    "section": "Hadoop",
    "text": "Hadoop\nIntroduction to Hadoop\n\n\n\n\n\n\nPurpose\n\nEnables distributed processing of large data sets across clusters of computers."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop-2",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop-2",
    "title": "Lecture 5",
    "section": "Hadoop",
    "text": "Hadoop\nHadoop Architecture - HDFS\n\n\n\n\n\n\n\n\n\nHDFS\n\nDivides data into blocks and distributes them across different servers for processing.\nProvides a highly redundant computing environment\n\nAllows the application to keep running even if individual servers fail."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop-3",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop-3",
    "title": "Lecture 5",
    "section": "Hadoop",
    "text": "Hadoop\nHadoop Architecture - MapReduce\n\nMapReduce: Distributes the processing of big data files across a large cluster of machines.\n\nHigh performance is achieved by breaking the processing into small units of work that can be run in parallel across nodes in the cluster.\n\nMap Phase: Filters and sorts data.\n\ne.g., Sorting customer orders based on their product IDs, with each group corresponding to a specific product ID.\n\nReduce Phase: Summarizes and aggregates results.\n\ne.g., Counting the number of orders within each group, thereby determining the frequency of each product ID."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop-4",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop-4",
    "title": "Lecture 5",
    "section": "Hadoop",
    "text": "Hadoop\nHadoop Architecture - MapReduce"
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop-5",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop-5",
    "title": "Lecture 5",
    "section": "Hadoop",
    "text": "Hadoop\nHow Hadoop Works\n\nData Distribution\n\nLarge data sets are split into smaller blocks.\n\nData Storage\n\nBlocks are stored across multiple servers in the cluster.\n\nProcessing with MapReduce\n\nMap Tasks: Executed on servers where data resides, minimizing data movement.\nReduce Tasks: Combine results from map tasks to produce final output.\n\nFault Tolerance\n\nData replication ensures processing continues even if servers fail."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop-6",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#hadoop-6",
    "title": "Lecture 5",
    "section": "Hadoop",
    "text": "Hadoop\nExtending Hadoop for Real-Time Processing\n\nLimitation of Hadoop\n\nHadoop is originally designed for batch processing.\n\nBatch Processing: Data or tasks are collected over a period of time and then processed all at once, typically at scheduled times or during periods of low activity.\nResults come after the entire dataset is analyzed.\n\n\nReal-Time Processing Limitation:\n\nHadoop cannot natively process real-time streaming data (e.g., stock prices flowing into stock exchanges, live sensor data)\n\nExtending Hadoop‚Äôs Capabilities\n\nBoth Apache Storm and Apache Spark can run on top of Hadoop clusters, utilizing HDFS for storage."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#spark",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#spark",
    "title": "Lecture 5",
    "section": "Spark",
    "text": "Spark\n\n\n\n\n\n\nApache Spark: distributed processing system used for big data workloads. a unified computing engine and computer clusters\n\nIt contains a set of libraries for parallel processing for data analysis, machine learning, graph analysis, and streaming live data."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#spark-vs.-hadoop",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#spark-vs.-hadoop",
    "title": "Lecture 5",
    "section": "Spark vs.¬†Hadoop",
    "text": "Spark vs.¬†Hadoop\nHadoop MapReduce: The Challenge\n\nSequential Multi-Step Process:\n\nReads data from the cluster.\nProcesses data.\nWrites results back to HDFS.\n\nDisk Input/Output Latency:\n\nEach step requires disk read/write.\nResults in slower performance due to latency."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#spark-vs.-hadoop-1",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#spark-vs.-hadoop-1",
    "title": "Lecture 5",
    "section": "Spark vs.¬†Hadoop",
    "text": "Spark vs.¬†Hadoop\nApache Spark: The Solution\n\nIn-Memory Processing:\n\nLoads data into memory once.\nPerforms all operations in-memory.\n\nData Reuse:\n\nCaches data for reuse in multiple operations (ideal for iterative tasks like machine learning).\n\nFaster Execution:\n\nEliminates multiple disk I/O steps.\nDramatically reduces latency for interactive analytics and real-time processing."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#spark-vs.-hadoop-2",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#spark-vs.-hadoop-2",
    "title": "Lecture 5",
    "section": "Spark vs.¬†Hadoop",
    "text": "Spark vs.¬†Hadoop\n\n\nApache Hadoop\n\nFramework Components:\n\nHDFS: Distributed storage system.\nMapReduce: Programming model for parallel processing.\n\nEcosystem:\n\nTypically integrates multiple execution engines (e.g., Spark) within a single deployment.\n\n\n\nSpark\n\nFocus Areas:\n\nInteractive queries, machine learning, and real-time analytics.\n\nStorage Agnostic:\n\nDoes not have its own storage system.\nOperates on data stored in systems like HDFS, etc.\n\nIntegration:\n\nCan run alongside Hadoop"
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#spark-vs.-hadoop-3",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#spark-vs.-hadoop-3",
    "title": "Lecture 5",
    "section": "Spark vs.¬†Hadoop",
    "text": "Spark vs.¬†Hadoop\nComplementary Use\n\nMany organizations store massive datasets in HDFS and utilize Spark for fast, interactive data processing.\n\nSpark can read data directly from HDFS, enabling seamless integration between storage and computation.\n\nHadoop provides robust storage and processing capabilities.\nSpark brings speed and versatility to data analytics, making them a powerful combination for solving complex business challenges.\nUse Case Example: An e-commerce company might store historical sales data in HDFS while using Spark to analyze customer behavior in real time to recommend products or detect fraudulent transactions."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#apache-spark-1",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#apache-spark-1",
    "title": "Lecture 5",
    "section": "Apache Spark",
    "text": "Apache Spark\nMedscape: Real-Time Medical News for Healthcare Professionals\n\n\nA medical news app for smartphones and tablets designed to keep healthcare professionals informed.\n\nProvides up-to-date medical news and expert perspectives.\n\n\n\n\n\n\n\n\n\n\n\n\nReal-Time Updates:\n\nUses Apache Storm/Spark to process about 500 million tweets per day.\nAutomatic Twitter feed integration helps users track important medical trends shared by physicians and medical commentators."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf-5",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf-5",
    "title": "Lecture 5",
    "section": "Distributed Computing Framework (DCF)",
    "text": "Distributed Computing Framework (DCF)\nSpark Application Structure on a Cluster of Computers\n\n\n\n\n\n\nDriver Process\n\nCommunicates with the cluster manager to acquire worker nodes.\nBreaks the application into smaller tasks if resources are allocated."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf-6",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf-6",
    "title": "Lecture 5",
    "section": "Distributed Computing Framework (DCF)",
    "text": "Distributed Computing Framework (DCF)\nSpark Application Structure on a Cluster of Computers\n\n\n\n\n\n\nCluster Manager\n\nDecides if Spark can use cluster resources (machines/nodes).\nAllocates necessary nodes to Spark applications."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf-7",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#distributed-computing-framework-dcf-7",
    "title": "Lecture 5",
    "section": "Distributed Computing Framework (DCF)",
    "text": "Distributed Computing Framework (DCF)\nSpark Application Structure on a Cluster of Computers\n\n\n\n\n\n\nWorker Nodes \n\nExecute tasks assigned by the driver program.\nSend results back to the driver after execution.\nCan communicate with each other if needed during task execution."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#spark-application-structure-on-a-cluster-of-computers",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#spark-application-structure-on-a-cluster-of-computers",
    "title": "Lecture 5",
    "section": "Spark Application Structure on a Cluster of Computers",
    "text": "Spark Application Structure on a Cluster of Computers\n\n\n\n\n\n\nDriver Process\n\nCommunicates with the cluster manager to acquire worker nodes.\nBreaks the application into smaller tasks if resources are allocated."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#spark-application-structure-on-a-cluster-of-computers-1",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#spark-application-structure-on-a-cluster-of-computers-1",
    "title": "Lecture 5",
    "section": "Spark Application Structure on a Cluster of Computers",
    "text": "Spark Application Structure on a Cluster of Computers\n\n\n\n\n\n\nCluster Manager\n\nDecides if Spark can use cluster resources (machines/nodes).\nAllocates necessary nodes to Spark applications."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#spark-application-structure-on-a-cluster-of-computers-2",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#spark-application-structure-on-a-cluster-of-computers-2",
    "title": "Lecture 5",
    "section": "Spark Application Structure on a Cluster of Computers",
    "text": "Spark Application Structure on a Cluster of Computers\n\n\n\n\n\n\nWorker Nodes \n\nExecute tasks assigned by the driver program.\nSend results back to the driver after execution.\nCan communicate with each other if needed during task execution."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#pyspark-on-google-colab",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#pyspark-on-google-colab",
    "title": "Lecture 5",
    "section": "PySpark on Google Colab",
    "text": "PySpark on Google Colab\nPySpark = Spark + Python\n\npyspark is a Python API to Apache Spark.\n\nAPI: application programming interface, the set of functions, classes, and variables provided for you to interact with.\nSpark itself is coded in a different programming language, called Scala.\n\nWe can combine Python, pandas, and PySpark in one program.\n\nKoalas (now called pyspark.pandas) provides a pandas-like porcelain on top of PySpark."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#google-drive-on-google-colab",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#google-drive-on-google-colab",
    "title": "Lecture 5",
    "section": "Google Drive on Google Colab",
    "text": "Google Drive on Google Colab\n\n\nfrom google.colab import drive, files\ndrive.mount('/content/drive')\nfiles.upload()\n\ndrive.mount('/content/drive')\n\nTo mount your Google Drive on Google colab:\n\nfiles.upload()\n\nTo initiate uploading a file on Google Drive:\n\n\n\n\nTo find a pathname of a CSV file in Google Drive:\n\nClick üìÅ from the sidebar menu\ndrive ‚û°Ô∏è MyDrive ‚Ä¶\nHover a mouse cursor on the CSV file\nClick the vertical dots\nClick ‚ÄúCopy path‚Äù"
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#the-sparksession-entry-point",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#the-sparksession-entry-point",
    "title": "Lecture 5",
    "section": "The SparkSession Entry Point",
    "text": "The SparkSession Entry Point\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\nThe SparkSession entry point provides the functionality for data transformation with data frames and SQL.\nfrom pyspark.sql import SparkSession\n\nImports the SparkSession class from PySpark‚Äôs SQL module.\n\nSparkSession.builder\n\nBegins the configuration of a new SparkSession.\n\n.master(\"local[*]\")\n\nConfigures Spark to run locally using all available CPU cores.\n\n.getOrCreate()\n\nRetrieves an existing SparkSession if one exists, or creates a new one otherwise."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#reading-a-csv-file-into-the-spark-framework",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#reading-a-csv-file-into-the-spark-framework",
    "title": "Lecture 5",
    "section": "Reading a CSV file into the Spark Framework",
    "text": "Reading a CSV file into the Spark Framework\npath = '/content/drive/MyDrive/lecture-data/cces.csv'\ndf = spark.read.csv(path, \n                    inferSchema=True,\n                    header=True)\ndf.show()  # Displays the first 20 rows\n\nspark.read.csv(path, ...):\n\nRead a CSV file from the location specified by the path variable.\n\ninferSchema=True:\n\nWhen set to True, Spark will automatically detect (or ‚Äúinfer‚Äù) the data types of the columns in the CSV file.\n\nWithout this, Spark would treat all columns as strings by default.\n\nheader=True:\n\nThe first row of the CSV file contains the column headers, and will be used as names of the columns."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#reading-a-csv-file-into-the-spark-framework-1",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#reading-a-csv-file-into-the-spark-framework-1",
    "title": "Lecture 5",
    "section": "Reading a CSV file into the Spark Framework",
    "text": "Reading a CSV file into the Spark Framework\npath = 'https://bcdanl.github.io/data/dominick_oj.csv'\ndf = spark.read.csv(path, \n                    inferSchema=True,\n                    header=True)\ndf.show()\n\nSpark‚Äôs spark.read.csv() function relies on the Hadoop FileSystem API to access files.\nBy default, Hadoop does not support reading files directly from HTTPS URLs.\n\nIt expects a local file system path, HDFS path, or another supported distributed file system.\n\nWhat should we do then?\n\nWe can convert the Pandas DataFrame to a Spark DataFrame\nspark_df = spark.createDataFrame(pandas_df)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#reading-a-csv-file-into-the-spark-framework-2",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#reading-a-csv-file-into-the-spark-framework-2",
    "title": "Lecture 5",
    "section": "Reading a CSV file into the Spark Framework",
    "text": "Reading a CSV file into the Spark Framework"
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#df.show",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#df.show",
    "title": "Lecture 5",
    "section": "df.show()",
    "text": "df.show()\nThree optional parameters\n\nBy default, displays the first 20 rows.\ndf.show(n, truncate, vertical) accepts three optional parameters:\n\nn (int): Number of rows to display.\n\n\nExample: df.show(5)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#df.show-1",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#df.show-1",
    "title": "Lecture 5",
    "section": "df.show()",
    "text": "df.show()\nThree optional parameters\n\ntruncate (bool or int):\n\n\nIf True (default), long strings are truncated to 20 characters.\nIf False, displays full column contents.\nIf an integer is provided, it specifies the maximum number of characters to display.\nExample: df.show(truncate=False), df.show(truncate=30)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#df.show-2",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#df.show-2",
    "title": "Lecture 5",
    "section": "df.show()",
    "text": "df.show()\nThree optional parameters\n\nvertical (bool):\n\n\nIf True, displays each row vertically (useful for wide tables).\nExample: df.show(vertical=True)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#df.printschema-and-df.dtypes",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#df.printschema-and-df.dtypes",
    "title": "Lecture 5",
    "section": "df.printSchema() and df.dtypes",
    "text": "df.printSchema() and df.dtypes\ndf.printSchema()\ndf.dtypes\n\ndf.printSchema() prints the DataFrame schema in a tree format.\n\nnullable = true means that a column can contain null values.\nWe may need to handle missing data appropriately.\n\ndf.dtypes returns a list of tuples representing each column‚Äôs name and data type."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#pyspark-on-google-colab-1",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#pyspark-on-google-colab-1",
    "title": "Lecture 5",
    "section": "PySpark on Google Colab",
    "text": "PySpark on Google Colab\nPySpark = Spark + Python\n\npyspark is a Python API to Apache Spark.\n\nAPI: application programming interface, the set of functions, classes, and variables provided for you to interact with.\nSpark itself is coded in a different programming language, called Scala.\n\nWe can combine Python, pandas, and PySpark in one program.\n\nKoalas (now called pyspark.pandas) provides a pandas-like porcelain on top of PySpark."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#displaying-the-first-rows-with-df.show",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#displaying-the-first-rows-with-df.show",
    "title": "Lecture 5",
    "section": "Displaying the First Rows with df.show()",
    "text": "Displaying the First Rows with df.show()\nThree Optional Parameters\n\nBy default, displays the first 20 rows.\ndf.show(n, truncate, vertical) accepts three optional parameters:\n\nn (int): Number of rows to display.\n\n\nExample: df.show(5)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#displaying-the-first-rows-with-df.show-1",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#displaying-the-first-rows-with-df.show-1",
    "title": "Lecture 5",
    "section": "Displaying the First Rows with df.show()",
    "text": "Displaying the First Rows with df.show()\nThree Optional Parameters\n\ntruncate (bool or int):\n\n\nIf True (default), long strings are truncated to 20 characters.\nIf False, displays full column contents.\nIf an integer is provided, it specifies the maximum number of characters to display.\nExample: df.show(truncate=False), df.show(truncate=30)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#displaying-the-first-rows-with-df.show-2",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#displaying-the-first-rows-with-df.show-2",
    "title": "Lecture 5",
    "section": "Displaying the First Rows with df.show()",
    "text": "Displaying the First Rows with df.show()\nThree Optional Parameters\n\nvertical (bool):\n\n\nIf True, displays each row vertically (useful for wide tables).\nExample: df.show(vertical=True)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#displaying-variable-types-with-df.printschema-and-df.dtypes",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#displaying-variable-types-with-df.printschema-and-df.dtypes",
    "title": "Lecture 5",
    "section": "Displaying Variable Types with df.printSchema() and df.dtypes",
    "text": "Displaying Variable Types with df.printSchema() and df.dtypes\ndf.printSchema()\ndf.dtypes\n\ndf.printSchema() prints the DataFrame schema in a tree format.\n\nnullable = true means that a column can contain null values.\nWe may need to handle missing data appropriately.\n\ndf.dtypes returns a list of tuples representing each column‚Äôs name and data type."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#generating-descriptive-statistics-with-df.describe.show",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#generating-descriptive-statistics-with-df.describe.show",
    "title": "Lecture 5",
    "section": "Generating Descriptive Statistics with df.describe().show()",
    "text": "Generating Descriptive Statistics with df.describe().show()\ndf.describe().show()\n\ndf.describe() computes summary statistics (e.g., count, mean, stddev, min, max) for the DataFrame‚Äôs numeric columns.\n.show() prints these statistics in a readable table format."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#displaying-variable-information-with-df.printschema-df.dtypes-and-df.columns",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#displaying-variable-information-with-df.printschema-df.dtypes-and-df.columns",
    "title": "Lecture 5",
    "section": "Displaying Variable Information with df.printSchema(), df.dtypes, and df.columns",
    "text": "Displaying Variable Information with df.printSchema(), df.dtypes, and df.columns\ndf.printSchema()\ndf.dtypes\ndf.columns\n\ndf.printSchema() prints the DataFrame schema in a tree format.\n\nnullable = true means that a column can contain null values.\nWe may need to handle missing data appropriately.\n\ndf.dtypes returns a list of tuples representing each column‚Äôs name and data type.\ndf.columns returns a list of colunm names."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#generating-descriptive-statistics-with-df.describe.show-and-df.count",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#generating-descriptive-statistics-with-df.describe.show-and-df.count",
    "title": "Lecture 5",
    "section": "Generating Descriptive Statistics with df.describe().show() and df.count()",
    "text": "Generating Descriptive Statistics with df.describe().show() and df.count()\ndf.describe().show()\ndf.count()\n\ndf.describe() computes summary statistics (e.g., count, mean, stddev, min, max) for the DataFrame‚Äôs numeric columns.\n.show() prints these statistics in a readable table format.\ndf.count() returns the number of rows in df."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#spark-dataframe-vs.-pandas-dataframe",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#spark-dataframe-vs.-pandas-dataframe",
    "title": "Lecture 5",
    "section": "Spark DataFrame vs.¬†Pandas DataFrame",
    "text": "Spark DataFrame vs.¬†Pandas DataFrame\n\nWhat makes a Spark DataFrame different from other DataFrame?\n\nSpark DataFrames are designed for big data and distributed computing.\n\nSpark DataFrame:\n\nData is distributed across a cluster of machines.\nOperations are executed in parallel on multiple nodes.\nCan process datasets that exceed the memory of a single machine.\n\nOther DataFrames (e.g., Pandas):\n\nOperate on a single machine.\nEntire dataset must fit into memory.\nLimited by local system resources."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#spark-dataframe-vs.-pandas-dataframe-1",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#spark-dataframe-vs.-pandas-dataframe-1",
    "title": "Lecture 5",
    "section": "Spark DataFrame vs.¬†Pandas DataFrame",
    "text": "Spark DataFrame vs.¬†Pandas DataFrame\nLazy Evaluation and Optimization\n\nSpark DataFrame:\n\nUses lazy evaluation: transformations are not computed until an action is called.\nOptimize query execution.\n\nOther DataFrames:\n\nOperations are evaluated eagerly (immediately).\nNo built-in query optimization across multiple operations."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#spark-dataframe-vs.-pandas-dataframe-2",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#spark-dataframe-vs.-pandas-dataframe-2",
    "title": "Lecture 5",
    "section": "Spark DataFrame vs.¬†Pandas DataFrame",
    "text": "Spark DataFrame vs.¬†Pandas DataFrame\nScalability\n\nSpark DataFrame:\n\nDesigned to scale to petabytes of data.\nUtilizes distributed storage and computing resources.\nIdeal for large-scale data processing and analytics.\n\nOther DataFrames:\n\nBest suited for small to medium datasets.\nLimited by the hardware of a single computer."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#spark-dataframe-vs.-pandas-dataframe-3",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#spark-dataframe-vs.-pandas-dataframe-3",
    "title": "Lecture 5",
    "section": "Spark DataFrame vs.¬†Pandas DataFrame",
    "text": "Spark DataFrame vs.¬†Pandas DataFrame\nFault Tolerance\n\nSpark DataFrame:\n\nBuilt on Resilient Distributed Datasets (RDDs).\nAutomatically recovers lost data if a node fails.\nEnsures high reliability in distributed environments.\n\nOther DataFrames:\n\nTypically lack built-in fault tolerance.\nFailures on a single machine can result in data loss."
  },
  {
    "objectID": "danl-hw/danl-320-hw-01.html",
    "href": "danl-hw/danl-320-hw-01.html",
    "title": "Homework 1",
    "section": "",
    "text": "Please submit your Jupyter Notebook for Part 3 in Homework 1 to Brightspace with the name below:\n\ndanl-320-hw1-LASTNAME-FIRSTNAME.ipynb\n( e.g., danl-320-hw1-choe-byeonghak.ipynb )\n\nThe due is February 17, 2024, 3:30 P.M.\nPlease send Byeong-Hak an email (bchoe@geneseo.edu) if you have any questions."
  },
  {
    "objectID": "danl-hw/danl-320-hw-01.html#question-0",
    "href": "danl-hw/danl-320-hw-01.html#question-0",
    "title": "Homework 1",
    "section": "Question 0",
    "text": "Question 0\nProvide your GitHub username."
  },
  {
    "objectID": "danl-hw/danl-320-hw-01.html#question-1",
    "href": "danl-hw/danl-320-hw-01.html#question-1",
    "title": "Homework 1",
    "section": "Question 1",
    "text": "Question 1\n\nQ1a\n\nCreate a list of integers from 1 to 10.\nAppend the number 11 to the list and remove the number 5.\n\n\nlist_numbers = list(range(1, 11))\nlist_numbers.append(11)\nlist_numbers.remove(5)\n\n\n\n\nQ1b\n\nConsider the following dictionary of three employees and their salaries:\n\ndict_salaries = {'Alice': 50000, 'Bob': 60000, 'Charlie': 70000}\n\nAdd a new employee 'Dana' with a salary of 65000.\nUpdate 'Alice'‚Äôs salary to 55000.\nPrint all employee names and their salaries.\n\n\n\ndict_salaries = {'Alice': 50000, 'Bob': 60000, 'Charlie': 70000}\ndict_salaries['Dana'] = 65000\ndict_salaries['Alice'] = 55000\n\nfor name, salary in dict_salaries.items():\n    print(name, \":\", salary)\n    \n# An f-string (formatted string literal) is a way to embed expressions \n  # inside string literals using curly braces `{}`.\nfor name, salary in dict_salaries.items():\n    print(f'{name}: {salary}')"
  },
  {
    "objectID": "danl-hw/danl-320-hw-01.html#question-2",
    "href": "danl-hw/danl-320-hw-01.html#question-2",
    "title": "Homework 1",
    "section": "Question 2",
    "text": "Question 2\n\nQ2a\n\nAssign a variable salary to 75000.\nUse an if-elif-else statement to print:\n\n'Low' if salary is less than 50,000\n'Medium' if salary is between 50,000 and 99,999\n'High' if salary is 100,000 or more\n\n\n\n\nsalary = 75000\nif salary &lt; 50000:\n    print('Low')\nelif 50000 &lt;= salary &lt; 100000:\n    print('Medium')\nelse:\n    print('High')\n\n\n\n\nQ2b\n\nAssign two variables, role and salary, to 'Manager' and 85000, respectively.\nUse nested if-else statements to print:\n\n'Eligible for bonus' if the role is 'Manager' and the salary is greater than 80,000.\n'Eligible for raise' if the role is 'Analyst' and the salary is less than 60,000.\n'No action needed' for all other cases.\n\n\n\n\nrole = 'Manager'\nsalary = 85000\n\nif role == 'Manager':\n    if salary &gt; 80000:\n        print('Eligible for bonus')\n    else:\n        print('No action needed')\nelif role == 'Analyst':\n    if salary &lt; 60000:\n        print('Eligible for raise')\n    else:\n        print('No action needed')\nelse:\n    print('No action needed')"
  },
  {
    "objectID": "danl-hw/danl-320-hw-01.html#question-3",
    "href": "danl-hw/danl-320-hw-01.html#question-3",
    "title": "Homework 1",
    "section": "Question 3",
    "text": "Question 3\n\nQ3a\n\nConsider the following list of salaries:\n\nlist_salaries = [45000, 60000, 75000, 120000, 30000]\n\nCalculate the average salary.\nUse a for loop to print whether each salary is 'Above Average' or 'Below Average'.\n\n\n\nlist_salaries = [45000, 60000, 75000, 120000, 30000]\naverage_salary = sum(list_salaries) / len(list_salaries)\n\nfor salary in list_salaries:\n    if salary &gt; average_salary:\n        print(f'{salary} is Above Average')\n    else:\n        print(f'{salary} is Below Average')\n\n\n\n\nQ3b\n\nStart with a salary of 50000.\nUse a while loop to increase the salary by 5000 each year until it exceeds 80000.\nPrint the salary after each increment.\n\n\n\nsalary = 50000\nwhile salary &lt;= 80000:\n    print(f'Salary: {salary}')\n    salary += 5000\n\n\n\n\nQ3c\n\nConsider the following dictionary of employee salaries:\n\nsalaries = {'Alice': 50000, 'Bob': 60000, 'Charlie': 70000, 'Dana': 45000}\n\nUse a for loop to print the names of employees who earn more than 55000.\n\n\n\nsalaries = {'Alice': 50000, 'Bob': 60000, 'Charlie': 70000, 'Dana': 45000}\n\nfor name, salary in salaries.items():\n    if salary &gt; 55000:\n        print(f'{name} earns more than 55000')\n\n\n\n\nQ3d\ndata_list = [42, 3.14, 'Data Analytics', True, None, [1, 2, 3], {'key': 'value'}, (4, 5)]\n\nGiven the list above, print the data type of each element using the type() function in a for loop. In the loop:\n\nConvert the integer 42 to a string.\nConvert the float 3.14 to a string, then back to a float.\nConvert the boolean True to an integer.\n\n\n\n\ndata_list = [42, 3.14, 'Data Analytics', True, None, [1, 2, 3], {'key': 'value'}, (4, 5)]\n\nfor item in data_list:\n    print(f'Original: {item}, Type: {type(item)}')\n    \n    if item == 42:\n        item = str(item)\n        print(f'Converted 42 to string: {item}, Type: {type(item)}')\n    \n    if item == 3.14:\n        item = float(str(item))\n        print(f'Converted \"3.14\" to float: {item}, Type: {type(item)}')\n\n    if item is True:\n        item = int(item)\n        print(f'Converted True to integer: {item}, Type: {type(item)}')"
  },
  {
    "objectID": "danl-hw/danl-320-hw-01.html#question-4",
    "href": "danl-hw/danl-320-hw-01.html#question-4",
    "title": "Homework 1",
    "section": "Question 4",
    "text": "Question 4\n\nQ4a\nConsider the variables a and b:\na = 10\nb = 0\n\nUse a try-except block to print the result of a / b.\n\nIf there is an error, print 'Cannot divide by zero!'.\n\n\n\n\na = 10\nb = 0\n\ntry:\n    result = a / b\n    print(result)\nexcept ZeroDivisionError:\n    print('Cannot divide by zero!')\n\n\n\n\nQ4b\n\nConsider the following dictionary of salaries with some missing (None) values:\n\nsalaries = {'Alice': 50000, 'Bob': None, 'Charlie': 70000, 'Dana': None, 'Eve': 80000}\n\nUse a for loop with a try-except block to calculate the total of non-missing salaries.\n\n\n\nsalaries = {'Alice': 50000, 'Bob': None, 'Charlie': 70000, 'Dana': None, 'Eve': 80000}\ntotal = 0\n\nfor name, salary in salaries.items():\n    try:\n        total += salary\n    except TypeError:\n        continue"
  },
  {
    "objectID": "danl-hw/danl-320-hw-01.html#question-5",
    "href": "danl-hw/danl-320-hw-01.html#question-5",
    "title": "Homework 1",
    "section": "Question 5",
    "text": "Question 5\n\nImport the math library and calculate the square root of 81 using the sqrt() function provided by the math library.\n\n\n\nimport math\nmath.sqrt(81)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#list-comprehension",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#list-comprehension",
    "title": "Lecture 3",
    "section": "List Comprehension",
    "text": "List Comprehension\nWhat is List Comprehension?\n\nA concise way to create or modify lists.\nSyntax: [expression for item in iterable if condition]\n\n\nCreating a List of Squares:\n\nsquares = [x**2 for x in range(5)]\n\nFiltering Items:\n\nnumbers = [1, 2, 3, 4, 5, 6]\nevens = [x for x in numbers if x != 2]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#dictionary-comprehension",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#dictionary-comprehension",
    "title": "Lecture 3",
    "section": "Dictionary Comprehension",
    "text": "Dictionary Comprehension\nWhat is Dictionary Comprehension?\n\nA concise way to create or modify dictionaries.\nSyntax: {key_expression: value_expression for item in iterable if condition}\n\n\nCreating a Dictionary of Squares:\n\nsquares_dict = {x: x**2 for x in range(5)}\n\nFiltering Dictionary Items:\n\n   my_dict = {'a': 1, 'b': 2, 'c': 3, 'd': 4}\n   filtered_dict = {k: v for k, v in my_dict.items() if v != 2}\n\nSwapping Keys and Values:\n\noriginal_dict = {'a': 1, 'b': 2, 'c': 3}\nswapped_dict = {v: k for k, v in original_dict.items()}"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#modifying-a-list",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#modifying-a-list",
    "title": "Lecture 3",
    "section": "Modifying a List",
    "text": "Modifying a List\nAdding Items\n\nappend(): Adds an item to the end of the list.\n\nmy_list = [1, 2, 3]\nmy_list.append(4)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#modifying-a-list-1",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#modifying-a-list-1",
    "title": "Lecture 3",
    "section": "Modifying a List",
    "text": "Modifying a List\nDeleting Items\n\nremove(): Deletes the first occurrence of value in the list.\n\nmy_list = [1, 2, 3, 4, 2]\nmy_list.remove(2)\n\nList Comprehension: Removes items based on a condition.\n\nmy_list = [1, 2, 3, 4, 2]\nmy_list = [x for x in my_list if x != 2]  \n\ndel statement: Deletes an item by index or a slice of items.\n\nmy_list = [1, 2, 3, 4]\ndel my_list[1] \ndel my_list[1:3]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#modifying-a-dictionary",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#modifying-a-dictionary",
    "title": "Lecture 3",
    "section": "Modifying a Dictionary",
    "text": "Modifying a Dictionary\nAdding/Updating Items\n\nupdate(): Adds new key-value pairs or updates existing ones.\n\nmy_dict = {'a': 1, 'b': 2}\nmy_dict.update({'c': 3})  \nmy_dict.update({'a': 10})"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#modifying-a-dictionary-1",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#modifying-a-dictionary-1",
    "title": "Lecture 3",
    "section": "Modifying a Dictionary",
    "text": "Modifying a Dictionary\nDeleting Items\n\nDictionary Comprehension: Removes items based on a condition.\n\nmy_dict = {'a': 1, 'b': 2, 'c': 3}\nmy_dict = {k: v for k, v in my_dict.items() if v != 2}  \n\ndel statement: Deletes an item by key.\n\nmy_dict = {'a': 1, 'b': 2, 'c': 3}\ndel my_dict['b']"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#importing-modules-packages-and-libraries",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#importing-modules-packages-and-libraries",
    "title": "Lecture 3",
    "section": "Importing Modules, Packages, and Libraries",
    "text": "Importing Modules, Packages, and Libraries\n\nPython is a general-purpose programming language and is not specialized for numerical or statistical computation.\nThe core libraries that enable Python to store and analyze data efficiently are:\n\npandas\nnumpy\nmatplotlib and seaborn"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#importing-modules-packages-and-libraries-2",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#importing-modules-packages-and-libraries-2",
    "title": "Lecture 3",
    "section": "Importing Modules, Packages, and Libraries",
    "text": "Importing Modules, Packages, and Libraries\nnumpy\n\n\n\n\nnumpy, numerical Python, provides the array block (np.array()) for doing fast and efficient computations;"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#importing-modules-packages-and-libraries-3",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#importing-modules-packages-and-libraries-3",
    "title": "Lecture 3",
    "section": "Importing Modules, Packages, and Libraries",
    "text": "Importing Modules, Packages, and Libraries\nmatplotlib and seaborn\n\n\n\n\nmatplotlib provides graphics. The most important submodule would be matplotlib.pyplot.\nseaborn provides a general improvement in the default appearance of matplotlib-produced plots."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#importing-modules-packages-and-libraries-4",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#importing-modules-packages-and-libraries-4",
    "title": "Lecture 3",
    "section": "Importing Modules, Packages, and Libraries",
    "text": "Importing Modules, Packages, and Libraries\nimport statement\n\nA module is basically a bunch of related codes saved in a file with the extension .py.\nA package is basically a directory of a collection of modules.\nA library is a collection of packages\nWe refer to code of other module/package/library by using the Python import statement.\n\nimport LIBRARY_NAME\n\nThis makes the code and variables in the imported module available to our programming codes."
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#importing-modules-packages-and-libraries-5",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#importing-modules-packages-and-libraries-5",
    "title": "Lecture 3",
    "section": "Importing Modules, Packages, and Libraries",
    "text": "Importing Modules, Packages, and Libraries\nimport statement\n\n\nas\n\nWe can use the as keyword when importing the module/package/library using its canonical names.\n\nimport LIBRARY as SOMETHING_SHORT\n\nfrom\n\nWe can use the from keyword when specifying Python module/package/library from which we want to import something.\n\nfrom LIBRARY import FUNCTION, PACKAGE, MODULE"
  },
  {
    "objectID": "danl-lec/danl-320-lec-03-2025-0129.html#installing-modules-packages-and-libraries",
    "href": "danl-lec/danl-320-lec-03-2025-0129.html#installing-modules-packages-and-libraries",
    "title": "Lecture 3",
    "section": "Installing Modules, Packages, and Libraries",
    "text": "Installing Modules, Packages, and Libraries\npip tool\n\nTo install a library LIBRARY on your Google Colab or Anaconda Python, run the following:\n\npip install LIBRARY\n\nQ. Classwork 4.8"
  },
  {
    "objectID": "danl-cw/danl-320-cw-05.html",
    "href": "danl-cw/danl-320-cw-05.html",
    "title": "Classwork 5",
    "section": "",
    "text": "Direction\n\n\n\nThe nfl.csv file contains a list of players in the National Football League with similar Name, Team, Position, Birthday, and Salary variables in the nba.csv file.\n\nimport pandas as pd\nfrom pyspark.sql import SparkSession\ndf = pd.read_csv(\"https://bcdanl.github.io/data/nfl.csv\")\nnfl = spark.createDataFrame(df)\n\n\n\n\n\n\n\n\n\n\n\nQuestion 1\n\nHow can we read the nfl.csv file, and assign it to a PySpark DataFrame object, nfl?\n\nAnswer:\n\n\n\nQuestion 2\n\nHow many observations are in nfl?\nWhat are the mean, standard deviation, minimum, and maximum of Salary in nfl?\n\nAnswer:\n\n\n\nQuestion 3\n\nHow can we count the number of players per team in nfl?\nHow many unique teams are in nfl?\n\nAnswer:\n\n\n\nQuestion 4\n\nWhat is an effective way to convert the values in its Birthday variable to date?\n\nThe format of Birthday is ‚ÄúM/d/yy‚Äù\n\n\nAnswer:\n\n\n\nQuestion 5\n\nWho are the five highest-paid players?\nWho is the oldest player?\n\nAnswer:\n\n\n\nQuestion 6\nHow can we sort the DataFrame first by Team in alphabetical order and then by Salary in descending order?\nAnswer:\n\n\n\nQuestion 7\nWho is the oldest player on the Kansas City Chiefs roster, and what is his birthday?\nAnswer:\n\n\n\nQuestion 8\n\nWhat is the median of Salary in nfl?\n\nAnswer:\n\n\n\nDiscussion\nWelcome to our Classwork 5 Discussion Board! üëã \nThis space is designed for you to engage with your classmates about the material covered in Classwork 5.\nWhether you are looking to delve deeper into the content, share insights, or have questions about the content, this is the perfect place for you.\nIf you have any specific questions for Byeong-Hak (@bcdanl) regarding the Classwork 5 materials or need clarification on any points, don‚Äôt hesitate to ask here.\nAll comments will be stored here.\nLet‚Äôs collaborate and learn from each other!\n\n\n\n\n Back to top"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html",
    "title": "Lecture 6",
    "section": "",
    "text": "Loading DataFrame with Spark‚Äôs read API (analogous to read_csv())\nGetting a Summary with printSchema() and describe()\nSelecting and Reordering Variables with select()\nCounting Values with groupBy().count(), countDistinct(), and count()\nSorting with orderBy()\nAdding a New Variable with withColumn()\nRemoving a Variable with drop()\nRenaming a Variable with withColumnRenamed()\nAggregation and Math Operations with selectExpr()\n\n\n\n\n\n\n\n\nConverting Data Types with cast()\nFiltering Observations with filter()\nDealing with Missing Values (na.drop(), na.fill(), etc.)\nDealing with Duplicates (dropDuplicates())   \nUsing Custom Functions (User-Defined Functions, UDFs) and Anonymous Functions\nGrouping DataFrames with groupBy().agg() and groupBy().applyInPandas() or UDFs (for advanced operations)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-on-google-colab-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-on-google-colab-1",
    "title": "Lecture 6",
    "section": "PySpark on Google Colab",
    "text": "PySpark on Google Colab\nPySpark = Spark + Python\n\npyspark is a Python API to Apache Spark.\n\nAPI: application programming interface, the set of functions, classes, and variables provided for you to interact with.\nSpark itself is coded in a different programming language, called Scala.\n\nWe can combine Python, pandas, and PySpark in one program.\n\nKoalas (now called pyspark.pandas) provides a pandas-like porcelain on top of PySpark."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#google-drive-on-google-colab",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#google-drive-on-google-colab",
    "title": "Lecture 6",
    "section": "Google Drive on Google Colab",
    "text": "Google Drive on Google Colab\n\n\nfrom google.colab import drive, files\ndrive.mount('/content/drive')\nfiles.upload()\n\ndrive.mount('/content/drive')\n\nTo mount your Google Drive on Google colab:\n\nfiles.upload()\n\nTo initiate uploading a file on Google Drive:\n\n\n\n\nTo find a pathname of a CSV file in Google Drive:\n\nClick üìÅ from the sidebar menu\ndrive ‚û°Ô∏è MyDrive ‚Ä¶\nHover a mouse cursor on the CSV file\nClick the vertical dots\nClick ‚ÄúCopy path‚Äù"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#the-sparksession-entry-point",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#the-sparksession-entry-point",
    "title": "Lecture 6",
    "section": "The SparkSession Entry Point",
    "text": "The SparkSession Entry Point\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\nThe SparkSession entry point provides the functionality for data transformation with data frames and SQL.\nfrom pyspark.sql import SparkSession\n\nImports the SparkSession class from PySpark‚Äôs SQL module.\n\nSparkSession.builder\n\nBegins the configuration of a new SparkSession.\n\n.master(\"local[*]\")\n\nConfigures Spark to run locally using all available CPU cores.\n\n.getOrCreate()\n\nRetrieves an existing SparkSession if one exists, or creates a new one otherwise."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#reading-a-csv-file-into-the-spark-framework",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#reading-a-csv-file-into-the-spark-framework",
    "title": "Lecture 6",
    "section": "Reading a CSV file into the Spark Framework",
    "text": "Reading a CSV file into the Spark Framework\npath = '/content/drive/MyDrive/lecture-data/cces.csv'\ndf = spark.read.csv(path, \n                    inferSchema=True,\n                    header=True)\ndf.show()  # Displays the first 20 rows\n\nspark.read.csv(path, ...):\n\nRead a CSV file from the location specified by the path variable.\n\ninferSchema=True:\n\nWhen set to True, Spark will automatically detect (or ‚Äúinfer‚Äù) the data types of the columns in the CSV file.\n\nWithout this, Spark would treat all columns as strings by default.\n\nheader=True:\n\nThe first row of the CSV file contains the column headers, and will be used as names of the columns."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#reading-a-csv-file-into-the-spark-framework-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#reading-a-csv-file-into-the-spark-framework-1",
    "title": "Lecture 6",
    "section": "Reading a CSV file into the Spark Framework",
    "text": "Reading a CSV file into the Spark Framework\npath = 'https://bcdanl.github.io/data/df.csv'\ndf = spark.read.csv(path, \n                    inferSchema=True,\n                    header=True)\ndf.show()\n\nSpark‚Äôs spark.read.csv() function relies on the Hadoop FileSystem API to access files.\nBy default, Hadoop does not support reading files directly from HTTPS URLs.\n\nIt expects a local file system path, HDFS path, or another supported distributed file system."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#displaying-the-first-rows-with-df.show",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#displaying-the-first-rows-with-df.show",
    "title": "Lecture 6",
    "section": "Displaying the First Rows with df.show()",
    "text": "Displaying the First Rows with df.show()\nThree Optional Parameters\ndf.show(5)\ndf.show(truncate = False)\ndf.show(truncate = 3)\ndf.show(vertical = True)\n\nBy default, displays the first 20 rows.\ndf.show(n, truncate, vertical) accepts three optional parameters:\n\nn (int): Number of rows to display.\n\n\nExample: df.show(5)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#displaying-the-first-rows-with-df.show-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#displaying-the-first-rows-with-df.show-1",
    "title": "Lecture 6",
    "section": "Displaying the First Rows with df.show()",
    "text": "Displaying the First Rows with df.show()\nThree Optional Parameters\n\ntruncate (bool or int):\n\n\nIf True (default), long strings are truncated to 20 characters.\nIf False, displays full column contents.\nIf an integer is provided, it specifies the maximum number of characters to display.\nExample: df.show(truncate=False), df.show(truncate=30)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#displaying-the-first-rows-with-df.show-2",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#displaying-the-first-rows-with-df.show-2",
    "title": "Lecture 6",
    "section": "Displaying the First Rows with df.show()",
    "text": "Displaying the First Rows with df.show()\nThree Optional Parameters\n\nvertical (bool):\n\n\nIf True, displays each row vertically (useful for wide tables).\nExample: df.show(vertical=True)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#displaying-variable-information-with-df.printschema-df.dtypes-and-df.columns",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#displaying-variable-information-with-df.printschema-df.dtypes-and-df.columns",
    "title": "Lecture 6",
    "section": "Displaying Variable Information with df.printSchema(), df.dtypes, and df.columns",
    "text": "Displaying Variable Information with df.printSchema(), df.dtypes, and df.columns\ndf.printSchema()\ndf.dtypes\ndf.columns\n\ndf.printSchema() prints the DataFrame schema in a tree format.\n\nnullable = true means that a column can contain null values.\nWe may need to handle missing data appropriately.\n\ndf.dtypes returns a list of tuples representing each column‚Äôs name and data type.\ndf.columns returns a list of colunm names."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#generating-descriptive-statistics-with-df.describe.show-and-df.count",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#generating-descriptive-statistics-with-df.describe.show-and-df.count",
    "title": "Lecture 6",
    "section": "Generating Descriptive Statistics with df.describe().show() and df.count()",
    "text": "Generating Descriptive Statistics with df.describe().show() and df.count()\ndf.describe().show()\ndf.count()\n\ndf.describe() computes summary statistics (e.g., count, mean, stddev, min, max) for the DataFrame‚Äôs numeric columns.\n.show() prints these statistics in a readable table format.\ndf.count() returns the number of rows in df."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#spark-dataframe-vs.-pandas-dataframe",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#spark-dataframe-vs.-pandas-dataframe",
    "title": "Lecture 6",
    "section": "Spark DataFrame vs.¬†Pandas DataFrame",
    "text": "Spark DataFrame vs.¬†Pandas DataFrame\n\nWhat makes a Spark DataFrame different from other DataFrame?\n\nSpark DataFrames are designed for big data and distributed computing.\n\nSpark DataFrame:\n\nData is distributed across a cluster of machines.\nOperations are executed in parallel on multiple nodes.\nCan process datasets that exceed the memory of a single machine.\n\nOther DataFrames (e.g., Pandas):\n\nOperate on a single machine.\nEntire dataset must fit into memory.\nLimited by local system resources."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#spark-dataframe-vs.-pandas-dataframe-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#spark-dataframe-vs.-pandas-dataframe-1",
    "title": "Lecture 6",
    "section": "Spark DataFrame vs.¬†Pandas DataFrame",
    "text": "Spark DataFrame vs.¬†Pandas DataFrame\nLazy Evaluation and Optimization\n\nSpark DataFrame:\n\nUses lazy evaluation: transformations are not computed until an action is called.\nOptimize query execution.\n\nOther DataFrames:\n\nOperations are evaluated eagerly (immediately).\nNo built-in query optimization across multiple operations."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#spark-dataframe-vs.-pandas-dataframe-2",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#spark-dataframe-vs.-pandas-dataframe-2",
    "title": "Lecture 6",
    "section": "Spark DataFrame vs.¬†Pandas DataFrame",
    "text": "Spark DataFrame vs.¬†Pandas DataFrame\nScalability\n\nSpark DataFrame:\n\nDesigned to scale to petabytes of data.\nUtilizes distributed storage and computing resources.\nIdeal for large-scale data processing and analytics.\n\nOther DataFrames:\n\nBest suited for small to medium datasets.\nLimited by the hardware of a single computer."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#spark-dataframe-vs.-pandas-dataframe-3",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#spark-dataframe-vs.-pandas-dataframe-3",
    "title": "Lecture 6",
    "section": "Spark DataFrame vs.¬†Pandas DataFrame",
    "text": "Spark DataFrame vs.¬†Pandas DataFrame\nFault Tolerance\n\nSpark DataFrame:\n\nBuilt on Resilient Distributed Datasets (RDDs).\nAutomatically recovers lost data if a node fails.\nEnsures high reliability in distributed environments.\n\nOther DataFrames:\n\nTypically lack built-in fault tolerance.\nFailures on a single machine can result in data loss."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#spark-dataframe-vs.-pandas-dataframe-4",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#spark-dataframe-vs.-pandas-dataframe-4",
    "title": "Lecture 6",
    "section": "Spark DataFrame vs.¬†Pandas DataFrame",
    "text": "Spark DataFrame vs.¬†Pandas DataFrame"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-1",
    "title": "Lecture 6",
    "section": "PySpark Basics",
    "text": "PySpark Basics\nLearning Objectives\n\nLoading DataFrame with Spark‚Äôs read API (analogous to read_csv())\nGetting a Summary with printSchema() and describe()\nSelecting and Reordering Variables with select()\nCounting Values with groupBy().count(), countDistinct(), and count()\nSorting with orderBy()\nAdding a New Variable with withColumn()\nRemoving a Variable with drop()\nRenaming a Variable with withColumnRenamed()\nAggregation and Math Operations with selectExpr()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-2",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-2",
    "title": "Lecture 6",
    "section": "PySpark Basics",
    "text": "PySpark Basics\nLearning Objectives\n\nConverting Data Types with cast()\nFiltering Observations with filter()\nDealing with Missing Values (na.drop(), na.fill(), etc.)\nDealing with Duplicates (dropDuplicates())    \nGrouping DataFrames with groupBy().agg()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#spark-dataframe",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#spark-dataframe",
    "title": "Lecture 6",
    "section": "Spark DataFrame",
    "text": "Spark DataFrame\n\nIn PySpark, a DataFrame is a distributed collection of data organized into named columns.\nUnlike Pandas DataFrame, a Spark DataFrame is evaluated lazily: many transformations are ‚Äúplanned‚Äù but not executed until an action (e.g., count(), collect()) triggers a computation on the cluster.\nNo dedicated row index is maintained like in Pandas; rows are conceptually identified by their values, not by a numeric position."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#reading-a-web-csv-file-into-the-spark-framework",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#reading-a-web-csv-file-into-the-spark-framework",
    "title": "Lecture 6",
    "section": "Reading a Web CSV file into the Spark Framework",
    "text": "Reading a Web CSV file into the Spark Framework\n\n\nWhat should we do then?\n\nWe can convert the Pandas DataFrame to a Spark DataFrame\ndf = spark.createDataFrame(df_pd)\n\n\n\nimport pandas as pd\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\ndf_pd = pd.read_csv('https://bcdanl.github.io/data/nba.csv')\ndf = spark.createDataFrame(df_pd)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#spark-dataframe-overview-methods",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#spark-dataframe-overview-methods",
    "title": "Lecture 6",
    "section": "Spark DataFrame Overview Methods",
    "text": "Spark DataFrame Overview Methods\n\ndf.printSchema(): prints the schema (column names and data types).\ndf.columns: returns the list of columns.\ndf.dtypes: returns a list of tuples (columnName, dataType).\ndf.count(): returns the total number of rows.\ndf.describe(): returns basic statistics of numerical/string columns (mean, count, std, min, max)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#getting-a-summary-of-a-dataframe-with-.describe",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#getting-a-summary-of-a-dataframe-with-.describe",
    "title": "Lecture 6",
    "section": "Getting a Summary of a DataFrame with .describe()",
    "text": "Getting a Summary of a DataFrame with .describe()\ndf.describe()\ndf.describe(include='all')\n\n.describe() method generates descriptive statistics that summarize the central tendency, dispersion, and distribution of each variable.\n\nIt can also process string-type variables if specified explicitly (include='all')."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#selecting-a-column-by-its-name",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#selecting-a-column-by-its-name",
    "title": "Lecture 6",
    "section": "Selecting a Column by its Name",
    "text": "Selecting a Column by its Name\n# Single column -&gt; returns a DataFrame with one column\ndf.select(\"Name\").show(5)\n\n# Multiple columns -&gt; pass a list-like of column names\ndf.select(\"Name\", \"Team\", \"Salary\").show(5)\n\nIn PySpark, we use select() to choose variables.\nIt returns a new DataFrame projection of the specified variables."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#counting-observations-distinct-values",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#counting-observations-distinct-values",
    "title": "Lecture 6",
    "section": "Counting Observations / Distinct Values",
    "text": "Counting Observations / Distinct Values\n# Counting how many total rows\nnba_count = df.count()\n\n# Count distinct values in one column\nfrom pyspark.sql.functions import countDistinct\nnum_teams = df.select(countDistinct(\"Team\")).collect()[0][0]\n\n# GroupBy a column and count occurrences\ndf.groupBy(\"Team\").count().show(5)\n\ndf.count() returns the number of rows in df.\nUnlike Pandas, there is no direct .value_counts() or .nunique() in PySpark.\n\nHowever, we can replicate these operations using Spark‚Äôs aggregations (groupBy().count(), countDistinct, etc.)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#pandas-basics",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#pandas-basics",
    "title": "Lecture 6",
    "section": "Pandas Basics",
    "text": "Pandas Basics\nLet‚Äôs do Questions 1-3 in Classwork 5!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#sorting-by-one-or-more-variables",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#sorting-by-one-or-more-variables",
    "title": "Lecture 6",
    "section": "Sorting by One or More Variables",
    "text": "Sorting by One or More Variables\n# Sort by a single column ascending\ndf.orderBy(\"Name\").show(5)\n\n# Sort by descending\nfrom pyspark.sql.functions import desc\ndf.orderBy(desc(\"Salary\")).show(5)\n\n# Sort by multiple columns\ndf.orderBy([\"Team\", desc(\"Salary\")]).show(5)\n\norderBy() can accept column names and ascending/descending instructions."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#equivalent-of-pandas-nsmallest-or-nlargest",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#equivalent-of-pandas-nsmallest-or-nlargest",
    "title": "Lecture 6",
    "section": "Equivalent of Pandas nsmallest or nlargest",
    "text": "Equivalent of Pandas nsmallest or nlargest\n# nsmallest example:\ndf.orderBy(\"Salary\").limit(5).show()\n\n# nlargest example:\ndf.orderBy(desc(\"Salary\")).limit(5).show()\n\nSpark does not have nsmallest() or nlargest(), but we can use limit() after sorting."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#row-based-access-in-pyspark",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#row-based-access-in-pyspark",
    "title": "Lecture 6",
    "section": "Row-Based Access in PySpark",
    "text": "Row-Based Access in PySpark\n\n\nUnlike Pandas, PySpark DataFrames do not use a row-based index, so there is no direct .loc[] or .iloc[].\nWe typically filter rows by conditions (using .filter()) or use transformations (limit(), take(), collect()) to access row data.\n\n\n# Example: filter by condition\ndf.filter(\"Team == 'New York Knicks'\").show()\ndf.limit(5).show()\ndf.take(5)\ndf.collect()\n\nTo get the first n rows, use df.limit(n) or df.take(n), which returns a list of Row objects.\ndf.collect(): Returns all the records as a list of Row."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#aggregations-math-methods",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#aggregations-math-methods",
    "title": "Lecture 6",
    "section": "Aggregations & Math Methods",
    "text": "Aggregations & Math Methods\n# Summaries for numeric columns\ndf.selectExpr(\n    \"mean(Salary) as mean_salary\",\n    \"min(Salary) as min_salary\",\n    \"max(Salary) as max_salary\",\n    \"stddev_pop(Salary) as std_salary\"\n).show()\n\nSpark has many SQL functions that can be used with selectExpr()."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#creating-or-transforming-columns",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#creating-or-transforming-columns",
    "title": "Lecture 6",
    "section": "Creating or Transforming Columns",
    "text": "Creating or Transforming Columns\nfrom pyspark.sql import functions as F\n# Pre-compute the average salary (pulls it back as a Python float)\nsalary_mean = df.select(F.avg(\"Salary\").alias(\"mean_salary\")).collect()[0][\"mean_salary\"]\n\ndf2 = (\n    df\n    .withColumn(\"Salary_2x\", F.col(\"Salary\") * 2)    # Add Salary_2x\n    .withColumn(\n        \"Name_w_Position\",           # Concatenate Name and Position\n        F.concat(F.col(\"Name\"), F.lit(\" (\"), F.col(\"Position\"), F.lit(\")\")))\n    .withColumn(\n        \"Salary_minus_Mean\",        # Subtract mean salary\n        F.col(\"Salary\") - F.lit(salary_mean))\n)\n\nAll transformations in Spark are ‚Äúlazy‚Äù until an action (show(), collect(), count()) is called.\n.alias() method is a way to give a temporary (or alternate) name to the column."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#vectorized-operations",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#vectorized-operations",
    "title": "Lecture 6",
    "section": "Vectorized Operations",
    "text": "Vectorized Operations\nnba[\"Salary_2x\"] = nba[\"Salary\"] + nba[\"Salary\"]\nnba[\"Name_w_Position\"] = nba[\"Name\"] + \" (\" + nba[\"Position\"] + \")\"\nnba[\"Salary_minus_Mean\"] = nba[\"Salary\"] - nba[\"Salary\"].mean()\n\npandas performs a vectorized operation on Series or a variable in DataFrame.\n\nThis means an element-by-element operation.\nThis enables us to apply functions and perform operations on the data efficiently, without the need for explicit loops."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#adding-and-removing-variables",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#adding-and-removing-variables",
    "title": "Lecture 6",
    "section": "Adding and Removing Variables",
    "text": "Adding and Removing Variables\n\n\nHere we use [] to add variables:\n\nnba['Salary_k'] = nba['Salary'] / 1000\nnba['Salary_2x'] = nba['Salary'] + nba['Salary']\nnba['Salary_3x'] = nba['Salary'] * 3"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#removing-variables-with-dropcolumns-...",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#removing-variables-with-dropcolumns-...",
    "title": "Lecture 6",
    "section": "Removing Variables with drop(columns = ... )",
    "text": "Removing Variables with drop(columns = ... )\n\n\nWe can use .drop(columns = ...) to drop variables:\n\nnba.drop(columns = \"Salary_k\")\nnba.drop(columns = [\"Salary_2x\", \"Salary_3x\"])"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#renaming-variables-with-nba.columns",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#renaming-variables-with-nba.columns",
    "title": "Lecture 6",
    "section": "Renaming Variables with nba.columns",
    "text": "Renaming Variables with nba.columns\n\n\nDo you recall the .columns attribute?\n\nnba.columns\n\nWe can rename any or all of a DataFrame‚Äôs columns by assigning a list of new names to the attribute:\n\nnba.columns = [\"Team\", \"Position\", \"Date of Birth\", \"Income\"]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#renaming-variables-with-rename-columns-existing-one-new-one",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#renaming-variables-with-rename-columns-existing-one-new-one",
    "title": "Lecture 6",
    "section": "Renaming Variables with rename( columns = { \"Existing One\" : \"New One\" } )",
    "text": "Renaming Variables with rename( columns = { \"Existing One\" : \"New One\" } )\nnba.rename( columns = { \"Date of Birth\": \"Birthday\" } )\n\nThe above rename() method renames the variable Date of Birth to Birthday."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#renaming-rows-with-rename-index-existing-one-new-one",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#renaming-rows-with-rename-index-existing-one-new-one",
    "title": "Lecture 6",
    "section": "Renaming rows with rename( index = { \"Existing One\" : \"New One\" } )",
    "text": "Renaming rows with rename( index = { \"Existing One\" : \"New One\" } )\nnba = nba.rename(\n    index = { \"LeBron James\": \"LeBron Raymone James\" }\n)\n\nThe above rename() method renames the observation LeBron James to LeBron Raymone James."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#relocating-variables-with-.columns.get_loc-.pop-and-.insert",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#relocating-variables-with-.columns.get_loc-.pop-and-.insert",
    "title": "Lecture 6",
    "section": "Relocating Variables with .columns.get_loc(), .pop(), and .insert()",
    "text": "Relocating Variables with .columns.get_loc(), .pop(), and .insert()\nref_var = nba.columns.get_loc('Team') \nvar_to_move = nba.pop('Salary')\nnba.insert(ref_var, 'Salary', var_to_move) # insert() directly alters 'nba'\n\nStep 1. DataFrame.columns.get_loc('Reference_Var')\n\nGet the integer position (right before the reference variable, ‚ÄòReference_Var‚Äô)\n\nStep 2. DataFrame.pop('Some_Var_To_Move')\n\nRemove the variable we want to relocate from the DataFrame and store it in a Series\n\nStep 3. DataFrame.insert(ref_var, 'Some_Var_To_Move', var_to_move)\n\nInsert the variable back into the DataFrame right after the reference variable."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-astype-method-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-astype-method-1",
    "title": "Lecture 6",
    "section": "Converting Data Types with the astype() Method",
    "text": "Converting Data Types with the astype() Method\n\n\nLet‚Äôs read employment.csv as emp.\n\nimport pandas as pd\n# Below is for an interactive display of DataFrame in Colab\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\nemp = pd.read_csv(\"https://bcdanl.github.io/data/employment.csv\")"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-astype-method-2",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-astype-method-2",
    "title": "Lecture 6",
    "section": "Converting Data Types with the astype() method",
    "text": "Converting Data Types with the astype() method\n\n\nWhat values are in the Mgmt variable?\n\n\nemp[\"Mgmt\"].astype(bool)\n\nThe astype() method converts a Series‚Äô values to a different data type.\n\nIt can accept a single argument: the new data type."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-astype-method-3",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-astype-method-3",
    "title": "Lecture 6",
    "section": "Converting Data Types with the astype() method",
    "text": "Converting Data Types with the astype() method\nemp[\"Mgmt\"] = emp[\"Mgmt\"].astype(bool)\n\nThe above code overwrites the Mgmt variable with our new Series of Booleans."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-astype-method-4",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-astype-method-4",
    "title": "Lecture 6",
    "section": "Converting Data Types with the astype() method",
    "text": "Converting Data Types with the astype() method\nemp[\"Salary\"].astype(int)\n\nThe above code tries to coerce the Salary variable‚Äôs values to integers with the astype() method.\n\nPandas is unable to convert the NaN values to integers."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#fill-missing-values-with-the-fillna-method",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#fill-missing-values-with-the-fillna-method",
    "title": "Lecture 6",
    "section": "Fill Missing Values with the fillna() method",
    "text": "Fill Missing Values with the fillna() method\nemp[\"Salary\"].fillna(0)\n\nThe fillna() method replaces a Series‚Äô missing values with the argument we pass in.\nThe above example provides a fill value of 0.\n\nNote that our choice of value can distort the data; 0 is passed solely for the sake of example."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-astype-method-5",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-astype-method-5",
    "title": "Lecture 6",
    "section": "Converting Data Types with the astype() method",
    "text": "Converting Data Types with the astype() method\nemp[\"Salary\"] = emp[\"Salary\"].fillna(0).astype(int)\n\nThe above code overwrites the Salary variable with our new Series of integers."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-astype-method-6",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-astype-method-6",
    "title": "Lecture 6",
    "section": "Converting Data Types with the astype() method",
    "text": "Converting Data Types with the astype() method\nemp[\"Gender\"] = emp[\"Gender\"].astype(\"category\")\n\nPandas includes a special data type called a category,\n\nIt is ideal for a variable consisting of a small number of unique values relative to its total size.\nE.g., gender, weekdays, blood types, planets, and income groups."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-pd.to_datetime-method",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-pd.to_datetime-method",
    "title": "Lecture 6",
    "section": "Converting Data Types with the pd.to_datetime() method",
    "text": "Converting Data Types with the pd.to_datetime() method\n# Below two are equivalent:\nemp[\"Start Date\"] = pd.to_datetime(emp[\"Start Date\"])\nemp[\"Start Date\"] = emp[\"Start Date\"].astype('datetime64')\n\nThe pd.to_datetime() function is used to convert a Series, DataFrame, or a single variable of a DataFrame from its current data type into datetime format."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-astype-method-7",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-astype-method-7",
    "title": "Lecture 6",
    "section": "Converting Data Types with the astype() method",
    "text": "Converting Data Types with the astype() method\nemp = pd.read_csv(\"https://bcdanl.github.io/data/employment.csv\")\n\nemp[\"Salary\"] = emp[\"Salary\"].fillna(0)\nemp = emp.astype({'Mgmt': 'bool', \n                  'Salary': 'int',\n                  'Gender': 'category',\n                  'Start Date': 'datetime64'})\n\nWe can provide a dictionary of variable-type pairs to astype()."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#pandas-basics-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#pandas-basics-1",
    "title": "Lecture 6",
    "section": "Pandas Basics",
    "text": "Pandas Basics\nLet‚Äôs do Question 1 in Classwork 6!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-by-a-condition-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-by-a-condition-1",
    "title": "Lecture 6",
    "section": "Filtering by a Condition",
    "text": "Filtering by a Condition\n\n\ndf.filter(\n    ( col(\"Team\") == \"Finance\" ) & \n    ( col(\"Salary\") &gt;= 100000 )\n).show()\n\ndf.filter(\n    (col(\"Team\") == \"Finance\") | \n    (col(\"Team\") == \"Legal\")   | \n    (col(\"Team\") == \"Sales\")\n).show()\n\n\n\nWe could combine multiple separate Boolean conditions with logical operators."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-by-a-single-condition",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-by-a-single-condition",
    "title": "Lecture 6",
    "section": "Filtering by a Single Condition",
    "text": "Filtering by a Single Condition\nemp[\"First Name\"] == \"Donna\"\n\nTo compare every value in Series with a constant value, we place the Series on one side of the equality operator (==) and the value on the other.\n\nSeries == value\n\nThe above example compares each First Name value with ‚ÄúDonna‚Äù.\n\npandas performs a vectorized operation (element-by-element operation) on Series."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-by-a-single-condition-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-by-a-single-condition-1",
    "title": "Lecture 6",
    "section": "Filtering by a Single Condition",
    "text": "Filtering by a Single Condition\nemp[ emp[\"First Name\"] == \"Donna\" ]\n\nTo filter observations, we provide the Boolean Series between square brackets following the DataFrame.\n\nDataFrame[ Boolean_Series ]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-by-a-single-condition-2",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-by-a-single-condition-2",
    "title": "Lecture 6",
    "section": "Filtering by a Single Condition",
    "text": "Filtering by a Single Condition\ndonnas = emp[\"First Name\"] == \"Donna\"\nemp[ donnas ]\n\nIf the use of multiple square brackets is confusing, we can assign the Boolean Series to an object and then pass it into the square brackets instead."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-by-a-single-condition-3",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-by-a-single-condition-3",
    "title": "Lecture 6",
    "section": "Filtering by a Single Condition",
    "text": "Filtering by a Single Condition\n\n\nWhat if we want to extract a subset of employees who are not on the ‚ÄúMarketing‚Äù team?\n\n\nnon_marketing = emp[\"Team\"] != \"Marketing\"  # != means \"not equal to\"\nemp[ non_marketing ]\n\nTrue denotes that the Team value for a given index is not ‚ÄúMarketing‚Äù, and False indicates the Team value is ‚ÄúMarketing‚Äù"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-by-a-single-condition-4",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-by-a-single-condition-4",
    "title": "Lecture 6",
    "section": "Filtering by a Single Condition",
    "text": "Filtering by a Single Condition\n\n\nWhat if we want to retrieve all the managers in the company?\n\nManagers have a value of True in the Mgmt variable.\n\n\n\nemp[ emp[\"Mgmt\"] ]\n\nWe could execute emp[\"Mgmt\"] == True, but we do not need to."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-by-a-single-condition-5",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-by-a-single-condition-5",
    "title": "Lecture 6",
    "section": "Filtering by a Single Condition",
    "text": "Filtering by a Single Condition\nhigh_earners = emp[\"Salary\"] &gt; 100000\nemp[ high_earners ]\n\nWe can also use arithmetic operands to filter observations based on mathematical conditions."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-by-a-condition-2",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-by-a-condition-2",
    "title": "Lecture 6",
    "section": "Filtering by a Condition",
    "text": "Filtering by a Condition\nsales = emp[\"Team\"] == \"Sales\"\nlegal = emp[\"Team\"] == \"Legal\"\nfnce = emp[\"Team\"] == \"Finance\"\nemp[ sales | legal | fnce ] # '|' is 'or' opeartor\n\nWe could provide three separate Boolean Series inside the square brackets and add the | symbol to declare OR criteria.\nWhat if our next report asked for employees from 30 teams instead of three?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-with-the-isin-method",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-with-the-isin-method",
    "title": "Lecture 6",
    "section": "Filtering with the isin() method",
    "text": "Filtering with the isin() method\n# Checking membership with \"isin\"\ndf.filter(col(\"Team\").isin(\"Finance\", \"Legal\", \"Sales\")).show()\n\nA better solution is the isin() method."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-by-a-condition-3",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-by-a-condition-3",
    "title": "Lecture 6",
    "section": "Filtering by a Condition",
    "text": "Filtering by a Condition\n\n\nWhen working with numbers or dates, we often want to extract values that fall within a range.\n\nE.g., Identify all employees with a salary between $90,000 and $100,000.\n\n\n\nhigher_than_90k = emp[\"Salary\"] &gt;= 90000\nlower_than_100k = emp[\"Salary\"] &lt; 100000\nemp[ higher_than_90k & lower_than_100k ] # '&' is 'and' opeartor\n\nWe can create two Boolean Series, one to declare the lower bound and one to declare the upper bound.\nThen we can use the & operator to mandate that both conditions are True."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-with-the-between-method",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-with-the-between-method",
    "title": "Lecture 6",
    "section": "Filtering with the between() method",
    "text": "Filtering with the between() method\ndf_between = df.filter(col(\"Salary\").between(90000, 100000))\ndf_between.show()\n\nIt returns a Boolean Series where True denotes that an observation‚Äôs value falls between the specified interval."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-with-the-between-method-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-with-the-between-method-1",
    "title": "Lecture 6",
    "section": "Filtering with the between() method",
    "text": "Filtering with the between() method\nname_starts_with_t = emp[\"First Name\"].between(\"T\", \"U\")\nemp[ name_starts_with_t ]\n\nWe can also apply the between() method to string variables."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-by-a-condition-with-the-query-method",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-by-a-condition-with-the-query-method",
    "title": "Lecture 6",
    "section": "Filtering by a Condition with the query() method!",
    "text": "Filtering by a Condition with the query() method!\nemp.query(\"Salary &gt;= 100000 & Team == 'Finance'\")\nemp.query(\"Salary &gt;= 100000 & `First Name` == 'Douglas'\")\n\nThe query() method filters observations using a concise, string-based query syntax.\n\nquery() accepts a string value that describes filtering conditions.\n\nWhen using the query() method, if we have variable names with spaces, we can wrap the variable names in backtick (`)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#pandas-basics-2",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#pandas-basics-2",
    "title": "Lecture 6",
    "section": "Pandas Basics",
    "text": "Pandas Basics\nLet‚Äôs do Questions 2-6 in Classwork 6!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-missing-values-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-missing-values-1",
    "title": "Lecture 6",
    "section": "Dealing with Missing Values",
    "text": "Dealing with Missing Values\n\n\nLet‚Äôs read employment.csv as emp.\n\nimport pandas as pd\n# Below is for an interactive display of DataFrame in Colab\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\nemp = pd.read_csv(\"https://bcdanl.github.io/data/employment.csv\")"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-missing-values-2",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-missing-values-2",
    "title": "Lecture 6",
    "section": "Dealing with Missing Values",
    "text": "Dealing with Missing Values\n\nPandas often marks (1) missing text values and (2) missing numeric values with a NaN (not a number);\n\nIt also marks missing datetime values with a NaT (not a time)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-missing-values-the-isna-and-notna-methods",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-missing-values-the-isna-and-notna-methods",
    "title": "Lecture 6",
    "section": "Dealing with Missing Values: The isna() and notna() methods",
    "text": "Dealing with Missing Values: The isna() and notna() methods\nemp[\"Team\"].isna()\nemp[\"Start Date\"].isna()\n\nThe isna() method returns a Boolean Series in which True denotes that an observation‚Äôs value is missing.\n\nIs a value of a variable ‚ÄúXYZ‚Äù missing?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-missing-values-the-isna-and-notna-methods-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-missing-values-the-isna-and-notna-methods-1",
    "title": "Lecture 6",
    "section": "Dealing with Missing Values: The isna() and notna() methods",
    "text": "Dealing with Missing Values: The isna() and notna() methods\n# Below two are equivalent.\nemp[\"Team\"].notna()\n~emp[\"Team\"].isna()\n\nThe notna() method returns the inverse Series, one in which True indicates that an observation‚Äôs value is present.\nWe use the tilde symbol (~) to invert a Boolean Series.\nQ. How can we pull out employees with non-missing Team values?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-missing-values-the-value_countsdropna-false-method",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-missing-values-the-value_countsdropna-false-method",
    "title": "Lecture 6",
    "section": "Dealing with Missing Values: The value_counts(dropna = False) method",
    "text": "Dealing with Missing Values: The value_counts(dropna = False) method\nemp[\"Mgmt\"].isna().sum()\nemp[\"Mgmt\"].value_counts()\nemp[\"Mgmt\"].value_counts(dropna = False)\n\nOne way to missing data counts is to use the isna().sum() on a Series.\n\nTrue is 1 and False is 0.\n\nAnother way to get missing data counts is to use the .value_counts() method on a Series.\n\nIf we use the dropna = False option, we can also get a missing value count."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-missing-values-the-dropna-method",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-missing-values-the-dropna-method",
    "title": "Lecture 6",
    "section": "Dealing with Missing Values: The dropna() method",
    "text": "Dealing with Missing Values: The dropna() method\nemp.dropna()\n\nThe dropna() method removes observations that hold any NaN or NaT values."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-missing-values-the-dropna-method-with-how",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-missing-values-the-dropna-method-with-how",
    "title": "Lecture 6",
    "section": "Dealing with Missing Values: The dropna() method with how",
    "text": "Dealing with Missing Values: The dropna() method with how\nemp.dropna(how = \"all\")\n\nWe can pass the how parameter an argument of \"all\" to remove observations in which all values are missing.\nNote that the how parameter‚Äôs default argument is \"any\"."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-missing-values-the-dropna-method-with-subset",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-missing-values-the-dropna-method-with-subset",
    "title": "Lecture 6",
    "section": "Dealing with Missing Values: The dropna() method with subset",
    "text": "Dealing with Missing Values: The dropna() method with subset\nemp.dropna(subset = [\"Gender\"])\n\nWe can use the subset parameter to target observations with a missing value in a specific variable.\n\nThe above example removes observations that have a missing value in the Gender variable."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-missing-values-the-dropna-method-with-subset-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-missing-values-the-dropna-method-with-subset-1",
    "title": "Lecture 6",
    "section": "Dealing with Missing Values: The dropna() method with subset",
    "text": "Dealing with Missing Values: The dropna() method with subset\nemp.dropna(subset = [\"Start Date\", \"Salary\"])\n\nWe can also pass the subset parameter a list of variables."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-missing-values-the-dropna-method-with-thresh",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-missing-values-the-dropna-method-with-thresh",
    "title": "Lecture 6",
    "section": "Dealing with Missing Values: The dropna() method with thresh",
    "text": "Dealing with Missing Values: The dropna() method with thresh\nemp.dropna(thresh = 4)\n\nThe thresh parameter specifies a minimum threshold of non-missing values that an observation must have for pandas to keep it."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-duplicates-with-the-duplicated-method",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-duplicates-with-the-duplicated-method",
    "title": "Lecture 6",
    "section": "Dealing with Duplicates with the duplicated() method",
    "text": "Dealing with Duplicates with the duplicated() method\n\n\nMissing values are a common occurrence in messy data sets, and so are duplicate values.\n\n\nemp[\"Team\"].duplicated()\n\nThe duplicated() method returns a Boolean Series that identifies duplicates in a variable."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-duplicates-with-the-duplicated-method-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-duplicates-with-the-duplicated-method-1",
    "title": "Lecture 6",
    "section": "Dealing with Duplicates with the duplicated() method",
    "text": "Dealing with Duplicates with the duplicated() method\nemp[\"Team\"].duplicated(keep = \"first\")\nemp[\"Team\"].duplicated(keep = \"last\")\n~emp[\"Team\"].duplicated()\n\nThe duplicated() method‚Äôs keep parameter informs pandas which duplicate occurrence to keep.\n\nIts default argument, \"first\", keeps the first occurrence of each duplicate value.\nIts argument, \"last\", keeps the last occurrence of each duplicate value.\n\nQ. How can we keep observations with the first occurrences of a value in the Team variable?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-duplicates-with-the-drop_duplicates-method",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-duplicates-with-the-drop_duplicates-method",
    "title": "Lecture 6",
    "section": "Dealing with Duplicates with the drop_duplicates() method",
    "text": "Dealing with Duplicates with the drop_duplicates() method\nemp.drop_duplicates()\n\nThe drop_duplicates() method removes observations in which all values are equal to those in a previously encountered observations."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-duplicates-with-the-drop_duplicates-method-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-duplicates-with-the-drop_duplicates-method-1",
    "title": "Lecture 6",
    "section": "Dealing with Duplicates with the drop_duplicates() method",
    "text": "Dealing with Duplicates with the drop_duplicates() method\n\n\nBelow is an example of the drop_duplicates() method:\n\n\n# Sample DataFrame with duplicate observations\ndata = {\n    'Name': ['John', 'Anna', 'John', 'Mike', 'Anna'],\n    'Age': [28, 23, 28, 32, 23],\n    'City': ['New York', 'Paris', 'New York', 'London', 'Paris']\n}\n\n# pd.DataFrame( Series, List, or Dict ) creates a DataFrame\ndf = pd.DataFrame(data)  \ndf_unique = df.drop_duplicates()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-duplicates-with-the-drop_duplicates-method-2",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-duplicates-with-the-drop_duplicates-method-2",
    "title": "Lecture 6",
    "section": "Dealing with Duplicates with the drop_duplicates() method",
    "text": "Dealing with Duplicates with the drop_duplicates() method\nemp.drop_duplicates(subset = [\"Team\"])\n\nWe can pass the drop_duplicates() method a subset parameter with a list of columns that pandas should use to determine an observation‚Äôs uniqueness.\n\nThe above example finds the first occurrence of each unique value in the Team variable."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-duplicates-with-the-drop_duplicates-method-3",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-duplicates-with-the-drop_duplicates-method-3",
    "title": "Lecture 6",
    "section": "Dealing with Duplicates with the drop_duplicates() method",
    "text": "Dealing with Duplicates with the drop_duplicates() method\nemp.drop_duplicates(subset = [\"Gender\", \"Team\"])\n\nThe above example uses a combination of values across the Gender and Team variables to identify duplicates."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-duplicates-with-the-drop_duplicates-method-4",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-duplicates-with-the-drop_duplicates-method-4",
    "title": "Lecture 6",
    "section": "Dealing with Duplicates with the drop_duplicates() method",
    "text": "Dealing with Duplicates with the drop_duplicates() method\nemp.drop_duplicates(subset = [\"Team\"], keep = \"last\")\nemp.drop_duplicates(subset = [\"Team\"], keep = False)\n\nThe drop_duplicates() method also accepts a keep parameter.\n\nWe can pass it an argument of \"last\" to keep the observations with each duplicate value‚Äôs last occurrence.\nWe can pass it an argument of False to exclude all observations with duplicate values.\n\nQ. What does emp.drop_duplicates(subset = [\"First Name\"], keep = False) do?\nQ. Find a subset of all employees with a First Name of ‚ÄúDouglas‚Äù and a Gender of ‚ÄúMale‚Äù. Then check which ‚ÄúDouglas‚Äù is in the DataFrame emp.drop_duplicates(subset = [\"Gender\", \"Team\"])."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#pandas-basics-3",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#pandas-basics-3",
    "title": "Lecture 6",
    "section": "Pandas Basics",
    "text": "Pandas Basics\nLet‚Äôs do Questions 7-8 in Classwork 6!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#reshaping-dataframes-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#reshaping-dataframes-1",
    "title": "Lecture 6",
    "section": "Reshaping DataFrames",
    "text": "Reshaping DataFrames\nTidy DataFrames\n\n\n\n\nThere are three interrelated rules that make a DataFrame tidy:\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#reshaping-dataframes-2",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#reshaping-dataframes-2",
    "title": "Lecture 6",
    "section": "Reshaping DataFrames",
    "text": "Reshaping DataFrames\nimport pandas as pd\n# Below is for an interactive display of DataFrame in Colab\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\nA DataFrame can be given in a format unsuited for the analysis that we would like to perform on it.\n\nA DataFrame may have larger structural problems that extend beyond the data.\nPerhaps the DataFrame stores its values in a format that makes it easy to extract a single row but difficult to aggregate the data.\n\nReshaping a DataFrame means manipulating it into a different shape.\nIn this section, we will discuss pandas techniques for molding a DataFrame into the shape we desire."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#long-vs.-wide-dataframes",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#long-vs.-wide-dataframes",
    "title": "Lecture 6",
    "section": "Long vs.¬†Wide DataFrames",
    "text": "Long vs.¬†Wide DataFrames\n\n\nThe following DataFrames measure temperatures in two cities over two days.\n\n\ndf_wide = pd.DataFrame({\n    'Weekday': ['Tuesday', 'Wednesday'],\n    'Miami': [80, 83],\n    'Rochester': [57, 62],\n    'St. Louis': [71, 75]\n})\n\ndf_long = pd.DataFrame({\n    'Weekday': ['Tuesday', 'Wednesday', 'Tuesday', 'Wednesday', 'Tuesday', 'Wednesday'],\n    'City': ['Miami', 'Miami', 'Rochester', 'Rochester', 'St. Louis', 'St. Louis'],\n    'Temperature': [80, 83, 57, 62, 71, 75]\n})"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#long-vs.-wide-dataframes-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#long-vs.-wide-dataframes-1",
    "title": "Lecture 6",
    "section": "Long vs.¬†Wide DataFrames",
    "text": "Long vs.¬†Wide DataFrames\n\nA DataFrame can store its values in wide or long format.\nThese names reflect the direction in which the data set expands as we add more values to it.\n\nA long DataFrame increases in height.\nA wide DataFrame increases in width."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#long-vs.-wide-dataframes-2",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#long-vs.-wide-dataframes-2",
    "title": "Lecture 6",
    "section": "Long vs.¬†Wide DataFrames",
    "text": "Long vs.¬†Wide DataFrames\n\nThe optimal storage format for a DataFrame depends on the insight we are trying to glean from it.\n\nWe consider making DataFrames longer if one variable is spread across multiple columns.\nWe consider making DataFrames wider if one observation is spread across multiple rows."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#reshaping-dataframes-3",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#reshaping-dataframes-3",
    "title": "Lecture 6",
    "section": "Reshaping DataFrames",
    "text": "Reshaping DataFrames\nmelt() and pivot()\n\n\n\n\nmelt() makes DataFrame longer.\npivot() and pivot_table() make DataFrame wider."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#make-dataframe-longer-with-melt",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#make-dataframe-longer-with-melt",
    "title": "Lecture 6",
    "section": "Make DataFrame Longer with melt()",
    "text": "Make DataFrame Longer with melt()\ndf_wide_to_long = (\n    df_wide\n    .melt()\n)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#make-dataframe-longer-with-melt-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#make-dataframe-longer-with-melt-1",
    "title": "Lecture 6",
    "section": "Make DataFrame Longer with melt()",
    "text": "Make DataFrame Longer with melt()\ndf_wide_to_long = (\n    df_wide\n    .melt(id_vars = \"Weekday\")\n)\n\n\n\n\n\nmelt() can take a few parameters:\n\nid_vars is a container (string, list, tuple, or array) that represents the variables that will remain as is.\nid_vars can indicate which column should be the ‚Äúidentifier‚Äù."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#make-dataframe-longer-with-melt-2",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#make-dataframe-longer-with-melt-2",
    "title": "Lecture 6",
    "section": "Make DataFrame Longer with melt()",
    "text": "Make DataFrame Longer with melt()\ndf_wide_to_long = (\n    df_wide\n    .melt(id_vars = \"Weekday\",\n          var_name = \"City\",\n          value_name = \"Temperature\")\n)\n\n\n\nmelt() can take a few parameters:\n\nvar_name is a string for the name of the variable whose values are taken from column names in a given wide-form DataFrame.\nvalue_name is a string for the name of the variable whose values are taken from the values in a given wide-form DataFrame."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#make-dataframe-longer-with-melt-3",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#make-dataframe-longer-with-melt-3",
    "title": "Lecture 6",
    "section": "Make DataFrame Longer with melt()",
    "text": "Make DataFrame Longer with melt()\ndf_wide_to_long = (\n    df_wide\n    .melt(id_vars = \"Weekday\",\n          var_name = \"City\",\n          value_name = \"Temperature\",\n          value_vars = ['Miami', 'Rochester'])\n)\n\n\nmelt() can take a few parameters:\n\nvalue_vars parameter allows us to select which specific columns we want to ‚Äúmelt‚Äù.\nBy default, it will melt all the columns not specified in the id_vars parameter."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#make-dataframe-wider-with-pivot",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#make-dataframe-wider-with-pivot",
    "title": "Lecture 6",
    "section": "Make DataFrame Wider with pivot()",
    "text": "Make DataFrame Wider with pivot()\ndf_long_to_wide = (\n    df_long\n    .pivot(index = \"Weekday\",\n           columns = \"City\",\n           values = \"Temperature\"  \n        )\n    .reset_index()\n    )\n\nWhen using pivot(), we need to specify a few parameters:\n\nindex that takes the column to pivot on;\ncolumns that takes the column to be used to make the new variable names of the wider DataFrame;\nvalues that takes the column that provides the values of the variables in the wider DataFrame."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#reshaping-dataframes-4",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#reshaping-dataframes-4",
    "title": "Lecture 6",
    "section": "Reshaping DataFrames",
    "text": "Reshaping DataFrames\n\n\nLet‚Äôs consider the following wide-form DataFrame, df, containing information about the number of courses each student took from each department in each year.\n\n\ndict_data = {\"Name\": [\"Donna\", \"Donna\", \"Mike\", \"Mike\"],\n             \"Department\": [\"ECON\", \"DANL\", \"ECON\", \"DANL\"],\n             \"2018\": [1, 2, 3, 1],\n             \"2019\": [2, 3, 4, 2],\n             \"2020\": [5, 1, 2, 2]}\ndf = pd.DataFrame(dict_data)\n\ndf_longer = df.melt(id_vars=[\"Name\", \"Department\"], \n                    var_name=\"Year\", \n                    value_name=\"Number of Courses\")\n\nThe pivot() method can also take a list of variable names for the index parameter."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#reshaping-dataframes-5",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#reshaping-dataframes-5",
    "title": "Lecture 6",
    "section": "Reshaping DataFrames",
    "text": "Reshaping DataFrames\n\n\nLet‚Äôs consider the following wide-form DataFrame, df, containing information about the number of courses each student took from each department in each year.\n\n\ndict_data = {\"Name\": [\"Donna\", \"Donna\", \"Mike\", \"Mike\"],\n             \"Department\": [\"ECON\", \"DANL\", \"ECON\", \"DANL\"],\n             \"2018\": [1, 2, 3, 1],\n             \"2019\": [2, 3, 4, 2],\n             \"2020\": [5, 1, 2, 2]}\ndf = pd.DataFrame(dict_data)\n\ndf_longer = df.melt(id_vars=[\"Name\", \"Department\"], \n                    var_name=\"Year\", \n                    value_name=\"Number of Courses\")\nQ. How can we use the df_longer to create the wide-form DataFrame, df_wider, which is equivalent to the df?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#reshaping-dataframes-6",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#reshaping-dataframes-6",
    "title": "Lecture 6",
    "section": "Reshaping DataFrames",
    "text": "Reshaping DataFrames\nLet‚Äôs do Part 1 of Classwork 7!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#joining-dataframes-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#joining-dataframes-1",
    "title": "Lecture 6",
    "section": "Joining DataFrames",
    "text": "Joining DataFrames\nRelational Data\n\nSometimes, one data set is scattered across multiple files.\n\nThe size of the files can be huge.\nThe data collection process can be scattered across time and space.\nE.g., DataFrame for county-level data and DataFrame for geographic information, such as longitude and latitude.\n\nSometimes we want to combine two or more DataFrames based on common data values in those DataFrames.\n\nThis task is known in the database world as performing a ‚Äújoin.‚Äù\nWe can do this with the merge() method in Pandas."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#joining-dataframes-2",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#joining-dataframes-2",
    "title": "Lecture 6",
    "section": "Joining DataFrames",
    "text": "Joining DataFrames\nRelational Data\n\n\nThe variables that are used to connect each pair of tables are called keys."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#joining-dataframes-with-merge",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#joining-dataframes-with-merge",
    "title": "Lecture 6",
    "section": "Joining DataFrames with merge()",
    "text": "Joining DataFrames with merge()\n\n\n\n\n\nx = pd.DataFrame({\n    'key': [1, 2, 3],\n    'val_x': ['x1', 'x2', 'x3']\n})\n\ny = pd.DataFrame({\n    'key': [1, 2, 4],\n    'val_y': ['y1', 'y2', 'y3']\n})\n\n\n\nThe colored column represents the ‚Äúkey‚Äù variable.\nThe grey column represents the ‚Äúvalue‚Äù column."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#joining-dataframes-with-merge-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#joining-dataframes-with-merge-1",
    "title": "Lecture 6",
    "section": "Joining DataFrames with merge()",
    "text": "Joining DataFrames with merge()\nInner Join\n\nAn inner join matches pairs of observations whenever their keys are equal:\n\n\n\n\n# the default value for 'how' is 'inner'\n# so it doesn't actually need to be specified\nmerge_inner = pd.merge(x, y, on='key', how='inner')\nmerge_inner_x = x.merge(y, on='key', how='inner')\nmerge_inner_x_how = x.merge(y, on='key')"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#joining-dataframes-with-merge-2",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#joining-dataframes-with-merge-2",
    "title": "Lecture 6",
    "section": "Joining DataFrames with merge()",
    "text": "Joining DataFrames with merge()\nLeft Join\n\nA left join keeps all observations in x.\n\n\n\n\nmerge_left = pd.merge(x, y, on='key', how='left')\nmerge_left_x = x.merge(y, on='key', how='left')\n\nThe most commonly used join is the left join."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#joining-dataframes-with-merge-3",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#joining-dataframes-with-merge-3",
    "title": "Lecture 6",
    "section": "Joining DataFrames with merge()",
    "text": "Joining DataFrames with merge()\nRight Join\n\nA right join keeps all observations in y.\n\n\n\n\nmerge_right = pd.merge(x, y, on='key', how='right')\nmerge_right_x = x.merge(y, on='key', how='right')"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#joining-dataframes-with-merge-4",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#joining-dataframes-with-merge-4",
    "title": "Lecture 6",
    "section": "Joining DataFrames with merge()",
    "text": "Joining DataFrames with merge()\nOuter (Full) Join\n\nA full join keeps all observations in x and y.\n\n\n\n\nmerge_outer = pd.merge(x, y, on='key', how='outer')\nmerge_outer_x = x.merge(y, on='key', how='outer')"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#joining-dataframes-with-merge-5",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#joining-dataframes-with-merge-5",
    "title": "Lecture 6",
    "section": "Joining DataFrames with merge()",
    "text": "Joining DataFrames with merge()\nDuplicate keys: one-to-many\n\nOne DataFrame has duplicate keys (a one-to-many relationship).\n\n\n\n\n\n\nx = pd.DataFrame({\n    'key':[1, 2, 2, 3],\n    'val_x':['x1', 'x2', 'x3', 'x4']})\n\ny = pd.DataFrame({\n    'key':[1, 2],\n    'val_y':['y1', 'y2'] })\none_to_many = x.merge(y, on='key', \n                         how='left')"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#joining-dataframes-with-merge-6",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#joining-dataframes-with-merge-6",
    "title": "Lecture 6",
    "section": "Joining DataFrames with merge()",
    "text": "Joining DataFrames with merge()\nDuplicate keys: many-to-many\n\nBoth DataFrames have duplicate keys (many-to-many relationship).\n\n\n\n\n\n\nx = pd.DataFrame({\n  'key':[1, 2, 2, 3],\n  'val_x':['x1','x2','x3','x4']})\n\ny = pd.DataFrame({\n  'key': [1, 2, 2, 3],\n  'val_y': ['y1', 'y2', 'y3', 'y4'] })\nmany_to_many = x.merge(y, on='key', \n                          how='left')"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#joining-dataframes-with-merge-7",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#joining-dataframes-with-merge-7",
    "title": "Lecture 6",
    "section": "Joining DataFrames with merge()",
    "text": "Joining DataFrames with merge()\nDefining the key columns\n\n\nIf the left and right columns do not have the same name for the key variables, we can use the left_on and right_on parameters instead.\n\n\n\n\nx = pd.DataFrame({\n  'key_x': [1, 2, 3],\n  'val_x': ['x1', 'x2', 'x3']\n})\n\ny = pd.DataFrame({\n  'key_y': [1, 2],\n  'val_y': ['y1', 'y2'] })\n\nkeys_xy = \n  x.merge(y, left_on = 'key_x', \n             right_on = 'key_y', \n             how = 'left')"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#joining-dataframes-with-merge-8",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#joining-dataframes-with-merge-8",
    "title": "Lecture 6",
    "section": "Joining DataFrames with merge()",
    "text": "Joining DataFrames with merge()\nLet‚Äôs do Part 2 of Classwork 7!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#data-concatenation-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#data-concatenation-1",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\n\n\nConcatenation can be thought of as appending a row or column to our data.\n\nThis approach is possible if our data was split into parts or if we performed a calculation that we want to append to our existing data set.\n\nLet‚Äôs consider the following example DataFrames:\n\ndf1 = pd.read_csv('https://bcdanl.github.io/data/concat_1.csv')\ndf2 = pd.read_csv('https://bcdanl.github.io/data/concat_2.csv')\ndf3 = pd.read_csv('https://bcdanl.github.io/data/concat_3.csv')\n\nWe will be working with .index and .columns in this Section.\n\ndf1.index\ndf1.columns"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#data-concatenation-2",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#data-concatenation-2",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nAdd Rows\n\n\nConcatenating the DataFrames on top of each other uses the concat() method.\n\nAll of the DataFrames to be concatenated are passed in a list.\n\n\nrow_concat = pd.concat([df1, df2, df3])\nrow_concat"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#data-concatenation-3",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#data-concatenation-3",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nAdd Rows\n\n\nLet‚Äôs consider a new Series and concatenate it with df1:\n\n# create a new row of data\nnew_row_series = pd.Series(['n1', 'n2', 'n3', 'n4'])\nnew_row_series\n\n\n# attempt to add the new row to a dataframe\ndf = pd.concat([df1, new_row_series])\ndf\n\nNot only did our code not append the values as a row, but it also created a new column completely misaligned with everything else.\nWhy?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#data-concatenation-4",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#data-concatenation-4",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nAdd Rows\n\n\nTo fix the problem, we need turn our Series into a DataFrame.\n\nThis data frame contains one row of data, and the column names are the ones the data will bind to.\n\n\nnew_row_df = pd.DataFrame(\n  # note the double brackets to create a \"row\" of data\n  data =[[\"n1\", \"n2\", \"n3\", \"n4\"]],\n  columns =[\"A\", \"B\", \"C\", \"D\"],\n)\n\ndf = pd.concat([df1, new_row_df])"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#data-concatenation-5",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#data-concatenation-5",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nAdd Columns\n\n\nConcatenating columns is very similar to concatenating rows.\n\nThe main difference is the axis parameter in the concat() method.\nThe default value of axis is 0 (or axis = \"index\"), so it will concatenate data in a row-wise fashion.\nIf we pass axis = 1 (or axis = \"columns\") to the function, it will concatenate data in a column-wise manner.\n\n\ncol_concat = pd.concat([df1, df2, df3], axis = \"columns\")"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#data-concatenation-6",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#data-concatenation-6",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nAdd Columns\n\n\nWe can use ignore_index=True to reset the column indices, so that we do not have duplicated column names.\n\npd.concat([df1, df2, df3], axis=\"columns\", ignore_index=True)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#concatenate-with-different-indices",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#concatenate-with-different-indices",
    "title": "Lecture 6",
    "section": "Concatenate with Different Indices",
    "text": "Concatenate with Different Indices\n\n\nWhat would happen when the row and column indices are not aligned?\nLet‚Äôs modify our DataFrames for the next few examples.\n\n# rename the columns of our dataframes\ndf1.columns = ['A', 'B', 'C', 'D']\ndf2.columns = ['E', 'F', 'G', 'H']\ndf3.columns = ['A', 'C', 'F', 'H']\n\nIf we try to concatenate these DataFrames as we did, the DataFrames now do much more than simply stack one on top of the other.\n\nrow_concat = pd.concat([df1, df2, df3])"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#concatenate-with-different-indices-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#concatenate-with-different-indices-1",
    "title": "Lecture 6",
    "section": "Concatenate with Different Indices",
    "text": "Concatenate with Different Indices\n\n\nWe can set join = 'inner' to keep only the columns that are shared among the data sets.\n\npd.concat([df1, df2, df3], join ='inner')\n\nIf we use the DataFrames that have columns in common, only the columns that all of them share will be returned.\n\npd.concat([df1, df3], join ='inner',  ignore_index =False)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#concatenate-with-different-indices-2",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#concatenate-with-different-indices-2",
    "title": "Lecture 6",
    "section": "Concatenate with Different Indices",
    "text": "Concatenate with Different Indices\n\n\nLet‚Äôs modify our DataFrames further.\n\n# re-indexing the rows of our DataFrames\ndf1.index = [0, 1, 2, 3]\ndf2.index = [4, 5, 6, 7]\ndf3.index = [0, 2, 5, 7]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#concatenate-with-different-indices-3",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#concatenate-with-different-indices-3",
    "title": "Lecture 6",
    "section": "Concatenate with Different Indices",
    "text": "Concatenate with Different Indices\n\n\nWhen we concatenate along axis=\"columns\" (axis=1), the new DataFrames will be added in a column-wise fashion and matched against their respective row indices.\n\ncol_concat = pd.concat([df1, df2, df3], axis=\"columns\")\n\nJust as we did when we concatenated in a row-wise manner, we can choose to keep the results only when there are matching indices by using join=\"inner\".\n\npd.concat([df1, df3], axis =\"columns\", join='inner')"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#data-concatenation-7",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#data-concatenation-7",
    "title": "Lecture 6",
    "section": "Data Concatenation",
    "text": "Data Concatenation\nLet‚Äôs do Part 3 of Classwork 7!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-3",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-3",
    "title": "Lecture 6",
    "section": "PySpark Basics",
    "text": "PySpark Basics\nLet‚Äôs do Questions 1-3 in Classwork 5!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#adding-removing-renaming-and-relocating-variables-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#adding-removing-renaming-and-relocating-variables-1",
    "title": "Lecture 6",
    "section": "Adding, Removing, Renaming, and Relocating Variables",
    "text": "Adding, Removing, Renaming, and Relocating Variables\nAdding Columns with withColumn()\n# Add a column \"Salary_k\" using a column expression col()\ndf = df.withColumn(\"Salary_k\", col(\"Salary\") / 1000) \nRemoving Columns with drop()\ndf = df.drop(\"Salary_k\")  # remove a single column\ndf = df.drop(\"Salary_2x\", \"Salary_3x\")  # remove multiple columns\nRenaming Columns with withColumnRenamed()\ndf = df.withColumnRenamed(\"Birthday\", \"DateOfBirth\")\nRearranging Columns\ndf = df.select(\"Name\", \"Team\", \"Position\", \"Salary\", \"DateOfBirth\")"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-cast-method-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-cast-method-1",
    "title": "Lecture 6",
    "section": "Converting Data Types with the cast() Method",
    "text": "Converting Data Types with the cast() Method\n\n\nSpark columns can be cast to other data types using cast():\n\n\n# Convert Salary to integer\ndf = df.withColumn(\"Salary_int\", col(\"Salary\").cast(\"int\"))"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types",
    "title": "Lecture 6",
    "section": "Converting Data Types",
    "text": "Converting Data Types\n\n\n\n\n\nIntegers\n\nByteType ‚Äî byte (8-bit)\nShortType ‚Äî short (16-bit)\nIntegerType ‚Äî int (32-bit)\nLongType ‚Äî long (64-bit)\n\nFloating points\n\nFloatType ‚Äî float (32-bit floating point)\nDoubleType ‚Äî double (64-bit floating point)\nDecimalType ‚Äî decimal (Arbitrary precision numeric type)\n\n\n\n\nStringType ‚Äî string (Text data)\nBooleanType ‚Äî boolean (Boolean values (True/False))\nDateType ‚Äî date (Represents a date (year, month, day))\nTimestampType ‚Äî timestamp (Represents a timestamp (date and time))"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-4",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-4",
    "title": "Lecture 6",
    "section": "PySpark Basics",
    "text": "PySpark Basics\n\nLet‚Äôs do\n\nQuestions 4-8 in Classwork 5!\nQuestion 1 in Classwork 6!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-rows-by-a-condition",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#filtering-rows-by-a-condition",
    "title": "Lecture 6",
    "section": "Filtering Rows by a Condition",
    "text": "Filtering Rows by a Condition\nimport pandas as pd\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\ndf_pd = pd.read_csv(\"https://bcdanl.github.io/data/employment.csv\")\ndf_pd = df_pd.where(pd.notnull(df_pd), None)  # Convert NaN to None\ndf = spark.createDataFrame(df_pd)\n\ndf.filter(col(\"Salary\") &gt; 100000).show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-5",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-5",
    "title": "Lecture 6",
    "section": "PySpark Basics",
    "text": "PySpark Basics\nLet‚Äôs do Questions 2-6 in Classwork 6!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#checking-for-missing-values-the-isnull-and-isnotnull-methods",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#checking-for-missing-values-the-isnull-and-isnotnull-methods",
    "title": "Lecture 6",
    "section": "Checking for Missing Values: The isNull() and isNotNull() methods",
    "text": "Checking for Missing Values: The isNull() and isNotNull() methods\n\n\nIn PySpark, missing values often appear as null.\n\n# Count how many null values in a given column\ndf.filter(col(\"Team\").isNull()).count()\n\n# Similarly, you can filter non-null\ndf.filter(col(\"Team\").isNotNull()).count()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dropping-rows-with-null-values-the-.na.drop-method",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dropping-rows-with-null-values-the-.na.drop-method",
    "title": "Lecture 6",
    "section": "Dropping Rows with Null Values: The .na.drop() method",
    "text": "Dropping Rows with Null Values: The .na.drop() method\n# Drop any row that has a null value in any column\ndf_drop_any = df.na.drop()\n\nThe .na.drop() method removes observations that hold any NULL values."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dropping-rows-with-null-values-the-.na.drop-method-with-how",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dropping-rows-with-null-values-the-.na.drop-method-with-how",
    "title": "Lecture 6",
    "section": "Dropping Rows with Null Values: The .na.drop() method with how",
    "text": "Dropping Rows with Null Values: The .na.drop() method with how\n# Drop rows that have all columns null\ndf_drop_all = df.na.drop(how=\"all\")\n\nWe can pass the how parameter an argument of \"all\" to remove observations in which all values are missing.\nNote that the how parameter‚Äôs default argument is \"any\"."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dropping-rows-with-null-values-the-.na.drop-method-with-subset",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dropping-rows-with-null-values-the-.na.drop-method-with-subset",
    "title": "Lecture 6",
    "section": "Dropping Rows with Null Values: The .na.drop() method with subset",
    "text": "Dropping Rows with Null Values: The .na.drop() method with subset\n# Drop rows with nulls in specific columns:\ndf_drop_subset = df.na.drop(subset=[\"Gender\", \"Team\"])\n\nWe can use the subset parameter to target observations with a missing value in a specific variable.\n\nThe above example removes observations that have a missing value in the Gender and Team variables."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#filling-null-values-the-.na.fill-method",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#filling-null-values-the-.na.fill-method",
    "title": "Lecture 6",
    "section": "Filling Null Values: The .na.fill() method",
    "text": "Filling Null Values: The .na.fill() method\n\n\n# Fill a specific column‚Äôs nulls with 0\ndf_fill = (\n  df.na\n  .fill(value = 0, \n        subset = [\"Salary\"])\n)\n\nWe can specify value and subset parameters to fill a specific column‚Äôs NULLs with a specific value\n\n\n# Fill multiple columns with a dictionary\ndf_fill_multi = (\n  df.na\n  .fill({\"Salary\": 0, \n           \"Team\": \"Unknown\"})\n)\n\nWe can fill multiple columns‚Äô NULLs with a dictionary."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-duplicates-with-the-dropduplicates-method",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#dealing-with-duplicates-with-the-dropduplicates-method",
    "title": "Lecture 6",
    "section": "Dealing with Duplicates with the dropDuplicates() method",
    "text": "Dealing with Duplicates with the dropDuplicates() method\n\n\nMissing values are a common occurrence in messy data sets, and so are duplicate values.\n\n\n# Drop all rows that are exact duplicates across all columns\ndf_no_dups = df.dropDuplicates()\n\n# Drop duplicates based on subset of columns\ndf_no_dups_subset = df.dropDuplicates([\"Team\"])\n\nBy default, dropDuplicates() keeps the first occurrence of each distinct combination."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-6",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-6",
    "title": "Lecture 6",
    "section": "PySpark Basics",
    "text": "PySpark Basics\nLet‚Äôs do Questions 7-8 in Classwork 6!"
  },
  {
    "objectID": "danl-cw/danl-320-cw-06.html",
    "href": "danl-cw/danl-320-cw-06.html",
    "title": "Classwork 6",
    "section": "",
    "text": "The netflix.csv file (with its pathname https://bcdanl.github.io/data/netflix.csv) contains a list of 6,000 titles that were available to watch in November 2019 on the video streaming service Netflix. It includes four variables: the video‚Äôs title, director, the date Netflix added it (date_added), and its type (category).\n\nimport pandas as pd\nfrom pyspark.sql import SparkSession\ndf = pd.read_csv(\"https://bcdanl.github.io/data/netflix.csv\")\ndf = df.where(pd.notnull(df), None)\nnetflix = spark.createDataFrame(df)"
  },
  {
    "objectID": "danl-cw/danl-320-cw-06.html#direction",
    "href": "danl-cw/danl-320-cw-06.html#direction",
    "title": "Classwork 6",
    "section": "",
    "text": "The netflix.csv file (with its pathname https://bcdanl.github.io/data/netflix.csv) contains a list of 6,000 titles that were available to watch in November 2019 on the video streaming service Netflix. It includes four variables: the video‚Äôs title, director, the date Netflix added it (date_added), and its type (category).\n\nimport pandas as pd\nfrom pyspark.sql import SparkSession\ndf = pd.read_csv(\"https://bcdanl.github.io/data/netflix.csv\")\ndf = df.where(pd.notnull(df), None)\nnetflix = spark.createDataFrame(df)"
  },
  {
    "objectID": "danl-cw/danl-320-cw-06.html#question-1",
    "href": "danl-cw/danl-320-cw-06.html#question-1",
    "title": "Classwork 6",
    "section": "Question 1",
    "text": "Question 1\n\nOptimize the DataFrame for limited memory use and maximum utility by using the cast() method.\n\nThe format of date_added is ‚Äúdd-MMM-yy‚Äù.\n\n\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-06.html#question-2",
    "href": "danl-cw/danl-320-cw-06.html#question-2",
    "title": "Classwork 6",
    "section": "Question 2",
    "text": "Question 2\nFind all observations with a director of ‚ÄúMartin Scorsese‚Äù.\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-06.html#question-3",
    "href": "danl-cw/danl-320-cw-06.html#question-3",
    "title": "Classwork 6",
    "section": "Question 3",
    "text": "Question 3\nFind all observations with a title of ‚ÄúLimitless‚Äù and a type of ‚ÄúMovie‚Äù.\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-06.html#question-4",
    "href": "danl-cw/danl-320-cw-06.html#question-4",
    "title": "Classwork 6",
    "section": "Question 4",
    "text": "Question 4\nFind all observations with either a date_added of ‚Äú2018-06-15‚Äù or a director of ‚ÄúBong Joon Ho‚Äù.\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-06.html#question-5",
    "href": "danl-cw/danl-320-cw-06.html#question-5",
    "title": "Classwork 6",
    "section": "Question 5",
    "text": "Question 5\nFind all observations with a director of ‚ÄúEthan Coen‚Äù, ‚ÄúJoel Coen‚Äù, and ‚ÄúQuentin Tarantino‚Äù.\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-06.html#question-6",
    "href": "danl-cw/danl-320-cw-06.html#question-6",
    "title": "Classwork 6",
    "section": "Question 6",
    "text": "Question 6\nFind all observations with a date_added value between January 1, 2019 and February 1, 2019.\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-06.html#question-7",
    "href": "danl-cw/danl-320-cw-06.html#question-7",
    "title": "Classwork 6",
    "section": "Question 7",
    "text": "Question 7\nDrop all observations with a NULL value in the director variable.\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-06.html#question-8",
    "href": "danl-cw/danl-320-cw-06.html#question-8",
    "title": "Classwork 6",
    "section": "Question 8",
    "text": "Question 8\nIdentify the days when Netflix added only one movie to its catalog.\nAnswer:"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#resilient-distributed-data-set-rdd-and-dataframe-in-pyspark",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#resilient-distributed-data-set-rdd-and-dataframe-in-pyspark",
    "title": "Lecture 6",
    "section": "Resilient distributed data set (RDD) and DataFrame in PySpark",
    "text": "Resilient distributed data set (RDD) and DataFrame in PySpark\n\n\n\n\n\n\nIn the RDD, we think of each record as an independent entity.\nWith the data frame, we mostly interact with columns, performing functions on them.\n\nWe still can access the rows of a DataFrame via RDD if necessary."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#resilient-distributed-data-set-rdd-and-dataframe",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#resilient-distributed-data-set-rdd-and-dataframe",
    "title": "Lecture 6",
    "section": "Resilient distributed data set (RDD) and DataFrame",
    "text": "Resilient distributed data set (RDD) and DataFrame\n\n\n\n\n\n\nIn the RDD, we think of each record as an independent entity.\nWith the data frame, we mostly interact with columns, performing functions on them.\n\nWe still can access the rows of a DataFrame via RDD if necessary."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#rdd-and-pyspark-dataframe",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#rdd-and-pyspark-dataframe",
    "title": "Lecture 6",
    "section": "RDD and PySpark DataFrame",
    "text": "RDD and PySpark DataFrame\n\n\n\n\n\n\nIn the RDD, we think of each row as an independent entity.\nWith the DataFrame, we mostly interact with columns, performing functions on them.\n\nWe still can access the rows of a DataFrame via RDD if necessary."
  },
  {
    "objectID": "danl-lec/danl-320-lec-05-2025-0205.html#rdd-and-pyspark-dataframe",
    "href": "danl-lec/danl-320-lec-05-2025-0205.html#rdd-and-pyspark-dataframe",
    "title": "Lecture 5",
    "section": "RDD and PySpark DataFrame",
    "text": "RDD and PySpark DataFrame\n\n\n\n\n\n\nIn the RDD, we think of each row as an independent entity.\nWith the DataFrame, we mostly interact with columns, performing functions on them.\n\nWe still can access the rows of a DataFrame via RDD if necessary."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#generating-descriptive-statistics-with-df.describe.show",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#generating-descriptive-statistics-with-df.describe.show",
    "title": "Lecture 6",
    "section": "Generating Descriptive Statistics with df.describe().show()",
    "text": "Generating Descriptive Statistics with df.describe().show()\ndf.describe().show()\n\ndf.describe() computes summary statistics (e.g., count, mean, stddev, min, max) for the DataFrame‚Äôs numeric columns.\n.show() prints these statistics in a readable table format."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-cast-method-2",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-cast-method-2",
    "title": "Lecture 6",
    "section": "Converting Data Types with the cast() Method",
    "text": "Converting Data Types with the cast() Method\n\n\nto_date() can be used with a given string format (e.g., ‚ÄúM/d/yy‚Äù)\n\n\n# Convert to date or timestamp\nfrom pyspark.sql.functions import to_date\n\n# To have 19xx years, not 20xx ones.\nspark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \n\n# Casting the \"Birthday\" column to a date type\ndf = df.withColumn(\"DateOfBirth_ts\", to_date(\"Birthday\", \"M/d/yy\"))"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-to_date-method",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#converting-data-types-with-the-to_date-method",
    "title": "Lecture 6",
    "section": "Converting Data Types with the to_date() Method",
    "text": "Converting Data Types with the to_date() Method\n\n\nto_date() can be used with a given string format (e.g., ‚ÄúM/d/yy‚Äù)\n\n\n# Convert to date or timestamp\nfrom pyspark.sql.functions import to_date\n\n# To have 19xx years, not 20xx ones.\nspark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \n\n# Casting the \"Birthday\" column to a date type\ndf = df.withColumn(\"DateOfBirth_ts\", to_date(\"Birthday\", \"M/d/yy\"))"
  },
  {
    "objectID": "danl-cw/danl-320-cw-07.html",
    "href": "danl-cw/danl-320-cw-07.html",
    "title": "Classwork 7",
    "section": "",
    "text": "The dataset ,cereals_oatmeal.csv,(with its pathname https://bcdanl.github.io/data/cereal_oatmeal.csv) is a listing of 76 popular breakfast cereals and oatmeal.\n\ncereal = pd.read_csv('https://bcdanl.github.io/data/cereal_oatmeal.csv')\n\n\n\n\n\n  \n\n\n\nUse PySpark to solve this classwork."
  },
  {
    "objectID": "danl-cw/danl-320-cw-07.html#direction",
    "href": "danl-cw/danl-320-cw-07.html#direction",
    "title": "Classwork 7",
    "section": "",
    "text": "The dataset ,cereals_oatmeal.csv,(with its pathname https://bcdanl.github.io/data/cereal_oatmeal.csv) is a listing of 76 popular breakfast cereals and oatmeal.\n\ncereal = pd.read_csv('https://bcdanl.github.io/data/cereal_oatmeal.csv')\n\n\n\n\n\n  \n\n\n\nUse PySpark to solve this classwork."
  },
  {
    "objectID": "danl-cw/danl-320-cw-07.html#question-1",
    "href": "danl-cw/danl-320-cw-07.html#question-1",
    "title": "Classwork 7",
    "section": "Question 1",
    "text": "Question 1\nGroup the cereal DataFrame, using the Manufacturer variable.\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-07.html#question-2",
    "href": "danl-cw/danl-320-cw-07.html#question-2",
    "title": "Classwork 7",
    "section": "Question 2",
    "text": "Question 2\nDetermine the total number of groups, and the number of cereals per group.\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-07.html#question-3",
    "href": "danl-cw/danl-320-cw-07.html#question-3",
    "title": "Classwork 7",
    "section": "Question 3",
    "text": "Question 3\nExtract the cereals that belong to the manufacturer \"Kellogg's\".\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-07.html#question-4",
    "href": "danl-cw/danl-320-cw-07.html#question-4",
    "title": "Classwork 7",
    "section": "Question 4",
    "text": "Question 4\nCalculate the average of values in the Calories, Fiber, and Sugars variables for each manufacturer.\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-07.html#question-5",
    "href": "danl-cw/danl-320-cw-07.html#question-5",
    "title": "Classwork 7",
    "section": "Question 5",
    "text": "Question 5\nFind the maximum value in the Sugars variable for each manufacturer.\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-07.html#question-6",
    "href": "danl-cw/danl-320-cw-07.html#question-6",
    "title": "Classwork 7",
    "section": "Question 6",
    "text": "Question 6\nFind the minimum value in the Fiber variable for each manufacturer.\nAnswer:"
  },
  {
    "objectID": "danl-cw/danl-320-cw-07.html#question-7",
    "href": "danl-cw/danl-320-cw-07.html#question-7",
    "title": "Classwork 7",
    "section": "Question 7",
    "text": "Question 7\n\nCalculate a ‚ÄòNormalized_Sugars‚Äô variable for each product by Manufacturer, where the normalization formula is\n\n\\[\n\\text{Normalized\\_Sugars} = \\frac{\\text{Sugars} - \\text{mean(Sugars)}}{\\text{std(Sugars)}}\n\\]\nfor each Manufacturer group. This formula adjusts the sugar content of each product by subtracting the mean sugar content of its manufacturer and then dividing by the standard deviation of the sugar content within its manufacturer.\nAnswer:"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#group-operations-1",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#group-operations-1",
    "title": "Lecture 6",
    "section": "Group Operations",
    "text": "Group Operations\n\nIn PySpark, we use groupBy() (similar to Pandas‚Äô groupby()) to aggregate, analyze, or transform data at a grouped level.\nA GroupedData object is returned, which can then be used with aggregation methods such as sum(), avg(), count(), etc."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#creating-a-groupby-object",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#creating-a-groupby-object",
    "title": "Lecture 6",
    "section": "Creating a groupBy Object",
    "text": "Creating a groupBy Object\n\n\nWe can create a Spark DataFrame from a list (or other data sources like CSV, Parquet, etc.).\nThen call groupBy(\"Type\") on the DataFrame.\n\n\nfood_data = [\n    (\"Apple\", \"Fruit\", 1.05),\n    (\"Onion\", \"Vegie\", 1.00),\n    (\"Orange\", \"Fruit\", 1.25),\n    (\"Tomato\", \"Vegie\", 0.85),\n    (\"Watermelon\", \"Fruit\", 4.15)\n]\n\nfood_df = spark.createDataFrame(food_data, [\"Item\", \"Type\", \"Price\"])\n\n# Group by \"Type\"\ngroups = food_df.groupBy(\"Type\")\n\nIn this example, there are two types of items: ‚ÄúFruit‚Äù and ‚ÄúVegie‚Äù."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#aggregation-on-groups",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#aggregation-on-groups",
    "title": "Lecture 6",
    "section": "Aggregation on Groups",
    "text": "Aggregation on Groups\n# Calculate the average Price for each Type\ngroups.avg(\"Price\").show()\n\n# Calculate the sum of the Price for each Type\ngroups.sum(\"Price\").show()\n\n# Count how many rows in each Type\ngroups.count().show()\n\nThese group-based operations are executed once an action (like .show()) is called."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#group-aggregation-with-multiple-columns",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#group-aggregation-with-multiple-columns",
    "title": "Lecture 6",
    "section": "Group Aggregation with Multiple Columns",
    "text": "Group Aggregation with Multiple Columns\nfrom pyspark.sql.functions import min, max, mean\n\nfood_df.groupBy(\"Type\").agg(\n    min(\"Price\").alias(\"min_price\"),\n    max(\"Price\").alias(\"max_price\"),\n    mean(\"Price\").alias(\"mean_price\")\n).show()\n\nWe can pass multiple aggregations to .agg() to get multiple results at once."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#adding-group-level-statistics-to-each-row",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#adding-group-level-statistics-to-each-row",
    "title": "Lecture 6",
    "section": "Adding Group-Level Statistics to Each Row",
    "text": "Adding Group-Level Statistics to Each Row\n\n\nIn Pandas, .transform() is often used to add group-level statistics back onto the original DataFrame.\nIn PySpark, we typically use a Window function with the aggregated DataFrame.\n\n\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import avg, col\n\n# Define a window partitioned by \"Type\"\nw = Window.partitionBy(\"Type\")\n\nfood_df_with_mean = food_df.withColumn(\n    \"mean_price_by_type\",\n    avg(col(\"Price\")).over(w)\n)\nfood_df_with_mean.show()\n\nThis keeps each original row, adding the group-level mean price for its corresponding Type."
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-7",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#pyspark-basics-7",
    "title": "Lecture 6",
    "section": "PySpark Basics",
    "text": "PySpark Basics\nLet‚Äôs do Classwork 7!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-06-2025-0210.html#removing-duplicates-with-distinct",
    "href": "danl-lec/danl-320-lec-06-2025-0210.html#removing-duplicates-with-distinct",
    "title": "Lecture 6",
    "section": "Removing Duplicates with distinct()",
    "text": "Removing Duplicates with distinct()\n\n\nIn PySpark, the distinct() method returns a new DataFrame with duplicate rows removed.\nIt is similar to the SQL SELECT DISTINCT command.\n\n\ndf.select(\"Team\", \"Position\").distinct().show()\n\nAfter applying distinct(), only unique observation remain."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression",
    "title": "Lecture 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nModels and Assumptions\nLinear Model\n\nLinear regression assumes a linear relationship for \\(Y = f(X_{1})\\):\n\n\\[Y_{i} \\,=\\, \\beta_{0} \\,+\\, \\beta_{1} X_{1, i} \\,+\\, \\epsilon_{i}\\] for \\(i \\,=\\, 1, 2, \\dots, n\\), where \\(i\\) is the \\(i\\)-th observation in data.\n\n\\(Y_i\\) is the \\(i\\)-th value for the outcome/dependent/response/target variable \\(Y\\).\n\\(X_{1, i}\\) is the \\(i\\)-th value for the explanatory/independent/predictor/input variable or feature \\(X_{1}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-1",
    "title": "Lecture 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nModels and Assumptions\nBeta coefficients\n\nLinear regression assumes a linear relationship for \\(Y = f(X_{1})\\):\n\n\\[Y_{i} \\,=\\, \\beta_{0} \\,+\\, \\beta_{1} X_{1, i} \\,+\\, \\epsilon_{i}\\] for \\(i \\,=\\, 1, 2, \\dots, n\\), where \\(i\\) is the \\(i\\)-th observation in data.\n\n\\(\\beta_0\\) is an unknown true value of an intercept: average value for \\(Y\\) if \\(X_{1} = 0\\)\n\\(\\beta_1\\) is an unknown true value of a slope: increase in average value for \\(Y\\) for each one-unit increase in \\(X_{1}\\)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-2",
    "title": "Lecture 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nModels and Assumptions\nRandom Noises\n\nLinear regression assumes a linear relationship for \\(Y = f(X_{1})\\):\n\n\\[Y_{i} \\,=\\, \\beta_{0} \\,+\\, \\beta_{1} X_{1, i} \\,+\\, \\epsilon_{i}\\] for \\(i \\,=\\, 1, 2, \\dots, n\\), where \\(i\\) is the \\(i\\)-th observation in data.\n\n\\(\\epsilon_i\\) is a random noise, or a statistical error:\n\n\\[\n\\epsilon_i \\sim N(0, \\sigma^2)\n\\]\n\nErrors have a mean value of 0 with constant variance \\(\\sigma^2\\).\nErrors are uncorrelated with \\(X_{1, i}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-3",
    "title": "Lecture 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhat Is Linear Regression Doing?\nBest Fitting Line\n\nLinear regression finds the beta estimates \\(( \\hat{\\beta_{0}}, \\hat{\\beta_{1}} )\\) such that:\n‚Äì The linear function \\(f(X_{1}) = \\hat{\\beta_{0}} + \\hat{\\beta_{1}}X_{1}\\) is as near as possible to \\(Y\\) for all \\((X_{1, i}\\,,\\, Y_{i})\\) pairs in the data.\n\nIt is the best fitting line, or the predicted outcome, \\(\\hat{Y_{\\,}} = \\hat{\\beta_{0}} + \\hat{\\beta_{1}}X_{1}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-4",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-4",
    "title": "Lecture 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhat Is Linear Regression Doing?\nResidual errors\n\nThe estimated beta coefficients are chosen to minimize the sum of squares of the residual errors \\((SSR)\\): \\[\n\\begin{align}\nSSR &\\,=\\, (\\texttt{Residual_Error}_{1})^{2}\\\\\n&\\quad \\,+\\, (\\texttt{Residual_Error}_{2})^{2}\\\\\n&\\quad\\,+\\, \\cdots + (\\texttt{Residual_Error}_{n})^{2}\\\\\n\\text{where}\\qquad\\qquad\\qquad\\qquad&\\\\\n\\texttt{Residual_Error}_{i} &\\,=\\, Y_{i} \\,-\\, \\hat{Y_{i}},\\\\\n\\texttt{Predicted_Outcome}_{i}: \\hat{Y_{i}} &\\,=\\, \\hat{\\beta_{0}} \\,+\\, \\hat{\\beta_{1}}X_{1, i}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-5",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-5",
    "title": "Lecture 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhat Is Linear Regression Doing?\nHat Notation\n\nWe use the hat notation \\((\\,\\hat{\\texttt{ }_{\\,}}\\,)\\) to distinguish true values and estimated/predicted values.\nThe value of true beta coefficient is denoted by \\(\\beta_{1}\\).\nThe value of estimated beta coefficient is denoted by \\(\\hat{\\beta_{1}}\\).\nThe \\(i\\)-th value of true outcome variable is denoted by \\(Y_{i}\\).\nThe \\(i\\)-th value of predicted outcome variable is denoted by \\(\\hat{Y_{i}}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-6",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-6",
    "title": "Lecture 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhat Is Linear Regression Doing?\nRelationship\n1. Finding the relationship between \\(X_{1}\\) and \\(Y\\) \\[\\hat{\\beta_{1}}\\]: How is an increase in \\(X_1\\) by one unit associated with a change in \\(Y\\) on average? - Positive? Negative? Independent? - How strong?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-7",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-7",
    "title": "Lecture 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nStatistical Significance in Estimated Beta Coefficients\n\nWhat does it mean for a beta estimate \\(\\hat{\\beta_{\\,}}\\) to be statistically significant at 5% level?\n\nIt means that the null hypothesis \\(H_{0}: \\beta = 0\\) is rejected for a given significance level 5%.\n‚Äú2 standard error rule‚Äù of thumb: The true value of \\(\\beta\\) is 95% likely to be in the confidence interval \\((\\, \\hat{\\beta_{\\,}} - 2 * \\texttt{Std. Error}\\;,\\; \\hat{\\beta_{\\,}} + 2 * \\texttt{Std. Error} \\,)\\).\nThe standard error tells us how uncertain our beta estimate is.\nWe should look for the stars!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-8",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-8",
    "title": "Lecture 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhat Is Linear Regression Doing?\nPrediction\n2. Making a prediction on \\(Y\\): \\[\\hat{Y_{\\,}}\\] For unseen data point of \\(X_1\\), what is the predicted value of outcome, \\(\\hat{Y_{\\,}}\\)?\n\nE.g., For \\(X_{1} = 2\\), the predicted outcome is \\(\\hat{Y_{\\,}} = \\hat{\\beta_{0}} + \\hat{\\beta_{1}} \\times 2\\).\nE.g., For \\(X_{1} = 3\\), the predicted outcome is \\(\\hat{Y_{\\,}} = \\hat{\\beta_{0}} + \\hat{\\beta_{1}} \\times 3\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-9",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-9",
    "title": "Lecture 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nExample\n\nSuppose we want to predict a property‚Äôs sales price based on the property size.\n\nIn other words, for some house sale i, we want to predict sale_price[i] based on gross_square_feet[i].\n\nWe also want to focus on the relationship between a property‚Äôs sales price and a property size.\n\nIn other words, we estimate how an increase in gross_square_feet[i] is associated with sale_price[i]."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-10",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-10",
    "title": "Lecture 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nLinear Relationship\n\nLinear regression assumes that:\n\nThe outcome sale_price[i] is linearly related to the input gross_square_feet[i]:\n\n\n\\[\\texttt{sale_price[i]} \\;=\\quad \\texttt{b0} \\,+\\, \\texttt{b1*gross_square_feet[i]} \\,+\\, \\texttt{e[i]}\\] where e[i] is a statistical error term."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#best-fitting-line-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#best-fitting-line-1",
    "title": "Lecture 7",
    "section": "Best Fitting Line",
    "text": "Best Fitting Line\n\n\nWhat do the vertical lines visualize?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-11",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-11",
    "title": "Lecture 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nThe Linear Relationship between sale_price and gross_square_feet"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-12",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-12",
    "title": "Lecture 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nBest Fitting Line\n\n\nWhat do the vertical lines visualize?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-13",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-13",
    "title": "Lecture 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nGoals of Linear Regression\n\nThe goals of linear regression here are to:\n\n\nFind the relationship between gross_square_feet and sale_price by estimating a true value of b1.\n\n\nThe estimated value of b1 is denoted by \\(\\hat{\\texttt{b1}}\\).\n\n\nMake a prediction on sale_price[i] for new property i\n\n\nThe predicted value of sale_price[i] is denoted by \\(\\widehat{\\texttt{sale_price}}\\texttt{[i]}\\), where\n\n\\[\\widehat{\\texttt{sale_price}}\\texttt{[i]} \\;=\\quad \\hat{\\texttt{b0}} \\,+\\, \\hat{\\texttt{b1}}\\texttt{*gross_square_feet[i]}\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-14",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-14",
    "title": "Lecture 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nTraining and Test Data\n\nTraining data: When we‚Äôre building a linear regression model, we need data to train the model.\nTest data: We also need data to test whether the model works well on new data.\n\n\n\n\nSo, we start with splitting a given data.frame into training and test data.frames when building a linear regression model."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-15",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-15",
    "title": "Lecture 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nTraining and Test Data\nTraining vs.¬†Test\n\nWe use training data to train/fit the linear regression model.\n\nWe then make a prediction using test data, which are unseen/new from the viewpoint of the trained linear regression model.\n\nIn this way, we can test whether our model performs well in the real world, where unseen data points exist."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-16",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-16",
    "title": "Lecture 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nTraining and Test Data\nModel Construction and Evaluation"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-17",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-17",
    "title": "Lecture 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nExample of Linear Regression using PySpark\n\nWe will use the data for residential property sales from September 2017 and August 2018 in NYC.\nEach sales data recorded contains a number of interesting variables, but here we focus on the followings:\n\nsale_price: a property‚Äôs sales price;\ngross_square_feet: a property‚Äôs size;\nage: a property‚Äôs age;\nborough_name: a borough where a property is located.\n\nUse summary statistics and visualization to explore the data."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-18",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-18",
    "title": "Lecture 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nSplitting Data into Training and Testing Data\nStep 1. Importing Modules and Reading DataFrames\nimport pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, when\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n# 1. Read CSV data from URL\ndf_pd = pd.read_csv('https://bcdanl.github.io/data/home_sales_nyc.csv')\nsale_df = spark.createDataFrame(df_pd)\nsale_df.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-19",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-19",
    "title": "Lecture 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nA Little Bit of Statistics for the Uniform Distribution\n\n\n\nThe probability density function for the uniform distribution looks like:\nWith the uniform distribution, any values of \\(x\\) between 0 and 1 is equally likely drawn.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will use the uniform distribution when splitting data into training and testing data sets."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-20",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-20",
    "title": "Lecture 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nRandomization in the Sampling Process\n\nWhy do we randomize when splitting given data into training and test data?\n\nRandomizing the sampling process ensures that the training and test data sets are representative for the population data.\nIf the sample does not properly represent the entire population, the model result is biased toward the sample.\n\nSuppose the splitting process is not randomized, so that the observations with sale_price &gt; 10^6 are in the training data and the observations with sale_price &lt;= 10^6 are in the test data.\n\nWhat would be the model result then?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-using-pyspark-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-using-pyspark-1",
    "title": "Lecture 7",
    "section": "Linear Regression using PySpark",
    "text": "Linear Regression using PySpark\nDummy variables\n\nThe model does not include borough_nameManhattan, because \\(\\texttt{borough_nameManhattan[i]}\\) is represented by a combination of \\(\\texttt{borough}\\) variables:\n\n\\(\\texttt{borough_nameManhattan[i]} = 0\\) if any of the \\(\\texttt{borough_name}\\) variables is 1.\n\\(\\texttt{borough_nameManhattan[i]} = 1\\) if all of the \\(\\texttt{borough_name}\\) variables are 0.\n\nThe level Manhattan is a reference level when interpreting the beta estimate for the borough_name variables."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-using-pyspark-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-using-pyspark-2",
    "title": "Lecture 7",
    "section": "Linear Regression using PySpark",
    "text": "Linear Regression using PySpark\nMake the Summary Pretty\nimport numpy as np\nimport scipy.stats as stats\nfrom tabulate import tabulate\n\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler,\n    and inserts a dashed horizontal line after the Intercept row.\n    \n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute).\n        assembler: The VectorAssembler used to assemble the features for the model.\n    \n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Extract coefficients and all standard errors as NumPy arrays\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n    \n    # Check if the intercept's standard error is included (one extra element)\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n    \n    # Compute t-statistics for feature coefficients (t = beta / SE(beta))\n    t_stats = coeffs / std_errors\n    \n    # Degrees of freedom: number of instances minus number of predictors minus 1 (for intercept)\n    df = model.summary.numInstances - len(coeffs) - 1\n    \n    # Compute two-tailed p-values for each feature coefficient\n    p_values = [2 * (1 - stats.t.cdf(np.abs(t), df)) for t in t_stats]\n    \n    # Function to assign significance stars based on p-value\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n    \n    # Build the table: one row per feature coefficient using the assembler's input columns\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        table.append([\n            \"Beta: \" + feature,  # Metric name\n            beta,                # Beta estimate\n            se,                  # Standard error\n            p,                   # p-value\n            significance_stars(p)  # Significance stars\n        ])\n    \n    # Compute and add the intercept row with its SE, p-value, and significance (if available)\n    if intercept_se is not None:\n        intercept_t = model.intercept / intercept_se\n        intercept_p = 2 * (1 - stats.t.cdf(np.abs(intercept_t), df))\n        intercept_sig = significance_stars(intercept_p)\n    else:\n        intercept_se = \"\"\n        intercept_p = \"\"\n        intercept_sig = \"\"\n    \n    table.append([\"Intercept\", model.intercept, intercept_se, intercept_p, intercept_sig])\n    \n    # Append additional overall model metrics (with extra columns left blank)\n    table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\"])\n    table.append([\"R¬≤\", model.summary.r2, \"\", \"\", \"\"])\n    \n    # Format numeric values to 4 decimal places when applicable\n    formatted_table = [\n        [f\"{item:.4f}\" if isinstance(item, (int, float, np.floating)) and item != \"\" else item\n         for item in row]\n        for row in table\n    ]\n    \n    # Generate the table string using tabulate\n    table_str = tabulate(\n        formatted_table,\n        headers=[\"Metric\", \"Value\", \"Std. Error\", \"p-value\", \"Significance\"],\n        tablefmt=\"pretty\",\n        colalign=(\"left\", \"right\", \"right\", \"right\", \"center\")\n    )\n    \n    # Split the table string into lines\n    lines = table_str.split(\"\\n\")\n    \n    # Determine the width of the table from the first line\n    dash_line = '-' * len(lines[0])\n    \n    # Find the line that contains \"Intercept\" (skip border lines)\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            # Insert the dash_line after the intercept row\n            lines.insert(i+1, dash_line)\n            break\n    \n    return \"\\n\".join(lines)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-21",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-21",
    "title": "Lecture 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nSplitting Data into Training and Testing Data\nStep 2. rand(seed = ANY_NUMBER)\n# 2. Split data into training and testing sets by creating a random column \"gp\"\nsale_df = sale_df.withColumn(\"gp\", rand(seed=123)) # seed is set for replication\n\n# Splits 50-50 into training and test sets \ndtrain = sale_df.filter(col(\"gp\") &gt;= 0.5) \ndtest = sale_df.filter(col(\"gp\") &lt; 0.5)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression",
    "title": "Lecture 7",
    "section": "Multiple Regression",
    "text": "Multiple Regression\n\nWhat if the regression were missing something?\n\nMaybe prices are not just about size, but maybe there are certain parts of NYC that are categorically more expensive than other parts of NYC.\nMaybe Manhattan is just more expensive than Bronx.\nMaybe apartments are different than non-apartments.\nMaybe old houses are different than new houses.\n\nIt is often helpful to bring in multiple explanatory variables‚Äîa Multivariate Regression."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression-1",
    "title": "Lecture 7",
    "section": "Multiple Regression",
    "text": "Multiple Regression\nBest Fitting Plane\n\n\n\n\n\n\n\n\n\n\nAll else being equal, an increase in gross_square_feet by one unit is associated with an increase in sale_price by \\(\\hat{\\beta_{1}}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression-2",
    "title": "Lecture 7",
    "section": "Multiple Regression",
    "text": "Multiple Regression\nModels and Assumptions\nRandom Noises\n\nLinear regression assumes a linear relationship for \\(Y = f(X_{1}, X_{2})\\):\n\n\\[Y_{i} \\,=\\, \\beta_{0} \\,+\\, \\beta_{1} X_{1, i}\\,+\\, \\beta_{1} X_{2, i} \\,+\\, \\epsilon_{i}\\] for \\(i \\,=\\, 1, 2, \\dots, n\\), where \\(i\\) is the \\(i\\)-th observation in data.\n\n\\(\\epsilon_i\\) is a random noise, or a statistical error:\n\n\\[\n\\epsilon_i \\sim N(0, \\sigma^2)\n\\]\n\nErrors have a mean value of 0 with constant variance \\(\\sigma^2\\).\nErrors are uncorrelated with \\(X_{1, i}\\) and with \\(X_{2, i}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression-3",
    "title": "Lecture 7",
    "section": "Multiple Regression",
    "text": "Multiple Regression\nModels and Assumptions\nBest Fitting Plane\n\nLinear regression finds the beta estimates \\(( \\hat{\\beta_{0}}, \\hat{\\beta_{1}}, \\hat{\\beta}_{2} )\\) such that:\n‚Äì The linear function \\(f(X_{1}, X_{2}) = \\hat{\\beta_{0}} + \\hat{\\beta_{1}}X_{1} + \\hat{\\beta}_{2}X_{2}\\) is as near as possible to \\(Y\\) for all \\((X_{1, i}\\,,\\,X_{2, i}\\,,\\, Y_{i})\\) pairs in the data.\n\nIt is the best fitting plane, or the predicted outcome, \\(\\hat{Y_{\\,}} = \\hat{\\beta_{0}} + \\hat{\\beta_{1}}X_{1} + \\hat{\\beta}_{2}X_{2}\\)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression-4",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression-4",
    "title": "Lecture 7",
    "section": "Multiple Regression",
    "text": "Multiple Regression\nModels and Assumptions\nResidual Errors\n\nThe estimated beta coefficients are chosen to minimize the sum of squares of the residual errors \\((SSR)\\): \\[\n\\begin{align}\nSSR &\\,=\\, (\\texttt{Residual_Error}_{1})^{2}\\\\\n&\\quad \\,+\\, (\\texttt{Residual_Error}_{2})^{2}\\\\\n&\\quad\\,+\\, \\cdots + (\\texttt{Residual_Error}_{n})^{2}\\\\\n\\text{where}\\qquad\\qquad\\qquad\\qquad&\\\\\n\\texttt{Residual_Error}_{i} &\\,=\\, Y_{i} \\,-\\, \\hat{Y_{i}},\\\\\n\\texttt{Predicted_Outcome}_{i}: \\hat{Y_{i}} &\\,=\\, \\hat{\\beta_{0}} \\,+\\, \\hat{\\beta_{1}}X_{1, i} \\,+\\, \\hat{\\beta_{1}}X_{2, i}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression-using-pyspark",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression-using-pyspark",
    "title": "Lecture 7",
    "section": "Multiple Regression using PySpark",
    "text": "Multiple Regression using PySpark\n\nLet‚Äôs add a new predictor, age, to the model.\n\nassembler2 = VectorAssembler(\n                    inputCols=[\"gross_square_feet\", \"age\"], \n                    outputCol=\"predictors\")\ndtrain2 = assembler2.transform(dtrain)\ndtest2  = assembler2.transform(dtest)\n\nmodel2 = LinearRegression(\n                featuresCol=\"predictors\",\n                labelCol=\"sale_price\")\n        .fit(dtrain2)\ndtest2 = model2.transform(dtest2) \n\nprint(regression_table(model2, assembler2))"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression-using-pyspark-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression-using-pyspark-1",
    "title": "Lecture 7",
    "section": "Multiple Regression using PySpark",
    "text": "Multiple Regression using PySpark\nBest Fitting Plane"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression-using-pyspark-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression-using-pyspark-2",
    "title": "Lecture 7",
    "section": "Multiple Regression using PySpark",
    "text": "Multiple Regression using PySpark\n\nWe can have more than two explanatory variables \\((\\,X_{1}, X_{2}, X_{3}, \\cdots\\,)\\) in the linear regression model.\nLet‚Äôs add a new explanatory variable, borough_name to the model.\n\n\nmodel_3 &lt;- lm(formula = sale_price ~ gross_square_feet + age + borough_name, \n              data = dtrain)\ndtest$pred_3 &lt;-  predict(model_3, \n                         newdata = dtest)\nsummary(model_3)   \n\n\nCall:\nlm(formula = sale_price ~ gross_square_feet + age + borough_name, \n    data = dtrain)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3464383  -182991   -16702   142074  8426853 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               -2.349e+05  3.183e+04  -7.379 1.79e-13 ***\ngross_square_feet          4.233e+02  9.466e+00  44.718  &lt; 2e-16 ***\nage                        2.116e+02  2.348e+02   0.901    0.367    \nborough_nameBrooklyn       4.947e+05  2.275e+04  21.748  &lt; 2e-16 ***\nborough_nameManhattan      2.695e+06  9.646e+04  27.937  &lt; 2e-16 ***\nborough_nameQueens         2.537e+05  2.023e+04  12.539  &lt; 2e-16 ***\nborough_nameStaten Island  1.075e+05  2.277e+04   4.722 2.39e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 427200 on 6363 degrees of freedom\nMultiple R-squared:  0.3854,    Adjusted R-squared:  0.3848 \nF-statistic: 664.9 on 6 and 6363 DF,  p-value: &lt; 2.2e-16\n\n\n\nWhat happened?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-using-pyspark-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-using-pyspark-3",
    "title": "Lecture 7",
    "section": "Linear Regression using PySpark",
    "text": "Linear Regression using PySpark\nMake the Summary Pretty\n# Using the UDF, regression_table(model, assembler)\nprint(regression_table(model1, assembler1))"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-using-pyspark-4",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-using-pyspark-4",
    "title": "Lecture 7",
    "section": "Linear Regression using PySpark",
    "text": "Linear Regression using PySpark\nDummy variables\n\nThe model does not include borough_nameBronx, because \\(\\texttt{borough_nameBronx[i]}\\) is represented by a combination of \\(\\texttt{borough}\\) variables:\n\n\\(\\texttt{borough_nameBronx[i]} = 0\\) if any of the \\(\\texttt{borough_name}\\) variables is 1.\n\\(\\texttt{borough_nameBronx[i]} = 1\\) if all of the \\(\\texttt{borough_name}\\) variables are 0.\n\nThe level Bronx is a reference level when interpreting the beta estimate for the borough_name variables."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-using-pyspark-5",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-using-pyspark-5",
    "title": "Lecture 7",
    "section": "Linear Regression using PySpark",
    "text": "Linear Regression using PySpark\nSetting a reference level\n\nIf a linear regression model includes a categorical variable (factor or character variable), we can consider setting a reference level of a factor variable using relevel(VARIABLE, ref = \"LEVEL\").\n\n\ndtrain &lt;- dtrain %&gt;% \n  mutate(borough_name = factor(borough_name), # borough_name is given as character type.\n         borough_name = relevel(borough_name,\n                                \"Manhattan\") )\nmodel_4 &lt;- lm(sale_price ~ gross_square_feet + age + borough_name, \n              data = dtrain)\nsummary(model_4)   \n\n\nCall:\nlm(formula = sale_price ~ gross_square_feet + age + borough_name, \n    data = dtrain)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3464383  -182991   -16702   142074  8426853 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                2.460e+06  1.020e+05  24.121   &lt;2e-16 ***\ngross_square_feet          4.233e+02  9.466e+00  44.718   &lt;2e-16 ***\nage                        2.116e+02  2.348e+02   0.901    0.367    \nborough_nameBronx         -2.695e+06  9.646e+04 -27.937   &lt;2e-16 ***\nborough_nameBrooklyn      -2.200e+06  9.541e+04 -23.059   &lt;2e-16 ***\nborough_nameQueens        -2.441e+06  9.522e+04 -25.636   &lt;2e-16 ***\nborough_nameStaten Island -2.587e+06  9.636e+04 -26.849   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 427200 on 6363 degrees of freedom\nMultiple R-squared:  0.3854,    Adjusted R-squared:  0.3848 \nF-statistic: 664.9 on 6 and 6363 DF,  p-value: &lt; 2.2e-16\n\n\n\nThe level Manhattan now becomes a reference level.\n\nAccordingly, the interpretation of beta estimates for borough_name variables change."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-using-pyspark-6",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-using-pyspark-6",
    "title": "Lecture 7",
    "section": "Linear Regression using PySpark",
    "text": "Linear Regression using PySpark\nSetting a reference level\n\nChanging a reference level of a factor variable does not change the regression result.\n\ndtest$pred_4 &lt;-  predict(model_4, \n                         newdata = dtest)\n\nCompare pred_3 with pred_4."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-using-pyspark-7",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-using-pyspark-7",
    "title": "Lecture 7",
    "section": "Linear Regression using PySpark",
    "text": "Linear Regression using PySpark\nSetting a reference level\n\nChanging a reference level of a factor variable does not change the regression result.\n\ndtest$pred_4 &lt;-  predict(model_4, \n                         newdata = dtest)\n\nCompare pred_3 with pred_4."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-the-output-of-summary",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-the-output-of-summary",
    "title": "Lecture 7",
    "section": "Interpreting the output of summary()",
    "text": "Interpreting the output of summary()\nEstimated Beta Coefficients\nborough_nameBronx\n\nAll else being equal, an increase in borough_nameBronx by one unit is associated with an increase in sale_price by b3.\nAll else being equal, being in Bronx relative to being a in Manhattan is associated with a decrease in sale_price by |b2|."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-the-output-of-summary-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-the-output-of-summary-1",
    "title": "Lecture 7",
    "section": "Interpreting the output of summary()",
    "text": "Interpreting the output of summary()\nEstimated Beta Coefficients\ngross_square_feet\n\nAll else being equal, an increase in gross_square_feet by one unit is associated with an increase in sale_price by b1."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-the-output-of-summary-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-the-output-of-summary-2",
    "title": "Lecture 7",
    "section": "Interpreting the output of summary()",
    "text": "Interpreting the output of summary()\nEstimated Beta Coefficients\nborough_nameBronx\n\nAll else being equal, an increase in borough_nameBronx by one unit is associated with an increase in sale_price by b3.\nAll else being equal, being in Bronx relative to being a in Manhattan is associated with a decrease in sale_price by |b2|."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-estimated-beta-coefficients",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-estimated-beta-coefficients",
    "title": "Lecture 7",
    "section": "Interpreting Estimated Beta Coefficients",
    "text": "Interpreting Estimated Beta Coefficients\nExample\nThe model equation is \\[\\begin{align}\n\\texttt{sale_price[i]} \\;=\\;\\, &\\texttt{b0} \\,+\\,\\\\ &\\texttt{b1*gross_square_feet[i]} \\,+\\,\\texttt{b2*age[i]}\\,+\\,\\\\ &\\texttt{b3*Bronx[i]} \\,+\\,\\texttt{b4*Brooklyn[i]} \\,+\\,\\\\&\\texttt{b5*Queens[i]} \\,+\\,\\texttt{b6*Staten Island[i]}\\,+\\,\\\\ &\\texttt{e[i]}\n\\end{align}\\] - The reference level of borough_name variables is Manhattan."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-estimated-beta-coefficients-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-estimated-beta-coefficients-1",
    "title": "Lecture 7",
    "section": "Interpreting Estimated Beta Coefficients",
    "text": "Interpreting Estimated Beta Coefficients\n1. gross_square_feet\n\nConsider the predicted sales prices of the two houses, A and B.\n\nBoth A and B are in Bronx and with the same age.\ngross_square_feet of house A is 2001, while that of house B is 2000.\n\nAll else being equal, an increase in gross_square_feet by one unit is associated with an increase in sale_price by \\(\\hat{\\beta_{1}}\\).\n\nWhy?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-estimated-beta-coefficients-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-estimated-beta-coefficients-2",
    "title": "Lecture 7",
    "section": "Interpreting Estimated Beta Coefficients",
    "text": "Interpreting Estimated Beta Coefficients\n1. gross_square_feet\n\\[\n\\begin{align}\\widehat{\\texttt{sale_price[A]}} \\;=\\quad& \\hat{\\texttt{b0}} \\,+\\, \\hat{\\texttt{b1}}\\texttt{*gross_square_feet[A]} \\,+\\, \\hat{\\texttt{b2}}\\texttt{*age[A]} \\,+\\,\\\\\n&\\hat{\\texttt{b3}}\\texttt{*Bronx[A]}\\,+\\,\\hat{\\texttt{b4}}\\texttt{*Brooklyn[A]} \\,+\\,\\\\ &\\hat{\\texttt{b5}}\\texttt{*Queens[A]}\\,+\\, \\hat{\\texttt{b6}}\\texttt{*Staten Island[A]}\\\\\n\\widehat{\\texttt{sale_price[B]}} \\;=\\quad& \\hat{\\texttt{b0}} \\,+\\, \\hat{\\texttt{b1}}\\texttt{*gross_square_feet[B]} \\,+\\, \\hat{\\texttt{b2}}\\texttt{*age[B]}\\,+\\,\\\\\n&\\hat{\\texttt{b3}}\\texttt{*Bronx[B]}\\,+\\,\n\\hat{\\texttt{b4}}\\texttt{*Brooklyn[B]} \\,+\\,\\\\ &\\hat{\\texttt{b5}}\\texttt{*Queens[B]}\\,+\\, \\hat{\\texttt{b6}}\\texttt{*Staten Island[B]} \\end{align}\n\\]\n\\[\n\\begin{align}\\Leftrightarrow\\qquad&\\widehat{\\texttt{sale_price[A]}} \\,-\\, \\widehat{\\texttt{sale_price[B]}}\\qquad  \\\\\n\\;=\\quad &\\hat{\\texttt{b1}}\\texttt{*}(\\texttt{gross_square_feet[A]} - \\texttt{gross_square_feet[B]})\\\\\n\\;=\\quad &\\hat{\\texttt{b1}}\\texttt{*}\\texttt{(2001 - 2000)} \\,=\\, \\hat{\\texttt{b1}}\\qquad\\qquad\\quad\\;\\;\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-the-output-of-summary-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-the-output-of-summary-3",
    "title": "Lecture 7",
    "section": "Interpreting the output of summary()",
    "text": "Interpreting the output of summary()\nF-statistic\n\nF-statistic is the value of F-test statistic for the hypothesis for overall significance of beta estimates: \\[H_{0}: \\beta_{1} = \\beta_{2} = \\cdots = \\beta_{p} = 0\\]\nIt is possible for all individual coefficients to be insignificant while jointly they are significant."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-the-output-of-summary-4",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-the-output-of-summary-4",
    "title": "Lecture 7",
    "section": "Interpreting the output of summary()",
    "text": "Interpreting the output of summary()\nR-squared\n\nR-squared is a measure of how well the model ‚Äúfits‚Äù the data, or its ‚Äúgoodness of fit.‚Äù\n\nR-squared can be thought of as what fraction of the y‚Äôs variation is explained by the explanatory variables.\n\nWe want R-squared to be fairly large and R-squareds that are similar on testing and training.\nCaution: R-squared will be higher for models with more explanatory variables, regardless of whether the additional explanatory variables actually improve the model or not."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-1",
    "title": "Lecture 7",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nMean squared error (MSE)\n\nThe root MSE (RMSE) represents the overall deviation of \\(Y_{i}\\) from the best fitting regression line."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-2",
    "title": "Lecture 7",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nR-squared\n\nR-squared is a measure of how well the model ‚Äúfits‚Äù the data, or its ‚Äúgoodness of fit.‚Äù\n\nR-squared can be thought of as what fraction of the y‚Äôs variation is explained by the explanatory variables.\n\nWe want R-squared to be fairly large and R-squareds that are similar on testing and training.\nCaution: R-squared will be higher for models with more explanatory variables, regardless of whether the additional explanatory variables actually improve the model or not."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-3",
    "title": "Lecture 7",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nResidual Plots\n\nWe would like have a residual plot to be\n\nUnbiased: have an average value of zero in any thin vertical strip;\nHomoskedastic, which means ‚Äúsame stretch‚Äù: it is ideal to have the same spread of the residuals in any thin vertical strip."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-4",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-4",
    "title": "Lecture 7",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nResidual Plots"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-5",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-5",
    "title": "Lecture 7",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nResidual Plots"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-6",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-6",
    "title": "Lecture 7",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nResidual Plots\n\nModel equation: \\(Y_{i} \\,=\\, \\beta_{0} \\,+\\, \\beta_{1}X_{1,i}\\)\n\n\\(\\epsilon_i\\) is a random noise, or a statistical error:\n\n\n\\[\n\\epsilon_i \\sim N(0, \\sigma^2)\n\\] - Errors have a mean value of 0 with constant variance \\(\\sigma^2\\). - Errors are uncorrelated with \\(X_{1,i}\\)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-7",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-7",
    "title": "Lecture 7",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nResidual Plots\n\nIf we re-arrange the simple regression equation, \\[\\begin{align}\n{\\epsilon}_{i} \\,=\\, Y_{i} \\,-\\, (\\, {\\beta}_{0} \\,+\\, {\\beta}_{1}X_{1} \\,).\n\\end{align}\\]\n\\(\\texttt{residual_error}_{i}\\) can be thought of as the expected value of \\(\\epsilon_{i}\\), denoted by \\(\\hat{\\epsilon}_{i}\\). \\[\\begin{align}\n\\hat{\\epsilon}_{i} \\,=\\, Y_{i} \\,-\\, (\\, \\hat{\\beta}_{0} \\,+\\, \\hat{\\beta}_{1}X_{1} \\,)\n\\end{align}\\]\nBecause we assume that \\(\\epsilon_{i}\\) have a mean value of 0 with constant variance \\(\\sigma^2\\), a well-behaved residual plot should bounce randomly and form a cloud roughly around the perfect prediction line."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-8",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-8",
    "title": "Lecture 7",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nResidual Plots\n\nFrom the residual plot, we should ask the following the two questions ourselves:\n\nOn average, are the predictions correct?\nAre there systematic errors?\n\nA well-behaved plot will bounce randomly and form a cloud roughly around the perfect prediction line."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-9",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-9",
    "title": "Lecture 7",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nResidual Plots\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn average, are the predictions correct?\nAre there systematic errors?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-10",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-10",
    "title": "Lecture 7",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nResidual Plots\n\nWe would like have a residual plot to be\n\nUnbiased: have an average value of zero in any thin vertical strip;\nHomoskedastic, which means ‚Äúsame stretch‚Äù: it is ideal to have the same spread of the residuals in any thin vertical strip."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-11",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation-11",
    "title": "Lecture 7",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nResidual Plots"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-22",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-22",
    "title": "Lecture 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nBuilding an ML DataFrame using VectorAssembler()\n# Now assemble inputs using the renamed column\nassembler1 = VectorAssembler(\n    inputCols=[\"gross_square_feet\"], \n    outputCol=\"inputs\")\n\ndtrain1 = assembler1.transform(dtrain) # training data\ndtest1  = assembler1.transform(dtest)  # test data\n\nVectorAssembler is a transformer in PySpark‚Äôs ML library that is used to combine multiple columns into a single vector column.\n\nMany ML algorithms in Spark require the inputs to be represented as a single vector.\nVectorAssembler is often one of the first steps in a Spark ML pipeline.\n\nVectorAssembler.transform() returns a DataFrame with a new column, specified in outputCol in VectorAssembler()."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#model-evaluation",
    "title": "Lecture 7",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nMean squared error (MSE)\n\nOne of the most common metrics used to measure the prediction accuracy of a linear regression model is MSE, which stands for mean squared error.\n\n\\(MSE\\) is \\(SSR\\) divided by \\(n\\) (the number of observations in the data that are used in making predictions).\n\n\n\n\n\n\\[\nMSE = SSR / n\n\\] \\[\n\\begin{align}\nSSR &\\,=\\, (\\texttt{Residual_Error}_{1})^{2}\\\\\n&\\quad \\,+\\, (\\texttt{Residual_Error}_{2})^{2}\\\\\n&\\quad\\,+\\, \\cdots + (\\texttt{Residual_Error}_{n})^{2}\n\\end{align}\n\\]\n\n\n\n\nThe lower MSE, the higher accuracy of the model.\n\nThe root MSE (RMSE) is the square root of MSE."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression-5",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression-5",
    "title": "Lecture 7",
    "section": "Multiple Regression",
    "text": "Multiple Regression\nBest Fitting Plane"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-23",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-23",
    "title": "Lecture 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nBuilding a linear regression model using LinearRegression().fit()\n# Fit linear regression model using the new label column \"sale_price\"\nmodel1 = (\n    LinearRegression(\n        featuresCol = \"inputs\", \n        labelCol = \"sale_price\")\n    .fit(dtrain1)\n)\n\nLinearRegression(featuresCol=\"inputs\", labelCol=\"sale_price\")\n\nThis creates an instance of the LinearRegression class.\nThe features (independent variables) are in a column named ‚Äúinputs‚Äù.\nThe label (dependent variable) is in a column named ‚Äúsale_price‚Äù.\n\n.fit() trains (fits) the linear regression model using the training DataFrame dtrain1.\n\nThis training process estimates the coefficients (betas) and intercept that best predict the label from the features."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions",
    "title": "Lecture 7",
    "section": "Models and Assumptions",
    "text": "Models and Assumptions\nLinear Model\n\nLinear regression assumes a linear relationship for \\(Y = f(X_{1})\\):\n\n\\[Y_{i} \\,=\\, \\beta_{0} \\,+\\, \\beta_{1} X_{1, i} \\,+\\, \\epsilon_{i}\\] for \\(i \\,=\\, 1, 2, \\dots, n\\), where \\(i\\) is the \\(i\\)-th observation in data.\n\n\\(Y_i\\) is the \\(i\\)-th value for the outcome/dependent/response/target variable \\(Y\\).\n\\(X_{1, i}\\) is the \\(i\\)-th value for the explanatory/independent/predictor/input variable or feature \\(X_{1}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions-1",
    "title": "Lecture 7",
    "section": "Models and Assumptions",
    "text": "Models and Assumptions\nBeta coefficients\n\nLinear regression assumes a linear relationship for \\(Y = f(X_{1})\\):\n\n\\[Y_{i} \\,=\\, \\beta_{0} \\,+\\, \\beta_{1} X_{1, i} \\,+\\, \\epsilon_{i}\\] for \\(i \\,=\\, 1, 2, \\dots, n\\), where \\(i\\) is the \\(i\\)-th observation in data.\n\n\\(\\beta_0\\) is an unknown true value of an intercept: average value for \\(Y\\) if \\(X_{1} = 0\\)\n\\(\\beta_1\\) is an unknown true value of a slope: increase in average value for \\(Y\\) for each one-unit increase in \\(X_{1}\\)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions-2",
    "title": "Lecture 7",
    "section": "Models and Assumptions",
    "text": "Models and Assumptions\nRandom Noises\n\nLinear regression assumes a linear relationship for \\(Y = f(X_{1})\\):\n\n\\[Y_{i} \\,=\\, \\beta_{0} \\,+\\, \\beta_{1} X_{1, i} \\,+\\, \\epsilon_{i}\\] for \\(i \\,=\\, 1, 2, \\dots, n\\), where \\(i\\) is the \\(i\\)-th observation in data.\n\n\\(\\epsilon_i\\) is a random noise, or a statistical error:\n\n\\[\n\\epsilon_i \\sim N(0, \\sigma^2)\n\\]\n\nErrors have a mean value of 0 with constant variance \\(\\sigma^2\\).\nErrors are uncorrelated with \\(X_{1, i}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#what-is-linear-regression-doing",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#what-is-linear-regression-doing",
    "title": "Lecture 7",
    "section": "What Is Linear Regression Doing?",
    "text": "What Is Linear Regression Doing?\nBest Fitting Line\n\nLinear regression finds the beta estimates \\(( \\hat{\\beta_{0}}, \\hat{\\beta_{1}} )\\) such that:\n‚Äì The linear function \\(f(X_{1}) = \\hat{\\beta_{0}} + \\hat{\\beta_{1}}X_{1}\\) is as near as possible to \\(Y\\) for all \\((X_{1, i}\\,,\\, Y_{i})\\) pairs in the data.\n\nIt is the best fitting line, or the predicted outcome, \\(\\hat{Y_{\\,}} = \\hat{\\beta_{0}} + \\hat{\\beta_{1}}X_{1}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#what-is-linear-regression-doing-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#what-is-linear-regression-doing-1",
    "title": "Lecture 7",
    "section": "What Is Linear Regression Doing?",
    "text": "What Is Linear Regression Doing?\nResidual errors\n\nThe estimated beta coefficients are chosen to minimize the sum of squares of the residual errors \\((SSR)\\): \\[\n\\begin{align}\nSSR &\\,=\\, (\\texttt{Residual_Error}_{1})^{2}\\\\\n&\\quad \\,+\\, (\\texttt{Residual_Error}_{2})^{2}\\\\\n&\\quad\\,+\\, \\cdots + (\\texttt{Residual_Error}_{n})^{2}\\\\\n\\text{where}\\qquad\\qquad\\qquad\\qquad&\\\\\n\\texttt{Residual_Error}_{i} &\\,=\\, Y_{i} \\,-\\, \\hat{Y_{i}},\\\\\n\\texttt{Predicted_Outcome}_{i}: \\hat{Y_{i}} &\\,=\\, \\hat{\\beta_{0}} \\,+\\, \\hat{\\beta_{1}}X_{1, i}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#what-is-linear-regression-doing-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#what-is-linear-regression-doing-2",
    "title": "Lecture 7",
    "section": "What Is Linear Regression Doing?",
    "text": "What Is Linear Regression Doing?\nHat Notation\n\nWe use the hat notation \\((\\,\\hat{\\texttt{ }_{\\,}}\\,)\\) to distinguish true values and estimated/predicted values.\nThe value of true beta coefficient is denoted by \\(\\beta_{1}\\).\nThe value of estimated beta coefficient is denoted by \\(\\hat{\\beta_{1}}\\).\nThe \\(i\\)-th value of true outcome variable is denoted by \\(Y_{i}\\).\nThe \\(i\\)-th value of predicted outcome variable is denoted by \\(\\hat{Y_{i}}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#what-is-linear-regression-doing-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#what-is-linear-regression-doing-3",
    "title": "Lecture 7",
    "section": "What Is Linear Regression Doing?",
    "text": "What Is Linear Regression Doing?\nRelationship\n1. Finding the relationship between \\(X_{1}\\) and \\(Y\\) \\[\\hat{\\beta_{1}}\\]: How is an increase in \\(X_1\\) by one unit associated with a change in \\(Y\\) on average? - Positive? Negative? Independent? - How strong?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#statistical-significance-in-estimated-beta-coefficients",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#statistical-significance-in-estimated-beta-coefficients",
    "title": "Lecture 7",
    "section": "Statistical Significance in Estimated Beta Coefficients",
    "text": "Statistical Significance in Estimated Beta Coefficients\n\nWhat does it mean for a beta estimate \\(\\hat{\\beta_{\\,}}\\) to be statistically significant at 5% level?\n\nIt means that the null hypothesis \\(H_{0}: \\beta = 0\\) is rejected for a given significance level 5%.\n‚Äú2 standard error rule‚Äù of thumb: The true value of \\(\\beta\\) is 95% likely to be in the confidence interval \\((\\, \\hat{\\beta_{\\,}} - 2 * \\texttt{Std. Error}\\;,\\; \\hat{\\beta_{\\,}} + 2 * \\texttt{Std. Error} \\,)\\).\nThe standard error tells us how uncertain our beta estimate is.\nWe should look for the stars!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#what-is-linear-regression-doing-4",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#what-is-linear-regression-doing-4",
    "title": "Lecture 7",
    "section": "What Is Linear Regression Doing?",
    "text": "What Is Linear Regression Doing?\nPrediction\n2. Making a prediction on \\(Y\\): \\[\\hat{Y_{\\,}}\\] For unseen data point of \\(X_1\\), what is the predicted value of outcome, \\(\\hat{Y_{\\,}}\\)?\n\nE.g., For \\(X_{1} = 2\\), the predicted outcome is \\(\\hat{Y_{\\,}} = \\hat{\\beta_{0}} + \\hat{\\beta_{1}} \\times 2\\).\nE.g., For \\(X_{1} = 3\\), the predicted outcome is \\(\\hat{Y_{\\,}} = \\hat{\\beta_{0}} + \\hat{\\beta_{1}} \\times 3\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression---example",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression---example",
    "title": "Lecture 7",
    "section": "Linear Regression - Example",
    "text": "Linear Regression - Example\n\nSuppose we want to predict a property‚Äôs sales price based on the property size.\n\nIn other words, for some house sale i, we want to predict sale_price[i] based on gross_square_feet[i].\n\nWe also want to focus on the relationship between a property‚Äôs sales price and a property size.\n\nIn other words, we estimate how an increase in gross_square_feet[i] is associated with sale_price[i]."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-relationship",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-relationship",
    "title": "Lecture 7",
    "section": "Linear Relationship",
    "text": "Linear Relationship\n\nLinear regression assumes that:\n\nThe outcome sale_price[i] is linearly related to the input gross_square_feet[i]:\n\n\n\\[\\texttt{sale_price[i]} \\;=\\quad \\texttt{b0} \\,+\\, \\texttt{b1*gross_square_feet[i]} \\,+\\, \\texttt{e[i]}\\] where e[i] is a statistical error term."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#the-linear-relationship-between-sale_price-and-gross_square_feet",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#the-linear-relationship-between-sale_price-and-gross_square_feet",
    "title": "Lecture 7",
    "section": "The Linear Relationship between sale_price and gross_square_feet",
    "text": "The Linear Relationship between sale_price and gross_square_feet"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#goals-of-linear-regression",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#goals-of-linear-regression",
    "title": "Lecture 7",
    "section": "Goals of Linear Regression",
    "text": "Goals of Linear Regression\n\nThe goals of linear regression here are to:\n\n\nModeling for explanation: Find the relationship between gross_square_feet and sale_price by estimating a true value of b1.\n\n\nThe estimated value of b1 is denoted by \\(\\hat{\\texttt{b1}}\\).\n\n\nModeling for prediction: Make a prediction on sale_price[i] for new property i\n\n\nThe predicted value of sale_price[i] is denoted by \\(\\widehat{\\texttt{sale_price}}\\texttt{[i]}\\), where\n\n\\[\\widehat{\\texttt{sale_price}}\\texttt{[i]} \\;=\\quad \\hat{\\texttt{b0}} \\,+\\, \\hat{\\texttt{b1}}\\texttt{*gross_square_feet[i]}\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#training-and-test-data",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#training-and-test-data",
    "title": "Lecture 7",
    "section": "Training and Test Data",
    "text": "Training and Test Data\n\nTraining data: When we‚Äôre building a linear regression model, we need data to train the model.\nTest data: We also need data to test whether the model works well on new data.\n\n\n\n\nSo, we start with splitting a given data.frame into training and test data.frames when building a linear regression model."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#training-and-test-data-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#training-and-test-data-1",
    "title": "Lecture 7",
    "section": "Training and Test Data",
    "text": "Training and Test Data\nTraining vs.¬†Test\n\nWe use training data to train/fit the linear regression model.\n\nWe then make a prediction using test data, which are unseen/new from the viewpoint of the trained linear regression model.\n\nIn this way, we can test whether our model performs well in the real world, where unseen data points exist."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#training-and-test-data-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#training-and-test-data-2",
    "title": "Lecture 7",
    "section": "Training and Test Data",
    "text": "Training and Test Data\nOverfitting"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#example-of-linear-regression-using-pyspark",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#example-of-linear-regression-using-pyspark",
    "title": "Lecture 7",
    "section": "Example of Linear Regression using PySpark",
    "text": "Example of Linear Regression using PySpark\n\nWe will use the data for residential property sales from September 2017 and August 2018 in NYC.\nEach sales data recorded contains a number of interesting variables, but here we focus on the followings:\n\nsale_price: a property‚Äôs sales price;\ngross_square_feet: a property‚Äôs size;\nage: a property‚Äôs age;\nborough_name: a borough where a property is located.\n\nUse summary statistics and visualization to explore the data."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#splitting-data-into-training-and-testing-data",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#splitting-data-into-training-and-testing-data",
    "title": "Lecture 7",
    "section": "Splitting Data into Training and Testing Data",
    "text": "Splitting Data into Training and Testing Data\nA Little Bit of Statistics for the Uniform Distribution\n\n\n\nThe probability density function for the uniform distribution looks like:\nWith the uniform distribution, any values of \\(x\\) between 0 and 1 is equally likely drawn.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will use the uniform distribution when splitting data into training and testing data sets."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#splitting-data-into-training-and-testing-data-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#splitting-data-into-training-and-testing-data-1",
    "title": "Lecture 7",
    "section": "Splitting Data into Training and Testing Data",
    "text": "Splitting Data into Training and Testing Data\nRandomization in the Sampling Process\n\nWhy do we randomize when splitting given data into training and test data?\n\nRandomizing the sampling process ensures that the training and test data sets are representative for the population data.\nIf the sample does not properly represent the entire population, the model result is biased toward the sample.\n\nSuppose the splitting process is not randomized, so that the observations with sale_price &gt; 10^6 are in the training data and the observations with sale_price &lt;= 10^6 are in the test data.\n\nWhat would be the model result then?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#splitting-data-into-training-and-testing-data-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#splitting-data-into-training-and-testing-data-2",
    "title": "Lecture 7",
    "section": "Splitting Data into Training and Testing Data",
    "text": "Splitting Data into Training and Testing Data\nStep 1. Importing Modules and Reading DataFrames\nimport pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n# 1. Read CSV data from URL\ndf_pd = pd.read_csv('https://bcdanl.github.io/data/home_sales_nyc.csv')\nsale_df = spark.createDataFrame(df_pd)\nsale_df.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#splitting-data-into-training-and-testing-data-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#splitting-data-into-training-and-testing-data-3",
    "title": "Lecture 7",
    "section": "Splitting Data into Training and Testing Data",
    "text": "Splitting Data into Training and Testing Data\nStep 2. rand(seed = ANY_NUMBER)\n# 2. Split data into training and testing sets by creating a random column \"gp\"\nsale_df = sale_df.withColumn(\"gp\", rand(seed=123)) # seed is set for replication\n\n# Splits 60-40 into training and test sets \ndtrain = sale_df.filter(col(\"gp\") &gt;= 0.4) \ndtest = sale_df.filter(col(\"gp\") &lt; 0.4)\n\n# Or simply,\ndtrain, dtest = sale_df.randomSplit([0.6, 0.4], seed = 123)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#building-an-ml-dataframe-using-vectorassembler",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#building-an-ml-dataframe-using-vectorassembler",
    "title": "Lecture 7",
    "section": "Building an ML DataFrame using VectorAssembler()",
    "text": "Building an ML DataFrame using VectorAssembler()\n# Now assemble predictors using the renamed column\nassembler1 = VectorAssembler(\n    inputCols=[\"gross_square_feet\"], \n    outputCol=\"predictors\")\n\nVectorAssembler is a transformer in PySpark‚Äôs ML library that is used to combine multiple columns into a single vector column.\n\nMany ML algorithms in Spark require the predictors to be represented as a single vector.\nVectorAssembler is often one of the first steps in a Spark ML pipeline."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#building-a-linear-regression-model-using-linearregression.fit",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#building-a-linear-regression-model-using-linearregression.fit",
    "title": "Lecture 7",
    "section": "Building a Linear Regression Model using LinearRegression().fit()",
    "text": "Building a Linear Regression Model using LinearRegression().fit()\n# Fit linear regression model using the new label column \"sale_price\"\nmodel1 = (\n    LinearRegression(\n        featuresCol = \"predictors\", \n        labelCol = \"sale_price\")\n    .fit(dtrain1)\n)\n\nLinearRegression(featuresCol=\"predictors\", labelCol=\"sale_price\")\n\nThis creates an instance of the LinearRegression class.\nThe features (independent variables) are in a column named ‚Äúpredictors‚Äù.\nThe label (dependent variable) is in a column named ‚Äúsale_price‚Äù.\n\n.fit() trains (fits) the linear regression model using the training DataFrame dtrain1.\n\nThis training process estimates the beta coefficients that best predict the label from the features."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#summary-of-the-regression-result",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#summary-of-the-regression-result",
    "title": "Lecture 7",
    "section": "Summary of the Regression Result",
    "text": "Summary of the Regression Result\nmodel1.intercept\nmodel1.coefficients\nmodel1.summary.coefficientStandardErrors\nmodel1.summary.rootMeanSquaredError\nmodel1.summary.r2"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#summary-of-the-regression-result-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#summary-of-the-regression-result-1",
    "title": "Lecture 7",
    "section": "Summary of the Regression Result",
    "text": "Summary of the Regression Result\nExample of a Regression Table"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#summary-of-the-regression-result-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#summary-of-the-regression-result-2",
    "title": "Lecture 7",
    "section": "Summary of the Regression Result",
    "text": "Summary of the Regression Result\nMake the Summary Pretty\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model‚Äôs labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                ci_upper,\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R¬≤\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R¬≤\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: Exp(95% CI Lower), 8: 95% CI Upper, 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"Exp(95% CI Lower)\", \"95% CI Upper\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions-3",
    "title": "Lecture 7",
    "section": "Models and Assumptions",
    "text": "Models and Assumptions\n\nLinear regression assumes a linear relationship for \\(Y = f(X_{1}, X_{2})\\):\n\n\\[Y_{i} \\,=\\, \\beta_{0} \\,+\\, \\beta_{1} X_{1, i} \\,+\\,\\beta_{2} X_{2, i} \\,+\\, \\epsilon_{i}\\] for \\(i \\,=\\, 1, 2, \\dots, n\\), where \\(i\\) is the \\(i\\)-th observation in data.\n\n\\(\\beta_0\\) is an unknown true value of an intercept: average value for \\(Y\\) if \\(X_{1} = 0\\) and \\(X_{2} = 0\\)\n\\(\\beta_1\\) is an unknown true value of a slope: increase in average value for \\(Y\\) for each one-unit increase in \\(X_{1}\\)\n\\(\\beta_2\\) is an unknown true value of a slope: increase in average value for \\(Y\\) for each one-unit increase in \\(X_{2}\\)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions-4",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions-4",
    "title": "Lecture 7",
    "section": "Models and Assumptions",
    "text": "Models and Assumptions\nRandom Noises\n\nLinear regression assumes a linear relationship for \\(Y = f(X_{1}, X_{2})\\):\n\n\\[Y_{i} \\,=\\, \\beta_{0} \\,+\\, \\beta_{1} X_{1, i}\\,+\\, \\beta_{1} X_{2, i} \\,+\\, \\epsilon_{i}\\] for \\(i \\,=\\, 1, 2, \\dots, n\\), where \\(i\\) is the \\(i\\)-th observation in data.\n\n\\(\\epsilon_i\\) is a random noise, or a statistical error:\n\n\\[\n\\epsilon_i \\sim N(0, \\sigma^2)\n\\]\n\nErrors have a mean value of 0 with constant variance \\(\\sigma^2\\).\nErrors are uncorrelated with \\(X_{1, i}\\) and with \\(X_{2, i}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions-5",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions-5",
    "title": "Lecture 7",
    "section": "Models and Assumptions",
    "text": "Models and Assumptions\nBest Fitting Plane\n\nLinear regression finds the beta estimates \\(( \\hat{\\beta_{0}}, \\hat{\\beta_{1}}, \\hat{\\beta_{2}} )\\) such that:\n‚Äì The linear function \\(f(X_{1}, X_{2}) = \\hat{\\beta_{0}} + \\hat{\\beta_{1}}X_{1} + \\hat{\\beta_{2}}X_{2}\\) is as near as possible to \\(Y\\) for all \\((X_{1, i}\\,,\\,X_{2, i}\\,,\\, Y_{i})\\) pairs in the data.\n\nIt is the best fitting plane, or the predicted outcome, \\(\\hat{Y_{\\,}} = \\hat{\\beta_{0}} + \\hat{\\beta_{1}}X_{1} + \\hat{\\beta_{2}}X_{2}\\)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions-6",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#models-and-assumptions-6",
    "title": "Lecture 7",
    "section": "Models and Assumptions",
    "text": "Models and Assumptions\nResidual Errors\n\nThe estimated beta coefficients are chosen to minimize the sum of squares of the residual errors \\((SSR)\\): \\[\n\\begin{align}\nSSR &\\,=\\, (\\texttt{Residual_Error}_{1})^{2}\\\\\n&\\quad \\,+\\, (\\texttt{Residual_Error}_{2})^{2}\\\\\n&\\quad\\,+\\, \\cdots + (\\texttt{Residual_Error}_{n})^{2}\\\\\n\\text{where}\\qquad\\qquad\\qquad\\qquad&\\\\\n\\texttt{Residual_Error}_{i} &\\,=\\, Y_{i} \\,-\\, \\hat{Y_{i}},\\\\\n\\texttt{Predicted_Outcome}_{i}: \\hat{Y_{i}} &\\,=\\, \\hat{\\beta_{0}} \\,+\\, \\hat{\\beta_{1}}X_{1, i} \\,+\\, \\hat{\\beta_{2}}X_{2, i}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#training-and-test-data-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#training-and-test-data-3",
    "title": "Lecture 7",
    "section": "Training and Test Data",
    "text": "Training and Test Data\nModel Construction and Evaluation"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#big-data-and-machine-learning-ml",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#big-data-and-machine-learning-ml",
    "title": "Lecture 7",
    "section": "Big Data and Machine Learning (ML)",
    "text": "Big Data and Machine Learning (ML)\n\nThe term ‚ÄúBig Data‚Äù originated from computer scientists working with datasets too large to fit on a single machine.\n\nAs aggregation evolved into analysis, Big Data became closely linked with statistics and machine learning (ML).\nScalability of algorithms is crucial for handling large datasets.\n\nWe will use ML to:\n‚úÖ Identify patterns in big data\n‚úÖ Make data-driven decisions"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#what-does-it-mean-to-be-big---big-in-both-the-number-of-observations-sizen-and-in-the-number-of-variables-dimension-p.",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#what-does-it-mean-to-be-big---big-in-both-the-number-of-observations-sizen-and-in-the-number-of-variables-dimension-p.",
    "title": "Lecture 7",
    "section": "What does it mean to be big'? - Big in both the number of observations (sizen‚Äô) and in the number of variables (dimension `p‚Äô).",
    "text": "What does it mean to be big'? - Big in both the number of observations (sizen‚Äô) and in the number of variables (dimension `p‚Äô).\n\nIn these settings, we cannot:\n\nLook at each individual variable and make a decision.\nChoose among a small set of candidate models.\nPlot every variable to look for interactions or transformations.\n\nSome ML tools are straight out of previous statistics classes (linear regression) and some are totally new (ensemble models, principal component analysis).\n\nAll require a different approach when n and p get really big."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#ml-topics",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#ml-topics",
    "title": "Lecture 7",
    "section": "ML topics",
    "text": "ML topics\n\nRegression: inference and prediction\nRegularization: cross-validation\nPrincipal Component Analysis: dimension reduction\nTree-based models: decision trees, random forest, XGBoost\nClassfication: kNN\nClustering: k-means, association rules\nText Mining: sentiment analysis; topic models"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#what-does-it-mean-to-be-big",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#what-does-it-mean-to-be-big",
    "title": "Lecture 7",
    "section": "What does it mean to be ‚Äúbig‚Äù?",
    "text": "What does it mean to be ‚Äúbig‚Äù?\n\nBig in both the number of observations (size n) and in the number of variables (dimension p).\nIn these settings, we cannot:\n\nLook at each individual variable and make a decision.\nChoose among a small set of candidate models.\nPlot every variable to look for interactions or transformations.\n\nSome ML tools are straight out of previous statistics classes (linear regression) and some are totally new (ensemble models, principal component analysis).\n\nAll require a different approach when n and p get really big."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#building-an-ml-dataframe-using-vectorassembler-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#building-an-ml-dataframe-using-vectorassembler-1",
    "title": "Lecture 7",
    "section": "Building an ML DataFrame using VectorAssembler()",
    "text": "Building an ML DataFrame using VectorAssembler()\ndtrain1 = assembler1.transform(dtrain) # training data\ndtest1  = assembler1.transform(dtest)  # test data\n\ndtrain1.select(\"predictors\", \"sale_price\").show()\ndtest1.select(\"predictors\", \"sale_price\").show()\n\nVectorAssembler.transform() returns a DataFrame with a new column, specified in outputCol in VectorAssembler()."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#what-are-dummy-variables",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#what-are-dummy-variables",
    "title": "Lecture 7",
    "section": "What are Dummy Variables?",
    "text": "What are Dummy Variables?\n\nDefinition: Binary indicators (0 or 1) representing categorical data.\nPurpose: Transform qualitative data into a quantitative form for regression analysis.\n\nExample:\n\\[\nD_i = \\begin{cases}\n1, & \\text{if the observation belongs to the category} \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#dummy-variables-in-regression-models",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#dummy-variables-in-regression-models",
    "title": "Lecture 7",
    "section": "Dummy Variables in Regression Models",
    "text": "Dummy Variables in Regression Models\nConsider a regression model including a dummy variable:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 D_i + \\epsilon_i\n\\]\n\n\\(x_i\\): A continuous predictor.\n\\(D_i\\): Dummy variable (e.g., political party affiliation, type of car).\n\nInterpretation:\n\\(\\beta_2\\) captures the difference in the response \\(y\\) when the category is present (i.e., \\(D_i=1\\)) versus absent."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#the-dummy-variable-trap",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#the-dummy-variable-trap",
    "title": "Lecture 7",
    "section": "The Dummy Variable Trap",
    "text": "The Dummy Variable Trap\n\nProblem: Including a dummy for every category introduces redundancy.\nFor a categorical variable with \\(k\\) levels:\n\nIf you include all \\(k\\) dummy variables in the model, their values always sum to 1:\n\n\\[\nD_{1i} + D_{2i} + \\cdots + D_{ki} = 1 \\quad \\text{(for each observation)}\n\\]\nThis is problematic, because one dummy is completely predictable from the others.\nThe intercept already captures the constant part (1), making one of the dummy variables redundant."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#avoiding-the-dummy-variable-trap",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#avoiding-the-dummy-variable-trap",
    "title": "Lecture 7",
    "section": "Avoiding the Dummy Variable Trap",
    "text": "Avoiding the Dummy Variable Trap\n\nSolution: Drop one dummy (often called the reference category)\nThe reference category is represented by a combination of \\(\\texttt{borough}\\) variables.\n\nDummy for the reference category is 1 if all the rest of the dummies is 0.\nDummy for the reference category is 0 otherwise.\n\n\nProper model:\n\\[\ny_i = \\beta_0 + \\beta_1 D_{1, i} + \\beta_2 D_{2, i} + \\cdots + \\beta_{k-1} D_{(k-1), i} + \\epsilon_i\n\\]\n\nInterpretation:\n\nEach \\(\\beta_j\\) (for \\(j=1,2,\\ldots,k-1\\)) represents the difference from the reference category."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression---best-fitting-plane",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#multiple-regression---best-fitting-plane",
    "title": "Lecture 7",
    "section": "Multiple Regression - Best Fitting Plane",
    "text": "Multiple Regression - Best Fitting Plane\n\n\n\n\n\n\n\n\n\n\nAll else being equal, an increase in gross_square_feet by one unit is associated with an increase in sale_price by \\(\\hat{\\beta_{1}}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#motivation-treating-categorical-variables-in-linear-regression",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#motivation-treating-categorical-variables-in-linear-regression",
    "title": "Lecture 7",
    "section": "Motivation: Treating Categorical Variables in Linear Regression",
    "text": "Motivation: Treating Categorical Variables in Linear Regression\n\nLinear regression models require numerical predictors, but many variables are categorical.\n\ne.g., Consider the two equivalent houses, except for its location‚Äîone in Manhattan and the other in Bronx.\n\nThe Approach:\nConvert categorical variables into numerical format using dummy variables.\nWhy Do This?\n\nAllows the model to compare different categories.\nEach dummy variable indicates the presence (1) or absence (0) of a category."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#avoiding-the-dummy-variable-trap-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#avoiding-the-dummy-variable-trap-1",
    "title": "Lecture 7",
    "section": "Avoiding the Dummy Variable Trap",
    "text": "Avoiding the Dummy Variable Trap\n\nE.g., the dummy variable, borough_name_Brooklyn, is follows:\n\n\\[\n\\texttt{borough_name_Brooklyn[i] }\\\\\n= \\begin{cases}\n\\texttt{1} & \\text{if a property } \\texttt{i} \\text{ is in } \\texttt{Brooklyn};\\\\\\\\\n\\texttt{0} & \\text{otherwise}.\\qquad\\qquad\\quad\\,\n\\end{cases}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#dummy-variable-regression-using-pyspark",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#dummy-variable-regression-using-pyspark",
    "title": "Lecture 7",
    "section": "Dummy Variable Regression using PySpark",
    "text": "Dummy Variable Regression using PySpark\n\nThe UDF, add_dummy_variables(var_name, reference_level) convert a categorical variable into its dummy variables:\n\nvar_name: a string of categorical variable name\nreference_level: index position of alphabetically sorted categories\nThis function requires dtrain and dtest DataFrames.\n\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n    \n  \n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#dummy-variable-regression-using-pyspark-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#dummy-variable-regression-using-pyspark-1",
    "title": "Lecture 7",
    "section": "Dummy Variable Regression using PySpark",
    "text": "Dummy Variable Regression using PySpark\n\nLet‚Äôs check what categories are in ‚Äúborough_name‚Äù and how many categories are:\n\n\n\n# Distinct categories:\n(\n    dtrain\n    .select(\"borough_name\")\n    .distinct()\n    .orderBy(\"borough_name\")\n    .show()\n)\n\n# Number of categories\n(\n    dtrain\n    .select(\"borough_name\")\n    .distinct()\n    .count()\n)\n\n\n\nLet‚Äôs convert the borough_name variable into its dummies using the UDF, add_dummy_variables(var_name, reference_level):\n\n# 0: Bronx, 1: Brooklyn, 2: Manhattan, 3: Queens, 4: Staten Island\ndummy_cols, ref_category = add_dummy_variables(\"borough_name\", 2)\n# To see dummy variables\ndtrain.select(['borough_name'] + dummy_cols).show()\ndtrain.select(['borough_name'] + dummy_cols).filter(col('borough_name') == \"Manhattan\").show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#dummy-variable-regression-using-pyspark-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#dummy-variable-regression-using-pyspark-2",
    "title": "Lecture 7",
    "section": "Dummy Variable Regression using PySpark",
    "text": "Dummy Variable Regression using PySpark\n\nNow we‚Äôre ready to run a linear regression with dummy variables:\n\n# Define the list of predictor columns:\n# Two numeric predictors plus the dummy variables (excluding the reference dummy)\nconti_cols = [\"gross_square_feet\", \"age\"]\nassembler_predictors = conti_cols + dummy_cols\n\nassembler3 = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\ndtrain3 = assembler3.transform(dtrain)\ndtest3  = assembler3.transform(dtest)\nmodel3 = LinearRegression(featuresCol=\"predictors\", labelCol=\"sale_price\").fit(dtrain3)\ndtest3 = model3.transform(dtest3)\n\n# For model3 and assembler3:\nprint(regression_table(model3, assembler3))"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#residuals",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#residuals",
    "title": "Lecture 7",
    "section": "Residuals",
    "text": "Residuals\n\nModel equation: \\(Y_{i} \\,=\\, \\beta_{0} \\,+\\, \\beta_{1}X_{1,i} \\,+\\, \\beta_{2}X_{2,i}\\)\n\n\\(\\epsilon_i\\) is a random noise, or a statistical error:\n\n\n\\[\n\\epsilon_i \\sim N(0, \\sigma^2)\n\\] - Errors have a mean value of 0 with constant variance \\(\\sigma^2\\).\n\nErrors are uncorrelated with \\(X_{1,i}\\) and with \\(X_{2, i}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#residuals-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#residuals-1",
    "title": "Lecture 7",
    "section": "Residuals",
    "text": "Residuals\n\nIf we re-arrange the simple regression equation, \\[\\begin{align}\n{\\epsilon}_{i} \\,=\\, Y_{i} \\,-\\, (\\, {\\beta}_{0} \\,+\\, {\\beta}_{1}X_{1,i} \\,).\n\\end{align}\\]\n\\(\\texttt{residual_error}_{i}\\) can be thought of as the expected value of \\(\\epsilon_{i}\\), denoted by \\(\\hat{\\epsilon_{i}}\\). \\[\\begin{align}\n\\hat{\\epsilon_{i}} \\,=\\, Y_{i} \\,-\\, (\\, \\hat{\\beta_{0}} \\,+\\, \\hat{\\beta_{1}}X_{1,i} \\,)\n\\end{align}\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-1",
    "title": "Lecture 7",
    "section": "Residual Plots",
    "text": "Residual Plots\n\nResidual plot is a scatterplot of fitted values and residuals.\n\nA variable of fitted values on x-axis\nA variable of residuals on y-axis\n\nA residual plot can be used to diagnose the quality of model results.\nBecause we assume that \\(\\epsilon_{i}\\) have a mean value of 0 with constant variance \\(\\sigma^2\\), a well-behaved residual plot should bounce randomly and form a cloud roughly at the level of zero residual, the perfect prediction line.\nFrom the residual plot, we should ask the following the two questions ourselves:\n\nOn average, are the predictions correct?\nAre there systematic errors?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-4",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-4",
    "title": "Lecture 7",
    "section": "Residual Plots",
    "text": "Residual Plots\nExamples"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-in-python",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-in-python",
    "title": "Lecture 7",
    "section": "Residual Plots in Python",
    "text": "Residual Plots in Python\n\nPySpark itself does not have built-in visualization capabilities.\n\nFirstly, we convert PySpark DataFrame into Pandas DataFrame by using .toPandas()\n\n\n# Residual plot for Model 2:\n# Convert test predictions to Pandas\nrdf = dtest2.select([\"prediction\", \"sale_price\"]).toPandas()\nrdf[\"residual\"] = rdf[\"sale_price\"] - rdf[\"prediction\"]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-in-python-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-in-python-1",
    "title": "Lecture 7",
    "section": "Residual Plots in Python",
    "text": "Residual Plots in Python\n\nWe then use matplotlib.pyplot to draw a residual plot:\n\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm  # for lowess smoothing\n\nplt.scatter(rdf[\"prediction\"], rdf[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n# Use lowess smoothing for the trend line\nsmoothed = sm.nonparametric.lowess(rdf[\"residual\"], rdf[\"prediction\"])\nplt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\n\n# Perfect prediction line\n#   A horizontal line at residual = 0\nplt.axhline(y=0, color=\"red\", linestyle=\"--\")\n\n# Labeling\nplt.xlabel(\"Predicted sale.price (Model 2)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residual Plot for Model 2\")\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-3",
    "title": "Lecture 7",
    "section": "Residual Plots",
    "text": "Residual Plots\nUnbiasedness and Homoskedasticity\n\nWe would like have a residual plot to be\n\nUnbiased: have an average value of zero in any thin vertical strip;\nHomoskedastic, which means ‚Äúsame stretch‚Äù: it is ideal to have the same spread of the residuals in any thin vertical strip.\nWhen the variance of residuals changes across predicted values (e.g., residuals get larger as predicted values increase), the model suffers from heteroscedasticity."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-2",
    "title": "Lecture 7",
    "section": "Residual Plots",
    "text": "Residual Plots\n\n\nOn average, are the predictions correct?\nAre there systematic errors?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-estimated-beta-coefficients-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-estimated-beta-coefficients-3",
    "title": "Lecture 7",
    "section": "Interpreting Estimated Beta Coefficients",
    "text": "Interpreting Estimated Beta Coefficients\n2. borough_nameBronx\n\nConsider the predicted sales prices of the two houses, A and C.\n\nBoth A and C are with the same age and the same gross_square_feet.\nA is in Bronx, and C is in Manhattan.\n\nAll else being equal, an increase in borough_nameBronx by one unit is associated with an increase in sale_price by b3.\nEquivalently, all else being equal, being in Bronx relative to being a in Manhattan is associated with a decrease in sale_price by |b3|.\n\nWhy?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-estimated-beta-coefficients-4",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-estimated-beta-coefficients-4",
    "title": "Lecture 7",
    "section": "Interpreting Estimated Beta Coefficients",
    "text": "Interpreting Estimated Beta Coefficients\n2. borough_nameBronx\n\\[\n\\begin{align}\\widehat{\\texttt{sale_price[A]}} \\;=\\quad& \\hat{\\texttt{b0}} \\,+\\, \\hat{\\texttt{b1}}\\texttt{*gross_square_feet[A]} \\,+\\, \\hat{\\texttt{b2}}\\texttt{*age[A]} \\,+\\,\\\\ &\\hat{\\texttt{b3}}\\texttt{*Bronx[A]}\\,+\\,\n\\hat{\\texttt{b4}}\\texttt{*Brooklyn[A]} \\,+\\,\\\\ &\\hat{\\texttt{b5}}\\texttt{*Queens[A]}\\,+\\, \\hat{\\texttt{b6}}\\texttt{*Staten Island[A]}\\\\\n\\widehat{\\texttt{sale_price[C]}} \\;=\\quad& \\hat{\\texttt{b0}} \\,+\\, \\hat{\\texttt{b1}}\\texttt{*gross_square_feet[C]} \\,+\\, \\hat{\\texttt{b2}}\\texttt{*age[C]}\\,+\\,\\\\\n&\\hat{\\texttt{b3}}\\texttt{*Bronx[C]}\\,+\\,\n\\hat{\\texttt{b4}}\\texttt{*Brooklyn[C]} \\,+\\,\\\\ &\\hat{\\texttt{b5}}\\texttt{*Queens[C]}\\,+\\, \\hat{\\texttt{b6}}\\texttt{*Staten Island[C]} \\end{align}\n\\]\n\\[\n\\begin{align}\\Leftrightarrow\\qquad&\\widehat{\\texttt{sale_price[A]}} \\,-\\, \\widehat{\\texttt{sale_price[C]}}\\qquad  \\\\\n\\;=\\quad &\\hat{\\texttt{b3}}\\texttt{*}\\texttt{Bronx[A]} \\\\\n\\;=\\quad &\\hat{\\texttt{b3}}\\qquad\\qquad\\qquad\\qquad\\quad\\;\\;\\;\\,\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#coefficient-plots-in-python",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#coefficient-plots-in-python",
    "title": "Lecture 7",
    "section": "Coefficient Plots in Python",
    "text": "Coefficient Plots in Python\n\nA coefficient plot shows beta estimates and their confidence intervals.\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Create a Pandas DataFrame from model3's summary information\nterms = assembler3.getInputCols()\ncoefs = model3.coefficients.toArray()[:len(terms)]\nstdErrs = model3.summary.coefficientStandardErrors[:len(terms)]\n\ndf_summary = pd.DataFrame({\n    \"term\": terms,\n    \"estimate\": coefs,\n    \"std_error\": stdErrs\n})\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary[\"term\"], df_summary[\"estimate\"],\n             yerr = 1.96 * df_summary[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates (Model 2)\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#summary-of-the-regression-result-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#summary-of-the-regression-result-3",
    "title": "Lecture 7",
    "section": "Summary of the Regression Result",
    "text": "Summary of the Regression Result\nMake the Summary Pretty\n# Using the UDF, regression_table(model, assembler)\nprint(regression_table(model1, assembler1))"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#building-a-linear-regression-model-using-linearregression.fit-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#building-a-linear-regression-model-using-linearregression.fit-1",
    "title": "Lecture 7",
    "section": "Building a Linear Regression Model using LinearRegression().fit()",
    "text": "Building a Linear Regression Model using LinearRegression().fit()\n# Adding a \"prediction\" coulmn to dtest1 DataFrame\ndtest1  = model1.transform(dtest1)\n\ndtest1 = model1.transform(dtest1): Adds a new column prediction to dtest1 DataFrame.\n\nThis new column contains the predicted outcome based on the trained model1 to predict an outcome for the test dataset dtest1."
  },
  {
    "objectID": "danl-cw/danl-320-cw-08.html",
    "href": "danl-cw/danl-320-cw-08.html",
    "title": "Classwork 8",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, when, log\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n\n\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model‚Äôs labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                ci_upper,\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R¬≤\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R¬≤\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: Exp(95% CI Lower), 8: 95% CI Upper, 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"Exp(95% CI Lower)\", \"95% CI Upper\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))\n\n\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n    \n  \n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\n\n\nterms = assembler3.getInputCols()\ncoefs = model3.coefficients.toArray()[:len(terms)]\nstdErrs = model3.summary.coefficientStandardErrors[:len(terms)]\n\ndf_summary = pd.DataFrame({\n    \"term\": terms,\n    \"estimate\": coefs,\n    \"std_error\": stdErrs\n})\n\n# Filter df_summary if needed\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary[\"term\"], df_summary[\"estimate\"],\n             yerr = 1.96 * df_summary[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates (Model 2)\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n# Convert test predictions to Pandas\ndfpd = TEST_DATAFRAME.select([\"prediction\", \"Y_VARIABLE\"]).toPandas()\ndfpd[\"residual\"] = dfpd[\"Y_VARIABLE\"] - dfpd[\"prediction\"]\nplt.scatter(dfpd[\"prediction\"], dfpd[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n# Use lowess smoothing for the trend line\nsmoothed = sm.nonparametric.lowess(dfpd[\"residual\"], dfpd[\"prediction\"])\nplt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\nplt.axhline(y=0, color=\"red\", linestyle=\"--\")\nplt.xlabel(\"Predicted y (Model)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residual Plot for Model\")\nplt.show()\n\n\n\n\n# Create a histogram\ndfpd = DATAFRAME.select([\"Y_VARIABLE\"]).toPandas()\nsns.histplot(dfpd[\"Y_VARIABLE\"], bins=10, kde=True)"
  },
  {
    "objectID": "danl-cw/danl-320-cw-08.html#variable-description",
    "href": "danl-cw/danl-320-cw-08.html#variable-description",
    "title": "Classwork 8",
    "section": "Variable description",
    "text": "Variable description\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ncnt\nCount of total rental bikes\n\n\nyear\nYear\n\n\nmonth\nMonth\n\n\ndate\nDate\n\n\nhr\nHour\n\n\nwkday\nWeekday\n\n\nholiday\nHoliday indicator (1 if holiday, 0 otherwise)\n\n\nseasons\nSeason\n\n\nweather_cond\nWeather condition\n\n\ntemp\nTemperature (measured in standard deviations from average)\n\n\nhum\nHumidity (measured in standard deviations from average)\n\n\nwindspeed\nWind speed (measured in standard deviations from average)"
  },
  {
    "objectID": "danl-cw/danl-320-cw-08.html#required-libraries",
    "href": "danl-cw/danl-320-cw-08.html#required-libraries",
    "title": "Classwork 8",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, when\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
  },
  {
    "objectID": "danl-cw/danl-320-cw-08.html#udf-for-regression-tables",
    "href": "danl-cw/danl-320-cw-08.html#udf-for-regression-tables",
    "title": "Classwork 8",
    "section": "",
    "text": "def regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model‚Äôs labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                ci_upper,\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R¬≤\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R¬≤\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: Exp(95% CI Lower), 8: 95% CI Upper, 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"Exp(95% CI Lower)\", \"95% CI Upper\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))"
  },
  {
    "objectID": "danl-cw/danl-320-cw-08.html#udf-for-adding-dummy-variables",
    "href": "danl-cw/danl-320-cw-08.html#udf-for-adding-dummy-variables",
    "title": "Classwork 8",
    "section": "",
    "text": "def add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n    \n  \n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)"
  },
  {
    "objectID": "danl-cw/danl-320-cw-08.html#required-libraries-and-sparksession-entry-point",
    "href": "danl-cw/danl-320-cw-08.html#required-libraries-and-sparksession-entry-point",
    "title": "Classwork 8",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, when, log\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
  },
  {
    "objectID": "danl-cw/danl-320-cw-08.html#coefficient-plots",
    "href": "danl-cw/danl-320-cw-08.html#coefficient-plots",
    "title": "Classwork 8",
    "section": "",
    "text": "terms = assembler3.getInputCols()\ncoefs = model3.coefficients.toArray()[:len(terms)]\nstdErrs = model3.summary.coefficientStandardErrors[:len(terms)]\n\ndf_summary = pd.DataFrame({\n    \"term\": terms,\n    \"estimate\": coefs,\n    \"std_error\": stdErrs\n})\n\n# Filter df_summary if needed\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary[\"term\"], df_summary[\"estimate\"],\n             yerr = 1.96 * df_summary[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates (Model 2)\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()"
  },
  {
    "objectID": "danl-cw/danl-320-cw-08.html#residual-plots",
    "href": "danl-cw/danl-320-cw-08.html#residual-plots",
    "title": "Classwork 8",
    "section": "",
    "text": "# Convert test predictions to Pandas\ndfpd = TEST_DATAFRAME.select([\"prediction\", \"Y_VARIABLE\"]).toPandas()\ndfpd[\"residual\"] = dfpd[\"Y_VARIABLE\"] - dfpd[\"prediction\"]\nplt.scatter(dfpd[\"prediction\"], dfpd[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n# Use lowess smoothing for the trend line\nsmoothed = sm.nonparametric.lowess(dfpd[\"residual\"], dfpd[\"prediction\"])\nplt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\nplt.axhline(y=0, color=\"red\", linestyle=\"--\")\nplt.xlabel(\"Predicted y (Model)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residual Plot for Model\")\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#hypothesis-testing-on-beta-coefficient-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#hypothesis-testing-on-beta-coefficient-1",
    "title": "Lecture 7",
    "section": "Hypothesis Testing on Beta Coefficient",
    "text": "Hypothesis Testing on Beta Coefficient\n\nTo determine whether an independent variable has a statistically significant effect on the dependent variable in a linear regression model.\nConsider the following linear regression model: \\[\ny_{i} = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\dots + \\beta_k x_{k,i} + \\epsilon_{i}\n\\]\n\\(y\\): Outcome\n\\(x_1, x_2, \\dots, x_k\\): Predictors\n\\(\\beta_0\\): Intercept\n\n\\(\\beta_1, \\beta_2, \\dots, \\beta_k\\): Coefficients\n\n\\(\\epsilon_{i}\\): Error term"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#hypotheses-and-test-statistic",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#hypotheses-and-test-statistic",
    "title": "Lecture 7",
    "section": "Hypotheses and Test Statistic",
    "text": "Hypotheses and Test Statistic\n\nWe test whether a specific coefficient \\(\\beta_j\\) significantly differs from zero:\n\nNull Hypothesis (\\(H_0\\)): \\(\\beta_j = 0\\) (No relationship)\n\nAlternative Hypothesis (\\(H_A\\)): \\(\\beta_j \\neq 0\\) (Significant relationship)\n\nThe t-statistic is used to test each coefficient:\n\n\\[\nt = \\frac{\\hat{\\beta_j} - 0}{SE(\\hat{\\beta_j})}\n\\]\n\n\\(\\hat{\\beta_j}\\): Estimated coefficient\n\n\\(SE(\\hat{\\beta_j})\\): Standard error of the estimate"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#hypothesis-testing---decision-rule-and-interpretation",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#hypothesis-testing---decision-rule-and-interpretation",
    "title": "Lecture 7",
    "section": "Hypothesis Testing - Decision Rule and Interpretation",
    "text": "Hypothesis Testing - Decision Rule and Interpretation\n\nCalculate the p-value based on the t-distribution with \\(n - k - 1\\) degrees of freedom.\nCompare p-value with significance level \\(\\alpha\\) (e.g., 0.05):\n\nIf \\(p \\leq \\alpha\\): Reject \\(H_0\\) (Significant)\nIf \\(p &gt; \\alpha\\): Fail to reject \\(H_0\\) (Not significant)\n\n\n\nIn our course, stars in a regression table mean a significance level:\n\n* (10%); ** (5%); *** (1%)\n\nReject \\(H_0\\): There is sufficient evidence to suggest a statistically significant relationship between \\(x_j\\) and \\(y\\).\nFail to reject \\(H_0\\): No statistically significant evidence of a relationship between \\(x_j\\) and \\(y\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-log-transformation-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-log-transformation-1",
    "title": "Lecture 7",
    "section": "Linear Regression with Log Transformation",
    "text": "Linear Regression with Log Transformation\nThe model equation with log-transformed \\(\\texttt{sale.price[i]}\\) is \\[\\begin{align}\n\\log(\\texttt{sale.price[i]}) \\;=\\;\\, &\\texttt{b0} \\,+\\,\\\\ &\\texttt{b1*gross.square.feet[i]} \\,+\\,\\texttt{b2*age[i]}\\,+\\,\\\\ &\\texttt{b3*Bronx[i]} \\,+\\,\\texttt{b4*Brooklyn[i]} \\,+\\,\\\\&\\texttt{b5*Queens[i]} \\,+\\,\\texttt{b6*Staten Island[i]}\\,+\\,\\\\ &\\texttt{e[i]}.\n\\end{align}\\]\n\nNote that the reference level for borough_name is Manhattan."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-beta-estimates-on-log-transformed-variables",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-beta-estimates-on-log-transformed-variables",
    "title": "Lecture 7",
    "section": "Interpreting Beta Estimates on Log-transformed Variables",
    "text": "Interpreting Beta Estimates on Log-transformed Variables\n1. gross_square_feet\n\nLet‚Äôs re-consider the two properties \\(\\texttt{A}\\) and \\(\\texttt{B}\\).\n\n\\(\\texttt{gross.square.feet[A]} = 2001\\) and \\(\\texttt{gross.square.feet[B]} = 2000\\).\nBoth are in the same borough.\nBoth properties‚Äô ages are the same."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-beta-estimates-on-log-transformed-variables-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-beta-estimates-on-log-transformed-variables-1",
    "title": "Lecture 7",
    "section": "Interpreting Beta Estimates on Log-transformed Variables",
    "text": "Interpreting Beta Estimates on Log-transformed Variables\n1. gross_square_feet\n\nIf we apply the rule above for \\(\\widehat{\\texttt{sale.price}}\\texttt{[A]}\\) and \\(\\widehat{\\texttt{sale.price}}\\texttt{[B]}\\),\n\n\n\n\n\\[\\begin{align}&\\log(\\widehat{\\texttt{sale.price}}\\texttt{[A]}) - \\log(\\widehat{\\texttt{sale.price}}\\texttt{[B]}) \\\\\n\\,=\\, &\\hat{\\texttt{b1}}\\,*\\,(\\texttt{gross.square.feet[A]} \\,-\\, \\texttt{gross.square.feet[B]})\\\\\n\\,=\\, &\\hat{\\texttt{b1}}\\end{align}\\]\nSo we can have the following: \\[\n\\begin{align}\n&\\Leftrightarrow\\qquad\\frac{\\widehat{\\texttt{sale.price[A]}}}{ \\widehat{\\texttt{sale.price[B]}}} \\;=\\; \\texttt{exp(}\\hat{\\texttt{b1}}\\texttt{)}\\\\ \\quad&\\Leftrightarrow\\qquad\\widehat{\\texttt{sale.price[A]}} \\;=\\; \\widehat{\\texttt{sale.price[B]}} * \\texttt{exp(}\\hat{\\texttt{b1}}\\texttt{)}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-beta-estimates-on-log-transformed-variables-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-beta-estimates-on-log-transformed-variables-2",
    "title": "Lecture 7",
    "section": "Interpreting Beta Estimates on Log-transformed Variables",
    "text": "Interpreting Beta Estimates on Log-transformed Variables\n1. gross_square_feet\n\nSuppose \\(\\texttt{exp(}\\hat{\\texttt{b2}}\\texttt{)} = 1.000431\\).\n\nThen \\(\\widehat{\\texttt{sale.price[A]}}\\) is \\(1.000431\\times\\widehat{\\texttt{sale.price[B]}}\\).\nAll else being equal, an increase in \\(\\texttt{gross.square.feet}\\) by one unit is associated with an increase in \\(\\texttt{sale.price}\\) by 0.0431%."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-beta-estimates-on-log-transformed-variables-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-beta-estimates-on-log-transformed-variables-3",
    "title": "Lecture 7",
    "section": "Interpreting Beta Estimates on Log-transformed Variables",
    "text": "Interpreting Beta Estimates on Log-transformed Variables\n2. borough_nameBronx\n\nLet‚Äôs re-consider the two properties \\(\\texttt{A}\\) and \\(\\texttt{C}\\).\n\nA is in Bronx, and C is in Manhattan.\nBoth A and C‚Äôs age are the same.\nBoth A and C‚Äôs gross.square.feet are the same."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-beta-estimates-on-log-transformed-variables-4",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-beta-estimates-on-log-transformed-variables-4",
    "title": "Lecture 7",
    "section": "Interpreting Beta Estimates on Log-transformed Variables",
    "text": "Interpreting Beta Estimates on Log-transformed Variables\n2. borough_nameBronx\n\nIf we apply the log()-exp() rules for \\(\\widehat{\\texttt{sale.price}}\\texttt{[A]}\\) and \\(\\widehat{\\texttt{sale.price}}\\texttt{[C]}\\),\n\n\n\n\n\\[\\begin{align}&\\log(\\widehat{\\texttt{sale.price}}\\texttt{[A]}) - \\log(\\widehat{\\texttt{sale.price}}\\texttt{[C]}) \\\\\n\\,=\\, &\\hat{\\texttt{b3}}\\,*\\,(\\texttt{borough_Bronx[A]} \\,-\\, \\texttt{borough_Bronx[C]})\\,=\\, \\hat{\\texttt{b3}}\\end{align}\\]\nSo we can have the following: \\[\n\\begin{align}&\\Leftrightarrow\\qquad\\frac{\\widehat{\\texttt{sale.price[A]}}}{ \\widehat{\\texttt{sale.price[C]}}} \\;=\\; \\texttt{exp(}\\hat{\\texttt{b3}}\\texttt{)}\\\\ \\quad&\\Leftrightarrow\\qquad\\,\\widehat{\\texttt{sale.price[A]}} \\;=\\; \\widehat{\\texttt{sale.price[C]}} * \\texttt{exp(}\\hat{\\texttt{b3}}\\texttt{)}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-beta-estimates-on-log-transformed-variables-5",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-beta-estimates-on-log-transformed-variables-5",
    "title": "Lecture 7",
    "section": "Interpreting Beta Estimates on Log-transformed Variables",
    "text": "Interpreting Beta Estimates on Log-transformed Variables\n2. borough_nameBronx\n\nSuppose \\(\\texttt{exp(}\\hat{\\texttt{b3}}\\texttt{)} = 0.2831691\\).\n\nThen \\(\\widehat{\\texttt{sale.price[A]}}\\) is \\(0.2831691\\times\\widehat{\\texttt{sale.price[B]}}\\).\nAll else being equal, an increase in \\(\\texttt{borough_Bronx}\\) by one unit is associated with a decrease in \\(\\texttt{sale.price}\\) by (1 - 0.2831691) = 71.78%.\nAll else being equal, being in Bronx relative to being in Manhattan is associated with a decrease in \\(\\texttt{sale.price}\\) by 71.78%."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-beta-estimates-on-log-transformed-variables-6",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#interpreting-beta-estimates-on-log-transformed-variables-6",
    "title": "Lecture 7",
    "section": "Interpreting Beta Estimates on Log-transformed Variables",
    "text": "Interpreting Beta Estimates on Log-transformed Variables\n\nAll else being equal, an increase in gross.square.feet by one unit is associated with an increase in log(sale.price) by b1.\n\nAll else being equal, an increase in gross.square.feet by one unit is associated with an increase in sale.price by (exp(b1) - 1) * 100%.\n\nAll else being equal, an increase in broughBronx by one unit is associated with an increase in log(sale.price) by b3.\n\nAll else being equal, being in Bronx is associated with a decrease in sale.price by |(exp(b3) - 1)| * 100% relative to being in Manhattan."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables",
    "title": "Lecture 7",
    "section": "Beta Estimates for Log-transformed Variables",
    "text": "Beta Estimates for Log-transformed Variables\n1. gross_square_feet\n\nLet‚Äôs re-consider the two properties \\(\\texttt{A}\\) and \\(\\texttt{B}\\).\n\n\\(\\texttt{gross.square.feet[A]} = 2001\\) and \\(\\texttt{gross.square.feet[B]} = 2000\\).\nBoth are in the same borough.\nBoth properties‚Äô ages are the same."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables-1",
    "title": "Lecture 7",
    "section": "Beta Estimates for Log-transformed Variables",
    "text": "Beta Estimates for Log-transformed Variables\n1. gross_square_feet\n\nIf we apply the rule above for \\(\\widehat{\\texttt{sale.price}}\\texttt{[A]}\\) and \\(\\widehat{\\texttt{sale.price}}\\texttt{[B]}\\),\n\n\n\n\n\\[\\begin{align}&\\log(\\widehat{\\texttt{sale.price}}\\texttt{[A]}) - \\log(\\widehat{\\texttt{sale.price}}\\texttt{[B]}) \\\\\n\\,=\\, &\\hat{\\texttt{b1}}\\,*\\,(\\texttt{gross.square.feet[A]} \\,-\\, \\texttt{gross.square.feet[B]})\\\\\n\\,=\\, &\\hat{\\texttt{b1}}\\end{align}\\]\nSo we can have the following: \\[\n\\begin{align}\n&\\Leftrightarrow\\qquad\\frac{\\widehat{\\texttt{sale.price[A]}}}{ \\widehat{\\texttt{sale.price[B]}}} \\;=\\; \\texttt{exp(}\\hat{\\texttt{b1}}\\texttt{)}\\\\ \\quad&\\Leftrightarrow\\qquad\\widehat{\\texttt{sale.price[A]}} \\;=\\; \\widehat{\\texttt{sale.price[B]}} * \\texttt{exp(}\\hat{\\texttt{b1}}\\texttt{)}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables-2",
    "title": "Lecture 7",
    "section": "Beta Estimates for Log-transformed Variables",
    "text": "Beta Estimates for Log-transformed Variables\n1. gross_square_feet\n\nSuppose \\(\\texttt{exp(}\\hat{\\texttt{b2}}\\texttt{)} = 1.000431\\).\n\nThen \\(\\widehat{\\texttt{sale.price[A]}}\\) is \\(1.000431\\times\\widehat{\\texttt{sale.price[B]}}\\).\nAll else being equal, an increase in \\(\\texttt{gross.square.feet}\\) by one unit is associated with an increase in \\(\\texttt{sale.price}\\) by 0.0431%."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables-3",
    "title": "Lecture 7",
    "section": "Beta Estimates for Log-transformed Variables",
    "text": "Beta Estimates for Log-transformed Variables\n2. borough_nameBronx\n\nLet‚Äôs re-consider the two properties \\(\\texttt{A}\\) and \\(\\texttt{C}\\).\n\nA is in Bronx, and C is in Manhattan.\nBoth A and C‚Äôs age are the same.\nBoth A and C‚Äôs gross.square.feet are the same."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables-4",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables-4",
    "title": "Lecture 7",
    "section": "Beta Estimates for Log-transformed Variables",
    "text": "Beta Estimates for Log-transformed Variables\n2. borough_nameBronx\n\nIf we apply the log()-exp() rules for \\(\\widehat{\\texttt{sale.price}}\\texttt{[A]}\\) and \\(\\widehat{\\texttt{sale.price}}\\texttt{[C]}\\),\n\n\n\n\n\\[\\begin{align}&\\log(\\widehat{\\texttt{sale.price}}\\texttt{[A]}) - \\log(\\widehat{\\texttt{sale.price}}\\texttt{[C]}) \\\\\n\\,=\\, &\\hat{\\texttt{b3}}\\,*\\,(\\texttt{borough_Bronx[A]} \\,-\\, \\texttt{borough_Bronx[C]})\\,=\\, \\hat{\\texttt{b3}}\\end{align}\\]\nSo we can have the following: \\[\n\\begin{align}&\\Leftrightarrow\\qquad\\frac{\\widehat{\\texttt{sale.price[A]}}}{ \\widehat{\\texttt{sale.price[C]}}} \\;=\\; \\texttt{exp(}\\hat{\\texttt{b3}}\\texttt{)}\\\\ \\quad&\\Leftrightarrow\\qquad\\,\\widehat{\\texttt{sale.price[A]}} \\;=\\; \\widehat{\\texttt{sale.price[C]}} * \\texttt{exp(}\\hat{\\texttt{b3}}\\texttt{)}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables-5",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables-5",
    "title": "Lecture 7",
    "section": "Beta Estimates for Log-transformed Variables",
    "text": "Beta Estimates for Log-transformed Variables\n2. borough_nameBronx\n\nSuppose \\(\\texttt{exp(}\\hat{\\texttt{b3}}\\texttt{)} = 0.2831691\\).\n\nThen \\(\\widehat{\\texttt{sale.price[A]}}\\) is \\(0.2831691\\times\\widehat{\\texttt{sale.price[B]}}\\).\nAll else being equal, an increase in \\(\\texttt{borough_Bronx}\\) by one unit is associated with a decrease in \\(\\texttt{sale.price}\\) by (1 - 0.2831691) = 71.78%.\nAll else being equal, being in Bronx relative to being in Manhattan is associated with a decrease in \\(\\texttt{sale.price}\\) by 71.78%."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables-6",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#beta-estimates-for-log-transformed-variables-6",
    "title": "Lecture 7",
    "section": "Beta Estimates for Log-transformed Variables",
    "text": "Beta Estimates for Log-transformed Variables\n\nAll else being equal, an increase in gross.square.feet by one unit is associated with an increase in log(sale.price) by b1.\n\nAll else being equal, an increase in gross.square.feet by one unit is associated with an increase in sale.price by (exp(b1) - 1) * 100%.\n\nAll else being equal, an increase in broughBronx by one unit is associated with an increase in log(sale.price) by b3.\n\nAll else being equal, being in Bronx is associated with a decrease in sale.price by |(exp(b3) - 1)| * 100% relative to being in Manhattan."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-5",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-5",
    "title": "Lecture 7",
    "section": "Residual Plots",
    "text": "Residual Plots\nWhat happens if biased?\n\nThe model consistently overpredicts or underpredicts for certain values.\n\nIndicates that the model might be misspecified‚Äîperhaps missing important variables or using an incorrect functional form.\nLeads to biased parameter estimates, meaning the coefficients are systematically off, reducing the validity of predictions and inferences."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-6",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#residual-plots-6",
    "title": "Lecture 7",
    "section": "Residual Plots",
    "text": "Residual Plots\nWhat happens if heteroscedasticity is present?\n\nConsequences of heteroscedasticity:\n\nüìâ Inefficient coefficient estimates: Estimates remain unbiased but are no longer efficient (i.e., they don‚Äôt have the smallest variance).\n‚ùå Biased standard errors: Leads to unreliable p-values and confidence intervals, potentially resulting in invalid hypothesis tests.\n‚ö†Ô∏è Misleading inferences: Predictors may appear statistically significant or insignificant incorrectly.\nüéØ Poor predictive performance: The model might perform poorly on future data, especially if the residual variance grows with higher predicted values."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#adding-a-log-transformed-variable-to-a-pyspark-dataframe",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#adding-a-log-transformed-variable-to-a-pyspark-dataframe",
    "title": "Lecture 7",
    "section": "Adding a Log-transformed Variable to a PySpark DataFrame",
    "text": "Adding a Log-transformed Variable to a PySpark DataFrame\n\nWe use the log() function to add a log-transformed variable:\n\nfrom pyspark.sql.functions import log\nsale_df = sale_df.withColumn(\"log_sale_price\", \n                              log( sale_df['sale_price'] ) ) \n\nTo use a exponential function, we can use an exp() function from numpy:\n\nimport numpy as np\n\nnp.exp(1)\nnp.exp([1, 2, 3])"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-transformed-variables-in-pyspark",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-transformed-variables-in-pyspark",
    "title": "Lecture 7",
    "section": "Log-transformed Variables in PySpark",
    "text": "Log-transformed Variables in PySpark\n\nWe use the log() function to add a log-transformed variable:\n\nfrom pyspark.sql.functions import log\nsale_df = sale_df.withColumn(\"log_sale_price\", \n                              log( sale_df['sale_price'] ) ) \n\nTo use a exponential function, we can use an exp() function from numpy:\n\nimport numpy as np\n\nnp.exp(1)\nnp.exp([1, 2, 3])"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-interaction-terms-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-interaction-terms-1",
    "title": "Lecture 7",
    "section": "Linear Regression with Interaction Terms",
    "text": "Linear Regression with Interaction Terms\nMotivation\n\nDoes the relationship between sale.price and gross.square.feet vary by borough_name?\n\nHow can linear regression address the question above?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-interaction-terms-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-interaction-terms-2",
    "title": "Lecture 7",
    "section": "Linear Regression with Interaction Terms",
    "text": "Linear Regression with Interaction Terms\nModel\n\nThe linear regression with an interaction between predictors \\(X_{1}\\) and \\(X_{2}\\) are:\n\n\\[Y_{\\texttt{i}} \\,=\\, b_{0} \\,+\\, b_{1}\\,X_{1,\\texttt{i}} \\,+\\, b_{2}\\,X_{2,\\texttt{i}} \\,+\\, b_{3}\\,X_{1,\\texttt{i}}\\times \\color{Red}{X_{2,\\texttt{i}}} \\,+\\, e_{\\texttt{i}},\\]\n\nwhere\n\n\\(i\\;\\): \\(\\;\\;i\\)-th observation in the training DataFrame, \\(i = 1, 2, 3, \\cdots\\).\n\\(Y_{i}\\,\\): \\(\\;i\\)-th observation of outcome \\(Y\\).\n\\(X_{p, i}\\,\\): \\(i\\)-th observation of the \\(p\\)-th predictor \\(X_{p}\\).\n\\(e_{i}\\;\\): \\(\\;i\\)-th observation of statistical error."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-interaction-terms-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-interaction-terms-3",
    "title": "Lecture 7",
    "section": "Linear Regression with Interaction Terms",
    "text": "Linear Regression with Interaction Terms\nModel\n\nThe linear regression with an interaction between predictors \\(X_{1}\\) and \\(X_{2}\\) are:\n\n\\[Y_{\\texttt{i}} \\,=\\, b_{0} \\,+\\, b_{1}\\,X_{1,\\texttt{i}} \\,+\\, b_{2}\\,X_{2,\\texttt{i}} \\,+\\, b_{3}\\,X_{1,\\texttt{i}}\\times \\color{Red}{X_{2,\\texttt{i}}} \\,+\\, e_{\\texttt{i}}\\;.\\]\n\n\nInteraction\n\nThe relationship between \\(X_{1}\\) and \\(Y\\) varies by values of \\(b_{3}\\, X_{2}\\):\n\n\\[\\frac{\\Delta Y}{\\Delta X_{1}} \\,=\\, b_{1} + b_{3}\\, X_{2}\\].\n\nExample\n\n\\(X_{2}\\) is often a dummy variable. If \\(b_{3} \\neq 0\\) and \\(X_{2, \\texttt{i}} = 1\\),\n\n\\[\\frac{\\Delta Y}{\\Delta X_{1}} \\,=\\, b_{1} + b_{3}\\]."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-interaction-terms-4",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-interaction-terms-4",
    "title": "Lecture 7",
    "section": "Linear Regression with Interaction Terms",
    "text": "Linear Regression with Interaction Terms\nInteraction with a Dummy Variable\n\nThe linear regression with an interaction between predictors \\(X_{1}\\) and \\(X_{2}\\in\\{\\,0, 1\\,\\}\\) are:\n\n\\[Y_{\\texttt{i}} \\,=\\, b_{0} \\,+\\, b_{1}\\,X_{1,\\texttt{i}} \\,+\\, b_{2}\\,X_{2,\\texttt{i}} \\,+\\, b_{3}\\,X_{1,\\texttt{i}}\\times \\color{Red}{X_{2,\\texttt{i}}} \\,+\\, e_{\\texttt{i}},\\] where \\(X_{\\,2, \\texttt{i}}\\) is either 0 or 1.\n\nFor \\(\\texttt{i}\\) such that \\(X_{\\,2, \\texttt{i}} = 0\\), the model is \\[Y_{\\texttt{i}} \\,=\\, b_{0} \\,+\\, b_{1}\\,X_{1,\\texttt{i}} \\,+\\, e_{\\texttt{i}}\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\;\\;\\]\nFor \\(\\texttt{i}\\) such that \\(X_{\\,2, \\texttt{i}} = 1\\), the model is\n\n\\[Y_{\\texttt{i}} \\,=\\, (\\,b_{0} \\,+\\, b_{2}\\,) \\,+\\, (\\,b_{1}\\,+\\, b_{3}\\,)\\,X_{1,\\texttt{i}} \\,+\\, e_{\\texttt{i}}\\qquad\\qquad\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-interaction-terms-5",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-interaction-terms-5",
    "title": "Lecture 7",
    "section": "Linear Regression with Interaction Terms",
    "text": "Linear Regression with Interaction Terms\nMotivation\n\nIs sale.price related with gross.square.feet?\n\n\\[\n\\begin{align}\n\\texttt{sale_price[i]} \\;=\\;\\, &\\texttt{b0} \\,+\\,\\\\\n&\\texttt{b1*Bronx[i]} \\,+\\,\\texttt{b2*Brooklyn[i]} \\,+\\,\\\\&\\texttt{b3*Queens[i]} \\,+\\,\\texttt{b4*Staten Island[i]}\\,+\\,\\\\\n&\\texttt{b5*age[i]}\\,+\\,\\\\\n&\\texttt{b6*gross_square_feet[i]} \\,+\\,\\texttt{e[i]}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-interaction-terms-6",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#linear-regression-with-interaction-terms-6",
    "title": "Lecture 7",
    "section": "Linear Regression with Interaction Terms",
    "text": "Linear Regression with Interaction Terms\nMotivation\n\nDoes the relationship between sale.price and gross.square.feet vary by borough_name?\n\n\n\n\n\\[\n\\begin{align}\n\\texttt{sale_price[i]} \\;=\\;\\, &\\texttt{b0} \\,+\\,\\\\\n&\\texttt{b1*Bronx[i]} \\,+\\,\\texttt{b2*Brooklyn[i]} \\,+\\,\\\\\n&\\texttt{b3*Queens[i]} \\,+\\,\\texttt{b4*Staten Island[i]}\\,+\\, \\\\\n&\\texttt{b5*age[i]}\\,+\\,\\\\\n&\\texttt{b6*gross_square_feet[i]} \\,+\\,\\\\\n&\\texttt{b7*gross_square_feet[i]*Bronx[i]} \\,+\\,  \\\\\n&\\texttt{b8*gross_square_feet[i]*Brooklyn[i]} \\,+\\,  \\\\\n&\\texttt{b9*gross_square_feet[i]*Queens[i]} \\,+\\,  \\\\\n&\\texttt{b10*gross_square_feet[i]*Staten Island[i]} \\,+\\, \\texttt{e[i]} \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-1",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-1",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\n\nTo estimate the price elasticity of orange juice (OJ), we will use sales data for OJ from Dominick‚Äôs grocery stores in the 1990s.\n\nWeekly price and sales (in number of cartons ‚Äúsold‚Äù) for three OJ brandsTropicana, Minute Maid, Dominick‚Äôs\nA dummy, ad, showing whether each brand was advertised (in store or flyer) that week.\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nsales\nQuantity of OJ cartons sold\n\n\nprice\nPrice of OJ\n\n\nbrand\nBrand of OJ\n\n\nad\nAdvertisement status"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-2",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-2",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\nEstimating Price Elasticity\n\nLet‚Äôs prepare the OJ data:\n\noj = pd.read_csv('https://bcdanl.github.io/data/dominick_oj_feat.csv')"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-3",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-3",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\nEstimating Price Elasticity\n\nThe following model estimates the price elasticity of demand for a carton of OJ:\n\n\n\n\n\\[\\begin{align}\n\\log(\\texttt{sales}_{\\texttt{i}}) &\\,=\\, \\;\\; b_{\\texttt{intercept}} \\,+\\, b_{\\,\\texttt{mm}}\\,\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}} \\,+\\, b_{\\,\\texttt{tr}}\\,\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}\\\\\n&\\quad\\,+\\, b_{\\texttt{price}}\\,\\log(\\texttt{price}_{\\texttt{i}}) \\,+\\, e_{\\texttt{i}},\\\\\n\\text{where}\\qquad\\qquad&\\\\\n\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}\n&\\,=\\, \\begin{cases}\n\\texttt{1} & \\text{ if an orange juice } \\texttt{i} \\text{ is } \\texttt{Tropicana};\\\\\\\\\n\\texttt{0} & \\text{otherwise}.\\qquad\\qquad\\quad\\,\n\\end{cases}\\\\\n\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}} &\\,=\\, \\begin{cases}\n\\texttt{1} & \\text{ if an orange juice } \\texttt{i} \\text{ is } \\texttt{Minute Maid};\\\\\\\\\n\\texttt{0} & \\text{otherwise}.\\qquad\\qquad\\quad\\,\n\\end{cases}\n\\end{align}\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-4",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-4",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\nEstimating Price Elasticity\n\nThe following model estimates the price elasticity of demand for a carton of OJ:\n\n\\[\\log(\\texttt{sales}_{\\texttt{i}}) \\,=\\, \\quad\\;\\; b_{\\texttt{intercept}} \\,+\\, b_{\\,\\texttt{mm}}\\,\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}} \\,+\\, b_{\\,\\texttt{tr}}\\,\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}\\\\\n\\,+\\, b_{\\texttt{price}}\\,\\log(\\texttt{price}_{\\texttt{i}}) \\,+\\, e_{\\texttt{i}}\\]\n\nWhen \\(\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}\\,=\\,0\\) and \\(\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}}\\,=\\,0\\), the beta coefficient for the intercept \\(b_{\\texttt{intercept}}\\) gives the value of Dominick‚Äôs log sales at \\(\\log(\\,\\texttt{price[i]}\\,) = 0\\).\nThe beta coefficient \\(b_{\\texttt{price}}\\) is the price elasticity of demand.\n\nIt measures how sensitive the quantity demanded is to its price."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-5",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-5",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\nEstimating Price Elasticity\n\nFor small changes in variable \\(x\\) from \\(x_{0}\\) to \\(x_{1}\\), the following equation holds:\n\n\n\n\n\\[\\Delta \\log(x) \\,= \\, \\log(x_{1}) \\,-\\, \\log(x_{0})\n\\approx\\, \\frac{x_{1} \\,-\\, x_{0}}{x_{0}}\n\\,=\\, \\frac{\\Delta\\, x}{x_{0}}.\\]\n\nThe coefficient on \\(\\log(\\texttt{price}_{\\texttt{i}})\\), \\(b_{\\texttt{price}}\\), is therefore\n\n\n\n\n\\[b_{\\texttt{price}} \\,=\\, \\frac{\\Delta \\log(\\texttt{sales}_{\\texttt{i}})}{\\Delta \\log(\\texttt{price}_{\\texttt{i}})}\\,=\\, \\frac{\\frac{\\Delta \\texttt{sales}_{\\texttt{i}}}{\\texttt{sales}_{\\texttt{i}}}}{\\frac{\\Delta \\texttt{price}_{\\texttt{i}}}{\\texttt{price}_{\\texttt{i}}}}.\\]\n\nAll else being equal, an increase in \\(\\texttt{price}\\) by 1% is associated with a decrease in \\(\\texttt{sales}\\) by \\(b_{\\texttt{price}}\\)%."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-6",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-6",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\nEDA\n\nDescribe the relationship between log(price) and log(sales) by brand.\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\noj['log_price'] = oj['price'].apply(lambda x: np.log(x))\noj['log_sales'] = oj['sales'].apply(lambda x: np.log(x))\n\nsns.lmplot(\n    x='log_price', \n    y='log_sales', \n    hue='brand',     # color by brand\n    data=oj, \n    aspect=1.2, \n    height=5,\n    markers=[\"o\", \"s\", \"D\"],  # optional: different markers per brand\n    scatter_kws={'alpha': 0.025},  # Pass alpha to the scatter plot\n    legend=True\n)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-7",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-7",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\nEDA\n\nDescribe the relationship between log(price) and log(sales) by brand and ad.\n\nsns.lmplot(\n    x='log_price', \n    y='log_sales', \n    hue='brand',     # color by brand\n    col='ad',         # facet by feat\n    data=oj, \n    aspect=1.2, \n    height=5,\n    markers=[\"o\", \"s\", \"D\"],  # optional: different markers per brand\n    # Pass alpha to the scatter plot\n    scatter_kws={'alpha': 0.025},\n    col_wrap=2,         # how many facets per row\n    legend=True\n)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-8",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-8",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\nEstimating Price Elasticity\n\nLet‚Äôs train the first model, model_1:\n\n\\[\n\\begin{align}\n\\log(\\texttt{sales}_{\\texttt{i}}) \\,=\\, &\\quad\\;\\; b_{\\texttt{intercept}} \\\\ &\\,+\\,b_{\\,\\texttt{mm}}\\,\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}} \\,+\\, b_{\\,\\texttt{tr}}\\,\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}\\\\\n&\\,+\\, b_{\\texttt{price}}\\,\\log(\\texttt{price}_{\\texttt{i}}) \\,+\\, e_{\\texttt{i}}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-9",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-9",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\nEstimating Price Elasticity\n\nHow does the relationship between log(sales) and log(price) vary by brand?\n\nLet‚Äôs train the second model, model_2, that addresses the above question:\n\n\n\\[\n\\begin{align}\n\\log(\\texttt{sales}_{\\texttt{i}}) \\,=\\,&\\;\\; \\quad b_{\\texttt{intercept}} \\,+\\, \\color{Green}{b_{\\,\\texttt{mm}}\\,\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}}} \\,+\\, \\color{Blue}{b_{\\,\\texttt{tr}}\\,\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}}\\\\\n&\\,+\\, b_{\\texttt{price}}\\,\\log(\\texttt{price}_{\\texttt{i}})  \\\\\n&\\, +\\, b_{\\texttt{price*mm}}\\,\\log(\\texttt{price}_{\\texttt{i}})\\,\\times\\,\\color{Green} {\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}}} \\\\\n&\\,+\\, b_{\\texttt{price*tr}}\\,\\log(\\texttt{price}_{\\texttt{i}})\\,\\times\\,\\color{Blue} {\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}} \\,+\\, e_{\\texttt{i}}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-10",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-10",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\n\nFor \\(\\texttt{i}\\) such that \\(\\color{Green}{\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}}} = 0\\) and \\(\\color{Blue}{\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}} = 0\\), the model equation is: \\[\\log(\\texttt{sales}_{\\texttt{i}}) \\,=\\, \\; \\,b_{\\texttt{intercept}}\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\\\\n\\qquad\\,+\\, b_{\\texttt{price}} \\,\\log(\\texttt{price}_{\\texttt{i}}) \\,+\\, e_{\\texttt{i}}\\,.\\qquad\\qquad\\;\\]\nFor \\(\\texttt{i}\\) such that \\(\\color{Green}{\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}}} = 1\\) and \\(\\color{Blue}{\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}} = 0\\), the model equation is: \\[\\log(\\texttt{sales}_{\\texttt{i}}) \\,=\\, \\; (\\,b_{\\texttt{intercept}} \\,+\\, b_{\\,\\texttt{mm}}\\,)\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\\\\n\\qquad\\!\\,+\\,(\\, b_{\\texttt{price}} \\,+\\, \\color{Green}{b_{\\texttt{price*mm}}}\\,)\\,\\log(\\texttt{price}_{\\texttt{i}}) \\,+\\, e_{\\texttt{i}}\\,.\\]\nFor \\(\\texttt{i}\\) such that \\(\\color{Green}{\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}}} = 0\\) and \\(\\color{Blue}{\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}} = 1\\), the model equation is: \\[\\log(\\texttt{sales}_{\\texttt{i}}) \\,=\\, \\; (\\,b_{\\texttt{intercept}} \\,+\\, b_{\\,\\texttt{tr}}\\,)\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\\\\n\\qquad\\!\\,+\\,(\\, b_{\\texttt{price}} \\,+\\, \\color{Blue}{b_{\\texttt{price*tr}}}\\,)\\,\\log(\\texttt{price}_{\\texttt{i}}) \\,+\\, e_{\\texttt{i}}\\,.\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-11",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-11",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\n\nHow does advertisement play a role in the relationship between sales and prices in the OJ market?\n\nThe ads can increase sales at all prices.\nThey can change price sensitivity.\nThey can do both of these things in a brand-specific manner.\n\nWhat would be the formula that address the question above?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-12",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-12",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\n\nLet‚Äôs train the third model, model_3, that addresses the question in the previous page:\n\n\n\n\n\\[\n\\begin{align}\n\\log(\\texttt{sales}_{\\texttt{i}}) \\,=\\,\\quad\\;\\;& b_{\\texttt{intercept}} \\,+\\, \\color{Green}{b_{\\,\\texttt{mm}}\\,\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}}} \\,+\\, \\color{Blue}{b_{\\,\\texttt{tr}}\\,\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}}  \\\\\n&\\,+\\; b_{\\,\\texttt{ad}}\\,\\color{Orange}{\\texttt{ad}_{\\,\\texttt{i}}} \\qquad\\qquad\\qquad\\qquad\\quad   \\\\\n&\\,+\\, b_{\\texttt{mm*ad}}\\,\\color{Green} {\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}}}\\,\\times\\, \\color{Orange}{\\texttt{ad}_{\\,\\texttt{i}}}\\,+\\, b_{\\texttt{tr*ad}}\\,\\color{Blue} {\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}}\\,\\times\\, \\color{Orange}{\\texttt{ad}_{\\,\\texttt{i}}} \\\\\n&\\,+\\;  b_{\\texttt{price}}\\,\\log(\\texttt{price}_{\\texttt{i}}) \\qquad\\qquad\\qquad\\;\\;\\;\\;\\,  \\\\\n&\\,+\\, b_{\\texttt{price*mm}}\\,\\log(\\texttt{price}_{\\texttt{i}})\\,\\times\\,\\color{Green} {\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}}}\\qquad\\qquad\\qquad\\;\\, \\\\\n&\\,+\\, b_{\\texttt{price*tr}}\\,\\log(\\texttt{price}_{\\texttt{i}})\\,\\times\\,\\color{Blue} {\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}}\\qquad\\qquad\\qquad\\;\\, \\\\\n& \\,+\\, b_{\\texttt{price*ad}}\\,\\log(\\texttt{price}_{\\texttt{i}})\\,\\times\\,\\color{Orange}{\\texttt{ad}_{\\,\\texttt{i}}}\\qquad\\qquad\\qquad\\;\\;\\, \\\\\n&\\,+\\, b_{\\texttt{price*mm*ad}}\\,\\log(\\texttt{price}_{\\texttt{i}}) \\,\\times\\,\\,\\color{Green} {\\texttt{brand}_{\\,\\texttt{mm}, \\texttt{i}}}\\,\\times\\, \\color{Orange}{\\texttt{ad}_{\\,\\texttt{i}}} \\\\\n&\\,+\\, b_{\\texttt{price*tr*ad}}\\,\\log(\\texttt{price}_{\\texttt{i}}) \\,\\times\\,\\,\\color{Blue} {\\texttt{brand}_{\\,\\texttt{tr}, \\texttt{i}}}\\,\\times\\, \\color{Orange}{\\texttt{ad}_{\\,\\texttt{i}}}  \\,+\\, e_{\\texttt{i}}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-13",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-13",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\nEstimating Price Elasticity\n\nDescribe the relationship between ad and brand using stacked bar charts.\n\nsns.histplot(\n    data=oj,\n    x='ad_status',   # categorical variable on the x-axis\n    hue='brand',     # fill color by brand\n    multiple='fill'  # 100% stacked bars\n)\nplt.ylabel('Proportion')\n\nModel 3 assumes that the relationship between price and sales can vary by ad."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-14",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-14",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\nEstimating Price Elasticity\n\n\nHow would you explain different estimation results across different models?\nWhich model do you prefer? Why?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-15",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-15",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\nEstimating Price Elasticity\n\nHere are the inverse demand curves from model_3:\n\n.pull-left[\n\n\n\n\n\n]\n.pull-right[\n\n\n\n\n\n]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-16",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-16",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\nEstimating Price Elasticity\n\nDescribe the relationship between ad and brand using stacked bar charts.\n\n.pull-left[\n\n\n\n\n\n]\n.pull-right[\n\n\n\n\n\n]\n\nModel 3 assumes that the relationship between price and sales can vary by ad."
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-17",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#log-log-linear-regression-17",
    "title": "Lecture 7",
    "section": "Log-Log Linear Regression",
    "text": "Log-Log Linear Regression\nEstimating Price Elasticity\n\n\nHow would you explain different estimation results across different models?\nWhich model do you prefer? Why?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-07-2025-0217.html#adding-interaction-terms-in-pyspark",
    "href": "danl-lec/danl-320-lec-07-2025-0217.html#adding-interaction-terms-in-pyspark",
    "title": "Lecture 7",
    "section": "Adding Interaction Terms in PySpark",
    "text": "Adding Interaction Terms in PySpark\n\nThe UDF add_interaction_terms requires at least two lists of variable names.\nvar_list3 is optional.\n\nIf var_list3 is provided, the add_interaction_terms function adds all possible two-way interactions and three-way interaction to the dtrain and dtest DataFrames.\n\n\ndef add_interaction_terms(var_list1, var_list2, var_list3=None):\n    \"\"\"\n    Creates interaction term columns in the global DataFrames dtrain and dtest.\n\n    For two sets of variable names (which may represent categorical (dummy) or continuous variables),\n    this function creates two-way interactions by multiplying each variable in var_list1 with each\n    variable in var_list2.\n\n    Optionally, if a third list of variable names (var_list3) is provided, the function also creates\n    three-way interactions among each variable in var_list1, each variable in var_list2, and each variable\n    in var_list3.\n\n    Parameters:\n        var_list1 (list): List of column names for the first set of variables.\n        var_list2 (list): List of column names for the second set of variables.\n        var_list3 (list, optional): List of column names for the third set of variables for three-way interactions.\n\n    Returns:\n        A flat list of new interaction column names.\n    \"\"\"\n    global dtrain, dtest\n\n    interaction_cols = []\n\n    # Create two-way interactions between var_list1 and var_list2.\n    for var1 in var_list1:\n        for var2 in var_list2:\n            col_name = f\"{var1}_*_{var2}\"\n            dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list1 and var_list3.\n    if var_list3 is not None:\n      for var1 in var_list1:\n          for var3 in var_list3:\n              col_name = f\"{var1}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list2 and var_list3.\n    if var_list3 is not None:\n      for var2 in var_list2:\n          for var3 in var_list3:\n              col_name = f\"{var2}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # If a third list is provided, create three-way interactions.\n    if var_list3 is not None:\n        for var1 in var_list1:\n            for var2 in var_list2:\n                for var3 in var_list3:\n                    col_name = f\"{var1}_*_{var2}_*_{var3}\"\n                    dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    interaction_cols.append(col_name)\n\n    return interaction_cols\n\n # Example\n # interaction_cols_brand_price = add_interaction_terms(dummy_cols_brand, ['log_price'])\n # interaction_cols_brand_ad_price = add_interaction_terms(dummy_cols_brand, dummy_cols_ad, ['log_price'])"
  },
  {
    "objectID": "danl-cw/danl-320-cw-08.html#histogram",
    "href": "danl-cw/danl-320-cw-08.html#histogram",
    "title": "Classwork 8",
    "section": "",
    "text": "# Create a histogram\ndfpd = DATAFRAME.select([\"Y_VARIABLE\"]).toPandas()\nsns.histplot(dfpd[\"Y_VARIABLE\"], bins=10, kde=True)"
  },
  {
    "objectID": "danl-hw/danl-320-hw-02.html",
    "href": "danl-hw/danl-320-hw-02.html",
    "title": "Homework 2",
    "section": "",
    "text": "Please submit your Jupyter Notebook for Part 1 in Homework 2 to Brightspace with the name below:\n\ndanl-320-hw2-LASTNAME-FIRSTNAME.ipynb\n( e.g., danl-320-hw2-choe-byeonghak.ipynb )\n\nThe due is March 24, 2025, 3:15 P.M.\n\nIt is recommended to finish it before the Spring Break begins.\n\nPlease send Byeong-Hak an email (bchoe@geneseo.edu) if you have any questions."
  },
  {
    "objectID": "danl-hw/danl-320-hw-02.html#variable-description",
    "href": "danl-hw/danl-320-hw-02.html#variable-description",
    "title": "Homework 2",
    "section": "Variable Description",
    "text": "Variable Description\n\n\n\n\n\n\n\nVariable Name\nDescription\n\n\n\n\nhousehold\nUnique identifier for household\n\n\nX_purchase_desc\nDescription of beer purchase\n\n\nquantity\nNumber of beer packages purchased\n\n\nbrand\nBrand of beer purchased\n\n\ndollar_spent\nTotal amount spent on the purchase\n\n\nbeer_floz\nTotal volume of beer purchased (in fluid ounces)\n\n\nprice_floz\nPrice per fluid ounce of beer\n\n\ncontainer\nType of beer container (e.g., CAN, BOTTLE)\n\n\npromo\nIndicates if the purchase was part of a promotion (True/False)\n\n\nregion\nGeographic region of purchase\n\n\nmarital\nMarital status of household head\n\n\nincome\nIncome level of the household\n\n\nage\nAge group of household head\n\n\nemployment\nEmployment status of household head\n\n\ndegree\nEducation level of household head\n\n\noccupation\nOccupation category of household head\n\n\nethnic\nEthnicity of household head\n\n\nmicrowave\nIndicates if the household owns a microwave (True/False)\n\n\ndishwasher\nIndicates if the household owns a dishwasher (True/False)\n\n\ntvcable\nType of television subscription (e.g., basic, premium)\n\n\nsinglefamilyhome\nIndicates if the household is a single-family home (True/False)\n\n\nnpeople\nNumber of people in the household"
  },
  {
    "objectID": "danl-hw/danl-320-hw-02.html#question-1",
    "href": "danl-hw/danl-320-hw-02.html#question-1",
    "title": "Homework 2",
    "section": "Question 1",
    "text": "Question 1\nCreate the DataFrame that keeps all the observations whose value of container is either ‚ÄòCAN‚Äô or ‚ÄòNON REFILLABLE BOTTLE‚Äô in the beer_markets DataFrame."
  },
  {
    "objectID": "danl-hw/danl-320-hw-02.html#question-2",
    "href": "danl-hw/danl-320-hw-02.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\nSplit the resulting DataFrame of Question 1 into training and test DataFrames such that approximately 67% of observations in the resulting DataFrame of Question 1 belong to the training DataFrame and the rest observations belong to the test DataFrame."
  },
  {
    "objectID": "danl-hw/danl-320-hw-02.html#questions-3-",
    "href": "danl-hw/danl-320-hw-02.html#questions-3-",
    "title": "Homework 2",
    "section": "Questions 3-",
    "text": "Questions 3-\nConsider the following three linear regression models:\n\nModel 1.\n\\[\n\\begin{align}\n\\log(\\text{price_per_floz}) \\,=\\, &\\beta_{0} \\,+\\, \\sum_{i=1}^{N}\\beta_{i}\\,\\text{market}_{i} \\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j}\\,\\text{brand}_{j} \\,+\\, \\beta_{N+5}\\,\\text{container_CAN}  \\\\\n&\\,+\\, \\beta_{N+6}\\log(\\text{beer_floz})\\\\\n&\\,+\\, \\epsilon\n\\end{align}\n\\]\n\n\nModel 2.\n\\[\n\\begin{align}\n\\log(\\text{price_per_floz}) \\,=\\, & \\beta_{0} \\,+\\, \\sum_{i=1}^{N}\\beta_{i}\\,\\text{market}_{i} \\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j}\\,\\text{brand}_{j} \\,+\\, \\beta_{N+5}\\,\\text{container_CAN} \\\\\n&\\,+\\, \\beta_{N+6}\\log(\\text{beer_floz})\\\\\n&\\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j\\times\\text{beer_floz}}\\,\\text{brand}_{j}\\times \\log(\\text{beer_floz})\\\\\n&\\,+\\, \\epsilon\n\\end{align}\n\\]\n\n\nModel 3.\n\\[\n\\begin{align}\n\\log(\\text{price_per_floz}) \\,=\\, & \\beta_{0} \\,+\\, \\sum_{i=1}^{N}\\beta_{i}\\,\\text{market}_{i} \\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j}\\,\\text{brand}_{j} \\,+\\, \\beta_{N+5}\\,\\text{container_CAN} \\\\\n&\\,+\\, \\beta_{N+6}\\log(\\text{beer_floz})\\\\\n&\\,+\\, \\beta_{N+7}\\,\\text{promo} \\times\\log(\\text{beer_floz}) \\\\\n&\\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j\\times\\text{beer_floz}}\\,\\text{brand}_{j}\\times \\log(\\text{beer_floz})\\\\\n&\\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j\\times\\text{promo}}\\,\\text{brand}_{j}\\times \\text{promo}\\\\\n&\\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j\\times\\text{promo}}\\,\\text{brand}_{j}\\times \\text{promo}\\times \\log(\\text{beer_floz})\\\\\n&\\,+\\, \\epsilon\n\\end{align}\n\\] - Set ‚ÄúBUD LIGHT‚Äù as the reference level for \\(\\text{brand}\\). - Set ‚ÄúBUFFALO-ROCHESTER‚Äù as the reference level for \\(\\text{market}\\).\n\n\n\nQuestion 3\nProvide intuition behind each of the model.\n\n\n\nQuestion 4\n\nTrain each of the three linear regression models using the training DataFrame from Question 2.\n\nProvide the summary of the result for each linear regression model.\nWhat are the predicted beer prices for unseen data from each model?\n\n\n\n\n\nQuestion 5\n\nInterpret the beta estimates of the following variables from the Model 3:\n\nmarket_ALBANY\nmarket_EXURBAN NY\nmarket_RURAL NEW YORK\nmarket_SURBURBAN NY\nmarket_SYRACUSE\nmarket_URBAN NY\n\n\n\n\n\nQuestion 6\n\nAcross the three models, how is the percentage change in the price of beer sensitive to the percentage change in the volume of beer purchases for each brand?\nHow does promo affect such sensitivity in the Model 3?\n\n\n\n\nQuestion 7\n\nDraw a residual plot from each of the three models.\n\nOn average, are the prediction correct? Are there systematic errors?\n\n\n\n\n\nQuestion 8\nWhich model do you prefer most and why?"
  },
  {
    "objectID": "danl-hw/danl-320-hw-02.html#variable-description-1",
    "href": "danl-hw/danl-320-hw-02.html#variable-description-1",
    "title": "Homework 2",
    "section": "Variable Description",
    "text": "Variable Description\n\n\n\n\n\n\n\n\n\nVariable Name\nType\nDescription\nCategories / Notes\n\n\n\n\npriceper1\nNumeric\nThe unit price for one product serving (in dollars).\n\n\n\nflavor_descr\nCategorical\nDescription of the ice cream flavor.\n\n\n\nsize1_descr\nCategorical\nThe description of the package or serving size (in fluid ounces or a related measure).\n\n\n\nhousehold_id\nCategorical/ID\nUnique identifier for each household purchasing product(s).\n\n\n\nhousehold_income\nCategorical/Numeric\nThe household income bracket (in dollars).\nThe number (e.g., 60,000) corresponds to predefined income categories. (for instance, ‚Äú60,000‚Äù represents a particular income range from $60,000 to $70,000). Not a categorical variable per se, but can be grouped into categories if desired.\n\n\nhousehold_size\nCategorical\nThe number of persons in the household.\nDiscrete numeric count (e.g., 1, 2, 3‚Ä¶). Not a categorical variable per se, but can be grouped into categories if desired.\n\n\nusecoup\nCategorical\nIndicates whether a coupon was used in the purchase.\nBoolean category: True (coupon used) or False (coupon not used).\n\n\ncouponper1\nNumeric\nThe discount amount per unit applied when a coupon is used.\nContinuous numeric value; often zero if no coupon is used, and a positive number when a discount is applied.\n\n\nregion\nCategorical\nGeographic region where the household is located.\nCategories include ‚ÄúEast‚Äù, ‚ÄúCentral‚Äù, ‚ÄúWest‚Äù, and ‚ÄúSouth‚Äù.\n\n\nmarried\nCategorical\nMarital status of the household head (or the household overall status).\nBoolean category: True (married) or False (not married).\n\n\nrace\nCategorical\nRace of the household head or primary respondent.\nCategories include ‚Äúwhite‚Äù, ‚Äúblack‚Äù, ‚Äúasian‚Äù, and ‚Äúother‚Äù.\n\n\nhispanic_origin\nCategorical\nIndicates whether the household identifies as of Hispanic origin.\nBoolean category: True (Hispanic origin) or False (not Hispanic).\n\n\nmicrowave\nCategorical\nWhether the household owns a microwave.\nBoolean category: True (owns microwave) or False (does not own one).\n\n\ndishwasher\nCategorical\nWhether the household owns a dishwasher.\nBoolean category: True (owns dishwasher) or False (does not own one).\n\n\nsfh\nCategorical\nWhether the household resides in a single-family home.\nBoolean category: True (single-family home) or False (does not live in a single-family home).\n\n\ninternet\nCategorical\nWhether the household has internet service.\nBoolean category: True (has internet) or False (does not).\n\n\ntvcable\nCategorical\nIndicates whether the household subscribes to cable television service.\nBoolean category: True (has cable TV) or False (does not).\n\n\n\n\n\n\n\n\n\n\n\nWrite a blog post about Ben and Jerry‚Äôs ice cream in the ice_cream DataFrame using Jupyter Notebook, and add it to your online blog.\n\nIn your blog post, provide your linear regression model, as well as descriptive statistics, counting, filtering, and various group operations.\nOptionally, provide seaborn visualizations."
  },
  {
    "objectID": "danl-hw/danl-320-hw-02.html#questions-3-8",
    "href": "danl-hw/danl-320-hw-02.html#questions-3-8",
    "title": "Homework 2",
    "section": "Questions 3-8",
    "text": "Questions 3-8\nConsider the following three linear regression models:\n\nModel 1\n\\[\n\\begin{aligned}\n\\log(\\text{price\\_per\\_floz}) = &\\ \\beta_{0} + \\sum_{i=1}^{N} \\beta_{i} \\,\\text{market}_{i} + \\sum_{j=N+1}^{N+4} \\beta_{j} \\,\\text{brand}_{j}\n+ \\beta_{N+5} \\,\\text{container\\_CAN} \\\\\n&\\,+\\, \\beta_{N+6} \\log(\\text{beer\\_floz}) + \\epsilon\n\\end{aligned}\n\\]\n\n\nModel 2\n\\[\n\\begin{aligned}\n\\log(\\text{price\\_per\\_floz}) \\,=\\, & \\beta_{0} \\,+\\, \\sum_{i=1}^{N}\\beta_{i}\\,\\text{market}_{i} \\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j}\\,\\text{brand}_{j} \\,+\\, \\beta_{N+5}\\,\\text{container\\_CAN} \\\\\n&\\,+\\, \\beta_{N+6}\\log(\\text{beer\\_floz})\\\\\n&\\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j\\times\\text{beer\\_floz}}\\,\\text{brand}_{j}\\times \\log(\\text{beer\\_floz})\\\\\n&\\,+\\, \\epsilon\n\\end{aligned}\n\\]\n\n\nModel 3\n\\[\n\\begin{aligned}\n\\log(\\text{price\\_per\\_floz}) \\,=\\, & \\beta_{0} \\,+\\, \\sum_{i=1}^{N}\\beta_{i}\\,\\text{market}_{i} \\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j}\\,\\text{brand}_{j} \\,+\\, \\beta_{N+5}\\,\\text{container\\_CAN} \\\\\n&\\,+\\, \\beta_{N+6}\\log(\\text{beer\\_floz})\\\\\n&\\,+\\, \\beta_{N+7}\\,\\text{promo} \\times\\log(\\text{beer\\_floz}) \\\\\n&\\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j\\times\\text{beer\\_floz}}\\,\\text{brand}_{j}\\times \\log(\\text{beer\\_floz})\\\\\n&\\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j\\times\\text{promo}}\\,\\text{brand}_{j}\\times \\text{promo}\\\\\n&\\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j\\times\\text{promo}\\times\\text{beer\\_floz}}\\,\\text{brand}_{j}\\times \\text{promo}\\times \\log(\\text{beer\\_floz})\\\\\n&\\,+\\, \\epsilon\n\\end{aligned}\n\\]\n\nSet ‚ÄúBUD_LIGHT‚Äù as the reference level for the \\(\\text{brand}\\) variable.\nSet ‚ÄúBUFFALO-ROCHESTER‚Äù as the reference level for the \\(\\text{market}\\) variable..\n\nThere are \\(N+1\\) distinct categories in the \\(\\text{market}\\) variable.\n\n\n\n\n\nQuestion 3\nProvide intuition behind each of the model.\n\n\n\nQuestion 4\n\nTrain each of the three linear regression models using the training DataFrame from Question 2.\n\nProvide the summary of the result for each linear regression model.\nWhat are the predicted beer prices for unseen data from each model?\n\n\n\n\n\nQuestion 5\n\nInterpret the beta estimates of the following variables from the Model 3:\n\nmarket_ALBANY\nmarket_EXURBAN_NY\nmarket_RURAL_NEW_YORK\nmarket_SURBURBAN_NY\nmarket_SYRACUSE\nmarket_URBAN_NY\n\n\n\n\n\nQuestion 6\n\nAcross the three models, how is the percentage change in the price of beer sensitive to the percentage change in the volume of beer purchases for each brand?\nHow does promo affect such sensitivity in the Model 3?\n\n\n\n\nQuestion 7\n\nDraw a residual plot from each of the three models.\n\nOn average, are the prediction correct? Are there systematic errors?\n\n\n\n\n\nQuestion 8\nWhich model do you prefer most and why?"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html",
    "href": "danl-cw/danl-320-cw-09.html",
    "title": "Classwork 9",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n\n\n\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model‚Äôs labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                ci_upper,\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R¬≤\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R¬≤\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: Exp(95% CI Lower), 8: 95% CI Upper, 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"Exp(95% CI Lower)\", \"95% CI Upper\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))\n\n\n\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\n\ndef add_interaction_terms(var_list1, var_list2, var_list3=None):\n    \"\"\"\n    Creates interaction term columns in the global DataFrames dtrain and dtest.\n\n    For two sets of variable names (which may represent categorical (dummy) or continuous variables),\n    this function creates two-way interactions by multiplying each variable in var_list1 with each\n    variable in var_list2.\n\n    Optionally, if a third list of variable names (var_list3) is provided, the function also creates\n    three-way interactions among each variable in var_list1, each variable in var_list2, and each variable\n    in var_list3.\n\n    Parameters:\n        var_list1 (list): List of column names for the first set of variables.\n        var_list2 (list): List of column names for the second set of variables.\n        var_list3 (list, optional): List of column names for the third set of variables for three-way interactions.\n\n    Returns:\n        A flat list of new interaction column names.\n    \"\"\"\n    global dtrain, dtest\n\n    interaction_cols = []\n\n    # Create two-way interactions between var_list1 and var_list2.\n    for var1 in var_list1:\n        for var2 in var_list2:\n            col_name = f\"{var1}_*_{var2}\"\n            dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list1 and var_list3.\n    if var_list3 is not None:\n      for var1 in var_list1:\n          for var3 in var_list3:\n              col_name = f\"{var1}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list2 and var_list3.\n    if var_list3 is not None:\n      for var2 in var_list2:\n          for var3 in var_list3:\n              col_name = f\"{var2}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # If a third list is provided, create three-way interactions.\n    if var_list3 is not None:\n        for var1 in var_list1:\n            for var2 in var_list2:\n                for var3 in var_list3:\n                    col_name = f\"{var1}_*_{var2}_*_{var3}\"\n                    dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    interaction_cols.append(col_name)\n\n    return interaction_cols\n\n # Example\n # interaction_cols_brand_price = add_interaction_terms(dummy_cols_brand, ['log_price'])\n # interaction_cols_brand_ad_price = add_interaction_terms(dummy_cols_brand, dummy_cols_ad, ['log_price'])\n\n\n\n\ndef compare_reg_models(models, assemblers, names=None):\n    \"\"\"\n    Produces a single formatted table comparing multiple regression models.\n\n    For each predictor (the union across models, ordered by first appearance), the table shows\n    the beta estimate (with significance stars) from each model (blank if not used).\n    For a predictor, if a model's outcome (model.getLabelCol()) starts with \"log\", the cell displays\n    both the beta and its exponential (separated by \" / \"), except when the predictor's name includes \"log_\".\n    (The intercept row does not display exp(.))\n\n    Additional rows for Intercept, Observations, R¬≤, and RMSE are appended.\n\n    The header's first column is labeled \"Predictor\", and subsequent columns are\n    \"y: [outcome] ([name])\" for each model.\n\n    The table is produced in grid format (with vertical lines). A dashed line (using '-' characters)\n    is inserted at the top, immediately after the header, and at the bottom.\n    Additionally, immediately after the Intercept row, the border line is replaced with one using '='\n    (to appear as, for example, \"+==============================================+==========================+...\").\n\n    Parameters:\n        models (list): List of fitted LinearRegression models.\n        assemblers (list): List of corresponding VectorAssembler objects.\n        names (list, optional): List of model names; defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing the combined regression table.\n    \"\"\"\n    # Default model names.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(models))]\n\n    # For each model, get outcome and determine if that model is log-transformed.\n    outcomes = [m.getLabelCol() for m in models]\n    is_log_flags = [out.lower().startswith(\"log\") for out in outcomes]\n\n    # Build an ordered union of predictors based on first appearance.\n    ordered_predictors = []\n    for assembler in assemblers:\n        for feat in assembler.getInputCols():\n            if feat not in ordered_predictors:\n                ordered_predictors.append(feat)\n\n    # Helper for significance stars.\n    def significance_stars(p):\n        if p is None:\n            return \"\"\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build rows for each predictor.\n    rows = []\n    for feat in ordered_predictors:\n        row = [feat]\n        for m, a, is_log in zip(models, assemblers, is_log_flags):\n            feats_model = a.getInputCols()\n            if feat in feats_model:\n                idx = feats_model.index(feat)\n                beta = m.coefficients.toArray()[idx]\n                p_val = m.summary.pValues[idx] if m.summary.pValues is not None else None\n                stars = significance_stars(p_val)\n                cell = f\"{beta:.3f}{stars}\"\n                # Only add exp(beta) if model is log and predictor name does NOT include \"log_\"\n                if is_log and (\"log_\" not in feat.lower()):\n                    cell += f\" / {np.exp(beta):,.3f}\"\n                row.append(cell)\n            else:\n                row.append(\"\")\n        rows.append(row)\n\n    # Build intercept row (do NOT compute exp(intercept)).\n    intercept_row = [\"Intercept\"]\n    for m in models:\n        std_all = np.array(m.summary.coefficientStandardErrors)\n        coeffs = m.coefficients.toArray()\n        if len(std_all) == len(coeffs) + 1:\n            intercept_p = m.summary.pValues[0] if m.summary.pValues is not None else None\n        else:\n            intercept_p = None\n        sig = significance_stars(intercept_p)\n        cell = f\"{m.intercept:.3f}{sig}\"\n        intercept_row.append(cell)\n    rows.append(intercept_row)\n\n    # Add Observations row.\n    obs_row = [\"Observations\"]\n    for m in models:\n        obs = m.summary.numInstances\n        obs_row.append(f\"{int(obs):,}\")\n    rows.append(obs_row)\n\n    # Add R¬≤ row.\n    r2_row = [\"R¬≤\"]\n    for m in models:\n        r2_row.append(f\"{m.summary.r2:.3f}\")\n    rows.append(r2_row)\n\n    # Add RMSE row.\n    rmse_row = [\"RMSE\"]\n    for m in models:\n        rmse_row.append(f\"{m.summary.rootMeanSquaredError:.3f}\")\n    rows.append(rmse_row)\n\n    # Build header: first column \"Predictor\", then for each model: \"y: [outcome] ([name])\"\n    header = [\"Predictor\"]\n    for out, name in zip(outcomes, names):\n        header.append(f\"y: {out} ({name})\")\n\n    # Create table string using grid format.\n    table_str = tabulate(rows, headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(models))\n\n    # Split into lines.\n    lines = table_str.split(\"\\n\")\n\n    # Create a dashed line spanning the full width.\n    full_width = len(lines[0])\n    dash_line = '-' * full_width\n    # Create an equals line by replacing '-' with '='.\n    eq_line = dash_line.replace('-', '=')\n\n    # Insert a dashed line after the header row.\n    lines = table_str.split(\"\\n\")\n    # In grid format, header and separator are usually the first two lines.\n    lines.insert(2, dash_line)\n\n    # Insert an equals line after the Intercept row.\n    for i, line in enumerate(lines):\n        if line.startswith(\"|\") and \"Intercept\" in line:\n            if i+1 &lt; len(lines):\n                lines[i+1] = eq_line\n            break\n\n    # Add dashed lines at the very top and bottom.\n    final_table = dash_line + \"\\n\" + \"\\n\".join(lines) + \"\\n\" + dash_line\n\n    return final_table\n\n# Example usage:\n# print(compare_regression_models([model_1, model_2, model_3],\n#                                 [assembler_1, assembler_2, assembler_3],\n#                                 [\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\n\ndef compare_rmse(test_dfs, label_col, pred_col=\"prediction\", names=None):\n    \"\"\"\n    Computes and compares RMSE values for a list of test DataFrames.\n\n    For each DataFrame in test_dfs, this function calculates the RMSE between the actual outcome\n    (given by label_col) and the predicted value (given by pred_col, default \"prediction\"). It then\n    produces a formatted table where the first column header is empty and the first row's first cell is\n    \"RMSE\", with each model's RMSE in its own column.\n\n    Parameters:\n        test_dfs (list): List of test DataFrames.\n        label_col (str): The name of the outcome column.\n        pred_col (str, optional): The name of the prediction column (default \"prediction\").\n        names (list, optional): List of model names corresponding to the test DataFrames.\n                                Defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing a table that compares RMSE values for each test DataFrame,\n        with one model per column.\n    \"\"\"\n    # Set default model names if none provided.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(test_dfs))]\n\n    rmse_values = []\n    for df in test_dfs:\n        # Create a column for squared error.\n        df = df.withColumn(\"error_sq\", pow(col(label_col) - col(pred_col), 2))\n        # Calculate RMSE: square root of the mean squared error.\n        rmse = df.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n        rmse_values.append(rmse)\n\n    # Build a single row table: first cell \"RMSE\", then one cell per model with the RMSE value.\n    row = [\"RMSE\"] + [f\"{rmse:.3f}\" for rmse in rmse_values]\n\n    # Build header: first column header is empty, then model names.\n    header = [\"\"] + names\n\n    table_str = tabulate([row], headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(names))\n    return table_str\n\n# Example usage:\n# print(compare_rmse([dtest_1, dtest_2, dtest_3], \"log_sales\", names=[\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\n\n\ndef residual_plot(df, label_col, model_name):\n    \"\"\"\n    Generates a residual plot for a given test dataframe.\n\n    Parameters:\n        df (DataFrame): Spark DataFrame containing the test set with predictions.\n        label_col (str): The column name of the actual outcome variable.\n        title (str): The title for the residual plot.\n\n    Returns:\n        None (displays the plot)\n    \"\"\"\n    # Convert to Pandas DataFrame\n    df_pd = df.select([\"prediction\", label_col]).toPandas()\n    df_pd[\"residual\"] = df_pd[label_col] - df_pd[\"prediction\"]\n\n    # Scatter plot of residuals vs. predicted values\n    plt.scatter(df_pd[\"prediction\"], df_pd[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n    # Use LOWESS smoothing for trend line\n    smoothed = sm.nonparametric.lowess(df_pd[\"residual\"], df_pd[\"prediction\"])\n    plt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\n\n    # Add reference line at y=0\n    plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n\n    # Labels and title (model_name)\n    plt.xlabel(\"Predicted Values\")\n    plt.ylabel(\"Residuals\")\n    model_name = \"Residual Plot for \" + model_name\n    plt.title(model_name)\n\n    # Show plot\n    plt.show()\n\n# Example usage:\n# residual_plot(dtest_1, \"log_sales\", \"Model 1\")\n\n\n\nterms = assembler3.getInputCols()\ncoefs = model3.coefficients.toArray()[:len(terms)]\nstdErrs = model3.summary.coefficientStandardErrors[:len(terms)]\n\ndf_summary = pd.DataFrame({\n    \"term\": terms,\n    \"estimate\": coefs,\n    \"std_error\": stdErrs\n})\n\n# Filter df_summary if needed\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary[\"term\"], df_summary[\"estimate\"],\n             yerr = 1.96 * df_summary[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates (Model 2)\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n# Create a histogram\ndfpd = DATAFRAME.select([\"Y_VARIABLE\"]).toPandas()\nsns.histplot(dfpd[\"Y_VARIABLE\"], bins=10, kde=True)"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#required-libraries-and-sparksession-entry-point",
    "href": "danl-cw/danl-320-cw-09.html#required-libraries-and-sparksession-entry-point",
    "title": "Classwork 9",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#udf-for-regression-tables",
    "href": "danl-cw/danl-320-cw-09.html#udf-for-regression-tables",
    "title": "Classwork 9",
    "section": "",
    "text": "def regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model‚Äôs labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                ci_upper,\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R¬≤\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R¬≤\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: Exp(95% CI Lower), 8: 95% CI Upper, 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"Exp(95% CI Lower)\", \"95% CI Upper\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#udf-for-adding-dummy-variables",
    "href": "danl-cw/danl-320-cw-09.html#udf-for-adding-dummy-variables",
    "title": "Classwork 9",
    "section": "",
    "text": "def add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#udf-for-adding-interaction-terms",
    "href": "danl-cw/danl-320-cw-09.html#udf-for-adding-interaction-terms",
    "title": "Classwork 9",
    "section": "",
    "text": "def add_interaction_terms(var_list1, var_list2, var_list3=None):\n    \"\"\"\n    Creates interaction term columns in the global DataFrames dtrain and dtest.\n\n    For two sets of variable names (which may represent categorical (dummy) or continuous variables),\n    this function creates two-way interactions by multiplying each variable in var_list1 with each\n    variable in var_list2.\n\n    Optionally, if a third list of variable names (var_list3) is provided, the function also creates\n    three-way interactions among each variable in var_list1, each variable in var_list2, and each variable\n    in var_list3.\n\n    Parameters:\n        var_list1 (list): List of column names for the first set of variables.\n        var_list2 (list): List of column names for the second set of variables.\n        var_list3 (list, optional): List of column names for the third set of variables for three-way interactions.\n\n    Returns:\n        A flat list of new interaction column names.\n    \"\"\"\n    global dtrain, dtest\n\n    interaction_cols = []\n\n    # Create two-way interactions between var_list1 and var_list2.\n    for var1 in var_list1:\n        for var2 in var_list2:\n            col_name = f\"{var1}_*_{var2}\"\n            dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list1 and var_list3.\n    if var_list3 is not None:\n      for var1 in var_list1:\n          for var3 in var_list3:\n              col_name = f\"{var1}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list2 and var_list3.\n    if var_list3 is not None:\n      for var2 in var_list2:\n          for var3 in var_list3:\n              col_name = f\"{var2}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # If a third list is provided, create three-way interactions.\n    if var_list3 is not None:\n        for var1 in var_list1:\n            for var2 in var_list2:\n                for var3 in var_list3:\n                    col_name = f\"{var1}_*_{var2}_*_{var3}\"\n                    dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    interaction_cols.append(col_name)\n\n    return interaction_cols\n\n # Example\n # interaction_cols_brand_price = add_interaction_terms(dummy_cols_brand, ['log_price'])\n # interaction_cols_brand_ad_price = add_interaction_terms(dummy_cols_brand, dummy_cols_ad, ['log_price'])"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#udf-for-model-comparison",
    "href": "danl-cw/danl-320-cw-09.html#udf-for-model-comparison",
    "title": "Classwork 9",
    "section": "",
    "text": "def compare_reg_models(models, assemblers, names=None):\n    \"\"\"\n    Produces a single formatted table comparing multiple regression models.\n\n    For each predictor (the union across models, ordered by first appearance), the table shows\n    the beta estimate (with significance stars) from each model (blank if not used).\n    For a predictor, if a model's outcome (model.getLabelCol()) starts with \"log\", the cell displays\n    both the beta and its exponential (separated by \" / \"), except when the predictor's name includes \"log_\".\n    (The intercept row does not display exp(.))\n\n    Additional rows for Intercept, Observations, R¬≤, and RMSE are appended.\n\n    The header's first column is labeled \"Predictor\", and subsequent columns are\n    \"y: [outcome] ([name])\" for each model.\n\n    The table is produced in grid format (with vertical lines). A dashed line (using '-' characters)\n    is inserted at the top, immediately after the header, and at the bottom.\n    Additionally, immediately after the Intercept row, the border line is replaced with one using '='\n    (to appear as, for example, \"+==============================================+==========================+...\").\n\n    Parameters:\n        models (list): List of fitted LinearRegression models.\n        assemblers (list): List of corresponding VectorAssembler objects.\n        names (list, optional): List of model names; defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing the combined regression table.\n    \"\"\"\n    # Default model names.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(models))]\n\n    # For each model, get outcome and determine if that model is log-transformed.\n    outcomes = [m.getLabelCol() for m in models]\n    is_log_flags = [out.lower().startswith(\"log\") for out in outcomes]\n\n    # Build an ordered union of predictors based on first appearance.\n    ordered_predictors = []\n    for assembler in assemblers:\n        for feat in assembler.getInputCols():\n            if feat not in ordered_predictors:\n                ordered_predictors.append(feat)\n\n    # Helper for significance stars.\n    def significance_stars(p):\n        if p is None:\n            return \"\"\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build rows for each predictor.\n    rows = []\n    for feat in ordered_predictors:\n        row = [feat]\n        for m, a, is_log in zip(models, assemblers, is_log_flags):\n            feats_model = a.getInputCols()\n            if feat in feats_model:\n                idx = feats_model.index(feat)\n                beta = m.coefficients.toArray()[idx]\n                p_val = m.summary.pValues[idx] if m.summary.pValues is not None else None\n                stars = significance_stars(p_val)\n                cell = f\"{beta:.3f}{stars}\"\n                # Only add exp(beta) if model is log and predictor name does NOT include \"log_\"\n                if is_log and (\"log_\" not in feat.lower()):\n                    cell += f\" / {np.exp(beta):,.3f}\"\n                row.append(cell)\n            else:\n                row.append(\"\")\n        rows.append(row)\n\n    # Build intercept row (do NOT compute exp(intercept)).\n    intercept_row = [\"Intercept\"]\n    for m in models:\n        std_all = np.array(m.summary.coefficientStandardErrors)\n        coeffs = m.coefficients.toArray()\n        if len(std_all) == len(coeffs) + 1:\n            intercept_p = m.summary.pValues[0] if m.summary.pValues is not None else None\n        else:\n            intercept_p = None\n        sig = significance_stars(intercept_p)\n        cell = f\"{m.intercept:.3f}{sig}\"\n        intercept_row.append(cell)\n    rows.append(intercept_row)\n\n    # Add Observations row.\n    obs_row = [\"Observations\"]\n    for m in models:\n        obs = m.summary.numInstances\n        obs_row.append(f\"{int(obs):,}\")\n    rows.append(obs_row)\n\n    # Add R¬≤ row.\n    r2_row = [\"R¬≤\"]\n    for m in models:\n        r2_row.append(f\"{m.summary.r2:.3f}\")\n    rows.append(r2_row)\n\n    # Add RMSE row.\n    rmse_row = [\"RMSE\"]\n    for m in models:\n        rmse_row.append(f\"{m.summary.rootMeanSquaredError:.3f}\")\n    rows.append(rmse_row)\n\n    # Build header: first column \"Predictor\", then for each model: \"y: [outcome] ([name])\"\n    header = [\"Predictor\"]\n    for out, name in zip(outcomes, names):\n        header.append(f\"y: {out} ({name})\")\n\n    # Create table string using grid format.\n    table_str = tabulate(rows, headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(models))\n\n    # Split into lines.\n    lines = table_str.split(\"\\n\")\n\n    # Create a dashed line spanning the full width.\n    full_width = len(lines[0])\n    dash_line = '-' * full_width\n    # Create an equals line by replacing '-' with '='.\n    eq_line = dash_line.replace('-', '=')\n\n    # Insert a dashed line after the header row.\n    lines = table_str.split(\"\\n\")\n    # In grid format, header and separator are usually the first two lines.\n    lines.insert(2, dash_line)\n\n    # Insert an equals line after the Intercept row.\n    for i, line in enumerate(lines):\n        if line.startswith(\"|\") and \"Intercept\" in line:\n            if i+1 &lt; len(lines):\n                lines[i+1] = eq_line\n            break\n\n    # Add dashed lines at the very top and bottom.\n    final_table = dash_line + \"\\n\" + \"\\n\".join(lines) + \"\\n\" + dash_line\n\n    return final_table\n\n# Example usage:\n# print(compare_regression_models([model_1, model_2, model_3],\n#                                 [assembler_1, assembler_2, assembler_3],\n#                                 [\"Model 1\", \"Model 2\", \"Model 3\"]))"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#udf-for-rmses",
    "href": "danl-cw/danl-320-cw-09.html#udf-for-rmses",
    "title": "Classwork 9",
    "section": "",
    "text": "def compare_rmse(test_dfs, label_col, pred_col=\"prediction\", names=None):\n    \"\"\"\n    Computes and compares RMSE values for a list of test DataFrames.\n\n    For each DataFrame in test_dfs, this function calculates the RMSE between the actual outcome\n    (given by label_col) and the predicted value (given by pred_col, default \"prediction\"). It then\n    produces a formatted table where the first column header is empty and the first row's first cell is\n    \"RMSE\", with each model's RMSE in its own column.\n\n    Parameters:\n        test_dfs (list): List of test DataFrames.\n        label_col (str): The name of the outcome column.\n        pred_col (str, optional): The name of the prediction column (default \"prediction\").\n        names (list, optional): List of model names corresponding to the test DataFrames.\n                                Defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing a table that compares RMSE values for each test DataFrame,\n        with one model per column.\n    \"\"\"\n    # Set default model names if none provided.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(test_dfs))]\n\n    rmse_values = []\n    for df in test_dfs:\n        # Create a column for squared error.\n        df = df.withColumn(\"error_sq\", pow(col(label_col) - col(pred_col), 2))\n        # Calculate RMSE: square root of the mean squared error.\n        rmse = df.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n        rmse_values.append(rmse)\n\n    # Build a single row table: first cell \"RMSE\", then one cell per model with the RMSE value.\n    row = [\"RMSE\"] + [f\"{rmse:.3f}\" for rmse in rmse_values]\n\n    # Build header: first column header is empty, then model names.\n    header = [\"\"] + names\n\n    table_str = tabulate([row], headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(names))\n    return table_str\n\n# Example usage:\n# print(compare_rmse([dtest_1, dtest_2, dtest_3], \"log_sales\", names=[\"Model 1\", \"Model 2\", \"Model 3\"]))"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#udf-for-a-residual-plot",
    "href": "danl-cw/danl-320-cw-09.html#udf-for-a-residual-plot",
    "title": "Classwork 9",
    "section": "",
    "text": "def residual_plot(df, label_col, model_name):\n    \"\"\"\n    Generates a residual plot for a given test dataframe.\n\n    Parameters:\n        df (DataFrame): Spark DataFrame containing the test set with predictions.\n        label_col (str): The column name of the actual outcome variable.\n        title (str): The title for the residual plot.\n\n    Returns:\n        None (displays the plot)\n    \"\"\"\n    # Convert to Pandas DataFrame\n    df_pd = df.select([\"prediction\", label_col]).toPandas()\n    df_pd[\"residual\"] = df_pd[label_col] - df_pd[\"prediction\"]\n\n    # Scatter plot of residuals vs. predicted values\n    plt.scatter(df_pd[\"prediction\"], df_pd[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n    # Use LOWESS smoothing for trend line\n    smoothed = sm.nonparametric.lowess(df_pd[\"residual\"], df_pd[\"prediction\"])\n    plt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\n\n    # Add reference line at y=0\n    plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n\n    # Labels and title (model_name)\n    plt.xlabel(\"Predicted Values\")\n    plt.ylabel(\"Residuals\")\n    model_name = \"Residual Plot for \" + model_name\n    plt.title(model_name)\n\n    # Show plot\n    plt.show()\n\n# Example usage:\n# residual_plot(dtest_1, \"log_sales\", \"Model 1\")"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#coefficient-plots",
    "href": "danl-cw/danl-320-cw-09.html#coefficient-plots",
    "title": "Classwork 9",
    "section": "",
    "text": "terms = assembler3.getInputCols()\ncoefs = model3.coefficients.toArray()[:len(terms)]\nstdErrs = model3.summary.coefficientStandardErrors[:len(terms)]\n\ndf_summary = pd.DataFrame({\n    \"term\": terms,\n    \"estimate\": coefs,\n    \"std_error\": stdErrs\n})\n\n# Filter df_summary if needed\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary[\"term\"], df_summary[\"estimate\"],\n             yerr = 1.96 * df_summary[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates (Model 2)\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#histogram",
    "href": "danl-cw/danl-320-cw-09.html#histogram",
    "title": "Classwork 9",
    "section": "",
    "text": "# Create a histogram\ndfpd = DATAFRAME.select([\"Y_VARIABLE\"]).toPandas()\nsns.histplot(dfpd[\"Y_VARIABLE\"], bins=10, kde=True)"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#variable-description",
    "href": "danl-cw/danl-320-cw-09.html#variable-description",
    "title": "Classwork 9",
    "section": "Variable description",
    "text": "Variable description\n\n\n\nVariable\nDescription\n\n\n\n\nsales\nQuantity of OJ cartons sold\n\n\nprice\nPrice of OJ\n\n\nbrand\nBrand of OJ\n\n\nad\nAdvertisement status"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#model-1",
    "href": "danl-cw/danl-320-cw-09.html#model-1",
    "title": "Classwork 9",
    "section": "Model 1",
    "text": "Model 1\n\\[\n\\begin{align}\n\\log(\\text{sales}_{\\text{i}}) &\\,=\\, \\;\\; b_{\\text{intercept}} \\,+\\, b_{\\,\\text{mm}}\\,\\text{brand}_{\\,\\text{mm}, \\text{i}} \\,+\\, b_{\\,\\text{tr}}\\,\\text{brand}_{\\,\\text{tr}, \\text{i}}\\\\\n&\\quad\\,+\\, b_{\\text{price}}\\,\\log(\\text{price}_{\\text{i}}) \\,+\\, e_{\\text{i}},\\\\\n\\text{where}\\qquad\\qquad&\\\\\n\\text{brand}_{\\,\\text{tr}, \\text{i}}\n&\\,=\\, \\begin{cases}\n\\text{1} & \\text{ if an orange juice } \\text{i} \\text{ is } \\text{Tropicana};\\\\\\\\\n\\text{0} & \\text{otherwise}.\\qquad\\qquad\\quad\\,\n\\end{cases}\\\\\n\\text{brand}_{\\,\\text{mm}, \\text{i}} &\\,=\\, \\begin{cases}\n\\text{1} & \\text{ if an orange juice } \\text{i} \\text{ is } \\text{Minute Maid};\\\\\\\\\n\\text{0} & \\text{otherwise}.\\qquad\\qquad\\quad\\,\n\\end{cases}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#model-2",
    "href": "danl-cw/danl-320-cw-09.html#model-2",
    "title": "Classwork 9",
    "section": "Model 2",
    "text": "Model 2\n\\[\n\\begin{align}\n\\log(\\text{sales}_{\\text{i}}) \\,=\\,&\\;\\; \\quad b_{\\text{intercept}} \\,+\\, \\color{Green}{b_{\\,\\text{mm}}\\,\\text{brand}_{\\,\\text{mm}, \\text{i}}} \\,+\\, \\color{Blue}{b_{\\,\\text{tr}}\\,\\text{brand}_{\\,\\text{tr}, \\text{i}}}\\\\\n&\\,+\\, b_{\\text{price}}\\,\\log(\\text{price}_{\\text{i}})  \\\\\n&\\, +\\, b_{\\text{price*mm}}\\,\\log(\\text{price}_{\\text{i}})\\,\\times\\,\\color{Green} {\\text{brand}_{\\,\\text{mm}, \\text{i}}} \\\\\n&\\,+\\, b_{\\text{price*tr}}\\,\\log(\\text{price}_{\\text{i}})\\,\\times\\,\\color{Blue} {\\text{brand}_{\\,\\text{tr}, \\text{i}}} \\,+\\, e_{\\text{i}}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-cw/danl-320-cw-09.html#model-3",
    "href": "danl-cw/danl-320-cw-09.html#model-3",
    "title": "Classwork 9",
    "section": "Model 3",
    "text": "Model 3\n\\[\n\\begin{align}\n\\log(\\text{sales}_{\\text{i}}) \\,=\\,\\quad\\;\\;& b_{\\text{intercept}} \\,+\\, \\color{Green}{b_{\\,\\text{mm}}\\,\\text{brand}_{\\,\\text{mm}, \\text{i}}} \\,+\\, \\color{Blue}{b_{\\,\\text{tr}}\\,\\text{brand}_{\\,\\text{tr}, \\text{i}}}  \\\\\n&\\,+\\; b_{\\,\\text{ad}}\\,\\color{Orange}{\\text{ad}_{\\,\\text{i}}} \\qquad\\qquad\\qquad\\qquad\\quad   \\\\\n&\\,+\\, b_{\\text{mm*ad}}\\,\\color{Green} {\\text{brand}_{\\,\\text{mm}, \\text{i}}}\\,\\times\\, \\color{Orange}{\\text{ad}_{\\,\\text{i}}}\\,+\\, b_{\\text{tr*ad}}\\,\\color{Blue} {\\text{brand}_{\\,\\text{tr}, \\text{i}}}\\,\\times\\, \\color{Orange}{\\text{ad}_{\\,\\text{i}}} \\\\\n&\\,+\\;  b_{\\text{price}}\\,\\log(\\text{price}_{\\text{i}}) \\qquad\\qquad\\qquad\\;\\;\\;\\;\\,  \\\\\n&\\,+\\, b_{\\text{price*mm}}\\,\\log(\\text{price}_{\\text{i}})\\,\\times\\,\\color{Green} {\\text{brand}_{\\,\\text{mm}, \\text{i}}}\\qquad\\qquad\\qquad\\;\\, \\\\\n&\\,+\\, b_{\\text{price*tr}}\\,\\log(\\text{price}_{\\text{i}})\\,\\times\\,\\color{Blue} {\\text{brand}_{\\,\\text{tr}, \\text{i}}}\\qquad\\qquad\\qquad\\;\\, \\\\\n& \\,+\\, b_{\\text{price*ad}}\\,\\log(\\text{price}_{\\text{i}})\\,\\times\\,\\color{Orange}{\\text{ad}_{\\,\\text{i}}}\\qquad\\qquad\\qquad\\;\\;\\, \\\\\n&\\,+\\, b_{\\text{price*mm*ad}}\\,\\log(\\text{price}_{\\text{i}}) \\,\\times\\,\\,\\color{Green} {\\text{brand}_{\\,\\text{mm}, \\text{i}}}\\,\\times\\, \\color{Orange}{\\text{ad}_{\\,\\text{i}}} \\\\\n&\\,+\\, b_{\\text{price*tr*ad}}\\,\\log(\\text{price}_{\\text{i}}) \\,\\times\\,\\,\\color{Blue} {\\text{brand}_{\\,\\text{tr}, \\text{i}}}\\,\\times\\, \\color{Orange}{\\text{ad}_{\\,\\text{i}}}  \\,+\\, e_{\\text{i}}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#what-is-logistic-regression-doing",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#what-is-logistic-regression-doing",
    "title": "Lecture 8",
    "section": "What is Logistic Regression Doing?",
    "text": "What is Logistic Regression Doing?\nLogistic regression maps any input through a logistic (sigmoid) function to estimate the probability of a binary outcome (0/1):\n\\[\nG(z_i) = \\frac{\\exp(z_i)}{1 + \\exp(z_i)}\n\\]\nwhere ( z_i ) is a linear combination of predictors:\n\\[\nz_i = b_0 + b_{x1} x_{i,1} + b_{x2} x_{i,2} + \\dots + b_{xN}x_{i,N}\n\\]\nLogistic regression finds coefficients \\((b_0, b_{x1}, \\dots, b_{xN})\\) that maximize the likelihood of correctly predicting the binary outcome \\((y_i)\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#example-predicting-whether-a-newborn-needs-extra-medical-attention",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#example-predicting-whether-a-newborn-needs-extra-medical-attention",
    "title": "Lecture 8",
    "section": "Example: Predicting Whether a Newborn Needs Extra Medical Attention",
    "text": "Example: Predicting Whether a Newborn Needs Extra Medical Attention\nDataset records information about all US births, including facts about the mother and father, and about the delivery."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#marginal-effects",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#marginal-effects",
    "title": "Lecture 8",
    "section": "Marginal Effects",
    "text": "Marginal Effects\nWhat are Marginal Effects?\nMarginal effects show how the probability of the binary outcome changes when you vary explanatory variables, holding others constant.\nIn PySpark, this typically requires manual computation."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#properties-of-the-logistic-function",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#properties-of-the-logistic-function",
    "title": "Lecture 8",
    "section": "Properties of the Logistic Function",
    "text": "Properties of the Logistic Function\n\nThe logistic function \\(G(z_i)\\) maps the linear combination of predictors to the probability that the outcome \\(y_{i}\\) is \\(1\\).\n\\(z_{i} = b_{0} + b_{1}x_{1, i} + b_{2}x_{2, i} + \\,\\cdots\\, + x_{k, i}\\)\n\\(z_{i} \\overset{G}{\\rightarrow} \\texttt{ Prob} ( y_{i} = 1 )\\)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#pyspark-logistic-regression-example",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#pyspark-logistic-regression-example",
    "title": "Lecture 8",
    "section": "PySpark Logistic Regression Example",
    "text": "PySpark Logistic Regression Example\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import VectorAssembler\n\n# Assemble features\nassembler = VectorAssembler(inputCols=['PWGT', 'UPREVIS', 'CIG_REC', 'GESTREC3',\n                                       'DPLURAL_1', 'DPLURAL_2', 'ULD_MECO',\n                                       'ULD_PRECIP', 'ULD_BREECH', 'URF_DIAB'],\n                            outputCol='features')\n\ndata_assembled = assembler.transform(df).select('features', 'atRisk')\n\n# Fit logistic regression model\nfrom pyspark.ml.classification import LogisticRegression\nlog_reg = LogisticRegression(featuresCol='features', labelCol='atRisk')\n\nmodel = log_reg.fit(data_assembled)\n\n# Predictions\npredictions = model.transform(data_assembled)\n\n# Evaluate\npredictions.select('probability', 'prediction', 'atRisk').show(5)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#marginal-effects-1",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#marginal-effects-1",
    "title": "Lecture 8",
    "section": "Marginal Effects",
    "text": "Marginal Effects\nExample in PySpark:\n# Using PySpark SQL to calculate marginal effects numerically or via model coefficients directly\nInterpreting Marginal Effects\n\nMarginal effects are interpreted as percentage points:\n\n\n‚ÄúThe probability of a newborn being at risk increases by 4.50 percentage points if the baby is born prematurely.‚Äù"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#characterizing-prediction-quality",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#characterizing-prediction-quality",
    "title": "Lecture 8",
    "section": "Characterizing Prediction Quality",
    "text": "Characterizing Prediction Quality\nCommon evaluation metrics:\n\n\n\n\n\n\n\nMetric\nInterpretation\n\n\n\n\nAccuracy\nProbability model predictions are correct\n\n\nPrecision\nProbability predicted positives are true positives\n\n\nRecall\nProbability model identifies actual at-risk babies\n\n\nEnrichment\nHow much better the model does compared to random chance\n\n\n\nPySpark Example:\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nevaluator = BinaryClassificationEvaluator(labelCol='atRisk', metricName='areaUnderROC')\nauc = evaluator.evaluate(predictions)\nprint(\"AUC: \", auc)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#prediction-quality",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#prediction-quality",
    "title": "Lecture 8",
    "section": "Prediction Quality",
    "text": "Prediction Quality\nClassifying based on a threshold:\n\nPredict at-risk if \\(\\text{probability} &gt; 0.02\\)\nOtherwise, predict not-at-risk.\n\nPySpark Example:\npred_threshold = predictions.withColumn('atRisk_pred', (predictions.probability[1] &gt; 0.02).cast('integer'))\n\n# Confusion matrix\npred_threshold.groupBy('atRisk_pred', 'atRisk').count().show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#characterizing-prediction-quality-1",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#characterizing-prediction-quality-1",
    "title": "Lecture 8",
    "section": "Characterizing Prediction Quality",
    "text": "Characterizing Prediction Quality\nPySpark visualization example (via pandas conversion for plot):\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npdf = predictions.select('probability', 'atRisk').toPandas()\npdf['prob'] = pdf['probability'].apply(lambda x: x[1])\n\nsns.kdeplot(data=pdf, x='prob', hue='atRisk', fill=True)\nplt.xlabel('Predicted Probability')\nplt.title('Prediction Probability Distribution by At-Risk Status')\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#evaluating-prediction-quality",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#evaluating-prediction-quality",
    "title": "Lecture 8",
    "section": "Evaluating Prediction Quality",
    "text": "Evaluating Prediction Quality\nPySpark Confusion Matrix:\nctab = pred_threshold.groupBy('atRisk_pred', 'atRisk').count().toPandas()\nctab_pivot = ctab.pivot(index='atRisk_pred', columns='atRisk', values='count').fillna(0)\n\n# Accuracy, Precision, Recall calculations\naccuracy = (ctab_pivot.loc[0, 0] + ctab_pivot.loc[1, 1]) / ctab_pivot.values.sum()\nprecision = ctab_pivot.loc[1,1] / ctab_pivot.loc[1].sum()\nrecall = ctab_pivot.loc[1,1] / ctab_pivot[1].sum()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#marginal-effects-for-subgroups",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#marginal-effects-for-subgroups",
    "title": "Lecture 8",
    "section": "Marginal Effects for Subgroups",
    "text": "Marginal Effects for Subgroups\nIn PySpark, subset data explicitly:\nsubset_cig_true = data.filter(data.CIG_REC == 1)\nsubset_cig_false = data.filter(data.CIG_REC == 0)\n\n# Run separate models or marginal calculations"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#choosing-the-threshold",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#choosing-the-threshold",
    "title": "Lecture 8",
    "section": "Choosing the Threshold",
    "text": "Choosing the Threshold\n\nAdjust threshold based on available resources, trade-off precision vs recall.\n\nPlotting precision-recall or ROC curves helps determine optimal thresholds:\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nevaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', labelCol='atRisk')\nroc_auc = evaluator.evaluate(predictions)\nprint(f'ROC AUC: {roc_auc}')\nVisualize ROC Curve using pandas and matplotlib:\nfrom sklearn.metrics import roc_curve, auc\n\npdf = predictions.select('probability', 'atRisk').toPandas()\npdf['prob'] = pdf['probability'].apply(lambda x: x[1])\n\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(pdf['atRisk'], pdf['prob'])\n\nsns.lineplot(x=fpr, y=tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#interpretation",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#interpretation",
    "title": "Lecture 8",
    "section": "Interpretation:",
    "text": "Interpretation:\nChoose thresholds according to your resources and desired balance between recall and precision."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#motivation",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#motivation",
    "title": "Lecture 8",
    "section": "Motivation",
    "text": "Motivation\n\nRelationship: Suppose we want to know how much the flight‚Äôs origin/destination, weather, and air carrier is associated with the probability that a flight will be delayed.\nPrediction: Suppose we also want to predict whether or not a flight will be delayed, based on facts like the flight‚Äôs origin/destination, weather, and air carrier.\nFor every flight \\(\\texttt{i}\\), you want to predict \\(\\texttt{flight_delayed[i]}\\), a binary variable ( \\(\\texttt{TRUE}\\) or \\(\\texttt{FALSE}\\)), based on \\(\\texttt{origin[i]}\\), \\(\\texttt{destination[i]}\\), \\(\\texttt{weather[i]}\\), and \\(\\texttt{air_carrier[i]}\\).\n\nPrediction of a binary variable \\(y_{i}\\) (0 or 1) is the expected value of \\(y_{i}\\)‚Äîthe predicted probability that \\(y_{i} = 1\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#motivation-1",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#motivation-1",
    "title": "Lecture 8",
    "section": "Motivation",
    "text": "Motivation\n\n\nThe logistic regression model for the probability that a flight will be delayed is formulated as: \\[\n\\begin{align}\n&\\quad\\texttt{ Prob( flight_delayed[i] == TRUE ) } \\\\[.5em]\n&=\\, \\texttt{G( b$_{\\texttt{0}}$ + b$_{\\texttt{origin}}$*origin[i]  }\\\\\n&\\qquad\\quad\\,\\, + \\texttt{b$_{\\texttt{destination}}$*destination[i]  }\\\\\n&\\qquad\\quad\\,\\, + \\texttt{b$_{\\texttt{weather}}$*weather[i]  }\\\\\n&\\qquad\\quad\\,\\, + \\texttt{b$_{\\texttt{air_carrier}}$*air_carrier[i] )}.\n\\end{align}\n\\]\n\n\n\n\\(G(z_i)\\): the logistic function\n\n\n\n\n\\[\n\\begin{align}\n\\texttt{G(z[i]) = } \\dfrac{\\texttt{exp(z[i])}}{\\texttt{1 + exp(z[i])}}.\\notag\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#properties-of-the-logistic-function-1",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#properties-of-the-logistic-function-1",
    "title": "Lecture 8",
    "section": "Properties of the Logistic Function",
    "text": "Properties of the Logistic Function\n\n\nThe logistic function \\(G(z_i)\\) \\[\n\\begin{align}\nG(z_i) = \\frac{\\exp(z_i)}{1 + \\exp(z_i)}\n\\end{align}\n\\] ranges between 0 and 1 for any value of \\(z_i\\)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#logistic-regression-1",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#logistic-regression-1",
    "title": "Lecture 8",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nInterpreting the Marginal Effect\n\nHow do we interpret the ME? All else being equal,\n\nThere is a 2.05 percentage point increase in the probability of a newborn being at risk if the baby is prematurely born.\nThere is a 0.04 percentage point decrease in the probability of a newborn being at risk for each additional parental medical visit."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#logistic-regression-2",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#logistic-regression-2",
    "title": "Lecture 8",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nProperties of the Logistic Function\n\nThe function \\(G(z_i)\\) is called the logistic function because the function \\(G(z_i)\\) is the inverse function of a logit (or a log-odd) of the probability that the outcome \\(y_{i}\\) is 1. \\[\n\\begin{align}\nG^{-1}(z_i) &\\,\\equiv\\, \\text{logit} (\\text{Prob}(y_{i} = 1))\\\\\n&\\equiv \\log\\left(\\, \\frac{\\text{Prob}(y_{i} = 1)}{\\text{Prob}(y_{i} = 0)} \\,\\right)\\\\\n&\\,=\\, b_0 + b_{1}x_{1,i} + b_{2}x_{2,i} + \\,\\cdots\\, + b_{k}x_{k,i}\n\\end{align}\n\\]\nLogistic regression is a linear regression model for log odds."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#example-of-logistic-regression",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#example-of-logistic-regression",
    "title": "Lecture 8",
    "section": "Example of Logistic Regression",
    "text": "Example of Logistic Regression\nMarginal Effect of on }\n\nInterpreting only the estimated beta coefficient may not be informative.\n\nmodel_1.summary\n\nIf the baby is prematurely born, the log-odds of being at risk increases by 1.539 relative to the non-premature baby."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#example-of-logistic-regression-1",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#example-of-logistic-regression-1",
    "title": "Lecture 8",
    "section": "Example of Logistic Regression",
    "text": "Example of Logistic Regression\nMarginal Effect of \\(x_{k, i}\\) on \\(\\text{Prob}(y_{i} = 1)\\)\n\nIn logistic regression, the effect of \\(x_{k, i}\\) on \\(\\text{Prob}(y_{i} = 1)\\) is different for each observation \\(i\\).\n\n\n\n\n\n\n\n\nLinear regression\n\n\n\n\n\n\n\n\n\n\nLogistic regression"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#example-of-logistic-regression-2",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#example-of-logistic-regression-2",
    "title": "Lecture 8",
    "section": "Example of Logistic Regression",
    "text": "Example of Logistic Regression\n\nThe data set records information about all US births, including facts about the mother and father, and about the delivery.\nThe sample has just over 26,000 births in a DataFrame."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#building-a-logistic-regression-model",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#building-a-logistic-regression-model",
    "title": "Lecture 8",
    "section": "Building a logistic regression model",
    "text": "Building a logistic regression model\n\nThe function to build a logistic regression model in PySpark is \\(\\texttt{GeneralizedLinearRegression(family=\"binomial\", link=\"logit\")}\\).\nThe outcome variable \\(y\\) is the binary variable \\(\\texttt{atRisk}\\) (0 or 1).\nThe other variables in the table in the previous slide are predictors \\(x_{k}\\).\nThe arguments \\(\\texttt{family=\"binomial\"}\\) and \\(\\texttt{link=\"logit\"}\\) specify the logistic distribution of the outcome variable \\(y\\).\n\nfrom pyspark.ml.regression import GeneralizedLinearRegression\n\ndummy_cols_GESTREC3, ref_category_GESTREC3 = add_dummy_variables('GESTREC3', 1)\ndummy_cols_DPLURAL, ref_category_DPLURAL = add_dummy_variables('DPLURAL', 0)\n\n# assembling predictors\nx_cols = ['PWGT', 'UPREVIS', 'CIG_REC', \n          'ULD_MECO', 'ULD_PRECIP', 'ULD_BREECH', 'URF_DIAB',\n          'URF_CHYPER', 'URF_PHYPER', 'URF_ECLAM']\n\nassembler_predictors = (\n    x_cols +\n    dummy_cols_GESTREC3 + dummy_cols_DPLURAL\n)\n\nassembler_1 = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\n\ndtrain_1 = assembler_1.transform(dtrain)\ndtest_1  = assembler_1.transform(dtest)\n\n# training model\nmodel_1 = (\n    GeneralizedLinearRegression(featuresCol=\"predictors\",\n                                labelCol=\"atRisk\",\n                                family=\"binomial\", \n                                link=\"logit\")\n    .fit(dtrain_1)\n)\n\n# making prediction\ndtest_1 = model_1.transform(dtest_1)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#building-a-logistic-regression-model-1",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#building-a-logistic-regression-model-1",
    "title": "Lecture 8",
    "section": "Building a logistic regression model",
    "text": "Building a logistic regression model\n\nThe function to build a logistic regression model in PySpark is \\(\\texttt{GeneralizedLinearRegression(family=\"binomial\", link=\"logit\")}\\).\nThe outcome variable \\(y\\) is the binary variable \\(\\texttt{atRisk}\\) (0 or 1).\nThe other variables in the table in the previous slide are predictors \\(x_{k}\\).\nThe arguments \\(\\texttt{family=\"binomial\"}\\) and \\(\\texttt{link=\"logit\"}\\) specify the logistic distribution of the outcome variable \\(y\\).\n\ndummy_cols_GESTREC3, ref_category_GESTREC3 = add_dummy_variables('GESTREC3', 1)\ndummy_cols_DPLURAL, ref_category_DPLURAL = add_dummy_variables('DPLURAL', 0)\n\n# assembling predictors\nconti_cols = [\"PWGT\", 'UPREVIS', 'CIG_REC', \n              'ULD_MECO', 'ULD_PRECIP', 'ULD_BREECH', 'URF_DIAB',\n              'URF_CHYPER', 'URF_PHYPER', 'URF_ECLAM']\n\nassembler_predictors = (\n    conti_cols +\n    dummy_cols_GESTREC3 + dummy_cols_DPLURAL\n)\n\nassembler_1 = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\n\ndtrain_1 = assembler_1.transform(dtrain)\ndtest_1  = assembler_1.transform(dtest)\n\n# training model\nmodel_1 = (\n    GeneralizedLinearRegression(featuresCol=\"predictors\",\n                                labelCol=\"atRisk\",\n                                family=\"binomial\", \n                                link=\"logit\")\n    .fit(dtrain_1)\n)\n\n# making prediction\ndtest_1 = model_1.transform(dtest_1)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#example-of-logistic-regression-3",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#example-of-logistic-regression-3",
    "title": "Lecture 8",
    "section": "Example of Logistic Regression",
    "text": "Example of Logistic Regression\nMarginal Effect of on }\n\nInterpreting only the estimated beta coefficient may not be informative.\n\nmodel_1.summary\n\nIf the baby is prematurely born, the log-odds of being at risk increases by 1.539, relative to the non-premature baby."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#logistic-regression-3",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#logistic-regression-3",
    "title": "Lecture 8",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nProperties of the Logistic Regression\n\nLogistic regression\n\n\\[\n\\begin{align}\n\\text{Prob}(y_{i} = 1) = G( b_0 + b_{1}x_{1,i} + b_{2}x_{2,i} + \\,\\cdots\\, +  b_{k}x_{k,i} )\n\\end{align}\n\\]\ncan be expressed as linear regression of log odds of \\(\\texttt{y[i]==TRUE}\\) on predictors \\(\\texttt{x[i,1], x[i,2], ...}\\)\n\\[\n\\begin{align}\n\\log\\left(\\dfrac{\\text{Prob}( y_i = 1 )}{\\text{Prob}( y_i = 0 )}\\right) \\,=\\,  b_0 + b_{1}x_{1,i} + b_{2}x_{2,i} + \\,\\cdots\\, +  b_{k}x_{k,i}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#example-of-logistic-regression-4",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#example-of-logistic-regression-4",
    "title": "Lecture 8",
    "section": "Example of Logistic Regression",
    "text": "Example of Logistic Regression\nMarginal Effect of \\(x_{k, i}\\) on \\(\\text{Prob}(y_{i} = 1)\\)\n\nIn logistic regression, the effect of \\(x_{k, i}\\) on \\(\\text{Prob}(y_{i} = 1)\\) is different for each observation \\(i\\).\n\n\n\n\n\n\n\n\nLinear regression\n\n\n\n\n\n\n\n\n\n\nLogistic regression"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#logistic-regression-4",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#logistic-regression-4",
    "title": "Lecture 8",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nMarginal Effect of \\(x_{k, i}\\) on \\(\\text{Prob}(y_{i} = 1)\\)\n\nHow can we calculate the effect of \\(x_{k, i}\\) on the probability of \\(y_{i} = 1\\)?\n\nWe can average the marginal effects across the training data (average marginal effect, or AME).\n\n\nIt can be computationally challenging, especially for the large number of observations.\n\n\nWe can obtain the marginal effect at an average observation or representative observations in the training data (marginal effect at the mean (MEM) or at representative values (MER)).\n\n\ndef marginal_effects(model, means):\n    \"\"\"\n    Compute marginal effects for all predictors in a PySpark GeneralizedLinearRegression model (logit)\n    and return a formatted table with statistical significance.\n    \n    Parameters:\n        model: Fitted GeneralizedLinearRegression model (with binomial family and logit link).\n        means: List of mean values for the predictor variables.\n    \n    Returns:\n        A formatted string containing the marginal effects table.\n    \"\"\"\n    global assembler_predictors  # Use the global assembler_predictors list\n    \n    # Extract model coefficients, standard errors, and intercept\n    coeffs = np.array(model.coefficients)\n    std_errors = np.array(model.summary.coefficientStandardErrors)\n    intercept = model.intercept\n    \n    # Compute linear combination of means and coefficients (XB)\n    XB = np.dot(means, coeffs) + intercept\n    \n    # Compute derivative of logistic function (G'(XB))\n    G_prime_XB = np.exp(XB) / ((1 + np.exp(XB)) ** 2)\n    \n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n    \n    # Create table to store results\n    results = []\n    \n    for i, predictor in enumerate(assembler_predictors):\n        # Compute marginal effect\n        marginal_effect = G_prime_XB * coeffs[i]\n        \n        # Compute standard error of the marginal effect\n        std_error = G_prime_XB * std_errors[i]\n        \n        # Compute z-score and p-value\n        z_score = marginal_effect / std_error if std_error != 0 else np.nan\n        p_value = 2 * (1 - norm.cdf(abs(z_score))) if not np.isnan(z_score) else np.nan\n        \n        # Compute confidence interval (95%)\n        ci_lower = marginal_effect - 1.96 * std_error\n        ci_upper = marginal_effect + 1.96 * std_error\n        \n        # Append results\n        results.append([predictor, f\"{marginal_effect: .4f}\", significance_stars(p_value), f\"{ci_lower: .4f}\", f\"{ci_upper: .4f}\"])\n    \n    # Convert results to tabulated format\n    table_str = tabulate(results, headers=[\"Variable\", \"Marginal Effect\", \"Significance\", \"95% CI Lower\", \"95% CI Upper\"], \n                         tablefmt=\"pretty\", colalign=(\"left\", \"decimal\", \"left\", \"decimal\", \"decimal\"))\n    \n    return table_str\n\n# Example usage:\n# means = [0.5, 30]  # Mean values for x1 and x2\n# assembler_predictors = ['x1', 'x2']  # Define globally before calling the function\n# table_output = mfx_glm(fitted_model, means)\n# print(table_output)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#vs.-point",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#vs.-point",
    "title": "Lecture 8",
    "section": "% vs.¬†% point",
    "text": "% vs.¬†% point\n\nLet‚Äôs say you have money in a savings account. The interest is 3%.\nNow consider two scenarios:\n\nThe bank increases the interest rate by one percent.\nThe bank increases the interest rate by one percentage point.\n\nWhat is the new interest rate in each scenario? Which is better?"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#logistic-regression-5",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#logistic-regression-5",
    "title": "Lecture 8",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nInterpreting the Marginal Effect\n\nHow do we interpret the ME? All else being equal,\n\nThe probability of a new born baby being at risk increases by 2.05% points if the baby is prematurely born.\nThe probability of a new born baby being at risk decreases by 0.04% points if the number of parental medical visits increases by one unit."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#classifier",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#classifier",
    "title": "Lecture 8",
    "section": "Classifier",
    "text": "Classifier\n\nYour goal is to use the logistic regression model to classify newborn babies into one of two categories‚Äîat-risk or not.\nPrediction from the logistic regression with a threshold on the predicted probabilities can be used as a classifier.\n\nIf the predicted probability that the baby \\(\\texttt{i}\\) is at risk is greater than the threshold, the baby \\(\\texttt{i}\\) is classified as at-risk.\nOtherwise, the baby \\(\\texttt{i}\\) is classified as not-at-risk."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#classifier-1",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#classifier-1",
    "title": "Lecture 8",
    "section": "Classifier",
    "text": "Classifier\n\nDouble density plot is useful when picking the classifier threshold.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Filter data for atRisk == 1 and atRisk == 0\npdf = dtest_1.select(\"prediction\", \"atRisk\").toPandas()\n\ntrain_true = pdf[pdf[\"atRisk\"] == 1]\ntrain_false = pdf[pdf[\"atRisk\"] == 0]\n\n# Create the first density plot\nplt.figure(figsize=(8, 6))\nsns.kdeplot(train_true[\"prediction\"], label=\"TRUE\", color=\"red\", fill=True)\nsns.kdeplot(train_false[\"prediction\"], label=\"FALSE\", color=\"blue\", fill=True)\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Density\")\nplt.title(\"Density Plot of Predictions\")\nplt.legend(title=\"atRisk\")\nplt.show()\n\n# Define threshold for vertical line\nthreshold = 0.02  # Replace with actual value\n\n# Create the second density plot with vertical line\nplt.figure(figsize=(8, 6))\nsns.kdeplot(train_true[\"prediction\"], label=\"TRUE\", color=\"red\", fill=True)\nsns.kdeplot(train_false[\"prediction\"], label=\"FALSE\", color=\"blue\", fill=True)\nplt.axvline(x=threshold, color=\"blue\", linestyle=\"dashed\", label=f\"Threshold = {threshold}\")\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Density\")\nplt.title(\"Density Plot of Predictions with Threshold\")\nplt.legend(title=\"atRisk\")\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nConfusion matrix\n\n\nThe confusion matrix summarizes the classifier‚Äôs predictions against the actual known data categories.\n\nSuppose the threshold is set as 0.02.\n\n\n\n# Compute confusion matrix\ndtest_1 = dtest_1.withColumn(\"predicted_class\", when(col(\"prediction\") &gt; .02, 1).otherwise(0))\nconf_matrix = dtest_1.groupBy(\"atRisk\", \"predicted_class\").count().orderBy(\"atRisk\", \"predicted_class\")\n\nTP = dtest_1.filter((col(\"atRisk\") == 1) & (col(\"predicted_class\") == 1)).count()\nFP = dtest_1.filter((col(\"atRisk\") == 0) & (col(\"predicted_class\") == 1)).count()\nFN = dtest_1.filter((col(\"atRisk\") == 1) & (col(\"predicted_class\") == 0)).count()\nTN = dtest_1.filter((col(\"atRisk\") == 0) & (col(\"predicted_class\") == 0)).count()\n\n# Print formatted confusion matrix with labels\nprint(\"\\n Confusion Matrix:\\n\")\nprint(\"                     Predicted\")\nprint(\"            |  Negative  |  Positive  \")\nprint(\"------------+------------+------------\")\nprint(f\"Actual Neg. |    {TN:5}   |    {FP:5}  |\")\nprint(\"------------+------------+------------\")\nprint(f\"Actual Pos. |    {FN:5}   |    {TP:5}  |\")\nprint(\"------------+------------+------------\")"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-1",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-1",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nAccuracy\n\n\nAccuracy: When the classifier says this newborn baby is at risk or is not at risk, what is the probability that the model is correct?\n\nAccuracy is defined as the number of items categorized correctly divided by the total number of items."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-2",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-2",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nFalse positive/negative\n\nFalse positive rate (FPR): If the classifier says this newborn baby is at risk, what‚Äôs the probability that the baby is not really at risk?\n\nFPR is defined as the ratio of false positives to predicted positives.\n\nFalse negative rate (FNR): If the classifier says this newborn baby is not at risk, what‚Äôs the probability that the baby is really at risk?\n\nFNR is defined as the ratio of false negatives to predicted negatives."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-3",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-3",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nPrecision\n\n\nPrecision: If the classifier says this newborn baby is at risk, what‚Äôs the probability that the baby is really at risk?\n\nPrecision is defined as the ratio of true positives to predicted positives."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-4",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-4",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nRecall (or Sensitivity)\n\n\nRecall (or sensitivity): Of all the babies at risk, what fraction did the classifier detect?\n\nRecall (or sensitivity) is also called the true positive rate (TPR), the ratio of true positives over all actual positives."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-5",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-5",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nSpecificity\n\n\nSpecificity: Of all the not-at-risk babies, what fraction did the classifier detect?\n\nSpecificity is also called the true negative rate (TNR), the ratio of true negatives over all actual negatives."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-6",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-6",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nEnrichment\n\nAverage: Average rate of new born babies being at risk\nEnrichment: How does the classifier precisely choose babies at risk relative to the average rate of new born babies being at risk?\n\nWe want a classifier whose enrichment is greater than 2.\n\n\n\naccuracy = (TP + TN) / (TP + FP + FN + TN)\nprecision = TP / (TP + FP)\nrecall = TP / (TP + FN)\nspecificity = TN / (TN + FP)\naverage_rate = (TP + FN) / (TP + TN + FP + FN)  # Proportion of actual at-risk babies\nenrichment = precision / average_rate\n\nprint(f\"Accuracy:  {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall (Sensitivity): {recall:.4f}\")\nprint(f\"Specificity:  {specificity:.4f}\")\nprint(f\"Average Rate: {average_rate:.4f}\")\nprint(f\"Enrichment:   {enrichment:.4f} (Relative Precision)\")"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-7",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-7",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nTrade-off between recall and precision/enrichment\n\n\nThere is a trade-off between recall and precision/enrichment.\n\nWhat would be the optimal threshold?\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_recall_curve\n\npdf = dtest_1.select(\"prediction\", \"atRisk\").toPandas()\n\n# Extract predictions and true labels\ny_true = pdf[\"atRisk\"]  # True labels\ny_scores = pdf[\"prediction\"]  # Predicted probabilities\n\n# Compute precision, recall, and thresholds\nprecision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n\n# Compute enrichment: precision divided by average at-risk rate\naverage_rate = np.mean(y_true)\nenrichment = precision / average_rate\n\n# Define optimal threshold (example: threshold where recall ‚âà enrichment balance)\noptimal_threshold = 0.02  # Adjust based on the plot\n\n# Plot Enrichment vs. Recall vs. Threshold\nplt.figure(figsize=(8, 6))\nplt.plot(thresholds, enrichment[:-1], label=\"Enrichment\", color=\"blue\", linestyle=\"--\")\nplt.plot(thresholds, recall[:-1], label=\"Recall\", color=\"red\", linestyle=\"-\")\n\n# Add vertical line for chosen threshold\nplt.axvline(x=optimal_threshold, color=\"black\", linestyle=\"dashed\", label=f\"Optimal Threshold = {optimal_threshold}\")\n\n# Labels and legend\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Score\")\nplt.title(\"Enrichment vs. Recall\")\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-8",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-8",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nTrade-off between sensitivity and specificity\n\n\nThere is also a trade-off between sensitivity and specificity.\nThe receiver operating characteristic curve (or ROC curve) plot both the true positive rate (recall) and the false positive rate (or 1 - specificity) for all threshold levels.\n\nArea under the curve (or AUC) can be another measure of the quality of the model.\n\n\n\nfrom sklearn.metrics import roc_curve\n\n# Convert to Pandas\npdf = dtest_1.select(\"prediction\", \"atRisk\").toPandas()\n\n# Compute ROC curve\nfpr, tpr, _ = roc_curve(pdf[\"atRisk\"], pdf[\"prediction\"])\n\n# Plot ROC curve\nplt.figure(figsize=(8,6))\nplt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc:.4f})\")\nplt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-9",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-9",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nAUC\n\n\n(0,0)‚ÄîCorresponding to a classifier defined by the threshold \\(\\text{Prob}(y_{i} = 1) = 1\\):\n\nNothing gets classified as at-risk."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-10",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-10",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nAUC\n\n\n(1,1)‚ÄîCorresponding to a classifier defined by the threshold \\(\\text{Prob}(y_{i} = 1) = 0\\):\n\nEverything gets classified as at-risk."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-11",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-11",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nAUC\n\n\n(0,1)‚ÄîCorresponding to any classifier defined by a threshold between 0 and 1:\n\nEverything is classified perfectly!"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-12",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-12",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\nAUC\n\nThe AUC for the random model is 0.5.\n\nYou want a classifier whose AUC is close to 1, and greater than 0.5.\n\nWhen comparing multiple classifiers, you generally want to prefer classifiers that have a higher AUC.\nYou also want to examine the shape of the ROC to explore possible trade-offs."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-13",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-13",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\n\nSuppose that you have successfully trained a classifier with acceptable precision and recall using NY hospital data.\nNow you want to apply the same classifier on all hospital data in MA.\n\nWill the classifier perform as well?\n\nThe proportion of at-risk babies in MA hospitals may differ from that in NY.\n\nCan this difference impact classifier performance on MA data?\n\nLet‚Äôs examine how classifier performance changes with varying at-risk rates.\n\ndtrain, dtest = df.randomSplit([0.5, 0.5], seed = 1234)\n\npd_dtrain = dtrain.toPandas()\npd_dtest = dtest.toPandas()\n\n# Set seed for reproducibility\nnp.random.seed(23464)\n\n# Sample 1000 random indices from the test dataset without replacement\nsample_indices = np.random.choice(pd_dtest.index, size=1000, replace=False)\n\n# Separate the selected observations from testing data\nseparated = pd_dtest.loc[sample_indices]\n\n# Remove the selected observations from the testing data\n# Consider this as data from NY hospitals\npd_dtest_NY = pd_dtest.drop(sample_indices)\n\n# Split the separated sample into at-risk and not-at-risk groups\nat_risk_sample = separated[separated[\"atRisk\"] == 1]  # Only at-risk cases\nnot_at_risk_sample = separated[separated[\"atRisk\"] == 0]  # Only not-at-risk cases\n\n# Create test sets for MA hospitals with different at-risk rates\npd_dtest_MA_moreRisk = pd.concat([pd_dtest_NY, at_risk_sample])  # Adds back only at-risk cases\npd_dtest_MA_lessRisk = pd.concat([pd_dtest_NY, not_at_risk_sample])  # Adds back only not-at-risk cases\n\n# Show counts to verify results\nprint(\"Original Test Set Size:\", pd_dtest.shape[0])\nprint(\"Sampled Separated Size:\", separated.shape[0])\nprint(\"NY Hospitals Data Size:\", pd_dtest_NY.shape[0])\nprint(\"MA More Risk Data Size:\", pd_dtest_MA_moreRisk.shape[0])\nprint(\"MA Less Risk Data Size:\", pd_dtest_MA_lessRisk.shape[0])"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-14",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-14",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-15",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-15",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\n\nWhich classifier do you prefer for identifying at-risk babies?\n\nHigh accuracy, low recall, other things being equal;\nLow accuracy, high recall, other things being equal.\n\nAccuracy may not be a good measure for the classes that have unbalanced distribution of predicted probabilities (e.g., rare event)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-16",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-16",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\n\nPrecision and false negative rate change a lot with the average rate of positives (at-risk babies).\nAccuracy, sensitivity, and specificity is relatively independent of the class prevalence.\nSensitivity/specificity is good for fields, like medicine, where it‚Äôs important to have an idea how well a classifier separates positive from negative instances independently of the distribution of the different classes in the population.\nPrecision/recall gives you an idea how well a classifier will work on a specific population."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-17",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-17",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\n\nWhich classifier do you prefer for identifying at-risk babies?\n\nHigh accuracy, low sensitivity, other things being equal;\nLow accuracy, high sensitivity, other things being equal.\n\nAccuracy may not be a good measure for the classes that have unbalanced distribution of predicted probabilities (e.g., rare event)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-18",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#performance-of-classifier-18",
    "title": "Lecture 8",
    "section": "Performance of Classifier",
    "text": "Performance of Classifier\n\nWhich classifier do you prefer for identifying at-risk babies?\n\nHigh accuracy, low sensitivity, other things being equal;\nLow accuracy, high sensitivity, other things being equal.\n\nAccuracy may not be a good measure for the classes that have unbalanced distribution of predicted probabilities (e.g., rare event)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#assessing-newborn-babies-at-risk",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#assessing-newborn-babies-at-risk",
    "title": "Lecture 8",
    "section": "Assessing Newborn Babies at Risk",
    "text": "Assessing Newborn Babies at Risk\n\nNewborn babies are assessed at one and five minutes after birth using what‚Äôs called the Apgar test, which is designed to determine if a baby needs immediate emergency care or extra medical attention.\n\nA baby who scores below 7 (on a scale from 0 to 10) on the Apgar scale needs extra attention.\n\nSuch at-risk babies are rare, so the hospital doesn‚Äôt want to provision extra emergency equipment for every delivery.\n\nOn the other hand, at-risk babies may need attention quickly, so provisioning resources proactively to appropriate deliveries can save lives."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#assessing-newborn-babies-at-risk-1",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#assessing-newborn-babies-at-risk-1",
    "title": "Lecture 8",
    "section": "Assessing Newborn Babies at Risk",
    "text": "Assessing Newborn Babies at Risk\ndfpd = pd.read_csv('https://bcdanl.github.io/data/NatalRiskData.csv')\n\nWe‚Äôll use a sample dataset from the 2010 CDC natality public-use data file.\nThe data set records information about all US births, including facts about the mother and father, and about the delivery.\nThe sample has just over 26,000 births in a DataFrame."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#assessing-newborn-babies-at-risk-2",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#assessing-newborn-babies-at-risk-2",
    "title": "Lecture 8",
    "section": "Assessing Newborn Babies at Risk",
    "text": "Assessing Newborn Babies at Risk\n\n\n\n\n\nVariable\n\n\nType\n\n\nDescription\n\n\n\n\natRisk\n\n\nBool\n\n\n1 if Apgar &lt; 7, 0 otherwise\n\n\n\n\nPWGT\n\n\nNum\n\n\nPrepregnancy weight\n\n\n\n\nUPREVIS\n\n\nInt\n\n\nPrenatal visits\n\n\n\n\nCIG_REC\n\n\nBool\n\n\n1 if smoker, 0 otherwise\n\n\n\n\nGESTREC3\n\n\nCat\n\n\n&lt; 37 weeks or ‚â• 37 weeks\n\n\n\n\nDPLURAL\n\n\nCat\n\n\nSingle / Twin / Triplet+\n\n\n\n\nULD_MECO\n\n\nBool\n\n\n1 if heavy meconium\n\n\n\n\nULD_PRECIP\n\n\nBool\n\n\n1 if labor &lt; 3 hours\n\n\n\n\nULD_BREECH\n\n\nBool\n\n\n1 if breech birth\n\n\n\n\nURF_DIAB\n\n\nBool\n\n\n1 if diabetic\n\n\n\n\nURF_CHYPER\n\n\nBool\n\n\n1 if chronic hypertension\n\n\n\n\nURF_PHYPER\n\n\nBool\n\n\n1 if pregnancy hypertension\n\n\n\n\nURF_ECLAM\n\n\nBool\n\n\n1 if eclampsia"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#assessing-newborn-babies-at-risk-3",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#assessing-newborn-babies-at-risk-3",
    "title": "Lecture 8",
    "section": "Assessing Newborn Babies at Risk",
    "text": "Assessing Newborn Babies at Risk\n\nTask 1. Identify the relationship between a predictor and the probability of \\(\\texttt{atRisk == TRUE}\\).\nTask 2. Identify ahead of time situations with a higher probability of \\(\\texttt{atRisk == TRUE}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#what-is-the-logistic-regression-doing",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#what-is-the-logistic-regression-doing",
    "title": "Lecture 8",
    "section": "What is the Logistic Regression doing?",
    "text": "What is the Logistic Regression doing?\n\nThe logistic regression finds the beta coefficients, \\(b_0\\), \\(b_1\\), \\(b_2\\), \\(\\cdots\\), \\(b_{k}\\) such that the logistic function \\[\nG(b_0 + b_{1}x_{1,i} + b_{2}x_{2,i} + \\,\\cdots\\, + b_{k}x_{k,i})\n\\] is the best possible estimate of the binary outcome \\(y_{i}\\)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#what-is-the-logistic-regression-doing-1",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#what-is-the-logistic-regression-doing-1",
    "title": "Lecture 8",
    "section": "What is the Logistic Regression doing?",
    "text": "What is the Logistic Regression doing?\n\nThe function \\(G(z_i)\\) is called the logistic function because the function \\(G(z_i)\\) is the inverse function of a logit (or a log-odd) of the probability that the outcome \\(y_{i}\\) is 1. \\[\n\\begin{align}\nG^{-1}(z_i) &\\,\\equiv\\, \\text{logit} (\\text{Prob}(y_{i} = 1))\\\\\n&\\,\\equiv \\log\\left(\\, \\frac{\\text{Prob}(y_{i} = 1)}{\\text{Prob}(y_{i} = 0)} \\,\\right)\\\\\n&\\,=\\, b_0 + b_{1}x_{1,i} + b_{2}x_{2,i} + \\,\\cdots\\, + b_{k}x_{k,i}\n\\end{align}\n\\]\nLogistic regression is a linear regression model for log odds."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#what-is-the-logistic-regression-doing-2",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#what-is-the-logistic-regression-doing-2",
    "title": "Lecture 8",
    "section": "What is the Logistic Regression doing?",
    "text": "What is the Logistic Regression doing?\n\n\nLogistic regression can be expressed as linear regression of log odds of \\(y_{i} = 1\\) on predictors \\(x_1, x_2, \\cdots, x_k\\):\n\n\n\\[\n\\begin{align}\n\\text{Prob}(y_{i} = 1) &\\,=\\, G( b_0 + b_{1}x_{1,i} + b_{2}x_{2,i} + \\,\\cdots\\, +  b_{k}x_{k,i} )\\\\\n\\text{ }\\\\\n\\Leftrightarrow\\qquad \\log\\left(\\dfrac{\\text{Prob}( y_i = 1 )}{\\text{Prob}( y_i = 0 )}\\right) &\\,=\\,  b_0 + b_{1}x_{1,i} + b_{2}x_{2,i} + \\,\\cdots\\, +  b_{k}x_{k,i}\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#building-a-logistic-regression-model-in-pyspark",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#building-a-logistic-regression-model-in-pyspark",
    "title": "Lecture 8",
    "section": "Building a Logistic Regression Model in PySpark",
    "text": "Building a Logistic Regression Model in PySpark\n\nThe function to build a logistic regression model in PySpark is \\(\\texttt{GeneralizedLinearRegression(family=\"binomial\", link=\"logit\")}\\).\nThe outcome variable \\(y\\) is the binary variable \\(\\texttt{atRisk}\\) (0 or 1).\nThe other variables in the table in the previous slide are predictors \\(x_{k}\\).\nThe arguments \\(\\texttt{family=\"binomial\"}\\) and \\(\\texttt{link=\"logit\"}\\) specify the logistic distribution of the outcome variable \\(y\\).\n\nfrom pyspark.ml.regression import GeneralizedLinearRegression\n\ndummy_cols_GESTREC3, ref_category_GESTREC3 = add_dummy_variables('GESTREC3', 1)\ndummy_cols_DPLURAL, ref_category_DPLURAL = add_dummy_variables('DPLURAL', 0)\n\n# assembling predictors\nx_cols = ['PWGT', 'UPREVIS', 'CIG_REC', \n          'ULD_MECO', 'ULD_PRECIP', 'ULD_BREECH', 'URF_DIAB',\n          'URF_CHYPER', 'URF_PHYPER', 'URF_ECLAM']\n\nassembler_predictors = (\n    x_cols +\n    dummy_cols_GESTREC3 + dummy_cols_DPLURAL\n)\n\nassembler_1 = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\n\ndtrain_1 = assembler_1.transform(dtrain)\ndtest_1  = assembler_1.transform(dtest)\n\n# training model\nmodel_1 = (\n    GeneralizedLinearRegression(featuresCol=\"predictors\",\n                                labelCol=\"atRisk\",\n                                family=\"binomial\", \n                                link=\"logit\")\n    .fit(dtrain_1)\n)\n\n# making prediction\ndtrain_1 = model_1.transform(dtrain_1)\ndtest_1 = model_1.transform(dtest_1)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#marginal-effect-of-x_k-i-on-textproby_i-1",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#marginal-effect-of-x_k-i-on-textproby_i-1",
    "title": "Lecture 8",
    "section": "Marginal Effect of \\(x_{k, i}\\) on \\(\\text{Prob}(y_{i} = 1)\\)?",
    "text": "Marginal Effect of \\(x_{k, i}\\) on \\(\\text{Prob}(y_{i} = 1)\\)?\nmodel_1.summary\n\nIf the baby is prematurely born, the log-odds of being at risk increases by 1.539 relative to the non-premature baby."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#marginal-effect-of-x_k-i-on-textproby_i-1-1",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#marginal-effect-of-x_k-i-on-textproby_i-1-1",
    "title": "Lecture 8",
    "section": "Marginal Effect of \\(x_{k, i}\\) on \\(\\text{Prob}(y_{i} = 1)\\)",
    "text": "Marginal Effect of \\(x_{k, i}\\) on \\(\\text{Prob}(y_{i} = 1)\\)\n\n\nIn logistic regression, the effect of \\(x_{k, i}\\) on \\(\\text{Prob}(y_{i} = 1)\\) is different for each observation \\(i\\).\n\n\n\n\n\n\n\n\n\nLinear regression\n\n\n\n\n\n\n\n\n\n\nLogistic regression"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#marginal-effect-of-x_k-i-on-textproby_i-1-2",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#marginal-effect-of-x_k-i-on-textproby_i-1-2",
    "title": "Lecture 8",
    "section": "Marginal Effect of \\(x_{k, i}\\) on \\(\\text{Prob}(y_{i} = 1)\\)",
    "text": "Marginal Effect of \\(x_{k, i}\\) on \\(\\text{Prob}(y_{i} = 1)\\)\n\nHow can we calculate the effect of \\(x_{k, i}\\) on the probability of \\(y_{i} = 1\\)?\n\nMarginal effect at the mean (MEM): We can obtain the marginal effect at an average observation or representative observations in the training data (MEM) or at representative values (MER)).\nAverage marginal effect (AME): We can average the marginal effects across the training data.\n\n\ndef marginal_effects(model, means):\n    \"\"\"\n    Compute marginal effects for all predictors in a PySpark GeneralizedLinearRegression model (logit)\n    and return a formatted table with statistical significance.\n    \n    Parameters:\n        model: Fitted GeneralizedLinearRegression model (with binomial family and logit link).\n        means: List of mean values for the predictor variables.\n    \n    Returns:\n        A formatted string containing the marginal effects table.\n    \"\"\"\n    global assembler_predictors  # Use the global assembler_predictors list\n    \n    # Extract model coefficients, standard errors, and intercept\n    coeffs = np.array(model.coefficients)\n    std_errors = np.array(model.summary.coefficientStandardErrors)\n    intercept = model.intercept\n    \n    # Compute linear combination of means and coefficients (XB)\n    XB = np.dot(means, coeffs) + intercept\n    \n    # Compute derivative of logistic function (G'(XB))\n    G_prime_XB = np.exp(XB) / ((1 + np.exp(XB)) ** 2)\n    \n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n    \n    # Create table to store results\n    results = []\n    \n    for i, predictor in enumerate(assembler_predictors):\n        # Compute marginal effect\n        marginal_effect = G_prime_XB * coeffs[i]\n        \n        # Compute standard error of the marginal effect\n        std_error = G_prime_XB * std_errors[i]\n        \n        # Compute z-score and p-value\n        z_score = marginal_effect / std_error if std_error != 0 else np.nan\n        p_value = 2 * (1 - norm.cdf(abs(z_score))) if not np.isnan(z_score) else np.nan\n        \n        # Compute confidence interval (95%)\n        ci_lower = marginal_effect - 1.96 * std_error\n        ci_upper = marginal_effect + 1.96 * std_error\n        \n        # Append results\n        results.append([predictor, f\"{marginal_effect: .4f}\", significance_stars(p_value), f\"{ci_lower: .4f}\", f\"{ci_upper: .4f}\"])\n    \n    # Convert results to tabulated format\n    table_str = tabulate(results, headers=[\"Variable\", \"Marginal Effect\", \"Significance\", \"95% CI Lower\", \"95% CI Upper\"], \n                         tablefmt=\"pretty\", colalign=(\"left\", \"decimal\", \"left\", \"decimal\", \"decimal\"))\n    \n    return table_str\n\n# Example usage:\n# means = [0.5, 30]  # Mean values for x1 and x2\n# assembler_predictors = ['x1', 'x2']  # Define globally before calling the function\n# table_output = mfx_glm(fitted_model, means)\n# print(table_output)"
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#double-density-plot-choosing-the-optimal-threshold",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#double-density-plot-choosing-the-optimal-threshold",
    "title": "Lecture 8",
    "section": "Double Density Plot ‚Äì Choosing the Optimal Threshold",
    "text": "Double Density Plot ‚Äì Choosing the Optimal Threshold\n\n\nDouble density plot is useful when picking the classifier threshold.\n\nSince the classifier is built using the training data, the threshold should also be selected using the training data.\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Filter training data for atRisk == 1 and atRisk == 0\npdf = dtrain_1.select(\"prediction\", \"atRisk\").toPandas()\n\ntrain_true = pdf[pdf[\"atRisk\"] == 1]\ntrain_false = pdf[pdf[\"atRisk\"] == 0]\n\n# Create the first density plot\nplt.figure(figsize=(8, 6))\nsns.kdeplot(train_true[\"prediction\"], label=\"TRUE\", color=\"red\", fill=True)\nsns.kdeplot(train_false[\"prediction\"], label=\"FALSE\", color=\"blue\", fill=True)\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Density\")\nplt.title(\"Density Plot of Predictions\")\nplt.legend(title=\"atRisk\")\nplt.show()\n\n# Define threshold for vertical line\nthreshold = 0.02  # Replace with actual value\n\n# Create the second density plot with vertical line\nplt.figure(figsize=(8, 6))\nsns.kdeplot(train_true[\"prediction\"], label=\"TRUE\", color=\"red\", fill=True)\nsns.kdeplot(train_false[\"prediction\"], label=\"FALSE\", color=\"blue\", fill=True)\nplt.axvline(x=threshold, color=\"blue\", linestyle=\"dashed\", label=f\"Threshold = {threshold}\")\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Density\")\nplt.title(\"Density Plot of Predictions with Threshold\")\nplt.legend(title=\"atRisk\")\nplt.show()"
  },
  {
    "objectID": "danl-cw/danl-320-cw-10.html",
    "href": "danl-cw/danl-320-cw-10.html",
    "title": "Classwork 10",
    "section": "",
    "text": "# Below is for an interactive display of Pandas DataFrame in Colab\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\nimport pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_curve\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n\n\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\n\n# Increase figure size to prevent overlapping\nplt.figure(figsize=(10, 6))\n\n# Plot using the DataFrame columns\nplt.errorbar(df_ME[\"Variable\"], df_ME[\"Marginal Effect\"],\n             yerr=1.96 * df_ME[\"Std. Error\"], fmt='o', capsize=5)\n\n# Labels and title\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Marginal Effect\")\nplt.title(\"Marginal Effect at the Mean\")\n\n# Add horizontal line at 0 for reference\nplt.axhline(0, color=\"red\", linestyle=\"--\")\n\n# Adjust x-axis labels to avoid overlap\nplt.xticks(rotation=45, ha=\"right\")  # Rotate and align labels to the right\nplt.tight_layout()  # Adjust layout to prevent overlap\n\n# Show plot\nplt.show()"
  },
  {
    "objectID": "danl-cw/danl-320-cw-10.html#required-libraries-and-sparksession-entry-point",
    "href": "danl-cw/danl-320-cw-10.html#required-libraries-and-sparksession-entry-point",
    "title": "Classwork 10",
    "section": "",
    "text": "# Below is for an interactive display of Pandas DataFrame in Colab\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\nimport pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_curve\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
  },
  {
    "objectID": "danl-cw/danl-320-cw-10.html#udf-for-adding-dummy-variables",
    "href": "danl-cw/danl-320-cw-10.html#udf-for-adding-dummy-variables",
    "title": "Classwork 10",
    "section": "",
    "text": "def add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)"
  },
  {
    "objectID": "danl-cw/danl-320-cw-10.html#variable-description",
    "href": "danl-cw/danl-320-cw-10.html#variable-description",
    "title": "Classwork 10",
    "section": "Variable description",
    "text": "Variable description\n\n\n\n\n\n\n\n\nVariable\nType\nDescription\n\n\n\n\natRisk\nBoolean\nTRUE (1) if 5-minute Apgar score &lt; 7; FALSE (0) otherwise\n\n\nPWGT\nNumeric\nMother‚Äôs prepregnancy weight\n\n\nUPREVIS\nNumeric (integer)\nNumber of prenatal medical visits\n\n\nCIG_REC\nBoolean\nTRUE (1) if smoker; FALSE (0) otherwise\n\n\nGESTREC3\nCategorical\nTwo categories: &lt;37 weeks (premature) and &gt;=37 weeks\n\n\nDPLURAL\nCategorical\nBirth plurality, three categories: single/twin/triplet+\n\n\nULD_MECO\nBoolean\nTRUE (1) if moderate/heavy fecal staining of amniotic fluid\n\n\nULD_PRECIP\nBoolean\nTRUE (1) for unusually short labor (&lt; three hours)\n\n\nULD_BREECH\nBoolean\nTRUE (1) for breech (pelvis first) birth position\n\n\nURF_DIAB\nBoolean\nTRUE (1) if mother is diabetic\n\n\nURF_CHYPER\nBoolean\nTRUE (1) if mother has chronic hypertension\n\n\nURF_PHYPER\nBoolean\nTRUE (1) if mother has pregnancy-related hypertension\n\n\nURF_ECLAM\nBoolean\nTRUE (1) if mother experienced eclampsia: pregnancy-related seizures"
  },
  {
    "objectID": "danl-cw/danl-320-cw-10.html#marginal-effect-plot",
    "href": "danl-cw/danl-320-cw-10.html#marginal-effect-plot",
    "title": "Classwork 10",
    "section": "",
    "text": "# Increase figure size to prevent overlapping\nplt.figure(figsize=(10, 6))\n\n# Plot using the DataFrame columns\nplt.errorbar(df_ME[\"Variable\"], df_ME[\"Marginal Effect\"],\n             yerr=1.96 * df_ME[\"Std. Error\"], fmt='o', capsize=5)\n\n# Labels and title\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Marginal Effect\")\nplt.title(\"Marginal Effect at the Mean\")\n\n# Add horizontal line at 0 for reference\nplt.axhline(0, color=\"red\", linestyle=\"--\")\n\n# Adjust x-axis labels to avoid overlap\nplt.xticks(rotation=45, ha=\"right\")  # Rotate and align labels to the right\nplt.tight_layout()  # Adjust layout to prevent overlap\n\n# Show plot\nplt.show()"
  },
  {
    "objectID": "danl-cw/danl-320-cw-10.html#assessing-newborn-babies-at-risk",
    "href": "danl-cw/danl-320-cw-10.html#assessing-newborn-babies-at-risk",
    "title": "Classwork 10",
    "section": "Assessing Newborn Babies at Risk",
    "text": "Assessing Newborn Babies at Risk\n\n\n\n\n\nVariable\n\n\nType\n\n\nDescription\n\n\n\n\natRisk\n\n\nBool\n\n\n1 if Apgar &lt; 7, 0 otherwise\n\n\n\n\nPWGT\n\n\nNum\n\n\nPrepregnancy weight\n\n\n\n\nUPREVIS\n\n\nInt\n\n\nPrenatal visits\n\n\n\n\nCIG_REC\n\n\nBool\n\n\n1 if smoker, 0 otherwise\n\n\n\n\nGESTREC3\n\n\nCat\n\n\n&lt; 37 weeks or ‚â• 37 weeks\n\n\n\n\nDPLURAL\n\n\nCat\n\n\nSingle / Twin / Triplet+\n\n\n\n\nULD_MECO\n\n\nBool\n\n\n1 if heavy meconium\n\n\n\n\nULD_PRECIP\n\n\nBool\n\n\n1 if labor &lt; 3 hours\n\n\n\n\nULD_BREECH\n\n\nBool\n\n\n1 if breech birth\n\n\n\n\nURF_DIAB\n\n\nBool\n\n\n1 if diabetic\n\n\n\n\nURF_CHYPER\n\n\nBool\n\n\n1 if chronic hypertension\n\n\n\n\nURF_PHYPER\n\n\nBool\n\n\n1 if pregnancy hypertension\n\n\n\n\nURF_ECLAM\n\n\nBool\n\n\n1 if eclampsia"
  },
  {
    "objectID": "danl-hw/danl-320-hw-03.html",
    "href": "danl-hw/danl-320-hw-03.html",
    "title": "Homework 3",
    "section": "",
    "text": "Please submit your Jupyter Notebook for Part 1 in Homework 3 to Brightspace with the name below:\n\ndanl-320-hw3-LASTNAME-FIRSTNAME.ipynb\n( e.g., danl-320-hw3-choe-byeonghak.ipynb )\n\nThe due is April 2, 2025, 3:15 P.M.\nPlease send Byeong-Hak an email (bchoe@geneseo.edu) if you have any questions."
  },
  {
    "objectID": "danl-hw/danl-320-hw-03.html#variable-description",
    "href": "danl-hw/danl-320-hw-03.html#variable-description",
    "title": "Homework 3",
    "section": "Variable Description",
    "text": "Variable Description\n\n\n\nVariable\nDescription\n\n\n\n\nLPRICE\nPurchase price of unit and land\n\n\nVALUE\nCurrent market value of unit\n\n\nSTATE\nState code\n\n\nMETRO\nCentral city/suburban status\n\n\nZINC2\nHousehold income\n\n\nHHGRAD\nEducational level of householder\n\n\nBATHS\nNumber of full bathrooms in unit\n\n\nBEDRMS\nNumber of bedrooms in unit\n\n\nPER\nNumber of persons in household\n\n\nZADULT\nNumber of adults (18+) in household\n\n\nNUNITS\nNumber of units in building\n\n\nEAPTBL\nApartment buildings within 1/2 block of unit\n\n\nECOM1\nBusiness/institutions within 1/2 block\n\n\nECOM2\nFactories/other industry within 1/2 block\n\n\nEGREEN\nOpen spaces within 1/2 block of unit\n\n\nEJUNK\nTrash/junk in streets/properties in 1/2 block\n\n\nELOW1\nSingle-family town/rowhouses in 1/2 block\n\n\nESFD\nSingle-family homes within 1/2 block\n\n\nETRANS\nRR/airport/4-lane highway within 1/2 block\n\n\nEABAN\nAbandoned/vandalized buildings within 1/2 block\n\n\nHOWH\nRating of unit as a place to live\n\n\nHOWN\nRating of neighborhood as a place to live\n\n\nODORA\nNeighborhood has bad smells\n\n\nSTRNA\nNeighborhood has heavy street noise/traffic\n\n\nFRSTHO\nFirst home\n\n\nAMMORT\nAmount of 1st mortgage when acquired\n\n\nINTW\nInterest rate of 1st mortgage (whole number %)\n\n\nMATBUY\nGot 1st mortgage in the same year bought unit\n\n\nDWNPAY\nMain source of down payment on unit"
  },
  {
    "objectID": "danl-hw/danl-320-hw-03.html#question-1",
    "href": "danl-hw/danl-320-hw-03.html#question-1",
    "title": "Homework 3",
    "section": "Question 1",
    "text": "Question 1\nPlot some relationships and tell a story."
  },
  {
    "objectID": "danl-hw/danl-320-hw-03.html#question-2",
    "href": "danl-hw/danl-320-hw-03.html#question-2",
    "title": "Homework 3",
    "section": "Question 2",
    "text": "Question 2\n\nFit a linear regression model with the following specifications:\n\nOutcome variable: \\(\\log(VALUE)\\)\nPredictors: all but AMORT and LPRICE"
  },
  {
    "objectID": "danl-hw/danl-320-hw-03.html#questions-3-8",
    "href": "danl-hw/danl-320-hw-03.html#questions-3-8",
    "title": "Homework 3",
    "section": "Questions 3-8",
    "text": "Questions 3-8\nConsider the following three linear regression models:\n\nModel 1\n\\[\n\\begin{aligned}\n\\log(\\text{price\\_per\\_floz}) = &\\ \\beta_{0} + \\sum_{i=1}^{N} \\beta_{i} \\,\\text{market}_{i} + \\sum_{j=N+1}^{N+4} \\beta_{j} \\,\\text{brand}_{j}\n+ \\beta_{N+5} \\,\\text{container\\_CAN} \\\\\n&\\,+\\, \\beta_{N+6} \\log(\\text{beer\\_floz}) + \\epsilon\n\\end{aligned}\n\\]\n\n\nModel 2\n\\[\n\\begin{aligned}\n\\log(\\text{price\\_per\\_floz}) \\,=\\, & \\beta_{0} \\,+\\, \\sum_{i=1}^{N}\\beta_{i}\\,\\text{market}_{i} \\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j}\\,\\text{brand}_{j} \\,+\\, \\beta_{N+5}\\,\\text{container\\_CAN} \\\\\n&\\,+\\, \\beta_{N+6}\\log(\\text{beer\\_floz})\\\\\n&\\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j\\times\\text{beer\\_floz}}\\,\\text{brand}_{j}\\times \\log(\\text{beer\\_floz})\\\\\n&\\,+\\, \\epsilon\n\\end{aligned}\n\\]\n\n\nModel 3\n\\[\n\\begin{aligned}\n\\log(\\text{price\\_per\\_floz}) \\,=\\, & \\beta_{0} \\,+\\, \\sum_{i=1}^{N}\\beta_{i}\\,\\text{market}_{i} \\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j}\\,\\text{brand}_{j} \\,+\\, \\beta_{N+5}\\,\\text{container\\_CAN} \\\\\n&\\,+\\, \\beta_{N+6}\\log(\\text{beer\\_floz})\\\\\n&\\,+\\, \\beta_{N+7}\\,\\text{promo} \\times\\log(\\text{beer\\_floz}) \\\\\n&\\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j\\times\\text{beer\\_floz}}\\,\\text{brand}_{j}\\times \\log(\\text{beer\\_floz})\\\\\n&\\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j\\times\\text{promo}}\\,\\text{brand}_{j}\\times \\text{promo}\\\\\n&\\,+\\, \\sum_{j=N+1}^{N+4}\\beta_{j\\times\\text{promo}\\times\\text{beer\\_floz}}\\,\\text{brand}_{j}\\times \\text{promo}\\times \\log(\\text{beer\\_floz})\\\\\n&\\,+\\, \\epsilon\n\\end{aligned}\n\\]\n\nSet ‚ÄúBUD_LIGHT‚Äù as the reference level for the \\(\\text{brand}\\) variable.\nSet ‚ÄúBUFFALO-ROCHESTER‚Äù as the reference level for the \\(\\text{market}\\) variable..\n\nThere are \\(N+1\\) distinct categories in the \\(\\text{market}\\) variable.\n\n\n\n\n\nQuestion 3\nProvide intuition behind each of the model.\n\n\n\nQuestion 4\n\nTrain each of the three linear regression models using the training DataFrame from Question 2.\n\nProvide the summary of the result for each linear regression model.\nWhat are the predicted beer prices for unseen data from each model?\n\n\n\n\n\nQuestion 5\n\nInterpret the beta estimates of the following variables from the Model 3:\n\nmarket_ALBANY\nmarket_EXURBAN_NY\nmarket_RURAL_NEW_YORK\nmarket_SURBURBAN_NY\nmarket_SYRACUSE\nmarket_URBAN_NY\n\n\n\n\n\nQuestion 6\n\nAcross the three models, how is the percentage change in the price of beer sensitive to the percentage change in the volume of beer purchases for each brand?\nHow does promo affect such sensitivity in the Model 3?\n\n\n\n\nQuestion 7\n\nDraw a residual plot from each of the three models.\n\nOn average, are the prediction correct? Are there systematic errors?\n\n\n\n\n\nQuestion 8\nWhich model do you prefer most and why?"
  },
  {
    "objectID": "danl-hw/danl-320-hw-03.html#variable-description-1",
    "href": "danl-hw/danl-320-hw-03.html#variable-description-1",
    "title": "Homework 3",
    "section": "Variable Description",
    "text": "Variable Description\n\n\n\n\n\n\n\nVariable Name\nDescription\n\n\n\n\nquantity\nNumber of items purchased\n\n\nprice_paid_deal\nPrice paid under a deal (discounted price)\n\n\nprice_paid_non_deal\nPrice paid without a deal (regular price)\n\n\ncoupon_value\nValue of any coupon used\n\n\npromotion_type\nType of promotion applied\n\n\ntotal_spent\nTotal amount spent on purchase\n\n\nsize1_descr\nDescription of product size (e.g., 16.0 MLOZ)\n\n\nflavor_descr\nFlavor description of the product\n\n\nformula_descr\nType of product formula (e.g., Regular)\n\n\nhousehold_id\nUnique identifier for household\n\n\nprojection_factor\nWeighting factor for data projection\n\n\nfips_state_code\nFIPS state code for location\n\n\nfips_county_code\nFIPS county code for location\n\n\ncensus_tract_county_code\nCensus tract code within the county\n\n\ntype_of_residence\nType of housing/residence\n\n\nkitchen_appliances\nNumber of kitchen appliances owned\n\n\ntv_items\nNumber of televisions in the household\n\n\nfemale_head_birth\nBirthdate of female head of household (if applicable)\n\n\nmale_head_birth\nBirthdate of male head of household (if applicable)\n\n\nhousehold_internet_connection\nType of household internet connection\n\n\n\n\n\nWrite a blog post about Ben and Jerry‚Äôs ice cream in the ice_cream data.frame using Jupyter Notebook, and add it to your online blog.\n\nIn your blog post, provide your linear regression model, as well as descriptive statistics, counting, filtering, and various group operations.\nOptionally, provide seaborn visualizations."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#accuracy-can-be-misleading-in-imbalanced-data",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#accuracy-can-be-misleading-in-imbalanced-data",
    "title": "Lecture 8",
    "section": "Accuracy Can Be Misleading in Imbalanced Data",
    "text": "Accuracy Can Be Misleading in Imbalanced Data\n\nRare events (e.g., severe childbirth complications) occur in a very small percentage of cases (e.g., 1% of the population).\nA simple model that always predicts ‚Äúnot-at-risk‚Äù would be 99% accurate, as it correctly classifies 99% of cases where no complications occur.\nHowever, this does not mean the simple model is better‚Äîaccuracy alone does not capture the effectiveness of a model when class distributions are skewed.\nA better model that identifies 5% of cases as ‚Äúat-risk‚Äù and catches all true at-risk cases may appear to have lower overall accuracy than the simple model.\nMissing a severe complication (false negative) can be more costly than mistakenly flagging a healthy case as at risk (false positive)."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#accuracy-can-be-misleading-in-imbalanced-data-1",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#accuracy-can-be-misleading-in-imbalanced-data-1",
    "title": "Lecture 8",
    "section": "Accuracy Can Be Misleading in Imbalanced Data",
    "text": "Accuracy Can Be Misleading in Imbalanced Data\n\nThe cost of misclassification matters:\n\nMissing a severe complication (false negative) can be more costly than mistakenly flagging a healthy case as at risk (false positive).\n\nAlternative metrics are better suited for evaluating classifiers in imbalanced scenarios:\n\nPrecision, Recall, AUC-ROC\n\nThis is not a paradox‚Äîit simply reflects that accuracy alone does not capture the effectiveness of a model when class distributions are skewed."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#deviance-and-likelihood",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#deviance-and-likelihood",
    "title": "Lecture 8",
    "section": "Deviance and Likelihood",
    "text": "Deviance and Likelihood\nmodel_1.summary\n\n\nDeviance is a measure of the distance between the data and the estimated model.\n\n\\[\n\\text{Deviance} = -2 \\log(\\text{Likelihood}) + C,\n\\] where \\(C\\) is constant that we can ignore."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#deviance-and-likelihood-1",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#deviance-and-likelihood-1",
    "title": "Lecture 8",
    "section": "Deviance and Likelihood",
    "text": "Deviance and Likelihood\n\n\nLogistic regression finds the beta coefficients, \\(b_0, b_1, \\,\\cdots, b_k\\) , such that the logistic function\n\n\\[\nG(b_0 + b_{1}x_{1,i} + b_{2}x_{2,i} + \\,\\cdots\\, + b_{k}x_{k,i})\n\\]\nis the best possible estimate of the binary outcome \\(y_i\\).\n\n\nLogistic regression finds the beta parameters that maximize the log likelihood of the data, given the model, which is equivalent to minimizing the sum of the residual deviances.\n\nWhen you minimize the deviance, you are fitting the parameters to make the model and data look as close as possible."
  },
  {
    "objectID": "danl-lec/danl-320-lec-08-2025-0310.html#likelihood-function",
    "href": "danl-lec/danl-320-lec-08-2025-0310.html#likelihood-function",
    "title": "Lecture 8",
    "section": "Likelihood Function",
    "text": "Likelihood Function\n\n\nLikelihood is the probability of your data given the model.\n\n\n\n\n\n\n\n\n\nThe probability that the seven data points would be observed:\n\n\\(L = (1-P1)*(1-P2)* P3*(1-P4)*P5*P6*P7\\)\n\nThe log of the likelihood: \\[\n\\begin{align}\n\\log(L) &= \\log(1-P1) + \\log(1-P2) + \\log(P3) \\\\\n&\\quad+ \\log(1-P4) + \\log(P5) + \\log(P6) + \\log(P7)\n\\end{align}\n\\]"
  },
  {
    "objectID": "danl-hw/danl-320-hw-03.html#question-3",
    "href": "danl-hw/danl-320-hw-03.html#question-3",
    "title": "Homework 3",
    "section": "Question 3",
    "text": "Question 3\n\nRefit the linear regression model, retaining only statistically significant predictors from Question 1.\nCompare the revised model to the initial model from Question 2 using:\n\n\\(\\beta\\) estimates\n\\(R^2\\)\nRMSE\nResidual plots"
  },
  {
    "objectID": "danl-hw/danl-320-hw-03.html#question-4",
    "href": "danl-hw/danl-320-hw-03.html#question-4",
    "title": "Homework 3",
    "section": "Question 4",
    "text": "Question 4\n\nFit a logistic regression model with the following specifications:\n\nOutcome variable: \\(\\text{GT20DWN}\\) (indicating whether the buyer made a down payment of 20% or more)\nPredictors: All available variables except AMORT and LPRICE\n\nThe outcome variable is defined as: \\[\n\\begin{align}\n\\text{GT20DWN} \\,=\\,\\begin{cases}\n1 & \\text{if}\\; \\frac{\\text{LPRICE} - \\text{AMMORT}}{\\text{LPRICE}} &gt; 0.2 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\end{align}\n\\]\nAnalyze and interpret the following relationships:\nThe association between first-time homeownership (\\(\\text{FRSTHO}\\)) and the probability of making a 20%+ down payment.\nThe association between number of bedrooms (\\(\\text{BEDRMS}\\)) and the probability of making a 20%+ down payment."
  },
  {
    "objectID": "danl-hw/danl-320-hw-03.html#question-5",
    "href": "danl-hw/danl-320-hw-03.html#question-5",
    "title": "Homework 3",
    "section": "Question 5",
    "text": "Question 5\n\nRefit the logistic regression model, adding interaction terms:\n\nPredictors: all previously included predictors in Question 4 plus the interaction between \\(\\text{FRSTHO}\\) and \\(\\text{BEDRMS}\\)\n\nInterpret how the relationship between \\(\\text{BEDRMS}\\) and the probability of a 20%+ down payment varies depending on whether the buyer is a first-time homeowner (\\(\\text{FRSTHO}\\))."
  },
  {
    "objectID": "danl-hw/danl-320-hw-03.html#question-6",
    "href": "danl-hw/danl-320-hw-03.html#question-6",
    "title": "Homework 3",
    "section": "Question 6",
    "text": "Question 6\n\nFit separate logistic regression models (similar to Question 4) for two subsets of homes:\n\n\nHomes worth \\(\\text{VALUE} \\geq 175k\\).\nHomes worth \\(\\text{VALUE} &lt; 175k\\).\n\n\nCompare residual deviance, \\(RMSE\\), and classification performance."
  }
]